{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b48e5c3",
   "metadata": {},
   "source": [
    "## <center>Avaliação regular do perfil do corpo docente do PPCS IRR/Fiocruz <br /> Programa de Pós-graduação em Ciências da Saúde <br /> Instituto René Rachou – Fiocruz Minas</center>\n",
    "\n",
    "    Rubens Lima do Monte Neto – Instituto René Rachou\n",
    "    Antonio Marcos Aires Barbosa – Fiocruz Ceará\n",
    "    Equipe do PPGCS\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "    Reavaliação de meio termo do corpo docente do programa, a fim de acompanhar a manutenção dos parâmetros exigidos pela CAPES – Área de Medicina II e readequar a composição do grupo. São considerados os docentes permanentes (DP) e docentes colaboradores (DC), com base nos mesmos parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e961c",
   "metadata": {},
   "source": [
    "# Apuração do Indicadores de Gestão do PPGCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c3005",
   "metadata": {},
   "source": [
    "## Indicadores\n",
    "### 1. Índice de Produção Conjunta dos docentes com discentes (IPC >= 50%):\n",
    "    Indicador: Índice de publicações com discentes por orientador (IPC)\n",
    "     Objetivo: Em, pelo menos, 50 % dos artigos publicados deve constar discentes do programa\n",
    "         Meta: IPC >= 50,00\n",
    "      \n",
    "      Cálculo:\n",
    "\n",
    "$$\\sum_{k=1}^{n}\\, 100 * \\frac{QPCD}{QPAT}$$\n",
    "\n",
    "        onde:\n",
    "               n = Artigos completos publicado em periódicos indexados\n",
    "            QPCD = Qte. Publicação de Artigos com discentes do Programa na lista de autores\n",
    "            QPAT = Qte. Publicação de Artigos Total no período avaliado\n",
    "\n",
    "### 2. Pontuação por Fator de Impacto (PFI >= 600 em pelo menos 70% dos docentes):\n",
    "\n",
    "    Indicador: Total de pontos conforme periódicos das publicações no período\n",
    "     Objetivo: Publicar trabalhos em periódicos de elevado impacto\n",
    "               \n",
    "         Meta: 70% dos docentes permanentes com PFI >= 600 pontos no quadriênio\n",
    "               (150/ano e ao menos 03 artigos A, no mínimo 02 em A1, ou 04 artigos A2);\n",
    " \n",
    "      Cálculo: Soma ponderada pela estratificação Qualis de acordo com o que segue:\n",
    "        A1 = 100 pontos\n",
    "        A2 = 80 pontos\n",
    "        B1 = 60 pontos\n",
    "        B2 = 40 pontos\n",
    "        B3 = 20 pontos\n",
    "        B4 = 10 pontos\n",
    "        B5 = 2 pontos.\n",
    "\n",
    "\n",
    "Parâmetro para classificaçao do periódico:\n",
    "\n",
    "        A1: Periódicos com FI ou CPD >= 4,300\n",
    "        A2: Periódicos com FI ou CPD entre 2,950 e 4,299\n",
    "        B1: Periódicos com FI ou CPD entre 1,800 e 2,949\n",
    "        B2: Periódicos com FI ou CPD entre 1,100 e 1,799\n",
    "        B3: Periódicos com FI ou CPD entre 0,300 e 1,099\n",
    "        B4: Periódicos com FI ou CPD entre 0,001 e 0,299, (ou Scielo, Scimago, PubMed ou Web of Science)\n",
    "        B5: Periódicos sem FI ou CPD e indexado nas lases Lilacs ou Latindex    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3cf167",
   "metadata": {},
   "source": [
    "# <b>F00: Preparar Ambiente</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4cec7",
   "metadata": {},
   "source": [
    "#### Instalar Git, WSL, ambiente virtual ou Contêiner Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc48fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instalar o WSL e integrar ao VSCode, no terminal rodar:\n",
    "## Verificar versões já instaladas\n",
    "#wsl -l -v\n",
    "## Atualizar para WSL2 para que as instalações de distros já rodem em WSL2 por padrão\n",
    "#wsl --set-default-version 2\n",
    "## Reiniciar a máquina e instalar distribuição Linux Ubuntu LTS pelo store do windows\n",
    "## Configurar VScode: iniciar VScode e instalar a extensão \"Remote - WSL\" para desenvolver diretamente no VSCode dentro do ambiente do WSL.\n",
    "# Após a instalação, na parte inferior esquerda da janela do VSCode, aparecerá o ícone verde.\n",
    "# Clicar no ícone verde e selecionar \"New WSL Window\" ou \"Reopen in WSL\" se o projeto já estiver aberto.\n",
    "# Agora, o VSCode estará rodando dentro do contexto do seu WSL, pode-se abrir terminais dentro do VSCode que acessarão diretamente o Linux.\n",
    "\n",
    "## Instalar Python e o Pip:\n",
    "# sudo apt update\n",
    "# sudo apt install python3-pip\n",
    "\n",
    "## Inserir dados do Git no VScode, no terminal rodar:\n",
    "# git config --global user.name \"nome_usuario_gi\"\n",
    "# git config --global user.email \"email_git\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72ab53",
   "metadata": {},
   "source": [
    "#### Instalar gerenciadores de pacotes e aplicativos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3348635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intalar os gerenciador de pacotes PIP  no terminal do WSL executar:\n",
    "# sudo apt update\n",
    "# sudo apt upgrade\n",
    "# sudo apt install python3-pip\n",
    "# python3 -m pip install --upgrade pip\n",
    "\n",
    "## Atualizar o gerenciador de pacotes e ferramentas de compilação\n",
    "# %pip install --upgrade pip setuptools wheel\n",
    "\n",
    "## Instalar primeiro o GraphViz antes do ygraphviz, no terminal do WSL rodar e depois reiniciar o kernel:\n",
    "## Para Linux e WSL instalar a partir do terminal:\n",
    "# sudo apt-get install graphviz graphviz-dev\n",
    "\n",
    "## Para Windows, atualizar o gerenciador de pacotes e ferramentas de compilação:\n",
    "# %pip install --upgrade pip setuptools wheel\n",
    "\n",
    "## Instalar o Chocolatey (Só para Windows, não no WSL)\n",
    "## Baixar e instalar o gerenciador chocolatey em https://jcutrer.com/windows/install-chocolatey-choco-windows10\n",
    "# Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\n",
    "\n",
    "## Baixar e instalar o Graphviz em https://www.graphviz.org/download/\n",
    "## https://savleen307.medium.com/pygraphviz-installation-in-windows-f45cc6fed981\n",
    "## Seguir as instruções em https://pygraphviz.github.io/documentation/stable/install.html#id1\n",
    "# O comando na página acima está errado por conter duas aspas onde não deve, executar o comando abaixo:\n",
    "# python -m pip install --use-pep517 --config-settings=\"--global-option=build_ext\" --config-settings=\"--global-option=-IC:\\Program Files\\Graphviz\\include\" --config-settings=--global-option=\"-LC:\\Program Files\\Graphviz\\lib\" pygraphviz\n",
    "\n",
    "## Instalar o pacote no ambiente local\n",
    "# %pip install pygraphviz\n",
    "\n",
    "## Para instalar o Conda (Miniconda ou Anaconda) no WSL, baixar o script de instalação diretamente do site oficial e executá-lo manualmente.\n",
    "## Passos para instalar o Miniconda como exemplo:\n",
    "## Baixar script de instalação do Miniconda para Linux:\n",
    "# wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# bash Miniconda3-latest-Linux-x86_64.sh\n",
    "# conda --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3065f",
   "metadata": {},
   "source": [
    "#### Instalar pacotes do requirements.txt e importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a25a9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Localizar requirements.txt para instalar bibliotecas no ambente\n",
    "# os.listdir('./../../../../')\n",
    "# %pip install -r ./../../../../requirements.txt\n",
    "\n",
    "## Outras instalações pontuais quando necessário, por exemplo:\n",
    "# import os, tqdm\n",
    "# %pip install chardet\n",
    "# print(tqdm.__version__)\n",
    "# %pip3 install --upgrade plotly\n",
    "# %pip3 install omegaconf --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87ef1b",
   "metadata": {},
   "source": [
    "#### Instalar e configurar GPU e CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2bc898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instalar drivers da GPU e compilador CUDA Nvidia\n",
    "## Obter comandos adequados para cada sistema em: https://developer.nvidia.com/cuda-downloads\n",
    "## Exemplo para Linux com Ubuntu\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\n",
    "# sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda-repo-wsl-ubuntu-12-4-local_12.4.0-1_amd64.deb\n",
    "# sudo dpkg -i cuda-repo-wsl-ubuntu-12-4-local_12.4.0-1_amd64.deb\n",
    "# sudo cp /var/cuda-repo-wsl-ubuntu-12-4-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "# sudo apt-get update\n",
    "# sudo apt-get -y install cuda-toolkit-12-4\n",
    "\n",
    "## Instruções para instalação do PyTorch para usar a GPU em Windows\n",
    "# https://sh-tsang.medium.com/tutorial-cuda-cudnn-anaconda-jupyter-pytorch-installation-in-windows-10-96b2a2f0ac57\n",
    "\n",
    "## Para máquinas com apenas CPU\n",
    "# !pip install torch torchvision\n",
    "\n",
    "## Testar cálculo na GPU\n",
    "# import torch\n",
    "# x = torch.rand(5, 3)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551ffdc",
   "metadata": {},
   "source": [
    "#### Implementar classes para obter e preparar dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921ae65",
   "metadata": {},
   "source": [
    "    LattesScraper  (Extrair currículos do HTML para JSON)\n",
    "    SoupParser (Extrair currículos do Objeto Soup)\n",
    "    Neo4jPersister (Persistir em Neo4j)\n",
    "    DatasetArticlesGenerator (Gerar Datasets)\n",
    "    DictToHDF5 (converter dicionários para formato HDF5)\n",
    "    ArticlesCounter (Monta dataframe com quantidades de artigos e tempo de defasagem da atualização)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b344f4",
   "metadata": {},
   "source": [
    "# <b>F01: Preparar para extrair e montar datasets</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f6e3cb",
   "metadata": {},
   "source": [
    "#### Abrir Neo4j e depois preparar pastas locais e classes de processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfd06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Sempre construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(base_repo_dir, 'utils')\n",
    "folder_domain = os.path.join(base_repo_dir, 'source', 'domain')\n",
    "folder_data_input = os.path.join(base_repo_dir, '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(base_repo_dir, '_data', 'out_json')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "from scraper_pasteur import PasteurScraper\n",
    "from environment_setup import EnvironmentSetup\n",
    "from scraper_sucupira import SucupiraScraper\n",
    "from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DictToHDF5, attribute_to_be_non_empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45fefc",
   "metadata": {},
   "source": [
    "### Checar/atualizar Chromedriver e GoogleChrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71efa162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versões 123 Chrome e 123 Chromedriver estão compatíveis\n"
     ]
    }
   ],
   "source": [
    "# Cria instância da classe ChromeDriverManager e verifica compatibilidade entre versões do Chrome e Chromedriver\n",
    "actualizer = ChromeDriverManager()\n",
    "actualizer.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3962aad",
   "metadata": {},
   "source": [
    "### Obter lista Qualis de Periódicos da Plataforma Sucupira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03c0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper = SucupiraScraperEdge()\n",
    "# scraper.scrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb1692",
   "metadata": {},
   "source": [
    "# <b>F02: Extrair dados de Currículos Lattes</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd18ddb",
   "metadata": {},
   "source": [
    "## Rodar testes de ambiente e definir pastas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b7c93d",
   "metadata": {},
   "source": [
    "### Preparar pastas locais para processamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b06c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary directories are ensured.\n",
      "Processador em uso: \n",
      "Arquitetura modelo: AMD64 Family 25 Model 33 Stepping 0, AuthenticAMD\n",
      "Arquitetura em uso: 64bit\n",
      "Frequência das CPU: 3801.0 MHz\n",
      "  Qte CPUs físicas: 8\n",
      "  Qte CPUs lógicas: 16\n",
      "Carga total na CPU: 2.4%\n",
      "Ocupação atual CPU: user=1.9%, system=1.4%, idle=96.3%\n",
      "\n",
      "Espaço Total em disco: 465.15 GB\n",
      "Espaço em disco usado: 447.46 GB 96.2%\n",
      "Espaço em disco livre: 17.69 GB 3.8%\n",
      "\n",
      "Capacidade memórias RAM: 63.94 GB\n",
      "Utilização atual da RAM: 27.30 GB\n",
      "\n",
      "VERSÕES DOS DRIVERS CUDA, PYTORCH E GPU\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Wed_Nov_22_10:30:42_Pacific_Standard_Time_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.107\n",
      "Build cuda_12.3.r12.3/compiler.33567101_0\n",
      "\n",
      "    PyTorch: 2.2.0+cu121\n",
      "Dispositivo: cuda\n",
      "Disponível: cuda True  | Inicializado: False | Capacidade: (7, 5)\n",
      "Nome GPU: NVIDIA GeForce RTX 2060  | Quantidade: 1\n",
      "\n",
      "VERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT\n",
      "Ambiente Conda ativo: base\n",
      "Interpretador em uso: c:\\Users\\marco\\anaconda3\\python.exe\n",
      " Python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)] \n",
      "    Pip: pip 24.0 from c:\\Users\\marco\\anaconda3\\Lib\\site-packages\\pip (python 3.11)\n",
      "\n",
      "\n",
      "Pasta para xls_zip já existe!\n",
      "Pasta para csv já existe!\n",
      "Pasta para json já existe!\n",
      "Pasta para fig já existe!\n",
      "Pasta para output já existe!\n",
      "\n",
      "Caminho da pasta raiz: C:\\Users\\marco\\ppgcs_202404\n",
      "Caminho para xls_zip: xls_zip\n",
      "Caminho para csv: csv\n",
      "Caminho para json: json\n",
      "Caminho para fig: fig\n",
      "Caminho para output: output\n"
     ]
    }
   ],
   "source": [
    "# Cria instância da classe EnvironmenSetup e preparar pastas\n",
    "preparer = EnvironmentSetup()\n",
    "folder_name = input(\"Digite o nome da pasta principal: \")\n",
    "preparer.set_root_path(folder_name)\n",
    "preparer.try_cpu()\n",
    "preparer.try_gpu()\n",
    "preparer.try_amb()\n",
    "# preparer.try_browser()\n",
    "preparer.preparar_pastas()\n",
    "\n",
    "## Remover diretórios no linux\n",
    "# !rm -rf /home/mak/fioce\n",
    "# os.listdir('/home/mak/fioce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa67226",
   "metadata": {},
   "source": [
    "## Montar lista_nomes de arquivo .csv para extrair currículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc922df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 currículos a extrair da plataforma Lattes\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd\n",
    "pathfilename = os.path.join(folder_data_input,'nomesdocentes.csv')\n",
    "lista_busca = pd.read_csv(pathfilename,header=None)[0].values\n",
    "print(f'{len(lista_busca)} currículos a extrair da plataforma Lattes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55130d7a",
   "metadata": {},
   "source": [
    "## Montar lista_nomes de planilha para extrair currículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "279f1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ler dados do arquivo Excel do Setor de Recursos Humanos\n",
    "# pathdata = './../data/'\n",
    "# file_persons = 'fioce_colaboradores-2023.xls'\n",
    "\n",
    "# # Ler apenas os cabeçalhos do arquivo Excel\n",
    "# headers = pd.read_excel(pathdata+file_persons, skiprows=3, header=0, nrows=0).columns\n",
    "# # headers\n",
    "\n",
    "# # Usar função para indicar quais colunas devem ser eliminadas na leitura\n",
    "# def cols_to_keep(col_name):\n",
    "#     return col_name not in ['QUANT','Unnamed: 3','Unnamed: 6','Unnamed: 9','ADICIONAL OCUPACIONAL',\n",
    "#                             'EMPRESA/BOLSA/PROGRAMA','GESTOR','ADI','POSSE NA FIOCRUZ',\n",
    "#                             'VIGÊNCIA BOLSA/ENCERRAMENTO DO CONTRATO','Unnamed: 17',\n",
    "#                             'EMAIL INSTITUCIONAL','EMAIL PESSOAL','GENERO','DATA NASCIMENTO',\n",
    "#                             'Unnamed: 22','FORMAÇÃO','ENDEREÇO RESIDENCIAL']\n",
    "\n",
    "# # Filtrar cabeçalhos com base na função\n",
    "# selected_columns = [col for col in headers if cols_to_keep(col)]\n",
    "\n",
    "# # Ler dados do arquivo Excel do Setor de Recursos Humanos\n",
    "# fioce_pessoal = pd.read_excel(pathdata+file_persons, skiprows=3, header=0, usecols=selected_columns)\n",
    "# print(f'{len(fioce_pessoal.index)} nomes de colaboradores no total, todos vínculos e status')\n",
    "# print(f'{len(fioce_pessoal[\"VÍNCULO\"].unique()):3} tipos de vínculos')\n",
    "# print('\\nTipos de vínculos',list(fioce_pessoal['VÍNCULO'].unique()))\n",
    "# print('  Tipos de status',list(fioce_pessoal['STATUS'].unique()))\n",
    "# filtro1 = fioce_pessoal.VÍNCULO == 'SERVIDOR'\n",
    "# filtro2 = fioce_pessoal['STATUS'].isin(['ATIVO', 'AFASTADO'])\n",
    "# lista_nomes = fioce_pessoal[(filtro1) & (filtro2)]['NOME'].tolist()\n",
    "# print(f'\\n{len(lista_nomes)} nomes para extrair currículos')\n",
    "# for i,nome in enumerate(lista_nomes):\n",
    "#     print(f'{i+1:2}. {nome}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc89b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grupos_internos = fioce_pessoal['ÁREA'].unique()\n",
    "# coordenacoes=[]\n",
    "# grupos_tematicos=[]\n",
    "# for i in grupos_internos:\n",
    "#     # print(i)\n",
    "#     if 'coordenação' in str(i).lower():\n",
    "#         coordenacoes.append(i)\n",
    "#     elif str(i) != 'nan':\n",
    "#         grupos_tematicos.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "288fb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordenacoes.sort()\n",
    "# coordenacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7507b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grupos_tematicos.sort()\n",
    "# grupos_tematicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5adb022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Outras atividades não relacionadas diretamente com pesquisa\n",
    "# remover = ['Angela Christina De Moraes Ostritz',\n",
    "#            'Bruno Bezerra Carvalho',\n",
    "#            'Carlos Jose Araujo Pinheiro',\n",
    "#            'Charles Cerqueira De Abreu',\n",
    "#            'Ezequiel Valentim De Melo',\n",
    "#            'Ivanildo Lopes Farias',\n",
    "#            'João Baptista Estabile Neto',\n",
    "#            'Kamila Matos Albuquerque',\n",
    "#            'Luciana Coelho Serafim',\n",
    "#            'Luciano Pinto Zorzanelli',\n",
    "#            'Luciana Silvério Alleluia Higino Da Silva',\n",
    "#            'Luis Fernando Pessoa De Andrade',\n",
    "#            'Marcelo Jorge Lopes Coutinho',\n",
    "#            'Nilton Luiz Costa Machado',\n",
    "#            'Patricia Maria Ferreira da Silva',\n",
    "#            'Renato Caldeira De Souza',\n",
    "#            'Sergio Dos Santos Reis',\n",
    "#            'Clarice Gomes E Souza Dabés',\n",
    "#            'Luciana Pereira Lindenmeyer',\n",
    "#            'Rodrigo Carvalho Nogueira',\n",
    "           \n",
    "#         #    'Ana Camila Oliveira Alves',\n",
    "#         #    'Antonio Marcos Aires Barbosa',\n",
    "#         #    'Venúcia Bruna Magalhães Pereira',\n",
    "#            ]\n",
    "\n",
    "# trocar = {\n",
    "#     'Maximiliano Loiola Ponte De Souza': 'Maximiliano Ponte',\n",
    "#     'Raphael Trevizani Roque': 'Raphael Trevizani',\n",
    "#     }\n",
    "\n",
    "# lista_busca=[]\n",
    "# for i in lista_nomes:\n",
    "#     if i.strip() in trocar.keys():\n",
    "#         lista_busca.append(trocar.get(i))\n",
    "#     elif i.strip() not in remover:\n",
    "#         lista_busca.append(i)\n",
    "\n",
    "# # Limitando quantidade para testes\n",
    "# # lista_busca = lista_busca[:5]\n",
    "\n",
    "# print(f'{len(lista_busca)} currículos a extrair da plataforma Lattes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35849f4e",
   "metadata": {},
   "source": [
    "## <b>Processar com classe de extração</b> \n",
    "    (10-15min/40nomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d2a31",
   "metadata": {},
   "source": [
    "### Extrair currículos da plataforma Lattes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afa73b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment_setup import EnvironmentSetup\n",
    "preparer = EnvironmentSetup()\n",
    "repo_root = preparer.find_repo_root(os.getcwd())\n",
    "pasta_dados = os.path.join(repo_root,'data','output')\n",
    "instituicao = 'cruz'\n",
    "termo1 = 'cpqrr'\n",
    "termo2 = 'minas'\n",
    "termo3 = 'rachou'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ddfb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Teste de extração de currículos com homônimos\n",
    "# t1 = time.time()\n",
    "# lista_homonimos = [\n",
    "#     'Tania Maria Alves de Almeida',\n",
    "#     'Rodrigo Corrêa de Oliveira',\n",
    "# ]\n",
    "\n",
    "# scraper = LattesScraper(instituicao, termo1, termo2, 'bolt://localhost:7687', 'neo4j', 'password')\n",
    "# dom_dict_homonimos = scraper.scrape(lista_homonimos, instituicao, termo1, termo2, termo3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07a2e8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando currículos apenas entre nível de doutorado\n",
      " 1/55: Alessandra Aparecida Guarneri\n",
      "       057 artigos extraídos\n",
      "       DOI indisponível em 02 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      " 2/55: Alexandre de Magalhães Vieira Machado\n",
      "       034 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      " 3/55: Ana Lúcia Teles Rabello\n",
      "       149 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      " 4/55: Andrea Teixeira de Carvalho\n",
      "       268 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      " 5/55: Carlos Eduardo Calzavara Silva\n",
      "       040 artigos extraídos\n",
      "       DOI indisponível em 04 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      " 6/55: Caroline Furtado Junqueira\n",
      "       030 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      " 7/55: Célia Maria Ferreira Gontijo\n",
      "       107 artigos extraídos\n",
      "       DOI indisponível em 04 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      " 8/55: Cristiana Ferreira Alves de Brito\n",
      "       081 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      " 9/55: Cristina Toscano Fonseca\n",
      "       057 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "10/55: Edelberto Santos Dias\n",
      "       089 artigos extraídos\n",
      "       DOI indisponível em 02 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "11/55: Edward José de Oliveira\n",
      "        2 currículos homônimos: Edward José de Oliveira\n",
      "        Escolhido o homônimo 2: Edward José de Oliveira \n",
      "       Bolsista de Produtividade Desen. Tec. e Extensão Inovadora 2\n",
      "       Doutorado em Farmácia (Fisiopatologia e Toxicologia) pela Universidade de São Paulo, Brasil(2005)\n",
      "       Pesq e prof. no Curso de Pós-Gradução/CPqRR da Fundação Oswaldo Cruz , Brasil\n",
      "       062 artigos extraídos\n",
      "       DOI indisponível em 02 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "12/55: Flora Satiko Kano\n",
      "       042 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "13/55: Gláucia Fernandes Cota\n",
      "       082 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "14/55: Jaquelline Germano de Oliveira\n",
      "       039 artigos extraídos\n",
      "       DOI indisponível em 03 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "15/55: Jerônimo Conceição Ruiz\n",
      "       070 artigos extraídos\n",
      "       DOI indisponível em 03 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "16/55: José Dilermando Andrade Filho\n",
      "       131 artigos extraídos\n",
      "       DOI indisponível em 08 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "17/55: Liléia Gonçalves Diotaiuti\n",
      "       154 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "18/55: Lis Ribeiro do Valle Antonelli\n",
      "       084 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      "19/55: Luciano Andrade Moreira\n",
      "        2 currículos homônimos: Luciano Andrade Moreira\n",
      "       090 artigos extraídos\n",
      "       DOI indisponível em 05 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "20/55: Luzia Helena Carvalho\n",
      "       074 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      "21/55: Marcelo Antônio Pascoal Xavier\n",
      "       053 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "22/55: Marcelo Gustavo Lorenzo\n",
      "       075 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      "23/55: Márcio Sobreira Silva Araújo\n",
      "       082 artigos extraídos\n",
      "       DOI indisponível em 02 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "24/55: Marco Antônio da Silva Campos\n",
      "       052 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      "25/55: Marina de Moraes Mourão\n",
      "       048 artigos extraídos\n",
      "       DOI indisponível em 02 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "26/55: Nágila Francinete Costa Secundino\n",
      "       074 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "27/55: Olindo Assis Martins Filho\n",
      "       473 artigos extraídos\n",
      "       DOI indisponível em 03 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "28/55: Paulo Filemon Paolucci Pimenta\n",
      "       164 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      "29/55: Paulo Marcos Zech Coelho\n",
      "       239 artigos extraídos\n",
      "       DOI indisponível em 02 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "30/55: Ricardo Tostes Gazzinelli\n",
      "       302 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      "31/55: Roberta Lima Caldeira\n",
      "       081 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      "32/55: Rodrigo Corrêa de Oliveira\n",
      "       361 artigos extraídos\n",
      "       DOI indisponível em 28 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "33/55: Rodrigo Pedro Pinto Soares\n",
      "       111 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "34/55: Rubens Lima do Monte Neto\n",
      "       055 artigos extraídos\n",
      "       DOI indisponível em 02 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "35/55: Silvane Maria Fonseca Murta\n",
      "       117 artigos extraídos\n",
      "Seção de idiomas não encontrada\n",
      "       DOI indisponível em 03 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "36/55: Taís Nóbrega de Sousa\n",
      "       043 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "37/55: Vanessa Peruhype Magalhães Pascoal\n",
      "       078 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "38/55: Alvaro Gil Araujo Ferreira\n",
      "       014 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "39/55: Carina Margonari de Souza\n",
      "       033 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "40/55: Cristiana Couto Garcia\n",
      "       053 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "41/55: Daniel Moreira de Avelar\n",
      "       035 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "42/55: Érica Alessandra Rocha Alves\n",
      "       017 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "43/55: Erika Michalsky Monteiro\n",
      "       039 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "44/55: Gabriel da Rocha Fernandes\n",
      "       083 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      "45/55: Gustavo Fontes Paz\n",
      "       039 artigos extraídos\n",
      "       DOI indisponível em 04 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "46/55: Luiz Carlos Júnior Alcantara\n",
      "       195 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      "47/55: Paloma Helena Fernandes Shimabukuro\n",
      "       046 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      "48/55: Pedro Augusto Alves\n",
      "        2 currículos homônimos: Pedro Augusto Alves\n",
      "       042 artigos extraídos\n",
      "       DOI indisponível em 02 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "49/55: Rafaella Fortini Grenfell e Queiroz\n",
      "       035 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "50/55: Rita de Cassia Moreira de Souza\n",
      "       034 artigos extraídos\n",
      "       DOI indisponível para 01 artigo extraído\n",
      "       Extração bem-sucedida\n",
      "51/55: Rosiane Aparecida da Silva Pereira\n",
      "       023 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "52/55: Roney Santos Coimbra\n",
      "       054 artigos extraídos\n",
      "       DOI indisponível em 03 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "53/55: Soraya Torres Gaze Jangola\n",
      "       029 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "54/55: Tânia Maria de Almeida Alves\n",
      "       094 artigos extraídos\n",
      "       DOI indisponível em 02 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "55/55: Antonio Mauro Rezende\n",
      "       061 artigos extraídos\n",
      "       Extração bem-sucedida\n",
      "\n",
      "00:36:45 para busca de 55 nomes com extração de dados de 55 dicionários\n"
     ]
    }
   ],
   "source": [
    "# Iniciar a extração completa de todos currículos\n",
    "t1 = time.time()\n",
    "scraper = LattesScraper(instituicao, termo1, termo2, termo3, 'bolt://localhost:7687', 'neo4j', 'password')\n",
    "\n",
    "# Extrai e monta JSON com a lista de dicionários\n",
    "dom_dict_list = scraper.scrape(lista_busca, instituicao, termo1, termo2, termo3)\n",
    "print(f'\\n{preparer.tempo(t1,time.time())} para busca de {len(lista_busca)} nomes com extração de dados de {len(dom_dict_list)} dicionários')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca7dca23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adicionando Qualis Periódicos\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "00311820 | A1\n",
      "19352735 | A1\n",
      "09651748 | A1\n",
      "20492618 | A1\n",
      "17563305 | A2\n",
      "00311820 | A1\n",
      "22352988 | A2\n",
      "00311820 | A1\n",
      "20010370 | A2\n",
      "19352735 | A1\n",
      "00221910 | A1\n",
      "22352988 | A2\n",
      "20010370 | A2\n",
      "00311820 | A1\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "0001706X | A2\n",
      "16788060 | A4\n",
      "01666851 | A4\n",
      "09320113 | A1\n",
      "20452322 | A1\n",
      "20452322 | A1\n",
      "0950382X | A2\n",
      "00311820 | A1\n",
      "00221910 | A1\n",
      "03009084 | A2\n",
      "00311820 | A1\n",
      "00311820 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "03009084 | A2\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "00278424 | A1\n",
      "19326203 | A1\n",
      "09651748 | A1\n",
      "19352735 | A1\n",
      "00740276 | A4\n",
      "17560500 | B1\n",
      "15303667 | A4\n",
      "00222011 | A2\n",
      "00221910 | A1\n",
      "17563305 | A2\n",
      "14714922 | A1\n",
      "03076962 | A3\n",
      "0001706X | A2\n",
      "03076962 | A3\n",
      "03076962 | A3\n",
      "00221910 | A1\n",
      "0102311X | A1\n",
      "0102311X | A1\n",
      "13602276 | A1\n",
      "00222585 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "20411723 | A1\n",
      "10436618 | A1\n",
      "15178382 | A2\n",
      "10400605 | A1\n",
      "07302312 | A3\n",
      "19352735 | A1\n",
      "16643224 | A2\n",
      "23275081 | B4\n",
      "1743422X | A3\n",
      "01663542 | A2\n",
      "22352988 | A2\n",
      "16643224 | A2\n",
      "16643224 | A2\n",
      "15537374 | A1\n",
      "15537374 | A1\n",
      "10430342 | A2\n",
      "19326203 | A1\n",
      "09629351 | A3\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "0264410X | A2\n",
      "15537374 | A1\n",
      "00199567 | A2\n",
      "19326203 | A1\n",
      "0264410X | A2\n",
      "15537374 | A1\n",
      "00199567 | A2\n",
      "0264410X | A2\n",
      "00221767 | A1\n",
      "00426822 | A2\n",
      "10430342 | A2\n",
      "         | None\n",
      "10430342 | A2\n",
      "         | None\n",
      "16789849 | B1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "16789946 | B1\n",
      "16643224 | A2\n",
      "16643224 | A2\n",
      "16784464 | A1\n",
      "15167704 | B1\n",
      "12019712 | A2\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "16788060 | A4\n",
      "19352735 | A1\n",
      "16789849 | B1\n",
      "03057453 | A1\n",
      "19326203 | A1\n",
      "23146133 | A3\n",
      "19352735 | A1\n",
      "16788060 | A4\n",
      "16788060 | A4\n",
      "16789849 | B1\n",
      "17560500 | B1\n",
      "19326203 | A1\n",
      "16789946 | B1\n",
      "00378682 | B1\n",
      "16879686 | B1\n",
      "0001706X | A2\n",
      "19352735 | A1\n",
      "00311820 | A1\n",
      "00378682 | B1\n",
      "16784464 | A1\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "16789849 | B1\n",
      "19326203 | A1\n",
      "00359203 | A3\n",
      "16788060 | A4\n",
      "19326203 | A1\n",
      "00311820 | A1\n",
      "17563305 | A2\n",
      "00740276 | A4\n",
      "03057453 | A1\n",
      "16784464 | A1\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "19352735 | A1\n",
      "00029637 | A3\n",
      "19352735 | A1\n",
      "24660473 | None\n",
      "00378682 | B1\n",
      "19326203 | A1\n",
      "00359203 | A3\n",
      "00359203 | A3\n",
      "19352735 | A1\n",
      "00029637 | A3\n",
      "03057453 | A1\n",
      "00664804 | A1\n",
      "16006135 | A1\n",
      "19352735 | A1\n",
      "00359203 | A3\n",
      "00144894 | A4\n",
      "19352735 | A1\n",
      "00378682 | B1\n",
      "19352735 | A1\n",
      "10584838 | A1\n",
      "13602276 | A1\n",
      "19352735 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "16879635 | A4\n",
      "19352735 | A1\n",
      "14310651 | A4\n",
      "0102695X | A3\n",
      "00359203 | A3\n",
      "00359203 | A3\n",
      "0367326X | A4\n",
      "19352735 | A1\n",
      "00740276 | A4\n",
      "19352735 | A1\n",
      "15178382 | A2\n",
      "00378682 | B1\n",
      "00359203 | A3\n",
      "0102311X | A1\n",
      "00036072 | A4\n",
      "00740276 | A4\n",
      "00359203 | A3\n",
      "00740276 | A4\n",
      "00029637 | A3\n",
      "00359203 | A3\n",
      "00359203 | A3\n",
      "16794974 | A3\n",
      "03781097 | B1\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "19352735 | A1\n",
      "00359203 | A3\n",
      "07328893 | A4\n",
      "00099104 | A3\n",
      "0001706X | A2\n",
      "0102311X | A1\n",
      "07328893 | A4\n",
      "03009475 | A4\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "0102311X | A1\n",
      "00359203 | A3\n",
      "13982273 | A4\n",
      "00029637 | A3\n",
      "07328893 | A4\n",
      "00034983 | A4\n",
      "0001706X | A2\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "07328893 | A4\n",
      "00740276 | A4\n",
      "00029637 | A3\n",
      "00359203 | A3\n",
      "00359203 | A3\n",
      "00378682 | B1\n",
      "14138670 | B1\n",
      "00359203 | A3\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00359203 | A3\n",
      "10584838 | A1\n",
      "00311820 | A1\n",
      "00359203 | A3\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "03009475 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "08880786 | None\n",
      "00740276 | A4\n",
      "10584838 | A1\n",
      "00740276 | A4\n",
      "00029637 | A3\n",
      "00359203 | A3\n",
      "00413232 | None\n",
      "00413232 | None\n",
      "00359203 | A3\n",
      "00359203 | A3\n",
      "00359203 | A3\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00364665 | B1\n",
      "00739855 | B3\n",
      "00359203 | A3\n",
      "01419838 | A2\n",
      "20411723 | A1\n",
      "20799721 | C\n",
      "16643224 | A2\n",
      "10434666 | A4\n",
      "10799796 | A4\n",
      "00192805 | A1\n",
      "00144894 | A4\n",
      "15216616 | A3\n",
      "10584838 | A1\n",
      "16643224 | A2\n",
      "10434666 | A4\n",
      "16643224 | A2\n",
      "0264410X | A2\n",
      "25737732 | C\n",
      "01675273 | A2\n",
      "27727076 | None\n",
      "20452322 | A1\n",
      "25890042 | A2\n",
      "2234943X | A4\n",
      "12864579 | A3\n",
      "07533322 | A2\n",
      "0264410X | A2\n",
      "19326203 | A1\n",
      "10647481 | A1\n",
      "05315565 | A3\n",
      "16643224 | A2\n",
      "2297055X | A3\n",
      "16643224 | A2\n",
      "15675769 | A2\n",
      "16643224 | A2\n",
      "0264410X | A2\n",
      "16643224 | A2\n",
      "16643224 | A2\n",
      "14795876 | A2\n",
      "01712985 | A2\n",
      "16643224 | A2\n",
      "16643224 | A2\n",
      "00476374 | A2\n",
      "00951137 | A1\n",
      "10434666 | A4\n",
      "27546993 | None\n",
      "2297055X | A3\n",
      "19290748 | C\n",
      "22352988 | A2\n",
      "16878469 | B3\n",
      "21507511 | A1\n",
      "0264410X | A2\n",
      "01652478 | A3\n",
      "2234943X | A4\n",
      "20452322 | A1\n",
      "01663542 | A2\n",
      "05315565 | A3\n",
      "16643224 | A2\n",
      "08039488 | B1\n",
      "19994915 | A2\n",
      "00221759 | B2\n",
      "16639812 | A2\n",
      "01419838 | A2\n",
      "16788060 | A4\n",
      "20499957 | A2\n",
      "07415400 | A2\n",
      "00144894 | A4\n",
      "16788060 | A4\n",
      "07415400 | A2\n",
      "20452322 | A1\n",
      "16643224 | A2\n",
      "10584838 | A1\n",
      "15537374 | A1\n",
      "20452322 | A1\n",
      "0166445X | A1\n",
      "20452322 | A1\n",
      "16643224 | A2\n",
      "10434666 | A4\n",
      "14712105 | A1\n",
      "19352735 | A1\n",
      "01615890 | A4\n",
      "00099104 | A3\n",
      "14786362 | A2\n",
      "16784464 | A1\n",
      "15188787 | A1\n",
      "16643224 | A2\n",
      "07415400 | A2\n",
      "01634453 | A1\n",
      "16643224 | A2\n",
      "10806040 | A1\n",
      "01757598 | A2\n",
      "0264410X | A2\n",
      "16643224 | A2\n",
      "2076393X | A3\n",
      "16643224 | A2\n",
      "14712334 | A3\n",
      "20452322 | A1\n",
      "2076393X | A3\n",
      "1743422X | A3\n",
      "16643224 | A2\n",
      "00221759 | B2\n",
      "00664804 | A1\n",
      "19352735 | A1\n",
      "16643224 | A2\n",
      "16788060 | A4\n",
      "00221899 | A1\n",
      "19352735 | A1\n",
      "01419838 | A2\n",
      "1664302X | A2\n",
      "10788956 | A1\n",
      "08824010 | A3\n",
      "09394451 | A2\n",
      "20452322 | A1\n",
      "21645515 | A3\n",
      "17424933 | A3\n",
      "01663542 | A2\n",
      "00221759 | B2\n",
      "19352735 | A1\n",
      "10434666 | A4\n",
      "10532498 | A1\n",
      "19352735 | A1\n",
      "00221759 | B2\n",
      "01652478 | A3\n",
      "01663542 | A2\n",
      "01674943 | A1\n",
      "01712985 | A2\n",
      "05315565 | A3\n",
      "19326203 | A1\n",
      "00221759 | B2\n",
      "05315565 | A3\n",
      "00088749 | A3\n",
      "14712180 | A2\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "00221899 | A1\n",
      "19326203 | A1\n",
      "14752875 | A2\n",
      "00223956 | A2\n",
      "10434666 | A4\n",
      "01712985 | A2\n",
      "19326203 | A1\n",
      "03009475 | A4\n",
      "16765680 | B2\n",
      "22112839 | None\n",
      "0264410X | A2\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "17510147 | A2\n",
      "21645515 | A3\n",
      "1354750X | B1\n",
      "00221759 | B2\n",
      "01615890 | A4\n",
      "17563305 | A2\n",
      "00144894 | A4\n",
      "23148861 | A3\n",
      "09209964 | A1\n",
      "00098981 | A3\n",
      "14712334 | A3\n",
      "00221759 | B2\n",
      "01988859 | B1\n",
      "0264410X | A2\n",
      "15537374 | A1\n",
      "03044017 | A1\n",
      "0264410X | A2\n",
      "09629351 | A3\n",
      "00088749 | A3\n",
      "15254135 | A2\n",
      "10434666 | A4\n",
      "01466615 | B1\n",
      "10434666 | A4\n",
      "19458924 | A1\n",
      "20900023 | B1\n",
      "20900023 | B1\n",
      "03014851 | B3\n",
      "07328893 | A4\n",
      "00199567 | A2\n",
      "19352735 | A1\n",
      "01652478 | A3\n",
      "01988859 | B1\n",
      "19352735 | A1\n",
      "15566811 | A3\n",
      "03008916 | B2\n",
      "01652478 | A3\n",
      "01652427 | A1\n",
      "14765810 | A1\n",
      "14783223 | A2\n",
      "01676806 | A2\n",
      "00378682 | B1\n",
      "01712985 | A2\n",
      "19326203 | A1\n",
      "00429007 | A4\n",
      "19352735 | A1\n",
      "15412016 | A3\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "03009475 | A4\n",
      "15353702 | A3\n",
      "14712334 | A3\n",
      "03044017 | A1\n",
      "03009475 | A4\n",
      "0264410X | A2\n",
      "01652427 | A1\n",
      "00221759 | B2\n",
      "19326203 | A1\n",
      "10900233 | A1\n",
      "00144894 | A4\n",
      "00221759 | B2\n",
      "19352735 | A1\n",
      "0001706X | A2\n",
      "00221899 | A1\n",
      "01434004 | A2\n",
      "00243205 | A1\n",
      "1414431X | A3\n",
      "03009475 | A4\n",
      "01660934 | B2\n",
      "14712407 | A3\n",
      "13510002 | A3\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00099104 | A3\n",
      "19352727 | A1\n",
      "0100879X | A3\n",
      "14752875 | A2\n",
      "0264410X | A2\n",
      "01466615 | B1\n",
      "03009475 | A4\n",
      "03009475 | A4\n",
      "00740276 | A4\n",
      "00221759 | B2\n",
      "19352735 | A1\n",
      "15566811 | A3\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "09724567 | B4\n",
      "12864579 | A3\n",
      "0264410X | A2\n",
      "0264410X | A2\n",
      "0264410X | A2\n",
      "0264410X | A2\n",
      "00664804 | A1\n",
      "03009475 | A4\n",
      "1568010X | None\n",
      "00144894 | A4\n",
      "00199567 | A2\n",
      "01652427 | A1\n",
      "10217401 | B2\n",
      "10939946 | A3\n",
      "00099104 | A3\n",
      "00099104 | A3\n",
      "00221759 | B2\n",
      "12864579 | A3\n",
      "0264410X | A2\n",
      "13602276 | A1\n",
      "01652427 | A1\n",
      "00099104 | A3\n",
      "03009475 | A4\n",
      "00099104 | A3\n",
      "00740276 | A4\n",
      "00099104 | A3\n",
      "01652427 | A1\n",
      "03009475 | A4\n",
      "00345288 | A1\n",
      "03009475 | A4\n",
      "         | None\n",
      "03009475 | A4\n",
      "00378682 | B1\n",
      "1071412X | None\n",
      "00378682 | B1\n",
      "03009475 | A4\n",
      "00378682 | B1\n",
      "21650497 | A1\n",
      "03785173 | A1\n",
      "18686966 | A1\n",
      "15216616 | A3\n",
      "00951137 | A1\n",
      "19994915 | A2\n",
      "2296858X | A1\n",
      "2234943X | A4\n",
      "1664302X | A2\n",
      "1743422X | A3\n",
      "2673818X | None\n",
      "17435889 | A1\n",
      "19994915 | A2\n",
      "19352735 | A1\n",
      "19994915 | A2\n",
      "0264410X | A2\n",
      "2076393X | A3\n",
      "19994915 | A2\n",
      "15675769 | A2\n",
      "14694409 | A4\n",
      "19352735 | A1\n",
      "10969071 | B1\n",
      "00207519 | A1\n",
      "16788060 | A4\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "01988859 | B1\n",
      "00740276 | A4\n",
      "10933263 | A3\n",
      "21616620 | B4\n",
      "00426822 | A2\n",
      "19326203 | A1\n",
      "15178382 | A2\n",
      "00013765 | A2\n",
      "00145793 | A2\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "15216543 | A4\n",
      "00207519 | A1\n",
      "00740276 | A4\n",
      "00280836 | A1\n",
      "20590105 | A1\n",
      "22962565 | A1\n",
      "10747613 | A1\n",
      "00221899 | A1\n",
      "00280836 | A1\n",
      "00280836 | A1\n",
      "1940087X | B1\n",
      "15292908 | A1\n",
      "00278424 | A1\n",
      "23147156 | A3\n",
      "00280836 | A1\n",
      "00928674 | A1\n",
      "15537374 | A1\n",
      "20411723 | A1\n",
      "1600065X | None\n",
      "16643224 | A2\n",
      "10788956 | A1\n",
      "23747943 | None\n",
      "1664302X | A2\n",
      "15537374 | A1\n",
      "00221899 | A1\n",
      "22111247 | A1\n",
      "14320851 | None\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "00278424 | A1\n",
      "14623994 | None\n",
      "17443091 | None\n",
      "00221767 | A1\n",
      "00029637 | A3\n",
      "16788060 | A4\n",
      "0001706X | A2\n",
      "24146366 | A2\n",
      "00222585 | A1\n",
      "22352988 | A2\n",
      "0001706X | A2\n",
      "16784456 | B1\n",
      "25253409 | C\n",
      "03044017 | A1\n",
      "19352735 | A1\n",
      "01479571 | A2\n",
      "25253409 | C\n",
      "22132244 | A2\n",
      "0001706X | A2\n",
      "09320113 | A1\n",
      "16785150 | A4\n",
      "19842961 | A2\n",
      "19326203 | A1\n",
      "10434666 | A4\n",
      "01615890 | A4\n",
      "22352988 | A2\n",
      "16788060 | A4\n",
      "1414431X | A3\n",
      "03044017 | A1\n",
      "22352988 | A2\n",
      "00221759 | B2\n",
      "16995848 | A3\n",
      "00311820 | A1\n",
      "0001706X | A2\n",
      "09320113 | A1\n",
      "09320113 | A1\n",
      "19842961 | A2\n",
      "07533322 | A2\n",
      "20452322 | A1\n",
      "14712105 | A1\n",
      "16788060 | A4\n",
      "0001706X | A2\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "00222585 | A1\n",
      "16789946 | B1\n",
      "01666851 | A4\n",
      "13835769 | A3\n",
      "17466148 | A2\n",
      "19326203 | A1\n",
      "03009475 | A4\n",
      "19326203 | A1\n",
      "17563305 | A2\n",
      "01615890 | A4\n",
      "01041290 | A3\n",
      "17563305 | A2\n",
      "00222585 | A1\n",
      "17563305 | A2\n",
      "0001706X | A2\n",
      "8756971X | B1\n",
      "16765680 | B2\n",
      "19326203 | A1\n",
      "14154757 | A3\n",
      "16766024 | None\n",
      "0102311X | A1\n",
      "03044165 | A1\n",
      "00359203 | A3\n",
      "03044017 | A1\n",
      "03044017 | A1\n",
      "00740276 | A4\n",
      "00222585 | A1\n",
      "01025716 | B4\n",
      "01652427 | A1\n",
      "0001706X | A2\n",
      "00222585 | A1\n",
      "15303667 | A4\n",
      "01652427 | A1\n",
      "03044017 | A1\n",
      "15303667 | A4\n",
      "13602276 | A1\n",
      "01657380 | A2\n",
      "         | None\n",
      "14714922 | A1\n",
      "16679008 | None\n",
      "14759292 | None\n",
      "14714922 | A1\n",
      "16765680 | B2\n",
      "1415790X | A3\n",
      "00378682 | B1\n",
      "0001706X | A2\n",
      "         | None\n",
      "00411345 | B3\n",
      "         | None\n",
      "00740276 | A4\n",
      "         | None\n",
      "         | None\n",
      "01002430 | B2\n",
      "00029637 | A3\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "0034280  | None\n",
      "0001706X | A2\n",
      "0074276  | None\n",
      "0074276  | None\n",
      "0074276  | None\n",
      "         | None\n",
      "0074276  | None\n",
      "0074276  | None\n",
      "00740276 | A4\n",
      "14752875 | A2\n",
      "26737515 | None\n",
      "22352988 | A2\n",
      "00014079 | B3\n",
      "14752875 | A2\n",
      "1664302X | A2\n",
      "20524463 | None\n",
      "22352988 | A2\n",
      "17417007 | A1\n",
      "22352988 | A2\n",
      "14752875 | A2\n",
      "00664804 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "16784685 | A3\n",
      "14752875 | A2\n",
      "01044885 | C\n",
      "15537374 | A1\n",
      "09280987 | A1\n",
      "14752875 | A2\n",
      "19352735 | A1\n",
      "00740276 | A4\n",
      "20452322 | A1\n",
      "20452322 | A1\n",
      "00311820 | A1\n",
      "0001706X | A2\n",
      "20452322 | A1\n",
      "14752875 | A2\n",
      "15671348 | A2\n",
      "14752875 | A2\n",
      "24701343 | A4\n",
      "2214109X | A1\n",
      "15671348 | A2\n",
      "16788060 | A4\n",
      "20008686 | None\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "14752875 | A2\n",
      "19326203 | A1\n",
      "14752875 | A2\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "14752875 | A2\n",
      "14752875 | A2\n",
      "00029637 | A3\n",
      "15671348 | A2\n",
      "00740276 | A4\n",
      "13602276 | A1\n",
      "19326203 | A1\n",
      "14752875 | A2\n",
      "14752875 | A2\n",
      "19326203 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "03051978 | B1\n",
      "13602276 | A1\n",
      "00029637 | A3\n",
      "14752875 | A2\n",
      "14752875 | A2\n",
      "00099104 | A3\n",
      "00029637 | A3\n",
      "09621075 | A1\n",
      "00429007 | A4\n",
      "03009475 | A4\n",
      "01666851 | A4\n",
      "00029637 | A3\n",
      "0264410X | A2\n",
      "16765680 | B2\n",
      "00278424 | A1\n",
      "16784510 | None\n",
      "01419838 | A2\n",
      "03009475 | A4\n",
      "16784510 | None\n",
      "00740276 | A4\n",
      "00278424 | A1\n",
      "00740276 | A4\n",
      "00359203 | A3\n",
      "00740276 | A4\n",
      "18760341 | A3\n",
      "16643224 | A2\n",
      "16643224 | A2\n",
      "20452322 | A1\n",
      "1664302X | A2\n",
      "1664302X | A2\n",
      "16643224 | A2\n",
      "1664302X | A2\n",
      "17563305 | A2\n",
      "16643224 | A2\n",
      "0001706X | A2\n",
      "01615890 | A4\n",
      "16643224 | A2\n",
      "23148861 | A3\n",
      "01615890 | A4\n",
      "19352735 | A1\n",
      "16643224 | A2\n",
      "87567938 | A3\n",
      "19326203 | A1\n",
      "21645515 | A3\n",
      "00223395 | B1\n",
      "19326203 | A1\n",
      "02732289 | A2\n",
      "16643224 | A2\n",
      "19352735 | A1\n",
      "16643224 | A2\n",
      "00144894 | A4\n",
      "01419838 | A2\n",
      "0264410X | A2\n",
      "00364665 | B1\n",
      "20900023 | B1\n",
      "12864579 | A3\n",
      "00378682 | B1\n",
      "0001706X | A2\n",
      "20900023 | B1\n",
      "17560500 | B1\n",
      "00740276 | A4\n",
      "09547894 | A2\n",
      "01419838 | A2\n",
      "00099104 | A3\n",
      "18743188 | None\n",
      "01652478 | A3\n",
      "0001706X | A2\n",
      "00199567 | A2\n",
      "0001706X | A2\n",
      "14154757 | A3\n",
      "01712985 | A2\n",
      "12864579 | A3\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "12864579 | A3\n",
      "00099104 | A3\n",
      "0264410X | A2\n",
      "00740276 | A4\n",
      "01419838 | A2\n",
      "0100879X | A3\n",
      "0264410X | A2\n",
      "24059390 | A3\n",
      "20760817 | A3\n",
      "0001706X | A2\n",
      "19352735 | A1\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "16789849 | B1\n",
      "16784162 | B1\n",
      "19382928 | None\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "24734810 | C\n",
      "0001706X | A2\n",
      "19326203 | A1\n",
      "23144599 | B1\n",
      "0001706X | A2\n",
      "03044017 | A1\n",
      "0264410X | A2\n",
      "16788060 | A4\n",
      "16789849 | B1\n",
      "16765680 | B2\n",
      "16784162 | B1\n",
      "23146133 | A3\n",
      "0001706X | A2\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "00378682 | B1\n",
      "23146133 | A3\n",
      "10811710 | A4\n",
      "00378682 | B1\n",
      "23146133 | A3\n",
      "00359203 | A3\n",
      "00378682 | B1\n",
      "03044017 | A1\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "0102311X | A1\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "09320113 | A1\n",
      "00740276 | A4\n",
      "01675877 | A1\n",
      "0102311X | A1\n",
      "00740276 | A4\n",
      "1519566X | A4\n",
      "0264410X | A2\n",
      "00144894 | A4\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "03044017 | A1\n",
      "00740276 | A4\n",
      "1519566X | A4\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "         | None\n",
      "00378682 | B1\n",
      "03044017 | A1\n",
      "         | None\n",
      "00378682 | B1\n",
      "00222585 | A1\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00222585 | A1\n",
      "0001706X | A2\n",
      "00364665 | B1\n",
      "00378682 | B1\n",
      "00222585 | A1\n",
      "00740276 | A4\n",
      "00364665 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "         | None\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00855626 | A4\n",
      "16788060 | A4\n",
      "00311820 | A1\n",
      "09320113 | A1\n",
      "00664804 | A1\n",
      "16643224 | A2\n",
      "00364665 | B1\n",
      "1664302X | A2\n",
      "09320113 | A1\n",
      "15675769 | A2\n",
      "1664302X | A2\n",
      "16643224 | A2\n",
      "23147156 | A3\n",
      "19326203 | A1\n",
      "16788060 | A4\n",
      "00221759 | B2\n",
      "00740276 | A4\n",
      "00311820 | A1\n",
      "15167704 | B1\n",
      "00311820 | A1\n",
      "00029637 | A3\n",
      "24464732 | None\n",
      "16788060 | A4\n",
      "16643224 | A2\n",
      "16788060 | A4\n",
      "16788060 | A4\n",
      "09320113 | A1\n",
      "19352735 | A1\n",
      "16789849 | B1\n",
      "16789946 | B1\n",
      "17560500 | B1\n",
      "17563305 | A2\n",
      "16879686 | B1\n",
      "00311820 | A1\n",
      "03044017 | A1\n",
      "16789849 | B1\n",
      "16789849 | B1\n",
      "18743919 | A1\n",
      "00311820 | A1\n",
      "03786978 | B4\n",
      "00740276 | A4\n",
      "17563305 | A2\n",
      "16784464 | A1\n",
      "00029637 | A3\n",
      "00364665 | B1\n",
      "00359203 | A3\n",
      "00029637 | A3\n",
      "16788060 | A4\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "15376591 | None\n",
      "19352735 | A1\n",
      "00359203 | A3\n",
      "00359203 | A3\n",
      "16788060 | A4\n",
      "00359203 | A3\n",
      "00359203 | A3\n",
      "00359203 | A3\n",
      "00364665 | B1\n",
      "00740276 | A4\n",
      "16788060 | A4\n",
      "0102311X | A1\n",
      "16788060 | A4\n",
      "22352988 | A2\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "22352988 | A2\n",
      "16643224 | A2\n",
      "19352735 | A1\n",
      "1664302X | A2\n",
      "00664804 | A1\n",
      "19326203 | A1\n",
      "14752875 | A2\n",
      "21507511 | A1\n",
      "20452322 | A1\n",
      "00221899 | A1\n",
      "19326203 | A1\n",
      "15537374 | A1\n",
      "20452322 | A1\n",
      "19326203 | A1\n",
      "16788060 | A4\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "14752875 | A2\n",
      "14752875 | A2\n",
      "00029637 | A3\n",
      "13602276 | A1\n",
      "00345288 | A1\n",
      "0103846X | A2\n",
      "1198743X | A1\n",
      "00144894 | A4\n",
      "0264410X | A2\n",
      "16765680 | B2\n",
      "16765680 | B2\n",
      "00345288 | A1\n",
      "16790359 | A4\n",
      "09311793 | None\n",
      "16790359 | A4\n",
      "03781135 | A1\n",
      "16790359 | A4\n",
      "16790359 | A4\n",
      "16790359 | A4\n",
      "16784561 | A1\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "26913321 | B1\n",
      "19352735 | A1\n",
      "16789849 | B1\n",
      "10584838 | A1\n",
      "09320113 | A1\n",
      "17563305 | A2\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "1664302X | A2\n",
      "10453873 | A3\n",
      "16789849 | B1\n",
      "14712334 | A3\n",
      "16789946 | B1\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "00311820 | A1\n",
      "0001706X | A2\n",
      "20906951 | None\n",
      "16789946 | B1\n",
      "0001706X | A2\n",
      "14787210 | A2\n",
      "19352735 | A1\n",
      "16643224 | A2\n",
      "00311820 | A1\n",
      "0102311X | A1\n",
      "19352735 | A1\n",
      "16789849 | B1\n",
      "12019712 | A2\n",
      "16789849 | B1\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "15167704 | B1\n",
      "19352735 | A1\n",
      "11835702 | B2\n",
      "20446055 | A1\n",
      "00378682 | B1\n",
      "14733099 | A1\n",
      "16788060 | A4\n",
      "13602276 | A1\n",
      "19326203 | A1\n",
      "16789946 | B1\n",
      "01966553 | A3\n",
      "16879686 | B1\n",
      "0001706X | A2\n",
      "13865056 | A2\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "00378682 | B1\n",
      "19326203 | A1\n",
      "0102311X | A1\n",
      "19326203 | A1\n",
      "16788060 | A4\n",
      "19326203 | A1\n",
      "22383182 | B2\n",
      "1198743X | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "00029637 | A3\n",
      "18742793 | B2\n",
      "19352735 | A1\n",
      "15901874 | A3\n",
      "16879635 | A4\n",
      "09564624 | A4\n",
      "14787210 | A2\n",
      "19352727 | A1\n",
      "0103880X | B2\n",
      "0103880X | B2\n",
      "0103880X | B2\n",
      "14138670 | B1\n",
      "00347264 | B2\n",
      "11301406 | B1\n",
      "00029637 | A3\n",
      "00029637 | A3\n",
      "00347264 | B2\n",
      "00740276 | A4\n",
      "22352988 | A2\n",
      "1743422X | A3\n",
      "16643224 | A2\n",
      "2730664X | None\n",
      "00311820 | A1\n",
      "18786146 | A2\n",
      "14310651 | A4\n",
      "0264410X | A2\n",
      "14334909 | None\n",
      "2076393X | A3\n",
      "14334909 | None\n",
      "1414431X | A3\n",
      "15675769 | A2\n",
      "03048608 | B1\n",
      "14761645 | A3\n",
      "19326203 | A1\n",
      "14310651 | A4\n",
      "14622912 | A1\n",
      "16788060 | A4\n",
      "2164554X | A3\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "1743422X | A3\n",
      "01660934 | B2\n",
      "01652478 | A3\n",
      "10806040 | A1\n",
      "03403696 | A2\n",
      "00278424 | A1\n",
      "10434666 | A4\n",
      "15384101 | A3\n",
      "0022538X | A1\n",
      "0966842X | A1\n",
      "00278424 | A1\n",
      "03403696 | A2\n",
      "         | None\n",
      "03403696 | A2\n",
      "10736085 | A4\n",
      "01434004 | A2\n",
      "00013714 | None\n",
      "17563305 | A2\n",
      "22352988 | A2\n",
      "22352988 | A2\n",
      "0264410X | A2\n",
      "09320113 | A1\n",
      "17563305 | A2\n",
      "2076393X | A3\n",
      "18743919 | A1\n",
      "19326203 | A1\n",
      "17563305 | A2\n",
      "00489697 | A1\n",
      "2076393X | A3\n",
      "16643224 | A2\n",
      "0001706X | A2\n",
      "2314436X | A3\n",
      "20452322 | A1\n",
      "14220067 | A2\n",
      "17580463 | A1\n",
      "19326203 | A1\n",
      "09593993 | A3\n",
      "00740276 | A4\n",
      "00144894 | A4\n",
      "21698287 | None\n",
      "21698287 | None\n",
      "14712164 | A2\n",
      "00992240 | A1\n",
      "01666851 | A4\n",
      "14712164 | A2\n",
      "14712164 | A2\n",
      "17596653 | A1\n",
      "20458827 | A3\n",
      "0006291X | A2\n",
      "01666851 | A4\n",
      "00740276 | A4\n",
      "17563305 | A2\n",
      "14712105 | A1\n",
      "19326203 | A1\n",
      "14712164 | A2\n",
      "14752875 | A2\n",
      "19326203 | A1\n",
      "14712164 | A2\n",
      "0093691X | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "00219193 | A3\n",
      "14712164 | A2\n",
      "15303667 | A4\n",
      "00740276 | A4\n",
      "14712164 | A2\n",
      "00740276 | A4\n",
      "03051048 | A1\n",
      "0006291X | A2\n",
      "19821301 | B1\n",
      "0006291X | A2\n",
      "1526954X | B1\n",
      "18722156 | None\n",
      "09320113 | A1\n",
      "00207519 | A1\n",
      "09320113 | A1\n",
      "10614036 | A1\n",
      "15359778 | None\n",
      "00207519 | A1\n",
      "10465928 | B1\n",
      "1744117X | A2\n",
      "00207519 | A1\n",
      "00368075 | A1\n",
      "01666851 | A4\n",
      "03008584 | A3\n",
      "01666851 | A4\n",
      "01694758 | None\n",
      "0001706X | A2\n",
      "00029637 | A3\n",
      "00378682 | B1\n",
      "0001706X | A2\n",
      "15178382 | A2\n",
      "00222585 | A1\n",
      "0269283X | A1\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "27094715 | C\n",
      "2047217X | A1\n",
      "27635368 | C\n",
      "0269283X | A1\n",
      "0364152X | A2\n",
      "19326203 | A1\n",
      "00740276 | A4\n",
      "19352735 | A1\n",
      "8756971X | B1\n",
      "00222585 | A1\n",
      "00222585 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "16877098 | A4\n",
      "00222585 | A1\n",
      "20754450 | A2\n",
      "2412642X | None\n",
      "19808178 | B4\n",
      "16789849 | B1\n",
      "00222585 | A1\n",
      "2373437X | B3\n",
      "16788060 | A4\n",
      "00222585 | A1\n",
      "00222585 | A1\n",
      "00222585 | A1\n",
      "00222585 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "11755334 | A4\n",
      "00222585 | A1\n",
      "00855626 | A4\n",
      "21610983 | B4\n",
      "17563305 | A2\n",
      "23146133 | A3\n",
      "19326203 | A1\n",
      "17563305 | A2\n",
      "19326203 | A1\n",
      "17563305 | A2\n",
      "00378682 | B1\n",
      "17563305 | A2\n",
      "0001706X | A2\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "00855626 | A4\n",
      "0001706X | A2\n",
      "8756971X | B1\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "00740276 | A4\n",
      "16879686 | B1\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "8756971X | B1\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "0001706X | A2\n",
      "00222585 | A1\n",
      "17563305 | A2\n",
      "00364665 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00855626 | A4\n",
      "00222585 | A1\n",
      "1519566X | A4\n",
      "00740276 | A4\n",
      "00222585 | A1\n",
      "00222585 | A1\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "1809127X | B2\n",
      "03735680 | B4\n",
      "00222585 | A1\n",
      "00740276 | A4\n",
      "00222585 | A1\n",
      "1519566X | A4\n",
      "00740276 | A4\n",
      "1519566X | A4\n",
      "17563305 | A2\n",
      "1519566X | A4\n",
      "15303667 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "10811710 | A4\n",
      "00855626 | A4\n",
      "1252607X | A3\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00855626 | A4\n",
      "00740276 | A4\n",
      "00855626 | A4\n",
      "00740276 | A4\n",
      "0102311X | A1\n",
      "00740276 | A4\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "03280381 | None\n",
      "00740276 | A4\n",
      "03018059 | None\n",
      "8756971X | B1\n",
      "00740276 | A4\n",
      "00855626 | A4\n",
      "03018059 | None\n",
      "03018059 | None\n",
      "0102311X | A1\n",
      "00740276 | A4\n",
      "18094481 | A3\n",
      "00378682 | B1\n",
      "20760817 | A3\n",
      "16788060 | A4\n",
      "16788060 | A4\n",
      "0001706X | A2\n",
      "16789849 | B1\n",
      "09621075 | A1\n",
      "00221910 | A1\n",
      "14712156 | A2\n",
      "19352735 | A1\n",
      "2358291X | B1\n",
      "23583088 | B2\n",
      "74419021 | None\n",
      "16789849 | B1\n",
      "17563305 | A2\n",
      "19808178 | B4\n",
      "1809127X | B2\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "00166707 | A3\n",
      "00740276 | A4\n",
      "19352735 | A1\n",
      "00222585 | A1\n",
      "00740276 | A4\n",
      "16784561 | A1\n",
      "16788060 | A4\n",
      "19352735 | A1\n",
      "19808178 | B4\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "00378682 | B1\n",
      "16789849 | B1\n",
      "16789849 | B1\n",
      "16789849 | B1\n",
      "16788060 | A4\n",
      "17563305 | A2\n",
      "0001706X | A2\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "19352735 | A1\n",
      "16788060 | A4\n",
      "19808178 | B4\n",
      "17563305 | A2\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "17563305 | A2\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "23182598 | B4\n",
      "00740276 | A4\n",
      "15303667 | A4\n",
      "0269283X | A1\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "15671348 | A2\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "15671348 | A2\n",
      "15671348 | A2\n",
      "0001706X | A2\n",
      "15671348 | A2\n",
      "15671348 | A2\n",
      "00740276 | A4\n",
      "00221910 | A1\n",
      "00378682 | B1\n",
      "09493026 | C\n",
      "00740276 | A4\n",
      "03005771 | A1\n",
      "00740276 | A4\n",
      "15671348 | A2\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "03050270 | A1\n",
      "11755326 | A4\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00980331 | A2\n",
      "0001706X | A2\n",
      "14714922 | A1\n",
      "03076962 | A3\n",
      "0001706X | A2\n",
      "10557903 | A1\n",
      "00378682 | B1\n",
      "00013765 | A2\n",
      "00740276 | A4\n",
      "0001706X | A2\n",
      "00222585 | A1\n",
      "0001706X | A2\n",
      "         | None\n",
      "00222585 | A1\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "03076962 | A3\n",
      "00740276 | A4\n",
      "00062928 | A3\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "         | None\n",
      "00029637 | A3\n",
      "00378682 | B1\n",
      "10204989 | A3\n",
      "00740276 | A4\n",
      "00222585 | A1\n",
      "0102311X | A1\n",
      "0102311X | A1\n",
      "0102311X | A1\n",
      "0102311X | A1\n",
      "0102311X | A1\n",
      "0102311X | A1\n",
      "00222585 | A1\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00222585 | A1\n",
      "00740276 | A4\n",
      "10204989 | A3\n",
      "         | None\n",
      "00740276 | A4\n",
      "0001706X | A2\n",
      "00378682 | B1\n",
      "03279286 | C\n",
      "05912385 | A1\n",
      "         | None\n",
      "         | None\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00854638 | None\n",
      "0102311X | A1\n",
      "00364665 | B1\n",
      "00378682 | B1\n",
      "00364665 | B1\n",
      "         | None\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "20590105 | A1\n",
      "15216616 | A3\n",
      "16643224 | A2\n",
      "20452322 | A1\n",
      "10434666 | A4\n",
      "05315565 | A3\n",
      "16643224 | A2\n",
      "16643224 | A2\n",
      "20411723 | A1\n",
      "27546993 | None\n",
      "00029637 | A3\n",
      "14795876 | A2\n",
      "16643224 | A2\n",
      "22111247 | A1\n",
      "1664302X | A2\n",
      "21507511 | A1\n",
      "15216616 | A3\n",
      "21507511 | A1\n",
      "16643224 | A2\n",
      "19352735 | A1\n",
      "00221767 | A1\n",
      "16788060 | A4\n",
      "07415400 | A2\n",
      "00192805 | A1\n",
      "20452322 | A1\n",
      "16643224 | A2\n",
      "10806040 | A1\n",
      "20452322 | A1\n",
      "07415400 | A2\n",
      "14786362 | A2\n",
      "10806059 | A1\n",
      "13695274 | A1\n",
      "16643224 | A2\n",
      "01052896 | A1\n",
      "20452322 | A1\n",
      "1743422X | A3\n",
      "10985514 | A1\n",
      "19352735 | A1\n",
      "00221899 | A1\n",
      "16788060 | A4\n",
      "10788956 | A1\n",
      "07533322 | A2\n",
      "19330219 | A1\n",
      "1743422X | A3\n",
      "01663542 | A2\n",
      "01712985 | A2\n",
      "15537374 | A1\n",
      "00029637 | A3\n",
      "00221899 | A1\n",
      "21645515 | A3\n",
      "00221759 | B2\n",
      "22111247 | A1\n",
      "00221899 | A1\n",
      "23148861 | A3\n",
      "15537374 | A1\n",
      "00221759 | B2\n",
      "15537374 | A1\n",
      "03407004 | A1\n",
      "0264410X | A2\n",
      "16762444 | B3\n",
      "16762444 | B3\n",
      "19326203 | A1\n",
      "00221767 | A1\n",
      "00064971 | A1\n",
      "00221899 | A1\n",
      "00199567 | A2\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "00099104 | A3\n",
      "00219738 | A1\n",
      "00064971 | A1\n",
      "00064971 | A1\n",
      "00199567 | A2\n",
      "00029440 | A1\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "15675769 | A2\n",
      "12864579 | A3\n",
      "00199567 | A2\n",
      "01652478 | A3\n",
      "14714922 | A1\n",
      "01652478 | A3\n",
      "01655728 | A2\n",
      "00099104 | A3\n",
      "17417007 | A1\n",
      "23752548 | A1\n",
      "17456215 | B1\n",
      "09651748 | A1\n",
      "20760817 | A3\n",
      "14733099 | A1\n",
      "17563305 | A2\n",
      "20760817 | A3\n",
      "19994915 | A2\n",
      "20452322 | A1\n",
      "14714922 | A1\n",
      "19352735 | A1\n",
      "1664302X | A2\n",
      "1664302X | A2\n",
      "20452322 | A1\n",
      "19352735 | A1\n",
      "22145745 | A1\n",
      "2296858X | A1\n",
      "19994915 | A2\n",
      "19352735 | A1\n",
      "25724754 | A4\n",
      "17563305 | A2\n",
      "19352735 | A1\n",
      "00928674 | A1\n",
      "25724754 | A4\n",
      "19352735 | A1\n",
      "17563305 | A2\n",
      "20754450 | A2\n",
      "20461402 | A3\n",
      "09651748 | A1\n",
      "20452322 | A1\n",
      "17563305 | A2\n",
      "20585276 | A1\n",
      "14712164 | A2\n",
      "1519566X | A4\n",
      "17563305 | A2\n",
      "09651748 | A1\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "20452322 | A1\n",
      "19313128 | A1\n",
      "17563305 | A2\n",
      "20452322 | A1\n",
      "23112638 | A4\n",
      "17460913 | A3\n",
      "02659247 | A1\n",
      "19352735 | A1\n",
      "00074853 | A2\n",
      "19352735 | A1\n",
      "14714922 | A1\n",
      "19326203 | A1\n",
      "17563305 | A2\n",
      "19326203 | A1\n",
      "0001706X | A2\n",
      "01660934 | B2\n",
      "00740276 | A4\n",
      "0145305X | A1\n",
      "09621075 | A1\n",
      "17563305 | A2\n",
      "14752875 | A2\n",
      "00278424 | A1\n",
      "00280836 | A1\n",
      "00740276 | A4\n",
      "09628819 | A2\n",
      "00740276 | A4\n",
      "14752875 | A2\n",
      "19352735 | A1\n",
      "0034737X | B1\n",
      "19352735 | A1\n",
      "00928674 | A1\n",
      "09621075 | A1\n",
      "0001706X | A2\n",
      "00144894 | A4\n",
      "00740276 | A4\n",
      "01018515 | None\n",
      "09621075 | A1\n",
      "00166731 | A2\n",
      "00740276 | A4\n",
      "1519566X | A4\n",
      "09651748 | A1\n",
      "00280836 | A1\n",
      "00219258 | A1\n",
      "00207519 | A1\n",
      "00278424 | A1\n",
      "00142336 | A2\n",
      "03018059 | None\n",
      "         | None\n",
      "03018059 | None\n",
      "01018175 | B1\n",
      "00855626 | A4\n",
      "22352988 | A2\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "1664302X | A2\n",
      "22352988 | A2\n",
      "22352988 | A2\n",
      "16643224 | A2\n",
      "19352735 | A1\n",
      "22352988 | A2\n",
      "00664804 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "22111247 | A1\n",
      "14752875 | A2\n",
      "21507511 | A1\n",
      "20499957 | A2\n",
      "20452322 | A1\n",
      "00221899 | A1\n",
      "19326203 | A1\n",
      "15671348 | A2\n",
      "14752875 | A2\n",
      "20452322 | A1\n",
      "19326203 | A1\n",
      "15671348 | A2\n",
      "21507511 | A1\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "14752875 | A2\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "19722680 | B2\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "00029637 | A3\n",
      "13602276 | A1\n",
      "0001706X | A2\n",
      "14752875 | A2\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "16788060 | A4\n",
      "00029637 | A3\n",
      "13602276 | A1\n",
      "14752875 | A2\n",
      "14752875 | A2\n",
      "14752875 | A2\n",
      "00029637 | A3\n",
      "00099104 | A3\n",
      "03009475 | A4\n",
      "00429007 | A4\n",
      "01677012 | B1\n",
      "01666851 | A4\n",
      "00029637 | A3\n",
      "00029637 | A3\n",
      "09320113 | A1\n",
      "00221007 | A1\n",
      "00207519 | A1\n",
      "0960894X | A2\n",
      "10788956 | A1\n",
      "00029637 | A3\n",
      "10916490 | A1\n",
      "00221759 | B2\n",
      "00029637 | A3\n",
      "01419838 | A2\n",
      "00320943 | A1\n",
      "01018515 | None\n",
      "08880786 | None\n",
      "01018515 | None\n",
      "0100879X | A3\n",
      "01018515 | None\n",
      "0100879X | A3\n",
      "00740276 | A4\n",
      "0100879X | A3\n",
      "16788060 | A4\n",
      "21650497 | A1\n",
      "22535969 | C\n",
      "17564646 | A1\n",
      "10584838 | A1\n",
      "15216616 | A3\n",
      "1664302X | A2\n",
      "16643224 | A2\n",
      "18069460 | A4\n",
      "0066782X | B2\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "22840265 | B2\n",
      "22352988 | A2\n",
      "00221317 | A2\n",
      "00740276 | A4\n",
      "19994915 | A2\n",
      "00221759 | B2\n",
      "16789849 | B1\n",
      "16878469 | B3\n",
      "00311820 | A1\n",
      "19352735 | A1\n",
      "1936878X | A1\n",
      "19352735 | A1\n",
      "2296858X | A1\n",
      "15675769 | A2\n",
      "01419838 | A2\n",
      "00221759 | B2\n",
      "22361960 | C\n",
      "00029149 | A2\n",
      "09320113 | A1\n",
      "16789849 | B1\n",
      "00221759 | B2\n",
      "22352988 | A2\n",
      "19352735 | A1\n",
      "14767058 | A4\n",
      "03008916 | B2\n",
      "1354750X | B1\n",
      "14712334 | A3\n",
      "19326203 | A1\n",
      "00740276 | A4\n",
      "1414431X | A3\n",
      "15254135 | A2\n",
      "01615890 | A4\n",
      "20050380 | A2\n",
      "09337407 | A1\n",
      "14138670 | B1\n",
      "19326203 | A1\n",
      "18075932 | A4\n",
      "00221317 | A2\n",
      "14138670 | B1\n",
      "00378682 | B1\n",
      "00359203 | A3\n",
      "0001706X | A2\n",
      "09651748 | A1\n",
      "25890042 | A2\n",
      "00221910 | A1\n",
      "17596653 | A1\n",
      "1664302X | A2\n",
      "00311820 | A1\n",
      "19352735 | A1\n",
      "14712164 | A2\n",
      "00221910 | A1\n",
      "22145745 | A1\n",
      "0001706X | A2\n",
      "00980331 | A2\n",
      "19352735 | A1\n",
      "22145745 | A1\n",
      "0001706X | A2\n",
      "17563305 | A2\n",
      "20452322 | A1\n",
      "19352735 | A1\n",
      "00311820 | A1\n",
      "00311820 | A1\n",
      "00221910 | A1\n",
      "00221910 | A1\n",
      "19352735 | A1\n",
      "17563305 | A2\n",
      "09651748 | A1\n",
      "2296701X | A3\n",
      "00221910 | A1\n",
      "00278424 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "01035053 | A2\n",
      "0269283X | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "0269283X | A1\n",
      "00029637 | A3\n",
      "00332615 | B1\n",
      "15671348 | A2\n",
      "01633864 | A1\n",
      "17563305 | A2\n",
      "1519566X | A4\n",
      "15671348 | A2\n",
      "00980331 | A2\n",
      "00222011 | A2\n",
      "15237060 | A1\n",
      "00222585 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00980331 | A2\n",
      "00980331 | A2\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "00980331 | A2\n",
      "03076962 | A3\n",
      "00740276 | A4\n",
      "03076962 | A3\n",
      "0001706X | A2\n",
      "00222585 | A1\n",
      "03076962 | A3\n",
      "00740276 | A4\n",
      "10204989 | A3\n",
      "0102311X | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "00378682 | B1\n",
      "00221910 | A1\n",
      "00144754 | None\n",
      "20590105 | A1\n",
      "16788060 | A4\n",
      "20452322 | A1\n",
      "10434666 | A4\n",
      "0001706X | A2\n",
      "16643224 | A2\n",
      "16643224 | A2\n",
      "16643224 | A2\n",
      "10958355 | B2\n",
      "16643224 | A2\n",
      "13892010 | A3\n",
      "01418130 | A1\n",
      "25272179 | B2\n",
      "01712985 | A2\n",
      "00144894 | A4\n",
      "03034569 | B2\n",
      "03784320 | A2\n",
      "15178382 | A2\n",
      "10900233 | A1\n",
      "03044017 | A1\n",
      "16643224 | A2\n",
      "00144894 | A4\n",
      "09366768 | A3\n",
      "16799216 | B2\n",
      "08937648 | A1\n",
      "19842961 | A2\n",
      "00320943 | A1\n",
      "19352735 | A1\n",
      "16785150 | A4\n",
      "08824010 | A3\n",
      "01432044 | B1\n",
      "01652478 | A3\n",
      "01663542 | A2\n",
      "23182598 | B4\n",
      "03044017 | A1\n",
      "19326203 | A1\n",
      "00223956 | A2\n",
      "00740276 | A4\n",
      "16765680 | B2\n",
      "00740276 | A4\n",
      "22112839 | None\n",
      "17466148 | A2\n",
      "19326203 | A1\n",
      "0264410X | A2\n",
      "17466148 | A2\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "03044017 | A1\n",
      "22381589 | C\n",
      "15721000 | A2\n",
      "09209964 | A1\n",
      "22313184 | B3\n",
      "17466148 | A2\n",
      "0264410X | A2\n",
      "16765680 | B2\n",
      "03044017 | A1\n",
      "14765810 | A1\n",
      "03781135 | A1\n",
      "01652478 | A3\n",
      "15566811 | A3\n",
      "23182598 | B4\n",
      "01676806 | A2\n",
      "15353702 | A3\n",
      "01652427 | A1\n",
      "10902449 | A4\n",
      "0001706X | A2\n",
      "13510002 | A3\n",
      "14712407 | A3\n",
      "19352735 | A1\n",
      "0264410X | A2\n",
      "01652427 | A1\n",
      "03009475 | A4\n",
      "00221759 | B2\n",
      "0264410X | A2\n",
      "01652427 | A1\n",
      "00223395 | B1\n",
      "00207519 | A1\n",
      "00740276 | A4\n",
      "00222011 | A2\n",
      "14602091 | A1\n",
      "00740276 | A4\n",
      "27686701 | None\n",
      "10991654 | None\n",
      "08958696 | B1\n",
      "1743422X | A3\n",
      "14138670 | B1\n",
      "09167250 | A2\n",
      "1743422X | A3\n",
      "26886146 | None\n",
      "22352988 | A2\n",
      "03043940 | B1\n",
      "23275081 | B4\n",
      "01681591 | A1\n",
      "1743422X | A3\n",
      "15571890 | A1\n",
      "19352735 | A1\n",
      "03619230 | A3\n",
      "17563305 | A2\n",
      "13835769 | A3\n",
      "20013078 | A1\n",
      "17422094 | A1\n",
      "19326203 | A1\n",
      "14712202 | A4\n",
      "17563305 | A2\n",
      "13608185 | A1\n",
      "19326203 | A1\n",
      "1743422X | A3\n",
      "01664328 | A2\n",
      "19326203 | A1\n",
      "17422094 | A1\n",
      "19326203 | A1\n",
      "0004282X | B2\n",
      "12864579 | A3\n",
      "03043940 | B1\n",
      "00029440 | A1\n",
      "00778923 | A1\n",
      "03043940 | B1\n",
      "00221767 | A1\n",
      "00951137 | A1\n",
      "00029440 | A1\n",
      "00199567 | A2\n",
      "00221767 | A1\n",
      "09629351 | A3\n",
      "01052896 | A1\n",
      "00221767 | A1\n",
      "00219258 | A1\n",
      "12864579 | A3\n",
      "00221767 | A1\n",
      "07328893 | A4\n",
      "10656995 | B2\n",
      "03048608 | B1\n",
      "00013714 | None\n",
      "00426822 | A2\n",
      "28132424 | None\n",
      "1664302X | A2\n",
      "03781119 | A4\n",
      "16643224 | A2\n",
      "23738227 | A1\n",
      "22352988 | A2\n",
      "16643224 | A2\n",
      "1664302X | A2\n",
      "1664302X | A2\n",
      "23318325 | B3\n",
      "22111247 | A1\n",
      "16643224 | A2\n",
      "24701343 | A4\n",
      "1664302X | A2\n",
      "19352735 | A1\n",
      "17563305 | A2\n",
      "19994915 | A2\n",
      "16643224 | A2\n",
      "01615890 | A4\n",
      "16648021 | A4\n",
      "23148861 | A3\n",
      "17563305 | A2\n",
      "0145305X | A1\n",
      "20452322 | A1\n",
      "19352735 | A1\n",
      "00144894 | A4\n",
      "15671348 | A2\n",
      "01663542 | A2\n",
      "17563305 | A2\n",
      "21559597 | C\n",
      "14714922 | A1\n",
      "19352735 | A1\n",
      "19458924 | A1\n",
      "00740276 | A4\n",
      "15537374 | A1\n",
      "19326203 | A1\n",
      "20900023 | B1\n",
      "01419838 | A2\n",
      "15178382 | A2\n",
      "15178382 | A2\n",
      "1516635X | B1\n",
      "15216543 | A4\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "00311820 | A1\n",
      "00144894 | A4\n",
      "16765680 | B2\n",
      "19994915 | A2\n",
      "19352735 | A1\n",
      "0302766X | A1\n",
      "20462441 | A2\n",
      "14752875 | A2\n",
      "19994915 | A2\n",
      "16789849 | B1\n",
      "00222585 | A1\n",
      "17563305 | A2\n",
      "0001706X | A2\n",
      "20462441 | A2\n",
      "00221899 | A1\n",
      "19352735 | A1\n",
      "0001706X | A2\n",
      "00222585 | A1\n",
      "00221899 | A1\n",
      "17563305 | A2\n",
      "00222585 | A1\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "00222585 | A1\n",
      "24099279 | B1\n",
      "00222585 | A1\n",
      "19326203 | A1\n",
      "16788060 | A4\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "17563305 | A2\n",
      "00221899 | A1\n",
      "00222585 | A1\n",
      "14625814 | A2\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "16788060 | A4\n",
      "17563305 | A2\n",
      "19352735 | A1\n",
      "14752875 | A2\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "17563305 | A2\n",
      "03044017 | A1\n",
      "14752875 | A2\n",
      "17563305 | A2\n",
      "00029637 | A3\n",
      "00207519 | A1\n",
      "19352735 | A1\n",
      "19352727 | A1\n",
      "17563305 | A2\n",
      "19326203 | A1\n",
      "11107243 | None\n",
      "19326203 | A1\n",
      "14625814 | A2\n",
      "00368075 | A1\n",
      "15537366 | A1\n",
      "13835769 | A3\n",
      "13653083 | A4\n",
      "00348910 | A1\n",
      "00222585 | A1\n",
      "00278424 | A1\n",
      "00368075 | A1\n",
      "0971135X | None\n",
      "0102311X | A1\n",
      "00222585 | A1\n",
      "         | None\n",
      "0001706X | A2\n",
      "15192563 | C\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "07984545 | B3\n",
      "00740276 | A4\n",
      "16643224 | A2\n",
      "08915849 | A1\n",
      "20590105 | A1\n",
      "20799721 | C\n",
      "20452322 | A1\n",
      "16788060 | A4\n",
      "21650497 | A1\n",
      "10434666 | A4\n",
      "00257974 | A3\n",
      "13892010 | A3\n",
      "15216616 | A3\n",
      "2076393X | A3\n",
      "15178382 | A2\n",
      "10584838 | A1\n",
      "22962360 | A2\n",
      "10434666 | A4\n",
      "16643224 | A2\n",
      "0264410X | A2\n",
      "16643224 | A2\n",
      "10434666 | A4\n",
      "27727076 | None\n",
      "25890042 | A2\n",
      "16643224 | A2\n",
      "13648535 | A1\n",
      "20452322 | A1\n",
      "2076393X | A3\n",
      "16643224 | A2\n",
      "16643224 | A2\n",
      "03650596 | B2\n",
      "16789849 | B1\n",
      "24054577 | B1\n",
      "19326203 | A1\n",
      "07533322 | A2\n",
      "23288957 | A3\n",
      "07533322 | A2\n",
      "10647481 | A1\n",
      "05315565 | A3\n",
      "16643224 | A2\n",
      "0264410X | A2\n",
      "16643224 | A2\n",
      "10434666 | A4\n",
      "19994915 | A2\n",
      "10799796 | A4\n",
      "16643224 | A2\n",
      "01712985 | A2\n",
      "16643224 | A2\n",
      "2076393X | A3\n",
      "16643224 | A2\n",
      "00257974 | A3\n",
      "14795876 | A2\n",
      "16643224 | A2\n",
      "27546993 | None\n",
      "27546993 | None\n",
      "07533322 | A2\n",
      "15365964 | A3\n",
      "16643224 | A2\n",
      "16643224 | A2\n",
      "00951137 | A1\n",
      "2673818X | None\n",
      "16643224 | A2\n",
      "00221759 | B2\n",
      "01663542 | A2\n",
      "22964185 | A3\n",
      "05315565 | A3\n",
      "01418130 | A1\n",
      "2234943X | A4\n",
      "20452322 | A1\n",
      "01652478 | A3\n",
      "22352988 | A2\n",
      "0001706X | A2\n",
      "07533322 | A2\n",
      "01712985 | A2\n",
      "21507511 | A1\n",
      "19290748 | C\n",
      "16878469 | B3\n",
      "14138123 | A1\n",
      "2234943X | A4\n",
      "15216616 | A3\n",
      "0264410X | A2\n",
      "05315565 | A3\n",
      "16643224 | A2\n",
      "0264410X | A2\n",
      "05315565 | A3\n",
      "23148861 | A3\n",
      "19994915 | A2\n",
      "16788060 | A4\n",
      "00221759 | B2\n",
      "16639812 | A2\n",
      "01419838 | A2\n",
      "00664804 | A1\n",
      "07415400 | A2\n",
      "07415400 | A2\n",
      "16788060 | A4\n",
      "00311820 | A1\n",
      "20452322 | A1\n",
      "16643224 | A2\n",
      "20499957 | A2\n",
      "16643224 | A2\n",
      "00144894 | A4\n",
      "20452322 | A1\n",
      "2296858X | A1\n",
      "10434666 | A4\n",
      "03034569 | B2\n",
      "20452322 | A1\n",
      "25311379 | B3\n",
      "14712105 | A1\n",
      "22964185 | A3\n",
      "20499957 | A2\n",
      "15178382 | A2\n",
      "10900233 | A1\n",
      "10623264 | A2\n",
      "14786362 | A2\n",
      "16784464 | A1\n",
      "07415400 | A2\n",
      "16643224 | A2\n",
      "10806040 | A1\n",
      "03044017 | A1\n",
      "0264410X | A2\n",
      "16643224 | A2\n",
      "01757598 | A2\n",
      "15188787 | A1\n",
      "00493848 | A3\n",
      "03044017 | A1\n",
      "16643224 | A2\n",
      "0264410X | A2\n",
      "16643224 | A2\n",
      "08039488 | B1\n",
      "14712334 | A3\n",
      "20452322 | A1\n",
      "2076393X | A3\n",
      "22352988 | A2\n",
      "19805497 | A3\n",
      "00221759 | B2\n",
      "00144894 | A4\n",
      "00664804 | A1\n",
      "16643224 | A2\n",
      "19352735 | A1\n",
      "16799216 | B2\n",
      "1743422X | A3\n",
      "16788060 | A4\n",
      "20452322 | A1\n",
      "19842961 | A2\n",
      "19352735 | A1\n",
      "01419838 | A2\n",
      "1664302X | A2\n",
      "08824010 | A3\n",
      "19326203 | A1\n",
      "0264410X | A2\n",
      "22107401 | A4\n",
      "21645515 | A3\n",
      "17424933 | A3\n",
      "01663542 | A2\n",
      "00221759 | B2\n",
      "19352735 | A1\n",
      "10434666 | A4\n",
      "19352735 | A1\n",
      "01652478 | A3\n",
      "00221759 | B2\n",
      "01663542 | A2\n",
      "01674943 | A1\n",
      "01712985 | A2\n",
      "15537374 | A1\n",
      "08913668 | A2\n",
      "05315565 | A3\n",
      "00144894 | A4\n",
      "02780240 | A3\n",
      "13205358 | B1\n",
      "00221759 | B2\n",
      "00221759 | B2\n",
      "00088749 | A3\n",
      "05315565 | A3\n",
      "14760584 | A1\n",
      "09366768 | A3\n",
      "1806938X | B2\n",
      "00223956 | A2\n",
      "00098981 | A3\n",
      "14712180 | A2\n",
      "19352735 | A1\n",
      "00221899 | A1\n",
      "03044017 | A1\n",
      "19326203 | A1\n",
      "17563305 | A2\n",
      "14752875 | A2\n",
      "0001706X | A2\n",
      "00221899 | A1\n",
      "18787649 | B1\n",
      "00311820 | A1\n",
      "10434666 | A4\n",
      "00740276 | A4\n",
      "17563305 | A2\n",
      "00740276 | A4\n",
      "01712985 | A2\n",
      "16765680 | B2\n",
      "00063363 | A1\n",
      "19352735 | A1\n",
      "03603997 | A3\n",
      "03009475 | A4\n",
      "19326203 | A1\n",
      "01456008 | A2\n",
      "17466148 | A2\n",
      "03044017 | A1\n",
      "19352735 | A1\n",
      "14712334 | A3\n",
      "01434004 | A2\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "21645515 | A3\n",
      "17466148 | A2\n",
      "1354750X | B1\n",
      "23146133 | A3\n",
      "23582898 | A4\n",
      "19326203 | A1\n",
      "10623264 | A2\n",
      "20521839 | B1\n",
      "17510147 | A2\n",
      "22112839 | None\n",
      "23265191 | None\n",
      "20013078 | A1\n",
      "19183011 | None\n",
      "00221759 | B2\n",
      "22381589 | C\n",
      "00088749 | A3\n",
      "01615890 | A4\n",
      "16765680 | B2\n",
      "17563305 | A2\n",
      "23148861 | A3\n",
      "19326203 | A1\n",
      "0100879X | A3\n",
      "17466148 | A2\n",
      "14795876 | A2\n",
      "09209964 | A1\n",
      "00098981 | A3\n",
      "00221759 | B2\n",
      "14712334 | A3\n",
      "01988859 | B1\n",
      "09629351 | A3\n",
      "0264410X | A2\n",
      "00084166 | B1\n",
      "03044017 | A1\n",
      "16762444 | B3\n",
      "17424690 | A1\n",
      "0264410X | A2\n",
      "22313184 | B3\n",
      "20900430 | None\n",
      "09575235 | B1\n",
      "00221759 | B2\n",
      "15254135 | A2\n",
      "16762444 | B3\n",
      "00098981 | A3\n",
      "09502688 | A4\n",
      "0001706X | A2\n",
      "00221759 | B2\n",
      "01434004 | A2\n",
      "07328893 | A4\n",
      "19458924 | A1\n",
      "13518216 | A3\n",
      "10434666 | A4\n",
      "14783223 | A2\n",
      "01466615 | B1\n",
      "10434666 | A4\n",
      "07328893 | A4\n",
      "01988859 | B1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "01652478 | A3\n",
      "01652478 | A3\n",
      "01652427 | A1\n",
      "03044017 | A1\n",
      "03781135 | A1\n",
      "03044017 | A1\n",
      "01434004 | A2\n",
      "15566811 | A3\n",
      "14765810 | A1\n",
      "20900023 | B1\n",
      "23182598 | B4\n",
      "16765680 | B2\n",
      "23182598 | B4\n",
      "00378682 | B1\n",
      "01712985 | A2\n",
      "19326203 | A1\n",
      "00740276 | A4\n",
      "14712334 | A3\n",
      "19485956 | B2\n",
      "15566811 | A3\n",
      "07179553 | B1\n",
      "09725229 | B1\n",
      "07328893 | A4\n",
      "00365548 | A4\n",
      "15457885 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "00221759 | B2\n",
      "03009475 | A4\n",
      "15353702 | A3\n",
      "00098981 | A3\n",
      "1414431X | A3\n",
      "13866532 | A3\n",
      "03009475 | A4\n",
      "10799907 | B2\n",
      "08839441 | A3\n",
      "01652427 | A1\n",
      "01676806 | A2\n",
      "00221759 | B2\n",
      "03009475 | A4\n",
      "10900233 | A1\n",
      "19326203 | A1\n",
      "00144894 | A4\n",
      "00221759 | B2\n",
      "19352735 | A1\n",
      "03906078 | A1\n",
      "00411345 | B3\n",
      "0001706X | A2\n",
      "15168484 | B3\n",
      "00221899 | A1\n",
      "01434004 | A2\n",
      "00243205 | A1\n",
      "16782674 | A4\n",
      "1414431X | A3\n",
      "01479563 | A3\n",
      "01479563 | A3\n",
      "03784320 | A2\n",
      "15566811 | A3\n",
      "16879635 | A4\n",
      "01660934 | B2\n",
      "08839441 | A3\n",
      "14714922 | A1\n",
      "14712407 | A3\n",
      "00221759 | B2\n",
      "0100879X | A3\n",
      "13510002 | A3\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00099104 | A3\n",
      "16784510 | None\n",
      "03005526 | B2\n",
      "00378682 | B1\n",
      "0264410X | A2\n",
      "19352735 | A1\n",
      "00378682 | B1\n",
      "03044017 | A1\n",
      "01652427 | A1\n",
      "01652427 | A1\n",
      "09320113 | A1\n",
      "00311820 | A1\n",
      "0264410X | A2\n",
      "01466615 | B1\n",
      "00221759 | B2\n",
      "01660934 | B2\n",
      "01466615 | B1\n",
      "00740276 | A4\n",
      "00221759 | B2\n",
      "00378682 | B1\n",
      "19821263 | C\n",
      "19352735 | A1\n",
      "03009475 | A4\n",
      "00740276 | A4\n",
      "03044017 | A1\n",
      "17521947 | A4\n",
      "01616420 | A1\n",
      "01777726 | B4\n",
      "00378682 | B1\n",
      "0264410X | A2\n",
      "01652427 | A1\n",
      "00345288 | A1\n",
      "0264410X | A2\n",
      "00345288 | A1\n",
      "12864579 | A3\n",
      "0001706X | A2\n",
      "0264410X | A2\n",
      "00144894 | A4\n",
      "03009475 | A4\n",
      "0264410X | A2\n",
      "03057453 | A1\n",
      "00359203 | A3\n",
      "03009475 | A4\n",
      "00740276 | A4\n",
      "03009475 | A4\n",
      "00221759 | B2\n",
      "0100879X | A3\n",
      "00144894 | A4\n",
      "1568010X | None\n",
      "09724567 | B4\n",
      "10217401 | B2\n",
      "00378682 | B1\n",
      "0001706X | A2\n",
      "00378682 | B1\n",
      "00221759 | B2\n",
      "10939946 | A3\n",
      "00493848 | A3\n",
      "10939946 | A3\n",
      "0001706X | A2\n",
      "00378682 | B1\n",
      "15566811 | A3\n",
      "03009475 | A4\n",
      "00099104 | A3\n",
      "01652427 | A1\n",
      "0264410X | A2\n",
      "00099104 | A3\n",
      "1198743X | A1\n",
      "09320113 | A1\n",
      "00664804 | A1\n",
      "0264410X | A2\n",
      "00345288 | A1\n",
      "0100879X | A3\n",
      "13602276 | A1\n",
      "01652427 | A1\n",
      "00192805 | A1\n",
      "03044017 | A1\n",
      "00364665 | B1\n",
      "00199567 | A2\n",
      "09593993 | A3\n",
      "00099104 | A3\n",
      "03855600 | B2\n",
      "00099104 | A3\n",
      "01020935 | B1\n",
      "00219975 | A3\n",
      "00740276 | A4\n",
      "00099104 | A3\n",
      "00099104 | A3\n",
      "00099104 | A3\n",
      "00029394 | A1\n",
      "03009475 | A4\n",
      "00740276 | A4\n",
      "15051773 | A4\n",
      "01652427 | A1\n",
      "09284249 | None\n",
      "03009475 | A4\n",
      "03009475 | A4\n",
      "03009475 | A4\n",
      "01020935 | B1\n",
      "09629351 | A3\n",
      "09232508 | A3\n",
      "03044017 | A1\n",
      "1071412X | None\n",
      "1071412X | None\n",
      "00664804 | A1\n",
      "15675769 | A2\n",
      "08895406 | A1\n",
      "00199567 | A2\n",
      "01452126 | B1\n",
      "00144827 | A4\n",
      "03009475 | A4\n",
      "09724567 | B4\n",
      "15168484 | B3\n",
      "00378682 | B1\n",
      "03009475 | A4\n",
      "03009475 | A4\n",
      "1071412X | None\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "0001706X | A2\n",
      "01632116 | A4\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "0370369X | C\n",
      "00378682 | B1\n",
      "1071412X | None\n",
      "0001706X | A2\n",
      "00664804 | A1\n",
      "03057453 | A1\n",
      "01419838 | A2\n",
      "01632116 | A4\n",
      "00740276 | A4\n",
      "0100879X | A3\n",
      "00740276 | A4\n",
      "01419838 | A2\n",
      "03009475 | A4\n",
      "03009475 | A4\n",
      "1071412X | None\n",
      "09538178 | A2\n",
      "00740276 | A4\n",
      "19994915 | A2\n",
      "22352988 | A2\n",
      "20462441 | A2\n",
      "0302766X | A1\n",
      "14752875 | A2\n",
      "07394462 | A3\n",
      "00222585 | A1\n",
      "17563305 | A2\n",
      "19994915 | A2\n",
      "16789849 | B1\n",
      "26737515 | None\n",
      "0001706X | A2\n",
      "00221899 | A1\n",
      "22352988 | A2\n",
      "20462441 | A2\n",
      "19352735 | A1\n",
      "0001706X | A2\n",
      "16789849 | B1\n",
      "00221899 | A1\n",
      "17563305 | A2\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "00222585 | A1\n",
      "00222585 | A1\n",
      "00222585 | A1\n",
      "00410101 | A4\n",
      "00222585 | A1\n",
      "19326203 | A1\n",
      "00222585 | A1\n",
      "16789849 | B1\n",
      "16789849 | B1\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "00221899 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "0100736X | A4\n",
      "00222585 | A1\n",
      "19352735 | A1\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "16788060 | A4\n",
      "17563305 | A2\n",
      "14752875 | A2\n",
      "14752875 | A2\n",
      "0001706X | A2\n",
      "19326203 | A1\n",
      "00740276 | A4\n",
      "00664804 | A1\n",
      "14678039 | A2\n",
      "00740276 | A4\n",
      "17563305 | A2\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "14752875 | A2\n",
      "15537374 | A1\n",
      "03044017 | A1\n",
      "12864579 | A3\n",
      "17563305 | A2\n",
      "14773155 | A1\n",
      "00029637 | A3\n",
      "19326203 | A1\n",
      "10811710 | A4\n",
      "00222615 | A4\n",
      "00207519 | A1\n",
      "14384221 | A3\n",
      "00410101 | A4\n",
      "00408166 | A4\n",
      "00740276 | A4\n",
      "17563305 | A2\n",
      "17563305 | A2\n",
      "03048608 | B1\n",
      "1059910X | A2\n",
      "00740276 | A4\n",
      "17427584 | B2\n",
      "19352735 | A1\n",
      "17563305 | A2\n",
      "14712164 | A2\n",
      "0022538X | A1\n",
      "19326203 | A1\n",
      "11107243 | None\n",
      "13835769 | A3\n",
      "03009475 | A4\n",
      "1020895X | None\n",
      "00348910 | A1\n",
      "00410101 | A4\n",
      "00410101 | A4\n",
      "09621075 | A1\n",
      "19326203 | A1\n",
      "00223395 | B1\n",
      "00222585 | A1\n",
      "00222585 | A1\n",
      "00222585 | A1\n",
      "07179502 | B3\n",
      "00222585 | A1\n",
      "0102311X | A1\n",
      "00222585 | A1\n",
      "00222011 | A2\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "23579323 | None\n",
      "00207519 | A1\n",
      "14678039 | A2\n",
      "00223395 | B1\n",
      "00222585 | A1\n",
      "00278424 | A1\n",
      "00222585 | A1\n",
      "00207519 | A1\n",
      "00740276 | A4\n",
      "00222585 | A1\n",
      "09651748 | A1\n",
      "00029637 | A3\n",
      "00222011 | A2\n",
      "00364665 | B1\n",
      "01666851 | A4\n",
      "10665234 | A4\n",
      "16784510 | None\n",
      "00222585 | A1\n",
      "00311820 | A1\n",
      "07984545 | B3\n",
      "00278424 | A1\n",
      "01666851 | A4\n",
      "00311820 | A1\n",
      "01884409 | A4\n",
      "00219258 | A1\n",
      "         | None\n",
      "0269283X | A1\n",
      "00311820 | A1\n",
      "00144894 | A4\n",
      "00221007 | A1\n",
      "00311820 | A1\n",
      "         | None\n",
      "00144894 | A4\n",
      "10665234 | A4\n",
      "00278424 | A1\n",
      "         | None\n",
      "00368075 | A1\n",
      "00221554 | A3\n",
      "00443255 | None\n",
      "         | None\n",
      "         | None\n",
      "00144894 | A4\n",
      "00144894 | A4\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "00144894 | A4\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "03015564 | None\n",
      "00144894 | A4\n",
      "00223395 | B1\n",
      "03015564 | None\n",
      "         | None\n",
      "00223395 | B1\n",
      "15515044 | A3\n",
      "14759276 | A2\n",
      "16789849 | B1\n",
      "19994923 | A1\n",
      "0001706X | A2\n",
      "00378682 | B1\n",
      "13835769 | A3\n",
      "0001706X | A2\n",
      "03044017 | A1\n",
      "00740276 | A4\n",
      "00311820 | A1\n",
      "16788060 | A4\n",
      "19352735 | A1\n",
      "16789849 | B1\n",
      "16788060 | A4\n",
      "00221759 | B2\n",
      "00740276 | A4\n",
      "20452322 | A1\n",
      "16788060 | A4\n",
      "00223573 | A3\n",
      "16789849 | B1\n",
      "16789946 | B1\n",
      "00359203 | A3\n",
      "19352735 | A1\n",
      "17560500 | B1\n",
      "23146133 | A3\n",
      "23791764 | C\n",
      "00218820 | A4\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "21757925 | B4\n",
      "19326203 | A1\n",
      "00144894 | A4\n",
      "16789849 | B1\n",
      "20499957 | A2\n",
      "16789849 | B1\n",
      "19352727 | A1\n",
      "00740276 | A4\n",
      "00311820 | A1\n",
      "0001706X | A2\n",
      "19326203 | A1\n",
      "13835769 | A3\n",
      "00378682 | B1\n",
      "00144894 | A4\n",
      "23146133 | A3\n",
      "09684328 | A3\n",
      "19326203 | A1\n",
      "00364665 | B1\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00364665 | B1\n",
      "00740276 | A4\n",
      "2317269X | B2\n",
      "19352727 | A1\n",
      "19352727 | A1\n",
      "14786419 | A3\n",
      "00378682 | B1\n",
      "20900023 | B1\n",
      "1537744X | A3\n",
      "19326203 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "0145305X | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "19352727 | A1\n",
      "03009475 | A4\n",
      "00311820 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "01419838 | A2\n",
      "00740276 | A4\n",
      "00311820 | A1\n",
      "14714922 | A1\n",
      "0949944X | A4\n",
      "00740276 | A4\n",
      "00311820 | A1\n",
      "00740276 | A4\n",
      "00029637 | A3\n",
      "17434378 | None\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "00311820 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "01419838 | A2\n",
      "00740276 | A4\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "00359203 | A3\n",
      "03785173 | A1\n",
      "00378682 | B1\n",
      "0001706X | A2\n",
      "00142999 | A3\n",
      "1824307X | A4\n",
      "00311820 | A1\n",
      "00364665 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "01419838 | A2\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "09575235 | B1\n",
      "00311820 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00364665 | B1\n",
      "0100879X | A3\n",
      "00740276 | A4\n",
      "00429686 | A1\n",
      "09320113 | A1\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "15182983 | None\n",
      "15182983 | None\n",
      "00029637 | A3\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00778923 | A1\n",
      "00364665 | B1\n",
      "00740276 | A4\n",
      "0001706X | A2\n",
      "00218782 | A2\n",
      "00311820 | A1\n",
      "00311820 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "09629351 | A3\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00311820 | A1\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "0100879X | A3\n",
      "00359203 | A3\n",
      "00364665 | B1\n",
      "00207519 | A1\n",
      "00364665 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "         | None\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "0103880X | B2\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "0100879X | A3\n",
      "01020935 | B1\n",
      "01020935 | B1\n",
      "00378682 | B1\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "00740276 | A4\n",
      "00364665 | B1\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "00207519 | A1\n",
      "00034983 | A4\n",
      "00364665 | B1\n",
      "00740276 | A4\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "00378682 | B1\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "00223395 | B1\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "00740276 | A4\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "01020935 | B1\n",
      "00029637 | A3\n",
      "00359203 | A3\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "00311820 | A1\n",
      "00347256 | None\n",
      "00029637 | A3\n",
      "00359203 | A3\n",
      "00364665 | B1\n",
      "00364665 | B1\n",
      "00223395 | B1\n",
      "00029637 | A3\n",
      "00364665 | B1\n",
      "00311820 | A1\n",
      "00223395 | B1\n",
      "00029637 | A3\n",
      "00144894 | A4\n",
      "00223395 | B1\n",
      "00223395 | B1\n",
      "00347310 | None\n",
      "00347310 | None\n",
      "00359203 | A3\n",
      "00364665 | B1\n",
      "00223395 | B1\n",
      "00096725 | A1\n",
      "00364665 | B1\n",
      "0004525X | None\n",
      "01020935 | B1\n",
      "01020935 | B1\n",
      "01020935 | B1\n",
      "00364665 | B1\n",
      "00034983 | A4\n",
      "00364665 | B1\n",
      "01017608 | None\n",
      "00359203 | A3\n",
      "00144894 | A4\n",
      "00364665 | B1\n",
      "00359203 | A3\n",
      "00347256 | None\n",
      "00223395 | B1\n",
      "20590105 | A1\n",
      "20590105 | A1\n",
      "19352735 | A1\n",
      "16643224 | A2\n",
      "17435889 | A1\n",
      "20762607 | A3\n",
      "19994915 | A2\n",
      "21507511 | A1\n",
      "16788060 | A4\n",
      "19994915 | A2\n",
      "21678359 | A2\n",
      "20411723 | A1\n",
      "26670380 | None\n",
      "20452322 | A1\n",
      "15504131 | A1\n",
      "22111247 | A1\n",
      "00029637 | A3\n",
      "15292908 | A1\n",
      "23147156 | A3\n",
      "00221767 | A1\n",
      "22352988 | A2\n",
      "22111247 | A1\n",
      "21507511 | A1\n",
      "21507511 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "19994915 | A2\n",
      "19352735 | A1\n",
      "19313128 | A1\n",
      "07415400 | A2\n",
      "15537374 | A1\n",
      "20411723 | A1\n",
      "14741733 | A1\n",
      "19352735 | A1\n",
      "10806040 | A1\n",
      "15537374 | A1\n",
      "07415400 | A2\n",
      "10434666 | A4\n",
      "16788060 | A4\n",
      "19352735 | A1\n",
      "01052896 | A1\n",
      "00142980 | A2\n",
      "13695274 | A1\n",
      "16643224 | A2\n",
      "19313128 | A1\n",
      "19353456 | None\n",
      "00221007 | A1\n",
      "00221899 | A1\n",
      "10788956 | A1\n",
      "00221767 | A1\n",
      "22352988 | A2\n",
      "15537374 | A1\n",
      "23747943 | None\n",
      "16643224 | A2\n",
      "1664302X | A2\n",
      "15537374 | A1\n",
      "03044017 | A1\n",
      "15537374 | A1\n",
      "00221767 | A1\n",
      "15537374 | A1\n",
      "10788956 | A1\n",
      "20411723 | A1\n",
      "23738227 | A1\n",
      "19326203 | A1\n",
      "15537374 | A1\n",
      "19352735 | A1\n",
      "00221899 | A1\n",
      "21507511 | A1\n",
      "22111247 | A1\n",
      "19313128 | A1\n",
      "19326203 | A1\n",
      "15537374 | A1\n",
      "22111247 | A1\n",
      "14741733 | A1\n",
      "09629351 | A3\n",
      "09629351 | A3\n",
      "00142980 | A2\n",
      "10430342 | A2\n",
      "03044017 | A1\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "15537374 | A1\n",
      "15306984 | A1\n",
      "01666851 | A4\n",
      "03407004 | A1\n",
      "19313128 | A1\n",
      "1083351X | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "00221767 | A1\n",
      "19352735 | A1\n",
      "13608185 | A1\n",
      "03051048 | A1\n",
      "00740276 | A4\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "00903493 | A1\n",
      "00221899 | A1\n",
      "13695274 | A1\n",
      "15537374 | A1\n",
      "15537374 | A1\n",
      "00142999 | A3\n",
      "00199567 | A2\n",
      "01675273 | A2\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "00199567 | A2\n",
      "00278424 | A1\n",
      "00278424 | A1\n",
      "00142980 | A2\n",
      "00142980 | A2\n",
      "00199567 | A2\n",
      "0264410X | A2\n",
      "19326203 | A1\n",
      "15353893 | A2\n",
      "10747613 | A1\n",
      "15506606 | A1\n",
      "00278424 | A1\n",
      "00029440 | A1\n",
      "14623994 | None\n",
      "0264410X | A2\n",
      "14664879 | A3\n",
      "15537374 | A1\n",
      "00664804 | A1\n",
      "00278424 | A1\n",
      "00740276 | A4\n",
      "00278424 | A1\n",
      "03008428 | A1\n",
      "0264410X | A2\n",
      "01419838 | A2\n",
      "00199567 | A2\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "11396709 | B2\n",
      "12864579 | A3\n",
      "00013765 | A2\n",
      "12864579 | A3\n",
      "12864579 | A3\n",
      "00221767 | A1\n",
      "0264410X | A2\n",
      "00740276 | A4\n",
      "00199567 | A2\n",
      "18632297 | A1\n",
      "0264410X | A2\n",
      "00664804 | A1\n",
      "12864579 | A3\n",
      "15566811 | A3\n",
      "12864579 | A3\n",
      "0264410X | A2\n",
      "12864579 | A3\n",
      "00278424 | A1\n",
      "00221767 | A1\n",
      "00142980 | A2\n",
      "01655728 | A2\n",
      "15671348 | A2\n",
      "00199567 | A2\n",
      "10430342 | A2\n",
      "00063495 | A1\n",
      "14741733 | A1\n",
      "10430342 | A2\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "00029440 | A1\n",
      "00221767 | A1\n",
      "00199567 | A2\n",
      "00144827 | A4\n",
      "00740276 | A4\n",
      "02646021 | A1\n",
      "00221767 | A1\n",
      "00199567 | A2\n",
      "10970029 | A2\n",
      "0264410X | A2\n",
      "00199567 | A2\n",
      "08916934 | B2\n",
      "10430342 | A2\n",
      "09680519 | None\n",
      "00199567 | A2\n",
      "09629351 | A3\n",
      "00221899 | A1\n",
      "16765680 | B2\n",
      "01052896 | A1\n",
      "1083351X | A1\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "00097322 | A1\n",
      "00199567 | A2\n",
      "00199567 | A2\n",
      "00199567 | A2\n",
      "10953787 | A1\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "07415400 | A2\n",
      "14714922 | A1\n",
      "00951137 | A1\n",
      "12864579 | A3\n",
      "01666851 | A4\n",
      "12864579 | A3\n",
      "12864579 | A3\n",
      "00221899 | A1\n",
      "14714922 | A1\n",
      "0100879X | A3\n",
      "00199567 | A2\n",
      "00142980 | A2\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "00029440 | A1\n",
      "07415400 | A2\n",
      "12864579 | A3\n",
      "00029440 | A1\n",
      "00221899 | A1\n",
      "00219258 | A1\n",
      "00951137 | A1\n",
      "00221899 | A1\n",
      "00144894 | A4\n",
      "00221767 | A1\n",
      "00096725 | A1\n",
      "13695274 | A1\n",
      "12864579 | A3\n",
      "14602075 | A1\n",
      "01675699 | None\n",
      "00410101 | A4\n",
      "00071188 | A1\n",
      "00199567 | A2\n",
      "00199567 | A2\n",
      "00199567 | A2\n",
      "00199567 | A2\n",
      "07415400 | A2\n",
      "01419838 | A2\n",
      "         | None\n",
      "0100879X | A3\n",
      "00740276 | A4\n",
      "0100879X | A3\n",
      "0100879X | A3\n",
      "01666851 | A4\n",
      "00199567 | A2\n",
      "00664804 | A1\n",
      "00219738 | A1\n",
      "08938512 | A1\n",
      "00221767 | A1\n",
      "00364665 | B1\n",
      "00221767 | A1\n",
      "10182438 | A4\n",
      "00796034 | None\n",
      "00221007 | A1\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "00221007 | A1\n",
      "00199567 | A2\n",
      "00192805 | A1\n",
      "00221767 | A1\n",
      "00199567 | A2\n",
      "00142980 | A2\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "08892229 | B2\n",
      "13574310 | None\n",
      "00071161 | A1\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "09232494 | None\n",
      "09232494 | None\n",
      "09527915 | A1\n",
      "00221767 | A1\n",
      "00221007 | A1\n",
      "09232494 | None\n",
      "00029637 | A3\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "00029637 | A3\n",
      "00278424 | A1\n",
      "00221767 | A1\n",
      "00221007 | A1\n",
      "00278424 | A1\n",
      "10562044 | None\n",
      "01052896 | A1\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "00221767 | A1\n",
      "00142980 | A2\n",
      "00199567 | A2\n",
      "00221767 | A1\n",
      "00040622 | None\n",
      "09232494 | None\n",
      "00221767 | A1\n",
      "01419838 | A2\n",
      "00221767 | A1\n",
      "00378682 | B1\n",
      "10665234 | A4\n",
      "00199567 | A2\n",
      "00029394 | A1\n",
      "09232494 | None\n",
      "00221767 | A1\n",
      "00029637 | A3\n",
      "00951137 | A1\n",
      "00359203 | A3\n",
      "01020935 | B1\n",
      "0022149X | A2\n",
      "00762997 | A1\n",
      "00144894 | A4\n",
      "22352988 | A2\n",
      "19808178 | B4\n",
      "03781119 | A4\n",
      "16643224 | A2\n",
      "19844689 | B1\n",
      "1664302X | A2\n",
      "1664302X | A2\n",
      "20760817 | A3\n",
      "17563305 | A2\n",
      "2296858X | A1\n",
      "16643224 | A2\n",
      "1664302X | A2\n",
      "16788060 | A4\n",
      "0001706X | A2\n",
      "17563305 | A2\n",
      "00144894 | A4\n",
      "16788060 | A4\n",
      "16794973 | None\n",
      "20499957 | A2\n",
      "16789946 | B1\n",
      "21766215 | B3\n",
      "23739282 | C\n",
      "16789849 | B1\n",
      "20411723 | A1\n",
      "00222585 | A1\n",
      "16879686 | B1\n",
      "19326203 | A1\n",
      "20499957 | A2\n",
      "23146133 | A3\n",
      "21931801 | None\n",
      "19326203 | A1\n",
      "00364665 | B1\n",
      "21931801 | None\n",
      "00222933 | B1\n",
      "00740276 | A4\n",
      "19808178 | B4\n",
      "00740276 | A4\n",
      "10557903 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "0001706X | A2\n",
      "01655701 | B1\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "0001706X | A2\n",
      "0102311X | A1\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "0102695X | A3\n",
      "0001706X | A2\n",
      "13653024 | A2\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00311820 | A1\n",
      "00740276 | A4\n",
      "00311820 | A1\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "20452322 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "2297055X | A3\n",
      "12019712 | A2\n",
      "17435889 | A1\n",
      "19352735 | A1\n",
      "08824010 | A3\n",
      "20760817 | A3\n",
      "22964185 | A3\n",
      "16639812 | A2\n",
      "19352735 | A1\n",
      "03057453 | A1\n",
      "2076393X | A3\n",
      "00144894 | A4\n",
      "01712985 | A2\n",
      "22136711 | A1\n",
      "07415400 | A2\n",
      "16643224 | A2\n",
      "01988859 | B1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "2076393X | A3\n",
      "20452322 | A1\n",
      "01615890 | A4\n",
      "16643224 | A2\n",
      "20734409 | A3\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "17424933 | A3\n",
      "00664804 | A1\n",
      "19352735 | A1\n",
      "14220067 | A2\n",
      "19326203 | A1\n",
      "20499957 | A2\n",
      "16643224 | A2\n",
      "01419838 | A2\n",
      "10434666 | A4\n",
      "14366207 | A1\n",
      "01712985 | A2\n",
      "00029637 | A3\n",
      "00029637 | A3\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "16789849 | B1\n",
      "14712334 | A3\n",
      "00207519 | A1\n",
      "10420533 | A1\n",
      "17563305 | A2\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "2056676X | A1\n",
      "00199567 | A2\n",
      "0264410X | A2\n",
      "14712334 | A3\n",
      "01712985 | A2\n",
      "19352735 | A1\n",
      "14712334 | A3\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "01681702 | A2\n",
      "17563305 | A2\n",
      "15537374 | A1\n",
      "00034800 | B2\n",
      "03044017 | A1\n",
      "20900023 | B1\n",
      "0264410X | A2\n",
      "03044017 | A1\n",
      "03044017 | A1\n",
      "00359203 | A3\n",
      "01615890 | A4\n",
      "00199567 | A2\n",
      "01419838 | A2\n",
      "01652427 | A1\n",
      "19450493 | None\n",
      "22112839 | None\n",
      "01988859 | B1\n",
      "22112839 | None\n",
      "17402522 | None\n",
      "09628436 | A1\n",
      "01712985 | A2\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "12864579 | A3\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "14712105 | A1\n",
      "01712985 | A2\n",
      "03009475 | A4\n",
      "10623345 | None\n",
      "14712334 | A3\n",
      "00916749 | A1\n",
      "13602276 | A1\n",
      "00468177 | A2\n",
      "00468177 | A2\n",
      "10900233 | A1\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "09320113 | A1\n",
      "0264410X | A2\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "20900023 | B1\n",
      "0001706X | A2\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "00207519 | A1\n",
      "03090167 | A1\n",
      "03044017 | A1\n",
      "16784510 | None\n",
      "19352735 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00221899 | A1\n",
      "19352735 | A1\n",
      "19352735 | A1\n",
      "14138670 | B1\n",
      "14664879 | A3\n",
      "00468177 | A2\n",
      "09320113 | A1\n",
      "03009475 | A4\n",
      "0264410X | A2\n",
      "00036072 | A4\n",
      "01652427 | A1\n",
      "10584838 | A1\n",
      "         | None\n",
      "01988859 | B1\n",
      "0001706X | A2\n",
      "19352735 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "03076938 | A4\n",
      "00468177 | A2\n",
      "00740276 | A4\n",
      "01652427 | A1\n",
      "00345288 | A1\n",
      "00221899 | A1\n",
      "01652427 | A1\n",
      "0264410X | A2\n",
      "00345288 | A1\n",
      "0066782X | B2\n",
      "00740276 | A4\n",
      "00359203 | A3\n",
      "00311820 | A1\n",
      "0001706X | A2\n",
      "0264410X | A2\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "03008584 | A3\n",
      "08878013 | B1\n",
      "00664804 | A1\n",
      "00311820 | A1\n",
      "13602276 | A1\n",
      "0001706X | A2\n",
      "00144894 | A4\n",
      "         | None\n",
      "12864579 | A3\n",
      "12864579 | A3\n",
      "00740276 | A4\n",
      "01652427 | A1\n",
      "09320113 | A1\n",
      "00359203 | A3\n",
      "0001706X | A2\n",
      "01652427 | A1\n",
      "09320113 | A1\n",
      "14698161 | A1\n",
      "01632116 | A4\n",
      "00468177 | A2\n",
      "00099104 | A3\n",
      "10939946 | A3\n",
      "10939946 | A3\n",
      "01419838 | A2\n",
      "02776715 | A2\n",
      "00099104 | A3\n",
      "0264410X | A2\n",
      "15163180 | A4\n",
      "00187143 | None\n",
      "08926638 | A1\n",
      "12864579 | A3\n",
      "0001706X | A2\n",
      "01652427 | A1\n",
      "0100879X | A3\n",
      "10939946 | A3\n",
      "13602276 | A1\n",
      "00345288 | A1\n",
      "0001706X | A2\n",
      "00219975 | A3\n",
      "00199567 | A2\n",
      "01655728 | A2\n",
      "00199567 | A2\n",
      "00740276 | A4\n",
      "00099104 | A3\n",
      "10939946 | A3\n",
      "09593993 | A3\n",
      "00359203 | A3\n",
      "         | None\n",
      "01652427 | A1\n",
      "         | None\n",
      "00199567 | A2\n",
      "0001706X | A2\n",
      "12864579 | A3\n",
      "         | None\n",
      "03009475 | A4\n",
      "03009475 | A4\n",
      "09284249 | None\n",
      "03009475 | A4\n",
      "00187143 | None\n",
      "00199567 | A2\n",
      "00099104 | A3\n",
      "00099104 | A3\n",
      "00221899 | A1\n",
      "13602276 | A1\n",
      "00359203 | A3\n",
      "         | None\n",
      "00099104 | A3\n",
      "00740276 | A4\n",
      "00199567 | A2\n",
      "00187143 | None\n",
      "09629351 | A3\n",
      "         | None\n",
      "00207519 | A1\n",
      "00199567 | A2\n",
      "00144894 | A4\n",
      "         | None\n",
      "00378682 | B1\n",
      "00144827 | A4\n",
      "13602276 | A1\n",
      "00029637 | A3\n",
      "03009475 | A4\n",
      "00359203 | A3\n",
      "01632116 | A4\n",
      "00029637 | A3\n",
      "03009475 | A4\n",
      "00029637 | A3\n",
      "         | None\n",
      "00029637 | A3\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "01419838 | A2\n",
      "01419838 | A2\n",
      "00029637 | A3\n",
      "0100879X | A3\n",
      "0100879X | A3\n",
      "0001706X | A2\n",
      "08878013 | B1\n",
      "00740276 | A4\n",
      "         | None\n",
      "0100879X | A3\n",
      "00740276 | A4\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "13602276 | A1\n",
      "00740276 | A4\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "13602276 | A1\n",
      "00740276 | A4\n",
      "         | None\n",
      "00740276 | A4\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "00378682 | B1\n",
      "         | None\n",
      "         | None\n",
      "00411132 | A3\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "00029637 | A3\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "01652478 | A3\n",
      "00029637 | A3\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "10568751 | None\n",
      "00029637 | A3\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "         | None\n",
      "         | None\n",
      "00740276 | A4\n",
      "         | None\n",
      "00378682 | B1\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "00034983 | A4\n",
      "03009475 | A4\n",
      "         | None\n",
      "00378682 | B1\n",
      "         | None\n",
      "         | None\n",
      "00029637 | A3\n",
      "         | None\n",
      "01419838 | A2\n",
      "         | None\n",
      "00359203 | A3\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "00413232 | None\n",
      "         | None\n",
      "         | None\n",
      "00034983 | A4\n",
      "00144894 | A4\n",
      "00144894 | A4\n",
      "00359203 | A3\n",
      "00740276 | A4\n",
      "01666851 | A4\n",
      "00029637 | A3\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "00029637 | A3\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "00029637 | A3\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "0100879X | A3\n",
      "00029637 | A3\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "         | None\n",
      "00740276 | A4\n",
      "09320113 | A1\n",
      "00740276 | A4\n",
      "20760817 | A3\n",
      "00029637 | A3\n",
      "00740276 | A4\n",
      "27682811 | None\n",
      "20762607 | A3\n",
      "22352988 | A2\n",
      "00740276 | A4\n",
      "10656995 | B2\n",
      "10656995 | B2\n",
      "00013765 | A2\n",
      "22971769 | A2\n",
      "10656995 | B2\n",
      "08926638 | A1\n",
      "00311820 | A1\n",
      "19352735 | A1\n",
      "01002430 | B2\n",
      "19352735 | A1\n",
      "22971769 | A2\n",
      "23147156 | A3\n",
      "23147156 | A3\n",
      "22352988 | A2\n",
      "07415400 | A2\n",
      "21793700 | None\n",
      "23795042 | A2\n",
      "22352988 | A2\n",
      "23795042 | A2\n",
      "16788060 | A4\n",
      "22352988 | A2\n",
      "22111247 | A1\n",
      "16643224 | A2\n",
      "00144894 | A4\n",
      "16643224 | A2\n",
      "22352988 | A2\n",
      "16788060 | A4\n",
      "19352735 | A1\n",
      "22352988 | A2\n",
      "16788060 | A4\n",
      "13892029 | B1\n",
      "00903558 | A4\n",
      "16788060 | A4\n",
      "16788060 | A4\n",
      "20013078 | A1\n",
      "1664302X | A2\n",
      "07533322 | A2\n",
      "1664302X | A2\n",
      "20013078 | A1\n",
      "14344610 | A4\n",
      "15537374 | A1\n",
      "00099104 | A3\n",
      "17563305 | A2\n",
      "20013078 | A1\n",
      "20452322 | A1\n",
      "14698161 | A1\n",
      "14765810 | A1\n",
      "19352735 | A1\n",
      "00092797 | A2\n",
      "01666851 | A4\n",
      "13835769 | A3\n",
      "17563305 | A2\n",
      "13835769 | A3\n",
      "17466148 | A2\n",
      "20283997 | C\n",
      "23146133 | A3\n",
      "01666851 | A4\n",
      "20013078 | A1\n",
      "23148861 | A3\n",
      "17510147 | A2\n",
      "15734064 | B1\n",
      "00347612 | A2\n",
      "00221899 | A1\n",
      "03656233 | A3\n",
      "13835769 | A3\n",
      "19326203 | A1\n",
      "07328893 | A4\n",
      "17563305 | A2\n",
      "19326203 | A1\n",
      "15653633 | A2\n",
      "03044165 | A1\n",
      "19352735 | A1\n",
      "00032697 | A2\n",
      "00221767 | A1\n",
      "00029637 | A3\n",
      "17563305 | A2\n",
      "16879686 | B1\n",
      "00207519 | A1\n",
      "08926638 | A1\n",
      "07328893 | A4\n",
      "11107243 | None\n",
      "00222585 | A1\n",
      "07328893 | A4\n",
      "00278424 | A1\n",
      "02235234 | A1\n",
      "09248579 | A1\n",
      "0960894X | A2\n",
      "00207519 | A1\n",
      "00144894 | A4\n",
      "00219258 | A1\n",
      "00222585 | A1\n",
      "00013765 | A2\n",
      "01666851 | A4\n",
      "00062928 | A3\n",
      "00740276 | A4\n",
      "00029637 | A3\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "16788060 | A4\n",
      "26669587 | None\n",
      "20452322 | A1\n",
      "20762607 | A3\n",
      "18760341 | A3\n",
      "01620134 | A2\n",
      "16788060 | A4\n",
      "13873547 | A1\n",
      "20762607 | A3\n",
      "16788060 | A4\n",
      "10982299 | B1\n",
      "14715007 | None\n",
      "16788060 | A4\n",
      "00410101 | A4\n",
      "22352988 | A2\n",
      "10958355 | B2\n",
      "00144894 | A4\n",
      "2296858X | A1\n",
      "07533322 | A2\n",
      "00052736 | A1\n",
      "17563305 | A2\n",
      "21650497 | A1\n",
      "1664302X | A2\n",
      "00664804 | A1\n",
      "23738227 | A1\n",
      "20734425 | A3\n",
      "17470285 | A4\n",
      "16788060 | A4\n",
      "14786419 | A3\n",
      "20452322 | A1\n",
      "12019712 | A2\n",
      "07533322 | A2\n",
      "20452322 | A1\n",
      "2296634X | A2\n",
      "00664804 | A1\n",
      "02235234 | A1\n",
      "22113207 | A2\n",
      "19352735 | A1\n",
      "01666851 | A4\n",
      "01620134 | A2\n",
      "18672450 | A1\n",
      "0950382X | A2\n",
      "02235234 | A1\n",
      "22113207 | A2\n",
      "14203049 | A2\n",
      "09594973 | A4\n",
      "01004042 | A4\n",
      "19352735 | A1\n",
      "00144894 | A4\n",
      "01035053 | A2\n",
      "01039733 | A3\n",
      "0102695X | A3\n",
      "01422782 | None\n",
      "09395075 | B1\n",
      "09395075 | B1\n",
      "00664804 | A1\n",
      "14248247 | A1\n",
      "19352735 | A1\n",
      "00144894 | A4\n",
      "17563305 | A2\n",
      "02235234 | A1\n",
      "28132998 | None\n",
      "17568919 | A2\n",
      "14728222 | A1\n",
      "14203049 | A2\n",
      "16788060 | A4\n",
      "22352988 | A2\n",
      "15734064 | B1\n",
      "22277382 | A4\n",
      "17563305 | A2\n",
      "15675769 | A2\n",
      "18607179 | A1\n",
      "19352735 | A1\n",
      "01035053 | A2\n",
      "22352988 | A2\n",
      "17563305 | A2\n",
      "10542523 | A2\n",
      "19846835 | B2\n",
      "15734064 | B1\n",
      "14203049 | A2\n",
      "14310651 | A4\n",
      "16788060 | A4\n",
      "14310651 | A4\n",
      "16788060 | A4\n",
      "18786146 | A2\n",
      "17563305 | A2\n",
      "15734064 | B1\n",
      "14310651 | A4\n",
      "17470277 | A4\n",
      "17563305 | A2\n",
      "15734064 | B1\n",
      "16121872 | A4\n",
      "14786419 | A3\n",
      "14310651 | A4\n",
      "22352988 | A2\n",
      "14310651 | A4\n",
      "16788060 | A4\n",
      "20452322 | A1\n",
      "19352735 | A1\n",
      "16788060 | A4\n",
      "15734064 | B1\n",
      "00144894 | A4\n",
      "09680896 | A2\n",
      "0950382X | A2\n",
      "00029637 | A3\n",
      "01035053 | A2\n",
      "00144894 | A4\n",
      "14622912 | A1\n",
      "00664804 | A1\n",
      "17563305 | A2\n",
      "02235234 | A1\n",
      "00144894 | A4\n",
      "14203049 | A2\n",
      "00219258 | A1\n",
      "14786419 | A3\n",
      "19957645 | A1\n",
      "14786419 | A3\n",
      "13835769 | A3\n",
      "00664804 | A1\n",
      "14310651 | A4\n",
      "14786419 | A3\n",
      "16159853 | A2\n",
      "16788060 | A4\n",
      "02235234 | A1\n",
      "07224060 | A2\n",
      "03051978 | B1\n",
      "17563305 | A2\n",
      "19352735 | A1\n",
      "1742464X | A2\n",
      "00144894 | A4\n",
      "19326203 | A1\n",
      "01666851 | A4\n",
      "22113207 | A2\n",
      "00740276 | A4\n",
      "00029637 | A3\n",
      "17563305 | A2\n",
      "09320113 | A1\n",
      "15537374 | A1\n",
      "00740276 | A4\n",
      "00144894 | A4\n",
      "0950382X | A2\n",
      "0001706X | A2\n",
      "00278424 | A1\n",
      "01666851 | A4\n",
      "0001706X | A2\n",
      "00740276 | A4\n",
      "09320113 | A1\n",
      "00144894 | A4\n",
      "07328893 | A4\n",
      "0001706X | A2\n",
      "15353893 | A2\n",
      "0001706X | A2\n",
      "01666851 | A4\n",
      "0001706X | A2\n",
      "09320113 | A1\n",
      "09320113 | A1\n",
      "00221899 | A1\n",
      "01666851 | A4\n",
      "00029637 | A3\n",
      "01666851 | A4\n",
      "12864579 | A3\n",
      "00740276 | A4\n",
      "01419838 | A2\n",
      "01666851 | A4\n",
      "00664804 | A1\n",
      "00311820 | A1\n",
      "00740276 | A4\n",
      "09680896 | A2\n",
      "00320943 | A1\n",
      "0001706X | A2\n",
      "0951418X | A1\n",
      "10665234 | A4\n",
      "00664804 | A1\n",
      "00664804 | A1\n",
      "14752875 | A2\n",
      "26737515 | None\n",
      "22352988 | A2\n",
      "19352735 | A1\n",
      "1664302X | A2\n",
      "19352735 | A1\n",
      "17425255 | A1\n",
      "22352988 | A2\n",
      "14752875 | A2\n",
      "22352988 | A2\n",
      "22352988 | A2\n",
      "00664804 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "22113207 | A2\n",
      "16784685 | A3\n",
      "00664804 | A1\n",
      "14752875 | A2\n",
      "14752875 | A2\n",
      "14752875 | A2\n",
      "20452322 | A1\n",
      "19326203 | A1\n",
      "15671348 | A2\n",
      "14752875 | A2\n",
      "15671348 | A2\n",
      "16788060 | A4\n",
      "19326203 | A1\n",
      "00740276 | A4\n",
      "19352735 | A1\n",
      "14752875 | A2\n",
      "19326203 | A1\n",
      "14752875 | A2\n",
      "19326203 | A1\n",
      "00740276 | A4\n",
      "15671348 | A2\n",
      "13602276 | A1\n",
      "19326203 | A1\n",
      "14752875 | A2\n",
      "00099104 | A3\n",
      "00429007 | A4\n",
      "01666851 | A4\n",
      "20799721 | C\n",
      "10434666 | A4\n",
      "10434666 | A4\n",
      "16643224 | A2\n",
      "15216616 | A3\n",
      "20452322 | A1\n",
      "0264410X | A2\n",
      "05315565 | A3\n",
      "16643224 | A2\n",
      "15675769 | A2\n",
      "16643224 | A2\n",
      "0264410X | A2\n",
      "16643224 | A2\n",
      "0264410X | A2\n",
      "10434666 | A4\n",
      "16643224 | A2\n",
      "16643224 | A2\n",
      "01652478 | A3\n",
      "00221759 | B2\n",
      "0264410X | A2\n",
      "20452322 | A1\n",
      "15216616 | A3\n",
      "00221759 | B2\n",
      "16788060 | A4\n",
      "14698161 | A1\n",
      "16643224 | A2\n",
      "00144894 | A4\n",
      "10434666 | A4\n",
      "17563305 | A2\n",
      "14786362 | A2\n",
      "10806040 | A1\n",
      "16643224 | A2\n",
      "00221759 | B2\n",
      "19352735 | A1\n",
      "01419838 | A2\n",
      "00221759 | B2\n",
      "01663542 | A2\n",
      "17424933 | A3\n",
      "00221759 | B2\n",
      "01652478 | A3\n",
      "01663542 | A2\n",
      "00088749 | A3\n",
      "14760584 | A1\n",
      "03044017 | A1\n",
      "16789849 | B1\n",
      "01712985 | A2\n",
      "19326203 | A1\n",
      "03009475 | A4\n",
      "14712334 | A3\n",
      "16784464 | A1\n",
      "22381589 | C\n",
      "21645515 | A3\n",
      "01615890 | A4\n",
      "14712334 | A3\n",
      "15254135 | A2\n",
      "01652478 | A3\n",
      "19352735 | A1\n",
      "16006135 | A1\n",
      "01712985 | A2\n",
      "13653083 | A4\n",
      "16788060 | A4\n",
      "13653083 | A4\n",
      "00221759 | B2\n",
      "15566811 | A3\n",
      "00221759 | B2\n",
      "00221759 | B2\n",
      "00221759 | B2\n",
      "0264410X | A2\n",
      "00359203 | A3\n",
      "00221759 | B2\n",
      "0001706X | A2\n",
      "1568010X | None\n",
      "10217401 | B2\n",
      "00099104 | A3\n",
      "00099104 | A3\n",
      "00099104 | A3\n",
      "         | None\n",
      "         | None\n",
      "20585276 | A1\n",
      "23752548 | A1\n",
      "20760817 | A3\n",
      "19994915 | A2\n",
      "16643224 | A2\n",
      "14714922 | A1\n",
      "22145745 | A1\n",
      "19994915 | A2\n",
      "16788060 | A4\n",
      "20585276 | A1\n",
      "15537374 | A1\n",
      "14698161 | A1\n",
      "15457885 | A1\n",
      "09621083 | A1\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "16604601 | A1\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "01031104 | A4\n",
      "16877098 | A4\n",
      "19840470 | A3\n",
      "19326203 | A1\n",
      "2373437X | B3\n",
      "0001706X | A2\n",
      "19805497 | A3\n",
      "23146133 | A3\n",
      "21505608 | A1\n",
      "14152150 | A1\n",
      "0001706X | A2\n",
      "19832117 | A1\n",
      "00378682 | B1\n",
      "16879686 | B1\n",
      "0001706X | A2\n",
      "15167313 | A1\n",
      "11107243 | None\n",
      "00222585 | A1\n",
      "07328893 | A4\n",
      "00740276 | A4\n",
      "         | None\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00222585 | A1\n",
      "00222585 | A1\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "22352988 | A2\n",
      "2297055X | A3\n",
      "15250016 | A1\n",
      "09502688 | A4\n",
      "10434666 | A4\n",
      "19490976 | A1\n",
      "0264410X | A2\n",
      "20452322 | A1\n",
      "25233106 | B2\n",
      "16643224 | A2\n",
      "16643224 | A2\n",
      "21507511 | A1\n",
      "19994915 | A2\n",
      "23146141 | A3\n",
      "22962565 | A1\n",
      "19326203 | A1\n",
      "16643224 | A2\n",
      "16788060 | A4\n",
      "14248247 | A1\n",
      "09612033 | A4\n",
      "10400605 | A1\n",
      "22352988 | A2\n",
      "16643224 | A2\n",
      "16788060 | A4\n",
      "08926638 | A1\n",
      "15671348 | A2\n",
      "16643224 | A2\n",
      "01663542 | A2\n",
      "16643224 | A2\n",
      "00064971 | A1\n",
      "10233830 | A3\n",
      "07415400 | A2\n",
      "16643224 | A2\n",
      "03788741 | A1\n",
      "00222615 | A4\n",
      "10441549 | A1\n",
      "12864579 | A3\n",
      "00221767 | A1\n",
      "23265191 | None\n",
      "1744666X | A3\n",
      "00221767 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "15537374 | A1\n",
      "07415400 | A2\n",
      "10441549 | A1\n",
      "13676733 | None\n",
      "15537374 | A1\n",
      "15396509 | None\n",
      "0264410X | A2\n",
      "07415400 | A2\n",
      "10441549 | A1\n",
      "00221767 | A1\n",
      "09320113 | A1\n",
      "27673375 | None\n",
      "17563305 | A2\n",
      "1664302X | A2\n",
      "19326203 | A1\n",
      "0001706X | A2\n",
      "25253409 | C\n",
      "03044017 | A1\n",
      "12019712 | A2\n",
      "16789849 | B1\n",
      "16788060 | A4\n",
      "23146133 | A3\n",
      "19352735 | A1\n",
      "22363777 | B2\n",
      "16789946 | B1\n",
      "16788060 | A4\n",
      "23144599 | B1\n",
      "0269283X | A1\n",
      "23146133 | A3\n",
      "00222585 | A1\n",
      "0001706X | A2\n",
      "00378682 | B1\n",
      "00029637 | A3\n",
      "03044017 | A1\n",
      "09320113 | A1\n",
      "17761042 | A3\n",
      "14321955 | A1\n",
      "00222585 | A1\n",
      "01675877 | A1\n",
      "21769095 | None\n",
      "00222585 | A1\n",
      "03044017 | A1\n",
      "17563305 | A2\n",
      "00222585 | A1\n",
      "00222011 | A2\n",
      "03785173 | A1\n",
      "20452322 | A1\n",
      "18686966 | A1\n",
      "1743422X | A3\n",
      "09724567 | B4\n",
      "17435889 | A1\n",
      "22360867 | B4\n",
      "0264410X | A2\n",
      "16878728 | A3\n",
      "19352735 | A1\n",
      "19994915 | A2\n",
      "23275081 | B4\n",
      "15675769 | A2\n",
      "01712985 | A2\n",
      "08159319 | A3\n",
      "15675769 | A2\n",
      "15675769 | A2\n",
      "24059390 | A3\n",
      "20760817 | A3\n",
      "0001706X | A2\n",
      "19352735 | A1\n",
      "00378682 | B1\n",
      "16789849 | B1\n",
      "19352735 | A1\n",
      "00222585 | A1\n",
      "16784162 | B1\n",
      "24734810 | C\n",
      "0001706X | A2\n",
      "19326203 | A1\n",
      "23144599 | B1\n",
      "0001706X | A2\n",
      "0264410X | A2\n",
      "16765680 | B2\n",
      "23146133 | A3\n",
      "0001706X | A2\n",
      "17563305 | A2\n",
      "23146133 | A3\n",
      "00378682 | B1\n",
      "10811710 | A4\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "0102311X | A1\n",
      "00378682 | B1\n",
      "03044017 | A1\n",
      "00378682 | B1\n",
      "09320113 | A1\n",
      "0102311X | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "03044017 | A1\n",
      "03044017 | A1\n",
      "00740276 | A4\n",
      "16679008 | None\n",
      "00740276 | A4\n",
      "00364665 | B1\n",
      "20762607 | A3\n",
      "19490976 | A1\n",
      "08999007 | A1\n",
      "07394462 | A3\n",
      "22352988 | A2\n",
      "15565068 | A1\n",
      "20452322 | A1\n",
      "20726643 | A1\n",
      "2309608X | A2\n",
      "20452322 | A1\n",
      "19326203 | A1\n",
      "17417007 | A1\n",
      "17422094 | A1\n",
      "16789849 | B1\n",
      "03781119 | A4\n",
      "16643224 | A2\n",
      "16181905 | None\n",
      "16643224 | A2\n",
      "09441344 | A2\n",
      "09651748 | A1\n",
      "10759964 | A2\n",
      "00359203 | A3\n",
      "26442906 | None\n",
      "22352988 | A2\n",
      "14675463 | A1\n",
      "2730664X | None\n",
      "19352735 | A1\n",
      "22181989 | A3\n",
      "09543007 | A1\n",
      "2296858X | A1\n",
      "23593997 | B2\n",
      "00236438 | A1\n",
      "19805497 | A3\n",
      "00431354 | A1\n",
      "1432184X | A1\n",
      "0893133X | A1\n",
      "19490984 | None\n",
      "15746968 | None\n",
      "19994915 | A2\n",
      "18762891 | A2\n",
      "20452322 | A1\n",
      "14622920 | None\n",
      "15824934 | A2\n",
      "1365294X | None\n",
      "23144378 | A3\n",
      "01688278 | A1\n",
      "13500872 | A4\n",
      "19326203 | A1\n",
      "20452322 | A1\n",
      "00260495 | A1\n",
      "22352988 | A2\n",
      "20492618 | A1\n",
      "19326203 | A1\n",
      "21505594 | A1\n",
      "23752548 | A1\n",
      "09621083 | A1\n",
      "17585996 | A4\n",
      "1664302X | A2\n",
      "13624962 | A1\n",
      "20452322 | A1\n",
      "19352735 | A1\n",
      "00063525 | A2\n",
      "20452322 | A1\n",
      "21698287 | None\n",
      "15573125 | None\n",
      "21698287 | None\n",
      "19352735 | A1\n",
      "00165085 | A1\n",
      "20411723 | A1\n",
      "1664302X | A2\n",
      "21698287 | None\n",
      "00280836 | A1\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "14712164 | A2\n",
      "1664302X | A2\n",
      "15387445 | None\n",
      "09208569 | B1\n",
      "00280836 | A1\n",
      "14712164 | A2\n",
      "14712164 | A2\n",
      "16765680 | B2\n",
      "16765680 | B2\n",
      "03008177 | A3\n",
      "15178382 | A2\n",
      "16784456 | B1\n",
      "0001706X | A2\n",
      "0001706X | A2\n",
      "24059390 | A3\n",
      "16784464 | A1\n",
      "03044017 | A1\n",
      "09320113 | A1\n",
      "0001706X | A2\n",
      "24478822 | C\n",
      "19326203 | A1\n",
      "03044017 | A1\n",
      "0001706X | A2\n",
      "19842961 | A2\n",
      "03044017 | A1\n",
      "00222585 | A1\n",
      "24059390 | A3\n",
      "0100736X | A4\n",
      "01675877 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "03044017 | A1\n",
      "00378682 | B1\n",
      "00222585 | A1\n",
      "21799482 | None\n",
      "01675877 | A1\n",
      "16766024 | None\n",
      "00378682 | B1\n",
      "03044017 | A1\n",
      "00378682 | B1\n",
      "09320113 | A1\n",
      "01675877 | A1\n",
      "0103846X | A2\n",
      "0103846X | A2\n",
      "01020935 | B1\n",
      "01044257 | None\n",
      "00740276 | A4\n",
      "00351555 | B1\n",
      "19994915 | A2\n",
      "12467820 | B2\n",
      "19994915 | A2\n",
      "10806040 | A1\n",
      "20411723 | A1\n",
      "19994915 | A2\n",
      "14712458 | A1\n",
      "20411723 | A1\n",
      "19994915 | A2\n",
      "19994915 | A2\n",
      "10806040 | A1\n",
      "19994915 | A2\n",
      "01466615 | B1\n",
      "16643224 | A2\n",
      "10806040 | A1\n",
      "19994915 | A2\n",
      "15671348 | A2\n",
      "20762607 | A3\n",
      "24146366 | A2\n",
      "19994915 | A2\n",
      "20762607 | A3\n",
      "20760817 | A3\n",
      "10806040 | A1\n",
      "23752548 | A1\n",
      "22962565 | A1\n",
      "19994915 | A2\n",
      "22379622 | A3\n",
      "01634453 | A1\n",
      "01466615 | B1\n",
      "01681702 | A2\n",
      "01466615 | B1\n",
      "21650497 | A1\n",
      "15487091 | A1\n",
      "17460794 | B4\n",
      "16775090 | A4\n",
      "20571577 | A1\n",
      "01681702 | A2\n",
      "16643224 | A2\n",
      "00104825 | A2\n",
      "01681702 | A2\n",
      "2296858X | A1\n",
      "20762607 | A3\n",
      "19994915 | A2\n",
      "19994915 | A2\n",
      "10806059 | A1\n",
      "20585276 | A1\n",
      "14730502 | B1\n",
      "14622912 | A1\n",
      "19994915 | A2\n",
      "19326203 | A1\n",
      "26738112 | None\n",
      "19994915 | A2\n",
      "21650497 | A1\n",
      "19994915 | A2\n",
      "19994915 | A2\n",
      "19994915 | A2\n",
      "2667193X | None\n",
      "12019712 | A2\n",
      "10788956 | A1\n",
      "17417015 | A1\n",
      "15671348 | A2\n",
      "00280836 | A1\n",
      "20411723 | A1\n",
      "19352735 | A1\n",
      "20760817 | A3\n",
      "01634453 | A1\n",
      "01466615 | B1\n",
      "01466615 | B1\n",
      "19352735 | A1\n",
      "22962565 | A1\n",
      "15671348 | A2\n",
      "10806040 | A1\n",
      "20760817 | A3\n",
      "23993642 | A2\n",
      "19313128 | A1\n",
      "01466615 | B1\n",
      "12019712 | A2\n",
      "19994915 | A2\n",
      "22221751 | A1\n",
      "00207292 | A4\n",
      "20477724 | A4\n",
      "17563305 | A2\n",
      "13674803 | A1\n",
      "20461402 | A3\n",
      "20461402 | A3\n",
      "21678359 | A2\n",
      "22221751 | A1\n",
      "15537374 | A1\n",
      "22111247 | A1\n",
      "19326203 | A1\n",
      "19352735 | A1\n",
      "20734425 | A3\n",
      "20760817 | A3\n",
      "00221899 | A1\n",
      "25328689 | None\n",
      "12019712 | A2\n",
      "25956647 | C\n",
      "16789849 | B1\n",
      "19352735 | A1\n",
      "19290748 | C\n",
      "19352735 | A1\n",
      "25724754 | A4\n",
      "19326203 | A1\n",
      "23999772 | None\n",
      "14733099 | A1\n",
      "19352735 | A1\n",
      "07391102 | C\n",
      "15671348 | A2\n",
      "21650497 | A1\n",
      "08892229 | B2\n",
      "1664302X | A2\n",
      "0022538X | A1\n",
      "0022510X | A3\n",
      "1198743X | A1\n",
      "22352988 | A2\n",
      "12019712 | A2\n",
      "17512816 | C\n",
      "21573999 | A2\n",
      "19994915 | A2\n",
      "14602059 | A1\n",
      "00368075 | A1\n",
      "1570162X | B1\n",
      "00280836 | A1\n",
      "17542189 | A1\n",
      "00740276 | A4\n",
      "02197200 | B1\n",
      "2050084X | A1\n",
      "2454499X | C\n",
      "15671348 | A2\n",
      "21573999 | A2\n",
      "00034819 | A1\n",
      "19957645 | A1\n",
      "00368075 | A1\n",
      "15671348 | A2\n",
      "10434666 | A4\n",
      "20477724 | A4\n",
      "00029637 | A3\n",
      "1756994X | A1\n",
      "15671348 | A2\n",
      "08892229 | B2\n",
      "22146695 | C\n",
      "08892229 | B2\n",
      "08892229 | B2\n",
      "08892229 | B2\n",
      "1743422X | A3\n",
      "08892229 | B2\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "15537374 | A1\n",
      "1743422X | A3\n",
      "08892229 | B2\n",
      "11779322 | A2\n",
      "08892229 | B2\n",
      "0022538X | A1\n",
      "13674803 | A1\n",
      "19352735 | A1\n",
      "19326203 | A1\n",
      "08892229 | B2\n",
      "01466615 | B1\n",
      "08892229 | B2\n",
      "01466615 | B1\n",
      "08892229 | B2\n",
      "08892229 | B2\n",
      "03014851 | B3\n",
      "01466615 | B1\n",
      "01466615 | B1\n",
      "03051048 | A1\n",
      "01007203 | B1\n",
      "0016545X | None\n",
      "01466615 | B1\n",
      "01466615 | B1\n",
      "08892229 | B2\n",
      "01466615 | B1\n",
      "08892229 | B2\n",
      "08892229 | B2\n",
      "00740276 | A4\n",
      "14138670 | B1\n",
      "07415400 | A2\n",
      "15254135 | A2\n",
      "01466615 | B1\n",
      "00740276 | A4\n",
      "10204989 | A3\n",
      "02699370 | A1\n",
      "15168484 | B3\n",
      "08892229 | B2\n",
      "15346617 | None\n",
      "08892229 | B2\n",
      "08892229 | B2\n",
      "15254135 | A2\n",
      "00378682 | B1\n",
      "07374038 | A1\n",
      "         | None\n",
      "08892229 | B2\n",
      "01419838 | A2\n",
      "15254135 | A2\n",
      "19844689 | B1\n",
      "27094715 | C\n",
      "00311049 | B2\n",
      "23527714 | A1\n",
      "19326203 | A1\n",
      "0269283X | A1\n",
      "18069665 | A4\n",
      "27094715 | C\n",
      "27094715 | C\n",
      "27094715 | C\n",
      "11755334 | A4\n",
      "0269283X | A1\n",
      "00222585 | A1\n",
      "2047217X | A1\n",
      "00222585 | A1\n",
      "0364152X | A2\n",
      "16789849 | B1\n",
      "00222585 | A1\n",
      "0269283X | A1\n",
      "19326203 | A1\n",
      "19818122 | A1\n",
      "13142828 | A4\n",
      "11755334 | A4\n",
      "13132970 | A4\n",
      "00222585 | A1\n",
      "00222585 | A1\n",
      "00222585 | A1\n",
      "2317269X | B2\n",
      "00074853 | A2\n",
      "00855626 | A4\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "17563305 | A2\n",
      "00222585 | A1\n",
      "00734721 | B1\n",
      "00740276 | A4\n",
      "11755334 | A4\n",
      "15178382 | A2\n",
      "23182598 | B4\n",
      "00855626 | A4\n",
      "00311049 | B2\n",
      "17563305 | A2\n",
      "16760611 | B1\n",
      "11755326 | A4\n",
      "00855626 | A4\n",
      "00445231 | A3\n",
      "08915849 | A1\n",
      "26669587 | None\n",
      "22352988 | A2\n",
      "21650497 | A1\n",
      "15216616 | A3\n",
      "10584838 | A1\n",
      "18760341 | A3\n",
      "17422094 | A1\n",
      "16643224 | A2\n",
      "15178382 | A2\n",
      "16643224 | A2\n",
      "00740276 | A4\n",
      "2730664X | None\n",
      "00951137 | A1\n",
      "2673818X | None\n",
      "2296858X | A1\n",
      "2673818X | None\n",
      "15178382 | A2\n",
      "1743422X | A3\n",
      "1664302X | A2\n",
      "19994915 | A2\n",
      "00489697 | A1\n",
      "19352735 | A1\n",
      "18651674 | A1\n",
      "09031936 | A1\n",
      "15178382 | A2\n",
      "10806059 | A1\n",
      "16789946 | B1\n",
      "2076393X | A3\n",
      "23275081 | B4\n",
      "19352735 | A1\n",
      "16129202 | A2\n",
      "01479571 | A2\n",
      "10806040 | A1\n",
      "15353141 | A2\n",
      "00029637 | A3\n",
      "10806040 | A1\n",
      "03048608 | B1\n",
      "01466615 | B1\n",
      "1743422X | A3\n",
      "00221317 | A2\n",
      "19326203 | A1\n",
      "22352988 | A2\n",
      "16789849 | B1\n",
      "00359203 | A3\n",
      "16643224 | A2\n",
      "20452322 | A1\n",
      "10271005 | None\n",
      "35261794 | None\n",
      "25268732 | C\n",
      "1664302X | A2\n",
      "26442906 | None\n",
      "00359203 | A3\n",
      "16643224 | A2\n",
      "2730664X | None\n",
      "20477724 | A4\n",
      "20452322 | A1\n",
      "09320113 | A1\n",
      "16121872 | A4\n",
      "23527714 | A1\n",
      "16788060 | A4\n",
      "19352735 | A1\n",
      "00221759 | B2\n",
      "14698161 | A1\n",
      "00359203 | A3\n",
      "19352735 | A1\n",
      "15566811 | A3\n",
      "19326203 | A1\n",
      "0001706X | A2\n",
      "00364665 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "19352735 | A1\n",
      "00378682 | B1\n",
      "1537744X | A3\n",
      "00740276 | A4\n",
      "2358291X | B1\n",
      "18631959 | A1\n",
      "17563305 | A2\n",
      "20760817 | A3\n",
      "2667114X | None\n",
      "00378682 | B1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "19326203 | A1\n",
      "0001706X | A2\n",
      "19352735 | A1\n",
      "13132970 | A4\n",
      "19352735 | A1\n",
      "16789849 | B1\n",
      "00222585 | A1\n",
      "17563305 | A2\n",
      "16789849 | B1\n",
      "17596653 | A1\n",
      "1809127X | B2\n",
      "17563305 | A2\n",
      "17580463 | A1\n",
      "00222585 | A1\n",
      "19352735 | A1\n",
      "17563305 | A2\n",
      "00740276 | A4\n",
      "00378682 | B1\n",
      "00378682 | B1\n",
      "23182598 | B4\n",
      "00740276 | A4\n",
      "15671348 | A2\n",
      "00740276 | A4\n",
      "11755326 | A4\n",
      "0001706X | A2\n",
      "00378682 | B1\n",
      "20590105 | A1\n",
      "00740276 | A4\n",
      "18760341 | A3\n",
      "16643224 | A2\n",
      "1664302X | A2\n",
      "16643224 | A2\n",
      "05315565 | A3\n",
      "01615890 | A4\n",
      "09320113 | A1\n",
      "23148861 | A3\n",
      "20452322 | A1\n",
      "16788060 | A4\n",
      "19326203 | A1\n",
      "14712164 | A2\n",
      "19352735 | A1\n",
      "01666851 | A4\n",
      "13835769 | A3\n",
      "14712164 | A2\n",
      "00740276 | A4\n",
      "00145793 | A2\n",
      "00207519 | A1\n",
      "14321955 | A1\n",
      "00144894 | A4\n",
      "01655728 | A2\n",
      "16643224 | A2\n",
      "10434666 | A4\n",
      "17422094 | A1\n",
      "24058440 | A4\n",
      "16643224 | A2\n",
      "13594184 | A1\n",
      "17422094 | A1\n",
      "07533322 | A2\n",
      "22352988 | A2\n",
      "01419838 | A2\n",
      "17422094 | A1\n",
      "1664302X | A2\n",
      "14330768 | A3\n",
      "08937648 | A1\n",
      "19313128 | A1\n",
      "08937648 | A1\n",
      "10806059 | A1\n",
      "0022510X | A3\n",
      "20452322 | A1\n",
      "14384221 | A3\n",
      "19326203 | A1\n",
      "09042512 | A1\n",
      "09388990 | A4\n",
      "14712164 | A2\n",
      "14712164 | A2\n",
      "19326203 | A1\n",
      "25722050 | None\n",
      "17487188 | A2\n",
      "16134125 | A1\n",
      "03014851 | B3\n",
      "14726890 | A4\n",
      "19326203 | A1\n",
      "00951137 | A1\n",
      "00223069 | A2\n",
      "14712164 | A2\n",
      "09699961 | A1\n",
      "00313998 | A1\n",
      "09699961 | A1\n",
      "00199567 | A2\n",
      "17417007 | A1\n",
      "00740276 | A4\n",
      "10614036 | A1\n",
      "10614036 | A1\n",
      "10889051 | A1\n",
      "09232508 | A3\n",
      "00951137 | A1\n",
      "00951137 | A1\n",
      "09232508 | A3\n",
      "09232508 | A3\n",
      "07200536 | None\n",
      "00222615 | A4\n",
      "00029270 | A1\n",
      "00236772 | A2\n",
      "20452322 | A1\n",
      "1743422X | A3\n",
      "00144894 | A4\n",
      "09724567 | B4\n",
      "15537374 | A1\n",
      "00199567 | A2\n",
      "0264410X | A2\n",
      "15675769 | A2\n",
      "24761966 | B2\n",
      "19352735 | A1\n",
      "00207519 | A1\n",
      "22303162 | None\n",
      "16643224 | A2\n",
      "08189641 | A2\n",
      "00221899 | A1\n",
      "15461718 | A1\n",
      "15537374 | A1\n",
      "00207519 | A1\n",
      "00207519 | A1\n",
      "00199567 | A2\n",
      "01419838 | A2\n",
      "15537374 | A1\n",
      "15578100 | A3\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "15537374 | A1\n",
      "01666851 | A4\n",
      "03009475 | A4\n",
      "00192805 | A1\n",
      "21577064 | B4\n",
      "10986596 | A1\n",
      "16121872 | A4\n",
      "20796382 | A3\n",
      "1743422X | A3\n",
      "1981528X | A3\n",
      "14786419 | A3\n",
      "14310651 | A4\n",
      "23940514 | None\n",
      "14310651 | A4\n",
      "18786146 | A2\n",
      "0951418X | A1\n",
      "15675769 | A2\n",
      "14310651 | A4\n",
      "00740276 | A4\n",
      "14310651 | A4\n",
      "18657125 | B1\n",
      "1096620X | A4\n",
      "13693786 | A1\n",
      "09234748 | A1\n",
      "18743900 | A3\n",
      "02505991 | A3\n",
      "14622912 | A1\n",
      "18743900 | A3\n",
      "0102695X | A3\n",
      "14786419 | A3\n",
      "00740276 | A4\n",
      "00218820 | A4\n",
      "03051978 | B1\n",
      "14322056 | None\n",
      "14310651 | A4\n",
      "00013765 | A2\n",
      "00404039 | A3\n",
      "22313184 | B3\n",
      "00953628 | A1\n",
      "00320943 | A1\n",
      "00320943 | A1\n",
      "1996087X | None\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "0102695X | A3\n",
      "17517362 | A1\n",
      "00084166 | B1\n",
      "09298673 | A1\n",
      "09298673 | A1\n",
      "14310651 | A4\n",
      "09639969 | A1\n",
      "07177917 | B2\n",
      "19960875 | B3\n",
      "14602709 | A1\n",
      "0102695X | A3\n",
      "00013765 | A2\n",
      "03345114 | A2\n",
      "0102695X | A3\n",
      "09337407 | A1\n",
      "02235234 | A1\n",
      "0102695X | A3\n",
      "0102695X | A3\n",
      "0102695X | A3\n",
      "14760711 | A3\n",
      "18080804 | C\n",
      "15178382 | A2\n",
      "03051978 | B1\n",
      "16784790 | A2\n",
      "16787064 | A4\n",
      "01035053 | A2\n",
      "01035053 | A2\n",
      "03781097 | B1\n",
      "19352735 | A1\n",
      "10765174 | B1\n",
      "16787064 | A4\n",
      "10440305 | A2\n",
      "0102695X | A3\n",
      "0102695X | A3\n",
      "09593993 | A3\n",
      "00093130 | B2\n",
      "09514198 | A3\n",
      "02775387 | A2\n",
      "0100879X | A3\n",
      "00740276 | A4\n",
      "01633864 | A1\n",
      "00319422 | A1\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00740276 | A4\n",
      "00319422 | A1\n",
      "00404039 | A3\n",
      "01633864 | A1\n",
      "00320943 | A1\n",
      "13880209 | A1\n",
      "00320943 | A1\n",
      "13880209 | A1\n",
      "09580344 | A2\n",
      "00404039 | A3\n",
      "2218273X | B2\n",
      "19994915 | A2\n",
      "19994915 | A2\n",
      "03048608 | B1\n",
      "00034819 | A1\n",
      "20760817 | A3\n",
      "13866532 | A3\n",
      "10788956 | A1\n",
      "03028933 | B1\n",
      "2296889X | A2\n",
      "15607917 | A1\n",
      "25253409 | C\n",
      "0950382X | A2\n",
      "20760817 | A3\n",
      "22962646 | A3\n",
      "15671348 | A2\n",
      "21678359 | A2\n",
      "19352735 | A1\n",
      "25253409 | C\n",
      "16643224 | A2\n",
      "22137165 | A3\n",
      "1664302X | A2\n",
      "16789849 | B1\n",
      "18743919 | A1\n",
      "19352735 | A1\n",
      "20900023 | B1\n",
      "16648021 | A4\n",
      "19994915 | A2\n",
      "19326203 | A1\n",
      "14712164 | A2\n",
      "16174615 | A3\n",
      "19352735 | A1\n",
      "16643224 | A2\n",
      "17563305 | A2\n",
      "17580463 | A1\n",
      "14712105 | A1\n",
      "15476286 | A2\n",
      "16788060 | A4\n",
      "2314436X | A3\n",
      "09248579 | A1\n",
      "14712180 | A2\n",
      "01728083 | A3\n",
      "16643224 | A2\n",
      "09651748 | A1\n",
      "10986596 | A1\n",
      "09651748 | A1\n",
      "20458827 | A3\n",
      "19326203 | A1\n",
      "16643224 | A2\n",
      "14712164 | A2\n",
      "0006291X | A2\n",
      "17563305 | A2\n",
      "14752875 | A2\n",
      "14712105 | A1\n",
      "19326203 | A1\n",
      "19326203 | A1\n",
      "14752875 | A2\n",
      "16788060 | A4\n",
      "00219193 | A3\n",
      "13602276 | A1\n",
      "00029637 | A3\n"
     ]
    }
   ],
   "source": [
    "print('\\nAdicionando Qualis Periódicos')\n",
    "from lattes_scrapper import GetQualis\n",
    "stratifier = GetQualis()\n",
    "stratifier.buscar_qualis(dom_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e65a298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo JSON salvo em: c:\\Users\\marco\\ppgcs\\_data\\in_csv\\dict_list.json\n"
     ]
    }
   ],
   "source": [
    "# Função para salvar a lista de dicionários em um arquivo .json\n",
    "def save_to_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "       \n",
    "dict_list=dom_dict_list\n",
    "filepath = os.path.join(folder_data_input,'dict_list.json')\n",
    "save_to_json(dict_list, filepath)\n",
    "\n",
    "print(f\"Arquivo JSON salvo em: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11ae091a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 currículos buscados\n",
      "55 currículos extraídos com sucesso\n",
      "(01) Alessandra Aparecida Guarneri\n",
      "(02) Alexandre de Magalhaes Vieira Machado\n",
      "(03) Ana Lúcia Teles Rabello\n",
      "(04) Andréa Teixeira de Carvalho\n",
      "(05) Carlos Eduardo Calzavara Silva\n",
      "(06) Caroline Furtado Junqueira\n",
      "(07) Celia Maria Ferreira Gontijo\n",
      "(08) Cristiana Ferreira Alves de Brito\n",
      "(09) Cristina Toscano Fonseca\n",
      "(10) Edelberto Santos Dias\n",
      "(11) Edward José de Oliveira\n",
      "(12) Flora Satiko Kano\n",
      "(13) Glaucia Fernandes Cota\n",
      "(14) Jaquelline Germano de Oliveira\n",
      "(15) Jeronimo Conceição Ruiz\n",
      "(16) José Dilermando Andrade Filho\n",
      "(17) Lileia Gonçalves Diotaiuti\n",
      "(18) Lis Ribeiro do Valle Antonelli\n",
      "(19) Luciano Andrade Moreira\n",
      "(20) Luzia Helena Carvalho\n",
      "(21) Marcelo Antonio Pascoal Xavier\n",
      "(22) Marcelo Gustavo Lorenzo\n",
      "(23) Márcio Sobreira Silva Araújo\n",
      "(24) Marco Antônio da Silva Campos\n",
      "(25) Marina de Moraes Mourão\n",
      "(26) Nagila Francinete Costa Secundino\n",
      "(27) Olindo Assis Martins Filho\n",
      "(28) Paulo Filemon Paolucci Pimenta\n",
      "(29) Paulo Marcos Zech Coelho\n",
      "(30) Ricardo Tostes Gazzinelli\n",
      "(31) Roberta Lima Caldeira\n",
      "(32) Rodrigo Correa de Oliveira\n",
      "(33) Rodrigo Pedro Pinto Soares\n",
      "(34) Rubens Lima do Monte Neto\n",
      "(35) Silvane Maria Fonseca Murta\n",
      "(36) Taís Nóbrega de Sousa\n",
      "(37) Vanessa Peruhype Magalhães Pascoal\n",
      "(38) Álvaro Gil Araújo Ferreira\n",
      "(39) Carina Margonari de Souza\n",
      "(40) Cristiana Couto Garcia\n",
      "(41) Daniel Moreira de Avelar\n",
      "(42) Érica Alessandra Rocha Alves\n",
      "(43) Erika Michalsky Monteiro\n",
      "(44) Gabriel da Rocha Fernandes\n",
      "(45) Gustavo Fontes Paz\n",
      "(46) Luiz Carlos Júnior Alcantara\n",
      "(47) Paloma Helena Fernandes Shimabukuro\n",
      "(48) Pedro Augusto Alves\n",
      "(49) Rafaella Fortini Grenfell e Queiroz\n",
      "(50) Rita de Cássia Moreira de Souza\n",
      "(51) Rosiane Aparecida da Silva Pereira\n",
      "(52) Roney Santos Coimbra\n",
      "(53) Soraya Torres Gaze Jangola\n",
      "(54) Tânia Maria de Almeida Alves\n",
      "(55) Antonio Mauro Rezende\n",
      "\n",
      "0 currículos não extraídos\n"
     ]
    }
   ],
   "source": [
    "lista_restante = lista_busca[:]\n",
    "print(f'{len(lista_busca)} currículos buscados')\n",
    "print(f'{len(dom_dict_list)} currículos extraídos com sucesso')\n",
    "total_extraidos = 0\n",
    "total_nao_extraidos = 0\n",
    "\n",
    "# Função para normalizar os nomes\n",
    "def normalizar_nome(nome):\n",
    "    # Normaliza o nome para comparar de forma mais flexível\n",
    "    nome_normalizado = nome.lower().replace('  ', ' ').replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u').replace('â', 'a').replace('ê', 'e').replace('ô', 'o').replace('ã', 'a').replace('õ', 'o').replace('ç', 'c').replace('ü', 'u')\n",
    "    return nome_normalizado\n",
    "\n",
    "# Lista para armazenar nomes extraídos com sucesso\n",
    "nomes_extraidos = []\n",
    "\n",
    "for i in dom_dict_list:\n",
    "    nome = i.get('Identificação').get('Nome')\n",
    "    nome_normalizado = normalizar_nome(nome)\n",
    "    encontrado = False\n",
    "    lista_restante = list(lista_restante)\n",
    "\n",
    "    # Verifica se o nome ou uma forma similar já foi extraído\n",
    "    for nome_original in lista_restante:\n",
    "        nome_original_normalizado = normalizar_nome(nome_original)\n",
    "        if nome_original_normalizado == nome_normalizado:\n",
    "            lista_restante.remove(nome_original)\n",
    "            encontrado = True\n",
    "            break\n",
    "\n",
    "    # Incrementa o contador correspondente\n",
    "    if encontrado:\n",
    "        total_extraidos += 1\n",
    "    else:\n",
    "        total_nao_extraidos += 1\n",
    "\n",
    "    # Imprime o status da extração\n",
    "    if encontrado:\n",
    "        print(f'({total_extraidos:>02}) {nome}')\n",
    "    else:\n",
    "        print(f'({total_nao_extraidos}) Não extraído: {nome}')\n",
    "    # Adiciona à lista de nomes extraídos apenas se não for exatamente igual ao buscado\n",
    "    if encontrado and nome_original_normalizado != nome_normalizado:\n",
    "        nomes_extraidos.append(nome)\n",
    "\n",
    "print(f'\\n{len(lista_restante)} currículos não extraídos')\n",
    "\n",
    "for i in lista_restante:\n",
    "    print(f'   {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd6fb2",
   "metadata": {},
   "source": [
    "### Visualizar resumo quantitativo de artigos extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6dfc8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 dicionários montados\n",
      " 0C 057A 057T Dif:000 None \n",
      " 1C 034A 034T Dif:000 None \n",
      " 2C 149A 149T Dif:000 None \n",
      " 3C 268A 268T Dif:000 None \n",
      " 4C 040A 040T Dif:000 None \n",
      " 5C 030A 030T Dif:000 None \n",
      " 6C 107A 107T Dif:000 None \n",
      " 7C 081A 081T Dif:000 None \n",
      " 8C 057A 057T Dif:000 None \n",
      " 9C 089A 089T Dif:000 None \n",
      "10C 062A 062T Dif:000 None \n",
      "11C 042A 042T Dif:000 None \n",
      "12C 082A 082T Dif:000 None \n",
      "13C 039A 039T Dif:000 None \n",
      "14C 070A 070T Dif:000 None \n",
      "15C 131A 131T Dif:000 None \n",
      "16C 154A 154T Dif:000 None \n",
      "17C 084A 084T Dif:000 None \n",
      "18C 090A 090T Dif:000 None \n",
      "19C 074A 074T Dif:000 None \n",
      "20C 053A 053T Dif:000 None \n",
      "21C 075A 075T Dif:000 None \n",
      "22C 082A 082T Dif:000 None \n",
      "23C 052A 052T Dif:000 None \n",
      "24C 048A 048T Dif:000 None \n",
      "25C 074A 074T Dif:000 None \n",
      "26C 473A 473T Dif:000 None \n",
      "27C 164A 164T Dif:000 None \n",
      "28C 239A 239T Dif:000 None \n",
      "29C 302A 302T Dif:000 None \n",
      "30C 081A 081T Dif:000 None \n",
      "31C 361A 361T Dif:000 None \n",
      "32C 111A 111T Dif:000 None \n",
      "33C 055A 055T Dif:000 None \n",
      "34C 117A 117T Dif:000 None \n",
      "35C 043A 043T Dif:000 None \n",
      "36C 078A 078T Dif:000 None \n",
      "37C 014A 014T Dif:000 None \n",
      "38C 033A 033T Dif:000 None \n",
      "39C 053A 053T Dif:000 None \n",
      "40C 035A 035T Dif:000 None \n",
      "41C 017A 017T Dif:000 None \n",
      "42C 039A 039T Dif:000 None \n",
      "43C 083A 083T Dif:000 None \n",
      "44C 039A 039T Dif:000 None \n",
      "45C 195A 195T Dif:000 None \n",
      "46C 046A 046T Dif:000 None \n",
      "47C 042A 042T Dif:000 None \n",
      "48C 035A 035T Dif:000 None \n",
      "49C 034A 034T Dif:000 None \n",
      "50C 023A 023T Dif:000 None \n",
      "51C 054A 054T Dif:000 None \n",
      "52C 029A 029T Dif:000 None \n",
      "53C 094A 094T Dif:000 None \n",
      "54C 061A 061T Dif:000 None \n",
      "\n",
      "Total de artigos em todos períodos: 5074\n",
      "Total de títulos em todos períodos: 5074\n"
     ]
    }
   ],
   "source": [
    "## Contagem de artigos para simples confererência\n",
    "print(f'{len(dict_list)} dicionários montados')\n",
    "qte_artigos=0\n",
    "qte_titulos=0\n",
    "for k,i in enumerate(dict_list):\n",
    "    try:\n",
    "        qte_jcr = len(i.get('Produções').get('Artigos completos publicados em periódicos'))\n",
    "    except:\n",
    "        qte_jcr = 0\n",
    "    try:\n",
    "       qte_jcr2 = len(i['JCR2'])\n",
    "    except:\n",
    "       qte_jcr2 = 0\n",
    "    qte_artigos+=qte_jcr\n",
    "    qte_titulos+=qte_jcr2\n",
    "    status=qte_jcr2-qte_jcr\n",
    "    print(f\"{k:>2}C {qte_jcr:>03}A {qte_jcr2:>03}T Dif:{status:>03} {i.get('Identificação').get('name')} \")\n",
    "\n",
    "print(f'\\nTotal de artigos em todos períodos: {qte_artigos}')\n",
    "print(f'Total de títulos em todos períodos: {qte_titulos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d68fcc",
   "metadata": {},
   "source": [
    "### Extrair e salvar dados brutos da extração de remanescentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "806ae501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alessandra Aparecida Guarneri\n",
      "Alexandre de Magalhaes Vieira Machado\n",
      "Ana Lúcia Teles Rabello\n",
      "Andréa Teixeira de Carvalho\n",
      "Carlos Eduardo Calzavara Silva\n",
      "Caroline Furtado Junqueira\n",
      "Celia Maria Ferreira Gontijo\n",
      "Cristiana Ferreira Alves de Brito\n",
      "Cristina Toscano Fonseca\n",
      "Edelberto Santos Dias\n",
      "Edward José de Oliveira\n",
      "Flora Satiko Kano\n",
      "Glaucia Fernandes Cota\n",
      "Jaquelline Germano de Oliveira\n",
      "Jeronimo Conceição Ruiz\n",
      "José Dilermando Andrade Filho\n",
      "Lileia Gonçalves Diotaiuti\n",
      "Lis Ribeiro do Valle Antonelli\n",
      "Luciano Andrade Moreira\n",
      "Luzia Helena Carvalho\n",
      "Marcelo Antonio Pascoal Xavier\n",
      "Marcelo Gustavo Lorenzo\n",
      "Márcio Sobreira Silva Araújo\n",
      "Marco Antônio da Silva Campos\n",
      "Marina de Moraes Mourão\n",
      "Nagila Francinete Costa Secundino\n",
      "Olindo Assis Martins Filho\n",
      "Paulo Filemon Paolucci Pimenta\n",
      "Paulo Marcos Zech Coelho\n",
      "Ricardo Tostes Gazzinelli\n",
      "Roberta Lima Caldeira\n",
      "Rodrigo Correa de Oliveira\n",
      "Rodrigo Pedro Pinto Soares\n",
      "Rubens Lima do Monte Neto\n",
      "Silvane Maria Fonseca Murta\n",
      "Taís Nóbrega de Sousa\n",
      "Vanessa Peruhype Magalhães Pascoal\n",
      "Álvaro Gil Araújo Ferreira\n",
      "Carina Margonari de Souza\n",
      "Cristiana Couto Garcia\n",
      "Daniel Moreira de Avelar\n",
      "Érica Alessandra Rocha Alves\n",
      "Erika Michalsky Monteiro\n",
      "Gabriel da Rocha Fernandes\n",
      "Gustavo Fontes Paz\n",
      "Luiz Carlos Júnior Alcantara\n",
      "Paloma Helena Fernandes Shimabukuro\n",
      "Pedro Augusto Alves\n",
      "Rafaella Fortini Grenfell e Queiroz\n",
      "Rita de Cássia Moreira de Souza\n",
      "Rosiane Aparecida da Silva Pereira\n",
      "Roney Santos Coimbra\n",
      "Soraya Torres Gaze Jangola\n",
      "Tânia Maria de Almeida Alves\n",
      "Antonio Mauro Rezende\n"
     ]
    }
   ],
   "source": [
    "for i in dict_list:\n",
    "    print(i.get('Identificação').get('Nome'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35f55716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_restante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7eb2ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iniciar a extração de currículos remanescentes\n",
    "# t0 = time.time()\n",
    "\n",
    "# # Extrai e monta a lista de dicionários\n",
    "# dict_list_1 = scraper.scrape(lista_restante, instituicao, termo1, termo2, termo3,\n",
    "#                             pasta_dados+\"dicts_list1.json\", \n",
    "#                             pasta_dados+\"dicts_list1.hdf5\")\n",
    "\n",
    "# print(f'\\n{preparer.tempo(t0,time.time())} para busca de {len(lista_restante)} nomes com extração de dados de {len(dict_list_1)} dicionários')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b17011f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenando os dicionários\n",
    "# dict_concatenado = dom_dict_list.copy()  # Cria uma cópia de dict1 para não modificar o original\n",
    "# dict_concatenado.update(dom_dict_list_1)   # Atualiza dict_concatenado com os itens de dict2        \n",
    "# dict_list=dict_concatenado\n",
    "# pasta_dados = os.path.join(find_repo_root(os.getcwd()),'data','output','dict_list.json')\n",
    "# save_to_json(dict_list, pasta_dados)\n",
    "\n",
    "# print(f\"Arquivo JSON salvo em: {pasta_dados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b193bf",
   "metadata": {},
   "source": [
    "# <b>F03: Carregar JSON salvo</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dcfe43",
   "metadata": {},
   "source": [
    "### Visualizar arquivos gerados previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00a8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, pytz, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Sempre construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(base_repo_dir, 'utils')\n",
    "folder_domain = os.path.join(base_repo_dir, 'source', 'domain')\n",
    "folder_data_input = os.path.join(base_repo_dir, '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(base_repo_dir, 'data', 'output')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "from scraper_pasteur import PasteurScraper\n",
    "from environment_setup import EnvironmentSetup\n",
    "from scraper_sucupira import SucupiraScraper\n",
    "from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DiscentCollaborationCounter, DictToHDF5, attribute_to_be_non_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a8e183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos disponíveis na pasta para dados de entrada:\n",
      "  dict_list.json\n",
      "\n",
      "55 currículos carregados na lista de dicionários 'dict_list.json'\n",
      "Arquivo criado inicialmente em 14/04/2024 18:28:18 carregado com sucesso\n",
      "Extração realizada em 19/04/2024 19:31:28 a 21.9 horas\n"
     ]
    }
   ],
   "source": [
    "# Carregar o conteúdo do arquivo 'dict_list.json' para a variável dict_list\n",
    "jfm = JSONFileManager()\n",
    "jfm.list_json(folder_data_input)\n",
    "filename = 'dict_list.json'\n",
    "\n",
    "dict_list, formatted_creation_date, formatted_modification_date, time_count, unit = jfm.load_from_json(os.path.join(folder_data_input,filename))\n",
    "print(f\"\\n{len(dict_list)} currículos carregados na lista de dicionários '{filename}'\")\n",
    "print(f\"Arquivo criado inicialmente em {formatted_creation_date} carregado com sucesso\")\n",
    "print(f\"Extração realizada em {formatted_modification_date} a {time_count} {unit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143a095",
   "metadata": {},
   "source": [
    "### Quantidade de artigos e defasagem de atualização do currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d5a2aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>id_lattes</th>\n",
       "      <th>curriculos</th>\n",
       "      <th>ultima_atualizacao</th>\n",
       "      <th>dias_defasagem</th>\n",
       "      <th>qte_artigos_periodicos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5565463591721568</td>\n",
       "      <td>Alessandra Aparecida Guarneri</td>\n",
       "      <td>14/03/2024</td>\n",
       "      <td>37</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5804297852066640</td>\n",
       "      <td>Alexandre de Magalhaes Vieira Machado</td>\n",
       "      <td>21/09/2022</td>\n",
       "      <td>577</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7027792126979693</td>\n",
       "      <td>Ana Lúcia Teles Rabello</td>\n",
       "      <td>22/06/2023</td>\n",
       "      <td>303</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3798623797837575</td>\n",
       "      <td>Andréa Teixeira de Carvalho</td>\n",
       "      <td>21/02/2024</td>\n",
       "      <td>59</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8454953908544325</td>\n",
       "      <td>Carlos Eduardo Calzavara Silva</td>\n",
       "      <td>21/03/2024</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9405337414047265</td>\n",
       "      <td>Caroline Furtado Junqueira</td>\n",
       "      <td>12/04/2024</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7004385035447880</td>\n",
       "      <td>Celia Maria Ferreira Gontijo</td>\n",
       "      <td>08/03/2024</td>\n",
       "      <td>43</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0553918889655570</td>\n",
       "      <td>Cristiana Ferreira Alves de Brito</td>\n",
       "      <td>06/03/2024</td>\n",
       "      <td>45</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7920312439713999</td>\n",
       "      <td>Cristina Toscano Fonseca</td>\n",
       "      <td>09/02/2024</td>\n",
       "      <td>71</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0492208637289837</td>\n",
       "      <td>Edelberto Santos Dias</td>\n",
       "      <td>01/04/2024</td>\n",
       "      <td>19</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4533303011981452</td>\n",
       "      <td>Edward José de Oliveira</td>\n",
       "      <td>17/01/2024</td>\n",
       "      <td>94</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8688961957105096</td>\n",
       "      <td>Flora Satiko Kano</td>\n",
       "      <td>16/04/2024</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9894162318774616</td>\n",
       "      <td>Glaucia Fernandes Cota</td>\n",
       "      <td>09/04/2024</td>\n",
       "      <td>11</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2427045424896409</td>\n",
       "      <td>Jaquelline Germano de Oliveira</td>\n",
       "      <td>10/04/2024</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7769547194874833</td>\n",
       "      <td>Jeronimo Conceição Ruiz</td>\n",
       "      <td>18/03/2024</td>\n",
       "      <td>33</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7374065738760207</td>\n",
       "      <td>José Dilermando Andrade Filho</td>\n",
       "      <td>15/04/2024</td>\n",
       "      <td>5</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9211345386050612</td>\n",
       "      <td>Lileia Gonçalves Diotaiuti</td>\n",
       "      <td>01/08/2023</td>\n",
       "      <td>263</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4929168581709218</td>\n",
       "      <td>Lis Ribeiro do Valle Antonelli</td>\n",
       "      <td>18/03/2024</td>\n",
       "      <td>33</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5832739049925970</td>\n",
       "      <td>Luciano Andrade Moreira</td>\n",
       "      <td>01/11/2023</td>\n",
       "      <td>171</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5305938120287353</td>\n",
       "      <td>Luzia Helena Carvalho</td>\n",
       "      <td>15/03/2024</td>\n",
       "      <td>36</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2430268236881898</td>\n",
       "      <td>Marcelo Antonio Pascoal Xavier</td>\n",
       "      <td>02/04/2024</td>\n",
       "      <td>18</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2529945687863385</td>\n",
       "      <td>Marcelo Gustavo Lorenzo</td>\n",
       "      <td>13/02/2023</td>\n",
       "      <td>432</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2634004612117298</td>\n",
       "      <td>Márcio Sobreira Silva Araújo</td>\n",
       "      <td>02/04/2024</td>\n",
       "      <td>18</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1409006491220426</td>\n",
       "      <td>Marco Antônio da Silva Campos</td>\n",
       "      <td>15/04/2024</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0271842903270328</td>\n",
       "      <td>Marina de Moraes Mourão</td>\n",
       "      <td>10/04/2024</td>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0641258278314799</td>\n",
       "      <td>Nagila Francinete Costa Secundino</td>\n",
       "      <td>19/04/2024</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6260226537155026</td>\n",
       "      <td>Olindo Assis Martins Filho</td>\n",
       "      <td>10/04/2024</td>\n",
       "      <td>10</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4592140991723664</td>\n",
       "      <td>Paulo Filemon Paolucci Pimenta</td>\n",
       "      <td>08/11/2023</td>\n",
       "      <td>164</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6831626214829585</td>\n",
       "      <td>Paulo Marcos Zech Coelho</td>\n",
       "      <td>01/02/2024</td>\n",
       "      <td>79</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9148354106985023</td>\n",
       "      <td>Ricardo Tostes Gazzinelli</td>\n",
       "      <td>22/02/2024</td>\n",
       "      <td>58</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1735242943698067</td>\n",
       "      <td>Roberta Lima Caldeira</td>\n",
       "      <td>08/02/2024</td>\n",
       "      <td>72</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8712666530716107</td>\n",
       "      <td>Rodrigo Correa de Oliveira</td>\n",
       "      <td>30/06/2023</td>\n",
       "      <td>295</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5557656124686682</td>\n",
       "      <td>Rodrigo Pedro Pinto Soares</td>\n",
       "      <td>19/04/2024</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2817016666672297</td>\n",
       "      <td>Rubens Lima do Monte Neto</td>\n",
       "      <td>03/04/2024</td>\n",
       "      <td>17</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8285043752158198</td>\n",
       "      <td>Silvane Maria Fonseca Murta</td>\n",
       "      <td>03/04/2024</td>\n",
       "      <td>17</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6108351267696165</td>\n",
       "      <td>Taís Nóbrega de Sousa</td>\n",
       "      <td>11/04/2024</td>\n",
       "      <td>9</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>9698329171140813</td>\n",
       "      <td>Vanessa Peruhype Magalhães Pascoal</td>\n",
       "      <td>05/02/2024</td>\n",
       "      <td>75</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2621628661003591</td>\n",
       "      <td>Álvaro Gil Araújo Ferreira</td>\n",
       "      <td>03/04/2024</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1722996582628218</td>\n",
       "      <td>Carina Margonari de Souza</td>\n",
       "      <td>30/01/2024</td>\n",
       "      <td>81</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1125391664500711</td>\n",
       "      <td>Cristiana Couto Garcia</td>\n",
       "      <td>03/04/2024</td>\n",
       "      <td>17</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3544791535477213</td>\n",
       "      <td>Daniel Moreira de Avelar</td>\n",
       "      <td>06/11/2023</td>\n",
       "      <td>166</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>9853093673691832</td>\n",
       "      <td>Érica Alessandra Rocha Alves</td>\n",
       "      <td>22/03/2024</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1476232727069800</td>\n",
       "      <td>Erika Michalsky Monteiro</td>\n",
       "      <td>22/09/2023</td>\n",
       "      <td>211</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>8989178759075946</td>\n",
       "      <td>Gabriel da Rocha Fernandes</td>\n",
       "      <td>19/04/2024</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>7529793133786376</td>\n",
       "      <td>Gustavo Fontes Paz</td>\n",
       "      <td>20/03/2024</td>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>7428560072021675</td>\n",
       "      <td>Luiz Carlos Júnior Alcantara</td>\n",
       "      <td>02/02/2024</td>\n",
       "      <td>78</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3508138338414003</td>\n",
       "      <td>Paloma Helena Fernandes Shimabukuro</td>\n",
       "      <td>11/04/2024</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6935282825820547</td>\n",
       "      <td>Pedro Augusto Alves</td>\n",
       "      <td>21/03/2024</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5508945613911003</td>\n",
       "      <td>Rafaella Fortini Grenfell e Queiroz</td>\n",
       "      <td>10/04/2024</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2586445059585013</td>\n",
       "      <td>Rita de Cássia Moreira de Souza</td>\n",
       "      <td>03/04/2024</td>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6405912413079760</td>\n",
       "      <td>Rosiane Aparecida da Silva Pereira</td>\n",
       "      <td>03/04/2024</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3118219944250108</td>\n",
       "      <td>Roney Santos Coimbra</td>\n",
       "      <td>19/03/2024</td>\n",
       "      <td>32</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2290676372091813</td>\n",
       "      <td>Soraya Torres Gaze Jangola</td>\n",
       "      <td>02/04/2024</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5633763429024541</td>\n",
       "      <td>Tânia Maria de Almeida Alves</td>\n",
       "      <td>08/04/2024</td>\n",
       "      <td>12</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>8641629843495094</td>\n",
       "      <td>Antonio Mauro Rezende</td>\n",
       "      <td>26/02/2024</td>\n",
       "      <td>54</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_lattes         curriculos                              \\\n",
       "0   5565463591721568          Alessandra Aparecida Guarneri   \n",
       "1   5804297852066640  Alexandre de Magalhaes Vieira Machado   \n",
       "2   7027792126979693                Ana Lúcia Teles Rabello   \n",
       "3   3798623797837575            Andréa Teixeira de Carvalho   \n",
       "4   8454953908544325         Carlos Eduardo Calzavara Silva   \n",
       "5   9405337414047265             Caroline Furtado Junqueira   \n",
       "6   7004385035447880           Celia Maria Ferreira Gontijo   \n",
       "7   0553918889655570      Cristiana Ferreira Alves de Brito   \n",
       "8   7920312439713999               Cristina Toscano Fonseca   \n",
       "9   0492208637289837                  Edelberto Santos Dias   \n",
       "10  4533303011981452                Edward José de Oliveira   \n",
       "11  8688961957105096                      Flora Satiko Kano   \n",
       "12  9894162318774616                 Glaucia Fernandes Cota   \n",
       "13  2427045424896409         Jaquelline Germano de Oliveira   \n",
       "14  7769547194874833                Jeronimo Conceição Ruiz   \n",
       "15  7374065738760207          José Dilermando Andrade Filho   \n",
       "16  9211345386050612             Lileia Gonçalves Diotaiuti   \n",
       "17  4929168581709218         Lis Ribeiro do Valle Antonelli   \n",
       "18  5832739049925970                Luciano Andrade Moreira   \n",
       "19  5305938120287353                  Luzia Helena Carvalho   \n",
       "20  2430268236881898         Marcelo Antonio Pascoal Xavier   \n",
       "21  2529945687863385                Marcelo Gustavo Lorenzo   \n",
       "22  2634004612117298           Márcio Sobreira Silva Araújo   \n",
       "23  1409006491220426          Marco Antônio da Silva Campos   \n",
       "24  0271842903270328                Marina de Moraes Mourão   \n",
       "25  0641258278314799      Nagila Francinete Costa Secundino   \n",
       "26  6260226537155026             Olindo Assis Martins Filho   \n",
       "27  4592140991723664         Paulo Filemon Paolucci Pimenta   \n",
       "28  6831626214829585               Paulo Marcos Zech Coelho   \n",
       "29  9148354106985023              Ricardo Tostes Gazzinelli   \n",
       "30  1735242943698067                  Roberta Lima Caldeira   \n",
       "31  8712666530716107             Rodrigo Correa de Oliveira   \n",
       "32  5557656124686682             Rodrigo Pedro Pinto Soares   \n",
       "33  2817016666672297              Rubens Lima do Monte Neto   \n",
       "34  8285043752158198            Silvane Maria Fonseca Murta   \n",
       "35  6108351267696165                  Taís Nóbrega de Sousa   \n",
       "36  9698329171140813     Vanessa Peruhype Magalhães Pascoal   \n",
       "37  2621628661003591             Álvaro Gil Araújo Ferreira   \n",
       "38  1722996582628218              Carina Margonari de Souza   \n",
       "39  1125391664500711                 Cristiana Couto Garcia   \n",
       "40  3544791535477213               Daniel Moreira de Avelar   \n",
       "41  9853093673691832           Érica Alessandra Rocha Alves   \n",
       "42  1476232727069800               Erika Michalsky Monteiro   \n",
       "43  8989178759075946             Gabriel da Rocha Fernandes   \n",
       "44  7529793133786376                     Gustavo Fontes Paz   \n",
       "45  7428560072021675           Luiz Carlos Júnior Alcantara   \n",
       "46  3508138338414003    Paloma Helena Fernandes Shimabukuro   \n",
       "47  6935282825820547                    Pedro Augusto Alves   \n",
       "48  5508945613911003    Rafaella Fortini Grenfell e Queiroz   \n",
       "49  2586445059585013        Rita de Cássia Moreira de Souza   \n",
       "50  6405912413079760     Rosiane Aparecida da Silva Pereira   \n",
       "51  3118219944250108                   Roney Santos Coimbra   \n",
       "52  2290676372091813             Soraya Torres Gaze Jangola   \n",
       "53  5633763429024541           Tânia Maria de Almeida Alves   \n",
       "54  8641629843495094                  Antonio Mauro Rezende   \n",
       "\n",
       "   ultima_atualizacao  dias_defasagem  qte_artigos_periodicos  \n",
       "0   14/03/2024          37              57                     \n",
       "1   21/09/2022         577              34                     \n",
       "2   22/06/2023         303             149                     \n",
       "3   21/02/2024          59             268                     \n",
       "4   21/03/2024          30              40                     \n",
       "5   12/04/2024           8              30                     \n",
       "6   08/03/2024          43             107                     \n",
       "7   06/03/2024          45              81                     \n",
       "8   09/02/2024          71              57                     \n",
       "9   01/04/2024          19              89                     \n",
       "10  17/01/2024          94              62                     \n",
       "11  16/04/2024           4              42                     \n",
       "12  09/04/2024          11              82                     \n",
       "13  10/04/2024          10              39                     \n",
       "14  18/03/2024          33              70                     \n",
       "15  15/04/2024           5             131                     \n",
       "16  01/08/2023         263             154                     \n",
       "17  18/03/2024          33              84                     \n",
       "18  01/11/2023         171              90                     \n",
       "19  15/03/2024          36              74                     \n",
       "20  02/04/2024          18              53                     \n",
       "21  13/02/2023         432              75                     \n",
       "22  02/04/2024          18              82                     \n",
       "23  15/04/2024           5              52                     \n",
       "24  10/04/2024          10              48                     \n",
       "25  19/04/2024           1              74                     \n",
       "26  10/04/2024          10             473                     \n",
       "27  08/11/2023         164             164                     \n",
       "28  01/02/2024          79             239                     \n",
       "29  22/02/2024          58             302                     \n",
       "30  08/02/2024          72              81                     \n",
       "31  30/06/2023         295             361                     \n",
       "32  19/04/2024           1             111                     \n",
       "33  03/04/2024          17              55                     \n",
       "34  03/04/2024          17             117                     \n",
       "35  11/04/2024           9              43                     \n",
       "36  05/02/2024          75              78                     \n",
       "37  03/04/2024          17              14                     \n",
       "38  30/01/2024          81              33                     \n",
       "39  03/04/2024          17              53                     \n",
       "40  06/11/2023         166              35                     \n",
       "41  22/03/2024          29              17                     \n",
       "42  22/09/2023         211              39                     \n",
       "43  19/04/2024           1              83                     \n",
       "44  20/03/2024          31              39                     \n",
       "45  02/02/2024          78             195                     \n",
       "46  11/04/2024           9              46                     \n",
       "47  21/03/2024          30              42                     \n",
       "48  10/04/2024          10              35                     \n",
       "49  03/04/2024          17              34                     \n",
       "50  03/04/2024          17              23                     \n",
       "51  19/03/2024          32              54                     \n",
       "52  02/04/2024          18              29                     \n",
       "53  08/04/2024          12              94                     \n",
       "54  26/02/2024          54              61                     "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "atualizador = ArticlesCounter(dict_list)\n",
    "dtf_atualizado = atualizador.extrair_data_atualizacao(dict_list)\n",
    "dtf_atualizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbd686",
   "metadata": {},
   "source": [
    "#### Contagem de artigos completos publicados em periódicos estratificada pelo Qualis Periódicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ff30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ver dados da planilha da plataforma Sucupira com dados de todo Qualis Periódicos\n",
    "# fonte_planilha = 'classificações_publicadas_todas_as_areas_avaliacao1672761192111.xlsx'\n",
    "# dados_qualis = pd.read_excel(os.path.join(LattesScraper.find_repo_root(),'_data','in_xls',fonte_planilha))\n",
    "# dados_qualis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10f2f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de Publicações por Qualis Periódicos no período  a 2024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Qualis</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>C</th>\n",
       "      <th>Não encontrado</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Autor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alessandra Aparecida Guarneri</th>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alexandre de Magalhaes Vieira Machado</th>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ana Lúcia Teles Rabello</th>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>37</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andréa Teixeira de Carvalho</th>\n",
       "      <td>68</td>\n",
       "      <td>76</td>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antonio Mauro Rezende</th>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carina Margonari de Souza</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carlos Eduardo Calzavara Silva</th>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caroline Furtado Junqueira</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Celia Maria Ferreira Gontijo</th>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cristiana Couto Garcia</th>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cristiana Ferreira Alves de Brito</th>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cristina Toscano Fonseca</th>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daniel Moreira de Avelar</th>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edelberto Santos Dias</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edward José de Oliveira</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Erika Michalsky Monteiro</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flora Satiko Kano</th>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gabriel da Rocha Fernandes</th>\n",
       "      <td>39</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glaucia Fernandes Cota</th>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gustavo Fontes Paz</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaquelline Germano de Oliveira</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jeronimo Conceição Ruiz</th>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>José Dilermando Andrade Filho</th>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lileia Gonçalves Diotaiuti</th>\n",
       "      <td>27</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lis Ribeiro do Valle Antonelli</th>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luciano Andrade Moreira</th>\n",
       "      <td>44</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luiz Carlos Júnior Alcantara</th>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luzia Helena Carvalho</th>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marcelo Antonio Pascoal Xavier</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marcelo Gustavo Lorenzo</th>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marco Antônio da Silva Campos</th>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marina de Moraes Mourão</th>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Márcio Sobreira Silva Araújo</th>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nagila Francinete Costa Secundino</th>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Olindo Assis Martins Filho</th>\n",
       "      <td>108</td>\n",
       "      <td>111</td>\n",
       "      <td>84</td>\n",
       "      <td>69</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paloma Helena Fernandes Shimabukuro</th>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paulo Filemon Paolucci Pimenta</th>\n",
       "      <td>59</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paulo Marcos Zech Coelho</th>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>63</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pedro Augusto Alves</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rafaella Fortini Grenfell e Queiroz</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ricardo Tostes Gazzinelli</th>\n",
       "      <td>169</td>\n",
       "      <td>66</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rita de Cássia Moreira de Souza</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roberta Lima Caldeira</th>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rodrigo Correa de Oliveira</th>\n",
       "      <td>87</td>\n",
       "      <td>56</td>\n",
       "      <td>62</td>\n",
       "      <td>47</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rodrigo Pedro Pinto Soares</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roney Santos Coimbra</th>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rosiane Aparecida da Silva Pereira</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rubens Lima do Monte Neto</th>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Silvane Maria Fonseca Murta</th>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soraya Torres Gaze Jangola</th>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taís Nóbrega de Sousa</th>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tânia Maria de Almeida Alves</th>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanessa Peruhype Magalhães Pascoal</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Álvaro Gil Araújo Ferreira</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Érica Alessandra Rocha Alves</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Qualis                                 A1   A2   A3  A4  B1  B2  B3  B4  C  \\\n",
       "Autor                                                                        \n",
       "Alessandra Aparecida Guarneri           35   12   3   6   1   0  0   0   0   \n",
       "Alexandre de Magalhaes Vieira Machado   13   15   3   0   0   0  0   1   0   \n",
       "Ana Lúcia Teles Rabello                 43   10  32  37  22   0  1   0   0   \n",
       "Andréa Teixeira de Carvalho             68   76  45  40  12  15  2   1   3   \n",
       "Antonio Mauro Rezende                   19   21  11   3   4   1  0   0   2   \n",
       "Carina Margonari de Souza               11    5   3   7   4   0  1   0   0   \n",
       "Carlos Eduardo Calzavara Silva          15   11   4   6   2   0  0   1   0   \n",
       "Caroline Furtado Junqueira              21    2   1   0   1   0  0   0   0   \n",
       "Celia Maria Ferreira Gontijo            30   23   9  14   4   4  1   1   2   \n",
       "Cristiana Couto Garcia                  21   17   5   6   0   1  0   0   0   \n",
       "Cristiana Ferreira Alves de Brito       24   25   7  16   1   1  1   0   1   \n",
       "Cristina Toscano Fonseca                 5   25  12   8   6   0  0   0   0   \n",
       "Daniel Moreira de Avelar                14    7   4   2   4   1  0   0   1   \n",
       "Edelberto Santos Dias                   15   10   6  27  25   1  0   0   1   \n",
       "Edward José de Oliveira                 16    8  10  12  12   1  0   1   0   \n",
       "Erika Michalsky Monteiro                10    6   4   5  11   1  0   0   1   \n",
       "Flora Satiko Kano                       20    9   1   9   0   2  0   0   0   \n",
       "Gabriel da Rocha Fernandes              39   18   6   3   2   3  0   0   0   \n",
       "Glaucia Fernandes Cota                  36   10   7   6  14   8  0   0   0   \n",
       "Gustavo Fontes Paz                      16    8   3   2   6   0  0   0   1   \n",
       "Jaquelline Germano de Oliveira           9    9   8   6   1   1  0   0   0   \n",
       "Jeronimo Conceição Ruiz                 23   22   7  10   3   0  0   0   0   \n",
       "José Dilermando Andrade Filho           33   20   3  49  14   1  1   3   2   \n",
       "Lileia Gonçalves Diotaiuti              27   33   9  36  30   2  0   4   2   \n",
       "Lis Ribeiro do Valle Antonelli          41   21  14   3   0   2  2   0   0   \n",
       "Luciano Andrade Moreira                 44   22   4  12   3   1  0   0   0   \n",
       "Luiz Carlos Júnior Alcantara            62   49  16  11  21  20  2   1   6   \n",
       "Luzia Helena Carvalho                   29   19  11   8   1   2  0   0   0   \n",
       "Marcelo Antonio Pascoal Xavier          15   12   4   7   6   6  1   0   2   \n",
       "Marcelo Gustavo Lorenzo                 31   25   6  10   2   0  0   0   0   \n",
       "Marco Antônio da Silva Campos           21    7   9   2   6   2  0   1   0   \n",
       "Marina de Moraes Mourão                 15   18   1   8   2   1  1   0   1   \n",
       "Márcio Sobreira Silva Araújo            25   23   9  11   2   7  1   2   1   \n",
       "Nagila Francinete Costa Secundino       32   23   2   5   2   0  1   0   1   \n",
       "Olindo Assis Martins Filho             108  111  84  69  34  33  8   5   5   \n",
       "Paloma Helena Fernandes Shimabukuro     17    5   1  11   4   3  0   1   4   \n",
       "Paulo Filemon Paolucci Pimenta          59   34   8  27  10   1  2   0   0   \n",
       "Paulo Marcos Zech Coelho                37   21  27  63  76   3  0   1   1   \n",
       "Pedro Augusto Alves                     15   12   6   1   3   0  0   1   0   \n",
       "Rafaella Fortini Grenfell e Queiroz      9    5   5   6   4   1  0   0   1   \n",
       "Ricardo Tostes Gazzinelli              169   66  28  16   3   4  0   0   0   \n",
       "Rita de Cássia Moreira de Souza          9    7   1   7   7   1  0   1   0   \n",
       "Roberta Lima Caldeira                   10   19   3  33   9   0  1   2   1   \n",
       "Rodrigo Correa de Oliveira              87   56  62  47  14   2  0   0   0   \n",
       "Rodrigo Pedro Pinto Soares              31   31  15  23   3   4  0   0   1   \n",
       "Roney Santos Coimbra                    25   14   6   6   0   0  1   0   0   \n",
       "Rosiane Aparecida da Silva Pereira       7    6   4   6   0   0  0   0   0   \n",
       "Rubens Lima do Monte Neto               15   14   7  12   3   1  0   0   0   \n",
       "Silvane Maria Fonseca Murta             29   36   9  35   6   1  0   0   0   \n",
       "Soraya Torres Gaze Jangola              14    7   2   3   0   1  0   1   0   \n",
       "Taís Nóbrega de Sousa                   17   18   2   5   0   0  0   0   0   \n",
       "Tânia Maria de Almeida Alves            24   12  24  19   6   2  2   1   1   \n",
       "Vanessa Peruhype Magalhães Pascoal      11   23  16  11   1  11  0   0   2   \n",
       "Álvaro Gil Araújo Ferreira               9    3   1   1   0   0  0   0   0   \n",
       "Érica Alessandra Rocha Alves             5    6   3   0   0   0  0   3   0   \n",
       "\n",
       "Qualis                                 Não encontrado  \n",
       "Autor                                                  \n",
       "Alessandra Aparecida Guarneri           0              \n",
       "Alexandre de Magalhaes Vieira Machado   2              \n",
       "Ana Lúcia Teles Rabello                 4              \n",
       "Andréa Teixeira de Carvalho             6              \n",
       "Antonio Mauro Rezende                   0              \n",
       "Carina Margonari de Souza               2              \n",
       "Carlos Eduardo Calzavara Silva          1              \n",
       "Caroline Furtado Junqueira              5              \n",
       "Celia Maria Ferreira Gontijo           19              \n",
       "Cristiana Couto Garcia                  3              \n",
       "Cristiana Ferreira Alves de Brito       5              \n",
       "Cristina Toscano Fonseca                1              \n",
       "Daniel Moreira de Avelar                2              \n",
       "Edelberto Santos Dias                   4              \n",
       "Edward José de Oliveira                 2              \n",
       "Erika Michalsky Monteiro                1              \n",
       "Flora Satiko Kano                       1              \n",
       "Gabriel da Rocha Fernandes             12              \n",
       "Glaucia Fernandes Cota                  1              \n",
       "Gustavo Fontes Paz                      3              \n",
       "Jaquelline Germano de Oliveira          5              \n",
       "Jeronimo Conceição Ruiz                 5              \n",
       "José Dilermando Andrade Filho           5              \n",
       "Lileia Gonçalves Diotaiuti             11              \n",
       "Lis Ribeiro do Valle Antonelli          1              \n",
       "Luciano Andrade Moreira                 4              \n",
       "Luiz Carlos Júnior Alcantara            7              \n",
       "Luzia Helena Carvalho                   4              \n",
       "Marcelo Antonio Pascoal Xavier          0              \n",
       "Marcelo Gustavo Lorenzo                 1              \n",
       "Marco Antônio da Silva Campos           4              \n",
       "Marina de Moraes Mourão                 1              \n",
       "Márcio Sobreira Silva Araújo            1              \n",
       "Nagila Francinete Costa Secundino       8              \n",
       "Olindo Assis Martins Filho             16              \n",
       "Paloma Helena Fernandes Shimabukuro     0              \n",
       "Paulo Filemon Paolucci Pimenta         23              \n",
       "Paulo Marcos Zech Coelho               10              \n",
       "Pedro Augusto Alves                     4              \n",
       "Rafaella Fortini Grenfell e Queiroz     4              \n",
       "Ricardo Tostes Gazzinelli              16              \n",
       "Rita de Cássia Moreira de Souza         1              \n",
       "Roberta Lima Caldeira                   3              \n",
       "Rodrigo Correa de Oliveira             93              \n",
       "Rodrigo Pedro Pinto Soares              3              \n",
       "Roney Santos Coimbra                    2              \n",
       "Rosiane Aparecida da Silva Pereira      0              \n",
       "Rubens Lima do Monte Neto               3              \n",
       "Silvane Maria Fonseca Murta             1              \n",
       "Soraya Torres Gaze Jangola              1              \n",
       "Taís Nóbrega de Sousa                   1              \n",
       "Tânia Maria de Almeida Alves            3              \n",
       "Vanessa Peruhype Magalhães Pascoal      3              \n",
       "Álvaro Gil Araújo Ferreira              0              \n",
       "Érica Alessandra Rocha Alves            0              "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atualizador.contar_qualis(dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a81b8",
   "metadata": {},
   "source": [
    "#### Classificação dos artigos no período ponderada pelo Qualis Periódicos\n",
    "\n",
    "Obs.: 'Não encontrado' significa que o ISSN da revista da publicação não consta na lista de revistas avaliadas no Qualis Periódico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894b6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os anos de interesse\n",
    "ano_inicio = 2017\n",
    "ano_final = 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f98d76f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Ano</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "      <th>2024</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Autor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alessandra Aparecida Guarneri</th>\n",
       "      <td>A1, A2, A1, A1</td>\n",
       "      <td>A4, A1, A1</td>\n",
       "      <td>A1, A2, A4</td>\n",
       "      <td>A1, A2, A2, A1, A1</td>\n",
       "      <td>A2, A1, A2, A1</td>\n",
       "      <td>A1, A2, A1</td>\n",
       "      <td>A1, A1, A1, A1</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alexandre de Magalhaes Vieira Machado</th>\n",
       "      <td>A3, A2, A2, A2, A2</td>\n",
       "      <td>A2, B4</td>\n",
       "      <td>A3, A1</td>\n",
       "      <td>A1</td>\n",
       "      <td>A1, A2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ana Lúcia Teles Rabello</th>\n",
       "      <td>B1, A2, A1, A1, B1, A1, A4, A1, A1</td>\n",
       "      <td>A4, B1, B1, A1, B1, B1</td>\n",
       "      <td>A1, A4, A1, B1, A1, A1, A3, A1, A4</td>\n",
       "      <td>A2, A1, B1, A2, A1</td>\n",
       "      <td>B1, A2</td>\n",
       "      <td>A1, A1</td>\n",
       "      <td>B1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andréa Teixeira de Carvalho</th>\n",
       "      <td>A3, A3, A2, B2, A1, A4, A1, A1, B2, A3, A2, A1, A2, A3, A1, B2, A3, A3</td>\n",
       "      <td>A3, A2, B2, A1, A1, A2, A4, A1, A1, A2, A2, A1, A3, A2, A1</td>\n",
       "      <td>A4, A3, A2, A1, A1, A2, A2, A1, A2, A1, A2, A2, A2, A3, A2, A3, A1, A3</td>\n",
       "      <td>B1, A2, B2, A2, A2, A4, A2, A2, A4, A4, A2, A1, A2, A1, A1, A1, A1, A1, A2, A4, A1, A1</td>\n",
       "      <td>A3, C, A2, B3, A1, A2, A3, A4, A1, A2, A3, A2</td>\n",
       "      <td>A3, A2, A2, A1, A1, A3, A2, A3, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A1, A4, Não encontrado</td>\n",
       "      <td>A4, A4, A1, A4, A3, A1, A2, A4, A2, A2, C, A2, Não encontrado, A1, A2, A4</td>\n",
       "      <td>A2, A1, C, A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antonio Mauro Rezende</th>\n",
       "      <td>A1, A2, A3</td>\n",
       "      <td>A1, A1, A2, A4, A3</td>\n",
       "      <td>A1, A2, A3, A1, A2, A2</td>\n",
       "      <td>A2, A3, A2, B1, A1, A1, B1, A4, A2</td>\n",
       "      <td>A2, A3, A3, A2, A2, A1, C</td>\n",
       "      <td>A1, B1, A2, A1, C</td>\n",
       "      <td>A2, A2, B1, A1, A3, A3</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carina Margonari de Souza</th>\n",
       "      <td>A2</td>\n",
       "      <td>A1, B3</td>\n",
       "      <td>A3</td>\n",
       "      <td>A4, A4</td>\n",
       "      <td>A1, A4, A1, A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>A2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carlos Eduardo Calzavara Silva</th>\n",
       "      <td>B1</td>\n",
       "      <td>A2, A2, A4, A1</td>\n",
       "      <td>A3</td>\n",
       "      <td>A2, A1, A2, A2</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2, A1, A4, A2, A3, Não encontrado</td>\n",
       "      <td>A1, A1, A3, A1</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caroline Furtado Junqueira</th>\n",
       "      <td>Não encontrado, A2, A1</td>\n",
       "      <td>A2, A1</td>\n",
       "      <td>Não encontrado</td>\n",
       "      <td>A1, A1, A1, A1</td>\n",
       "      <td>A1, A1, A3</td>\n",
       "      <td>A1, A1, B1</td>\n",
       "      <td>A1, A1, A1, A1, A1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Celia Maria Ferreira Gontijo</th>\n",
       "      <td>A1, A1, A4, A2, A1, A1</td>\n",
       "      <td>B2, A3, A1, A2, A1, A1, A2, A2</td>\n",
       "      <td>A4, A2, A4, A3, A1, A2</td>\n",
       "      <td>A2, A2, A1, A4, A2, A1, A4</td>\n",
       "      <td>A1, A1, A2, C</td>\n",
       "      <td>A1, A2, A2, B1, C</td>\n",
       "      <td>A3, A4, A2, A2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cristiana Couto Garcia</th>\n",
       "      <td>A1, A3, A2, A2</td>\n",
       "      <td>A2, A2, A2</td>\n",
       "      <td>A2</td>\n",
       "      <td>A1, A2, A2, A4, A1</td>\n",
       "      <td>A4, A1, A4</td>\n",
       "      <td>A2, A2, A1, A2, A3, A1, A1, A2</td>\n",
       "      <td>A2, A3, A1, A4, A4, A1, A2, A1, B2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cristiana Ferreira Alves de Brito</th>\n",
       "      <td>A2, A2, A2, A4, A1</td>\n",
       "      <td>A1, A1, A1, A2, A1</td>\n",
       "      <td>A3, A2, C, A1, A1, A2, A1, A4</td>\n",
       "      <td>A1, A1, A1</td>\n",
       "      <td>A2, A1, A2, A2</td>\n",
       "      <td>A2, A2, Não encontrado</td>\n",
       "      <td>A2, Não encontrado, A2, B3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cristina Toscano Fonseca</th>\n",
       "      <td>A3, A1</td>\n",
       "      <td>A4, A1, A2</td>\n",
       "      <td>A4, A2, A3</td>\n",
       "      <td>A2</td>\n",
       "      <td>A2, A2, A2</td>\n",
       "      <td>A2, A2, A2</td>\n",
       "      <td>A4, A3, A2, A2, A1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daniel Moreira de Avelar</th>\n",
       "      <td>B1</td>\n",
       "      <td>B2, B1, A4</td>\n",
       "      <td>A4, A3, A1</td>\n",
       "      <td>A2, B1</td>\n",
       "      <td>A2, C, A1</td>\n",
       "      <td>A2, A2, A1</td>\n",
       "      <td>A1, Não encontrado</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edelberto Santos Dias</th>\n",
       "      <td>B1, A2</td>\n",
       "      <td>A2, A1</td>\n",
       "      <td>B1, B1, Não encontrado, A1, A1, C</td>\n",
       "      <td>A1, B1, B1</td>\n",
       "      <td></td>\n",
       "      <td>A2</td>\n",
       "      <td>A2, A3, A3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edward José de Oliveira</th>\n",
       "      <td>B1, A1</td>\n",
       "      <td>B1, A2</td>\n",
       "      <td>A2, A4, A4, A1, A1, B1, B1</td>\n",
       "      <td>A4, B2, A4, A1, B1, A1, A3, Não encontrado, A4</td>\n",
       "      <td>A2, A3, A1</td>\n",
       "      <td>B1, A2, A1, A2, A2</td>\n",
       "      <td>A4, A1, A1, A1, A2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Erika Michalsky Monteiro</th>\n",
       "      <td>B1, A2</td>\n",
       "      <td>A2, A1</td>\n",
       "      <td>A1, A1, B1, C</td>\n",
       "      <td>A1, B1, B1</td>\n",
       "      <td></td>\n",
       "      <td>A2</td>\n",
       "      <td>A3, A3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flora Satiko Kano</th>\n",
       "      <td>A1, A1</td>\n",
       "      <td>A1, A1, A1</td>\n",
       "      <td>A2, A1</td>\n",
       "      <td>A2, A1, A1</td>\n",
       "      <td>A2, A2, A1</td>\n",
       "      <td>A1, A1</td>\n",
       "      <td>A2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gabriel da Rocha Fernandes</th>\n",
       "      <td>A1, A2, A1, A1, A1, A1, A1, A4, A2, A1, A1</td>\n",
       "      <td>A3, A1, A4, A1, A1</td>\n",
       "      <td>A1, Não encontrado, A2, Não encontrado</td>\n",
       "      <td>A1, A1, A1, Não encontrado, Não encontrado, A2, A2</td>\n",
       "      <td>A1, A3</td>\n",
       "      <td>A2, A1, A2, A3, Não encontrado, A2, A1, Não encontrado, A1, A3, A1, A1, B2</td>\n",
       "      <td>A1, A2, A1, A1, A1, A1, B1, A4, A2, Não encontrado, A2</td>\n",
       "      <td>A3, A1, A1, A3, A2, A1, A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glaucia Fernandes Cota</th>\n",
       "      <td>A3, B1, A2, A2, A4, A1, B1, A1, A1</td>\n",
       "      <td>A1, B2, A1, B1, A1, A4, A1, A1, B1</td>\n",
       "      <td>B1, A1, A1, A1, B1</td>\n",
       "      <td>A2, A1, A2, A1, A1, A1, B1, A2</td>\n",
       "      <td>A3, B1, A3, B1, A1, A1, A1, A2, Não encontrado, B1, A2</td>\n",
       "      <td>A2, A1, A1, A1, A2</td>\n",
       "      <td>A1, B1, A1, A1</td>\n",
       "      <td>A1, A1, A1, A1, B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gustavo Fontes Paz</th>\n",
       "      <td>A1, A3, A4, A1, A1, A1, A1</td>\n",
       "      <td>A2, A1</td>\n",
       "      <td>A1, A2</td>\n",
       "      <td>A1, A2, C, A1</td>\n",
       "      <td>A1, A1</td>\n",
       "      <td>A2, B1, A2, A2, A3</td>\n",
       "      <td>A3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaquelline Germano de Oliveira</th>\n",
       "      <td></td>\n",
       "      <td>A3, A2, B1</td>\n",
       "      <td>A3, Não encontrado</td>\n",
       "      <td>A1, A2, A4, A2, Não encontrado</td>\n",
       "      <td></td>\n",
       "      <td>A3, A2, Não encontrado</td>\n",
       "      <td></td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jeronimo Conceição Ruiz</th>\n",
       "      <td>A1, A2, A1, A1, A3, A4</td>\n",
       "      <td>A2, A2, A3</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3, A1, A1, A2, A1</td>\n",
       "      <td>A2, A2, A1, A2</td>\n",
       "      <td>A2</td>\n",
       "      <td>A2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>José Dilermando Andrade Filho</th>\n",
       "      <td>A4, A1, A1, A1, A1, A1, A1, A1</td>\n",
       "      <td>B3</td>\n",
       "      <td>A1, A2, Não encontrado, B4, B1, A1</td>\n",
       "      <td>A1, A1, A1, A1, A4</td>\n",
       "      <td>A2, A1, A4, A1, B1</td>\n",
       "      <td>A2, A1, A1, A2, A2, C, A1, C, A1</td>\n",
       "      <td>A2, A3, B1, A2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lileia Gonçalves Diotaiuti</th>\n",
       "      <td>A2, A3, A4, A1, A1, A4</td>\n",
       "      <td>A2, B4, B2, A2</td>\n",
       "      <td>Não encontrado, B1</td>\n",
       "      <td>A2, A1, B1, B2</td>\n",
       "      <td>B1, A1, A1</td>\n",
       "      <td>B1, A3, A4, A4, A2</td>\n",
       "      <td>A3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lis Ribeiro do Valle Antonelli</th>\n",
       "      <td>A3, A2, A2, A1</td>\n",
       "      <td>A3, A1, A1, A1, A4, A1, A2, A1</td>\n",
       "      <td>A1, A2, A2, A1, A1, A2, A1, A1</td>\n",
       "      <td>A4, A2, A1, A1, A2, A1</td>\n",
       "      <td>A1, A2, A1, A3, A1, A2, A1, A1</td>\n",
       "      <td>A3, A2, A2, A1, Não encontrado, A3, A2, A2</td>\n",
       "      <td>A3, A2, A1, A4</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luciano Andrade Moreira</th>\n",
       "      <td>A2, A4, A2, A1, A1, A1, A1</td>\n",
       "      <td>A1, A1, A2, A1</td>\n",
       "      <td>A2, A1, A1, A4, A1, A2, A2, A3</td>\n",
       "      <td>A1, A1, A1, A2, A1, A4</td>\n",
       "      <td>A2, A3, A2, A1, A1, A1, A2, A2, A1</td>\n",
       "      <td>B1, A1, A3, A1</td>\n",
       "      <td>A1, A1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luiz Carlos Júnior Alcantara</th>\n",
       "      <td>B1, A1, A1, A4, B1, A1, C, A2</td>\n",
       "      <td>A3, A1, A2, A2, C, A2, A2, A1, A1</td>\n",
       "      <td>A1, C, A1, A4, A1, Não encontrado, A1, A1, C, A2, A1, B2, A2, A1</td>\n",
       "      <td>A1, A4, A4, A2, A1, A3, A3, A2, A1, A1, A1, A1, A1, A3, A3, A1, Não encontrado, A2, C, B1</td>\n",
       "      <td>A2, A1, A1, A2, A1, A1, A1, A3, A1, B1, B1, A1, A1, A2, A1, A3, A2, A1, B1, A2, A2</td>\n",
       "      <td>A1, B1, A2, B1, A1, A1, B4, A4, A1, A2, A2, A2, A2, A1, A3, A2, A2, A1, A1, B1, A1, A2, A1, Não encontrado, A2, A1, A2, A2, A2, Não encontrado</td>\n",
       "      <td>A1, A1, A2, A2, A1, A2, B1, A2, A1, A2, A2, A3, A2, A2, A3, A3, A1, A1, A1, A2, A3</td>\n",
       "      <td>A2, B2, A2, A1, A1, A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luzia Helena Carvalho</th>\n",
       "      <td>A2, A2, A1</td>\n",
       "      <td>A2, A1, A1, A1</td>\n",
       "      <td>A2, A1</td>\n",
       "      <td>A1, A1, A1, A1</td>\n",
       "      <td>A2, A2, A2, A1, A2</td>\n",
       "      <td>A1, A1, A2</td>\n",
       "      <td>A2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marcelo Antonio Pascoal Xavier</th>\n",
       "      <td>B2, A2, A1, A4</td>\n",
       "      <td>A1, A1, A2, A2, B2, C, A2, A1, B1</td>\n",
       "      <td></td>\n",
       "      <td>A4, A2, B2, B1, B3, A1, A1, A1</td>\n",
       "      <td>B2, A1, A1, B2, A2, A2</td>\n",
       "      <td>A2, A2, A4</td>\n",
       "      <td>C, A1, A1, A3</td>\n",
       "      <td>A4, A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marcelo Gustavo Lorenzo</th>\n",
       "      <td>A1, A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>A1, A2</td>\n",
       "      <td>A2, A1, A1, A2, A2, A1</td>\n",
       "      <td>A1, A1, A2, A1, A1</td>\n",
       "      <td>A1, A2</td>\n",
       "      <td>A2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marco Antônio da Silva Campos</th>\n",
       "      <td>A3</td>\n",
       "      <td>B4, A1</td>\n",
       "      <td></td>\n",
       "      <td>A2, B1</td>\n",
       "      <td>A2, A3, Não encontrado</td>\n",
       "      <td>B1</td>\n",
       "      <td>Não encontrado, B1, A3</td>\n",
       "      <td>Não encontrado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marina de Moraes Mourão</th>\n",
       "      <td>A1, A4, A2, A2, A2, C, A1</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2, A4, A4, A3, A2, A1</td>\n",
       "      <td>A2, A1, A2, A2</td>\n",
       "      <td>B3, A1, A2, A4</td>\n",
       "      <td>A1, A2, A2, A2, A2</td>\n",
       "      <td>A2, A4, A2</td>\n",
       "      <td>Não encontrado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Márcio Sobreira Silva Araújo</th>\n",
       "      <td>A3, A2, B4</td>\n",
       "      <td>A2, A4, A3, B2, A1, A2, A1, A1, A4, A3, B1</td>\n",
       "      <td>A1, A1</td>\n",
       "      <td>A4, B2, A2, A2</td>\n",
       "      <td>A1, B2, A2</td>\n",
       "      <td>A2, A2, A2, A2, B2, A2, A3</td>\n",
       "      <td>A4</td>\n",
       "      <td>A1, A4, A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nagila Francinete Costa Secundino</th>\n",
       "      <td>A2, A2, A2, A2</td>\n",
       "      <td>A4, A1, A1, A2, A1, A1</td>\n",
       "      <td>B1, A1, A1</td>\n",
       "      <td>A1, A2, A1, A1, A1, A1</td>\n",
       "      <td>A2, A2, A1, A1, A2, A1</td>\n",
       "      <td>A2, B1, A1, A2</td>\n",
       "      <td>A2, A1, A1, A2, A2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Olindo Assis Martins Filho</th>\n",
       "      <td>A3, A3, A2, B2, A1, A4, A1, A3, B2, A2, A1, A2, A1, A2, A3, A4, A3, B1, B2, B2, A3, A3, A1, A3, B2, A2</td>\n",
       "      <td>B2, A4, A1, A2, A1, B2, A3, A4, A1, A2, A1, A2, A2, A3, A1, A2, A4</td>\n",
       "      <td>A2, A1, A2, A2, A1, A2, A2, A1, A1, A2, A2, A2, A1, A3, A1, A2, A2, A2, B1, A3, A1, A3, A2, A3</td>\n",
       "      <td>A3, A3, A2, A4, B2, A2, A2, A1, A2, A2, A4, A1, A1, A2, A2, A2, A4, A1, A1, A4, B2, A1, B3, A1, A3, A2</td>\n",
       "      <td>A2, B2, A2, A3, A3, A1, A4, A1, A3, A2, A2, A2, A2, A1, C, B3, A1, A4, A3, A2, A3, A2, A2</td>\n",
       "      <td>A1, A2, A3, A2, A1, A3, A2, A2, A2, A4, A2, A4, A2, A2, A2, A3, A2, A3, A2, A2, Não encontrado, Não encontrado, A2, A3, A2, A2, A1, Não encontrado</td>\n",
       "      <td>A4, A3, A3, A3, A3, A2, A1, A2, A4, A2, A2, A2, A4, Não encontrado, A2, A2, A1, A1, A3, A2, A2, B2, B1, B1</td>\n",
       "      <td>A2, A1, A1, C, A1, A4, A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paloma Helena Fernandes Shimabukuro</th>\n",
       "      <td>A4, A1</td>\n",
       "      <td>A1, A4, A4</td>\n",
       "      <td></td>\n",
       "      <td>B1, A1, A1, A1</td>\n",
       "      <td>A1, A2</td>\n",
       "      <td>C, C, C, A4, A1, A1, A1</td>\n",
       "      <td>A1, A1, A1, A4</td>\n",
       "      <td>B1, C, B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paulo Filemon Paolucci Pimenta</th>\n",
       "      <td>A2, A2, A2, A2</td>\n",
       "      <td>A1, A1, A2, A2, A1, A1, A1, A4, A1, A1</td>\n",
       "      <td>A4, A1, A1, A1, B1, B1</td>\n",
       "      <td>B1, A1, A2, A1, A1, A1, A1, A1</td>\n",
       "      <td>A2, A1, A2, A2, A1, A2</td>\n",
       "      <td>A3, A1, A2, A2, B1, Não encontrado</td>\n",
       "      <td>A2, A2, A2, A1, A2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paulo Marcos Zech Coelho</th>\n",
       "      <td>A4, A1, A1, B4, A1, A4, B1</td>\n",
       "      <td>A3, A1, B1, A3, C</td>\n",
       "      <td>A1, B1, A4, B2, A4, A1, A4, A3, B1, B1</td>\n",
       "      <td>A1, A4, A1, A4</td>\n",
       "      <td>A3, A2</td>\n",
       "      <td>B1</td>\n",
       "      <td>B1, A1, A2</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pedro Augusto Alves</th>\n",
       "      <td></td>\n",
       "      <td>B4, A1, A2</td>\n",
       "      <td>A2, A1, B1, A3</td>\n",
       "      <td>A2, A1, A1, A1, A1</td>\n",
       "      <td>A3, A2</td>\n",
       "      <td>A2, A2, A4, Não encontrado, A1, Não encontrado, A1, Não encontrado, A2</td>\n",
       "      <td>A3, A1, A3, A1, A2</td>\n",
       "      <td>A1, Não encontrado, A2, A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rafaella Fortini Grenfell e Queiroz</th>\n",
       "      <td></td>\n",
       "      <td>A3</td>\n",
       "      <td>A4, A1, B2, A1</td>\n",
       "      <td>A1</td>\n",
       "      <td>A1, A1, A4</td>\n",
       "      <td>Não encontrado, C, A2, Não encontrado, A3, A2, Não encontrado, A4</td>\n",
       "      <td>B1, A3, A2, A1, Não encontrado</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ricardo Tostes Gazzinelli</th>\n",
       "      <td>A2, A1, Não encontrado, A2, A2, A1, A1</td>\n",
       "      <td>A2, A1, Não encontrado, A1, A1, A1, A1</td>\n",
       "      <td>A4, A4, A1, A1, A2, A1</td>\n",
       "      <td>A1, A2, A1, A1, A1, A1, A1, A1, A2</td>\n",
       "      <td>A1, A3, A1, A2, A1, A1, A1, A1, A1, A2, A1</td>\n",
       "      <td>A2, A1, A4, A2, A2, A1, Não encontrado, A1, A1, A1, A3</td>\n",
       "      <td>A1, A1, A1, A2, A1, A3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rita de Cássia Moreira de Souza</th>\n",
       "      <td>A1, A1, A1</td>\n",
       "      <td>B2, A2</td>\n",
       "      <td>A1, A2, B1, A1</td>\n",
       "      <td>A1, B1</td>\n",
       "      <td>B1, A4, A4, A1, A2, A1, A4</td>\n",
       "      <td>A2, A3, Não encontrado</td>\n",
       "      <td>B1, A1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roberta Lima Caldeira</th>\n",
       "      <td>B1, A1, A1, B1, A1</td>\n",
       "      <td>Não encontrado, A2, B1, B3, C</td>\n",
       "      <td>A4, A4</td>\n",
       "      <td>A2, A4, A2, A2</td>\n",
       "      <td>A2, A1, A2</td>\n",
       "      <td>A2, A2, A3</td>\n",
       "      <td>A1, A4, A2, B4, A4, A2, B1</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rodrigo Correa de Oliveira</th>\n",
       "      <td>A1, A3, A1, A1, A2, A1, A2, A2, A2, A4, A1, A2, A3, A3, A4, A1, A1</td>\n",
       "      <td>A4, A2, A3, A1, A1</td>\n",
       "      <td>A2, A1, A2, A2, B1, A1, A1, A1, A3, A1</td>\n",
       "      <td>A2, A1, A1, A3, A4</td>\n",
       "      <td>A1, A1, A3, A3, A3</td>\n",
       "      <td>A1, A3, A2</td>\n",
       "      <td>A1, A1, A1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rodrigo Pedro Pinto Soares</th>\n",
       "      <td>A4, A1, A3, A2, A1, A1, A1</td>\n",
       "      <td>B1, A4, A4, A4, A1, A2, A2, A2, A1</td>\n",
       "      <td>A1, A2, A4, A2, A2, A4, A1, A2, A4</td>\n",
       "      <td>A2, A2, Não encontrado, A2, A2, A2, A4, A2</td>\n",
       "      <td>A2, A2, B2, A1, A1, A1, B2, A1, A2, A3, A3</td>\n",
       "      <td>A2, A4, B2, B2</td>\n",
       "      <td>A3, A3, A4, Não encontrado, A3</td>\n",
       "      <td>A4, A1, A4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roney Santos Coimbra</th>\n",
       "      <td>A1, A3, A1, A3, A1, A1, A4</td>\n",
       "      <td>A2, A3, A1, A1, A1</td>\n",
       "      <td></td>\n",
       "      <td>A2, A1</td>\n",
       "      <td>A2, A2</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2, A4, A1, A4, A2, A1</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rosiane Aparecida da Silva Pereira</th>\n",
       "      <td></td>\n",
       "      <td>A1, A4</td>\n",
       "      <td>A4, A1, A3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>A2, A2, A3</td>\n",
       "      <td>A4, A3, A2</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rubens Lima do Monte Neto</th>\n",
       "      <td>A2</td>\n",
       "      <td>A3, A1, A2, A2, A1</td>\n",
       "      <td>A4</td>\n",
       "      <td>A1, A1, A3, A4</td>\n",
       "      <td>A2, A1, A2, A1, A2</td>\n",
       "      <td>A3, A4, B1, Não encontrado, A4, A4, A2, B2, A4, A1</td>\n",
       "      <td>A3, A3, A2, A4, A1</td>\n",
       "      <td>A4, Não encontrado, A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Silvane Maria Fonseca Murta</th>\n",
       "      <td>A4, A2, A2, A3, A2</td>\n",
       "      <td>A4, A4, A1, A1, A4, B1</td>\n",
       "      <td>A4, A2, B1, A4, A3, A4, A2</td>\n",
       "      <td>A2, A4, A4, A4, A4, A2, A2, B1, A4</td>\n",
       "      <td>A2, A2, A2, A2, B2, B1</td>\n",
       "      <td>A4, A2, B1, A4, A2, A2, A1, A1</td>\n",
       "      <td>A4, A2, A1, Não encontrado, A2, A1, A2</td>\n",
       "      <td>A1, A1, A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soraya Torres Gaze Jangola</th>\n",
       "      <td></td>\n",
       "      <td>A2, B2</td>\n",
       "      <td></td>\n",
       "      <td>A2</td>\n",
       "      <td>B4, A1, A2</td>\n",
       "      <td>A3, A4</td>\n",
       "      <td>A1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taís Nóbrega de Sousa</th>\n",
       "      <td>A2, A2</td>\n",
       "      <td>A2, A1, A1</td>\n",
       "      <td>A3, A1, A2, A2</td>\n",
       "      <td>A1, A1, A1, A2</td>\n",
       "      <td>A2, A2, A2, A2</td>\n",
       "      <td>A1, A2, A1, A1</td>\n",
       "      <td>A2, Não encontrado, A2</td>\n",
       "      <td>A1, A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tânia Maria de Almeida Alves</th>\n",
       "      <td>A3, A3</td>\n",
       "      <td>A4, A4, B1, A4, A1, A1</td>\n",
       "      <td>A1, A2, A4</td>\n",
       "      <td>A4, Não encontrado, A4, A2</td>\n",
       "      <td>A3, A3</td>\n",
       "      <td>A4, A3, A3</td>\n",
       "      <td>B4, A1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanessa Peruhype Magalhães Pascoal</th>\n",
       "      <td>B2, A2, A3, B2, A3, A2, A3, A1</td>\n",
       "      <td>B2, A1, A2</td>\n",
       "      <td>A2, A2, A1, A2</td>\n",
       "      <td>B2, A4, A1, A2, A4, A4</td>\n",
       "      <td>A3, B2, A2, A1, A3</td>\n",
       "      <td>A3, A2, A2, A2, A2, A2, A2, A4, A2, A2</td>\n",
       "      <td>A4, A4, A2, A3, A1, A2</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Álvaro Gil Araújo Ferreira</th>\n",
       "      <td></td>\n",
       "      <td>A1</td>\n",
       "      <td>A4</td>\n",
       "      <td>A1, A2</td>\n",
       "      <td>A2, A2, A1</td>\n",
       "      <td>A3</td>\n",
       "      <td>A1, A1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Érica Alessandra Rocha Alves</th>\n",
       "      <td>A2, A3</td>\n",
       "      <td>A2, B4, A2</td>\n",
       "      <td></td>\n",
       "      <td>A2, A3, A1</td>\n",
       "      <td>B4, A1, B4</td>\n",
       "      <td>A3</td>\n",
       "      <td>A1, A1, A1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ano                                   2017                                                                                                     \\\n",
       "Autor                                                                                                                                           \n",
       "Alessandra Aparecida Guarneri                                                                                                  A1, A2, A1, A1   \n",
       "Alexandre de Magalhaes Vieira Machado                                                                                      A3, A2, A2, A2, A2   \n",
       "Ana Lúcia Teles Rabello                                                                                    B1, A2, A1, A1, B1, A1, A4, A1, A1   \n",
       "Andréa Teixeira de Carvalho                                            A3, A3, A2, B2, A1, A4, A1, A1, B2, A3, A2, A1, A2, A3, A1, B2, A3, A3   \n",
       "Antonio Mauro Rezende                                                                                                              A1, A2, A3   \n",
       "Carina Margonari de Souza                                                                                                                  A2   \n",
       "Carlos Eduardo Calzavara Silva                                                                                                             B1   \n",
       "Caroline Furtado Junqueira                                                                                             Não encontrado, A2, A1   \n",
       "Celia Maria Ferreira Gontijo                                                                                           A1, A1, A4, A2, A1, A1   \n",
       "Cristiana Couto Garcia                                                                                                         A1, A3, A2, A2   \n",
       "Cristiana Ferreira Alves de Brito                                                                                          A2, A2, A2, A4, A1   \n",
       "Cristina Toscano Fonseca                                                                                                               A3, A1   \n",
       "Daniel Moreira de Avelar                                                                                                                   B1   \n",
       "Edelberto Santos Dias                                                                                                                  B1, A2   \n",
       "Edward José de Oliveira                                                                                                                B1, A1   \n",
       "Erika Michalsky Monteiro                                                                                                               B1, A2   \n",
       "Flora Satiko Kano                                                                                                                      A1, A1   \n",
       "Gabriel da Rocha Fernandes                                                                         A1, A2, A1, A1, A1, A1, A1, A4, A2, A1, A1   \n",
       "Glaucia Fernandes Cota                                                                                     A3, B1, A2, A2, A4, A1, B1, A1, A1   \n",
       "Gustavo Fontes Paz                                                                                                 A1, A3, A4, A1, A1, A1, A1   \n",
       "Jaquelline Germano de Oliveira                                                                                                                  \n",
       "Jeronimo Conceição Ruiz                                                                                                A1, A2, A1, A1, A3, A4   \n",
       "José Dilermando Andrade Filho                                                                                  A4, A1, A1, A1, A1, A1, A1, A1   \n",
       "Lileia Gonçalves Diotaiuti                                                                                             A2, A3, A4, A1, A1, A4   \n",
       "Lis Ribeiro do Valle Antonelli                                                                                                 A3, A2, A2, A1   \n",
       "Luciano Andrade Moreira                                                                                            A2, A4, A2, A1, A1, A1, A1   \n",
       "Luiz Carlos Júnior Alcantara                                                                                    B1, A1, A1, A4, B1, A1, C, A2   \n",
       "Luzia Helena Carvalho                                                                                                              A2, A2, A1   \n",
       "Marcelo Antonio Pascoal Xavier                                                                                                 B2, A2, A1, A4   \n",
       "Marcelo Gustavo Lorenzo                                                                                                                A1, A1   \n",
       "Marco Antônio da Silva Campos                                                                                                              A3   \n",
       "Marina de Moraes Mourão                                                                                             A1, A4, A2, A2, A2, C, A1   \n",
       "Márcio Sobreira Silva Araújo                                                                                                       A3, A2, B4   \n",
       "Nagila Francinete Costa Secundino                                                                                              A2, A2, A2, A2   \n",
       "Olindo Assis Martins Filho             A3, A3, A2, B2, A1, A4, A1, A3, B2, A2, A1, A2, A1, A2, A3, A4, A3, B1, B2, B2, A3, A3, A1, A3, B2, A2   \n",
       "Paloma Helena Fernandes Shimabukuro                                                                                                    A4, A1   \n",
       "Paulo Filemon Paolucci Pimenta                                                                                                 A2, A2, A2, A2   \n",
       "Paulo Marcos Zech Coelho                                                                                           A4, A1, A1, B4, A1, A4, B1   \n",
       "Pedro Augusto Alves                                                                                                                             \n",
       "Rafaella Fortini Grenfell e Queiroz                                                                                                             \n",
       "Ricardo Tostes Gazzinelli                                                                              A2, A1, Não encontrado, A2, A2, A1, A1   \n",
       "Rita de Cássia Moreira de Souza                                                                                                    A1, A1, A1   \n",
       "Roberta Lima Caldeira                                                                                                      B1, A1, A1, B1, A1   \n",
       "Rodrigo Correa de Oliveira                                                 A1, A3, A1, A1, A2, A1, A2, A2, A2, A4, A1, A2, A3, A3, A4, A1, A1   \n",
       "Rodrigo Pedro Pinto Soares                                                                                         A4, A1, A3, A2, A1, A1, A1   \n",
       "Roney Santos Coimbra                                                                                               A1, A3, A1, A3, A1, A1, A4   \n",
       "Rosiane Aparecida da Silva Pereira                                                                                                              \n",
       "Rubens Lima do Monte Neto                                                                                                                  A2   \n",
       "Silvane Maria Fonseca Murta                                                                                                A4, A2, A2, A3, A2   \n",
       "Soraya Torres Gaze Jangola                                                                                                                      \n",
       "Taís Nóbrega de Sousa                                                                                                                  A2, A2   \n",
       "Tânia Maria de Almeida Alves                                                                                                           A3, A3   \n",
       "Vanessa Peruhype Magalhães Pascoal                                                                             B2, A2, A3, B2, A3, A2, A3, A1   \n",
       "Álvaro Gil Araújo Ferreira                                                                                                                      \n",
       "Érica Alessandra Rocha Alves                                                                                                           A2, A3   \n",
       "\n",
       "Ano                                   2018                                                                 \\\n",
       "Autor                                                                                                       \n",
       "Alessandra Aparecida Guarneri                                                                  A4, A1, A1   \n",
       "Alexandre de Magalhaes Vieira Machado                                                              A2, B4   \n",
       "Ana Lúcia Teles Rabello                                                            A4, B1, B1, A1, B1, B1   \n",
       "Andréa Teixeira de Carvalho                    A3, A2, B2, A1, A1, A2, A4, A1, A1, A2, A2, A1, A3, A2, A1   \n",
       "Antonio Mauro Rezende                                                                  A1, A1, A2, A4, A3   \n",
       "Carina Margonari de Souza                                                                          A1, B3   \n",
       "Carlos Eduardo Calzavara Silva                                                             A2, A2, A4, A1   \n",
       "Caroline Furtado Junqueira                                                                         A2, A1   \n",
       "Celia Maria Ferreira Gontijo                                               B2, A3, A1, A2, A1, A1, A2, A2   \n",
       "Cristiana Couto Garcia                                                                         A2, A2, A2   \n",
       "Cristiana Ferreira Alves de Brito                                                      A1, A1, A1, A2, A1   \n",
       "Cristina Toscano Fonseca                                                                       A4, A1, A2   \n",
       "Daniel Moreira de Avelar                                                                       B2, B1, A4   \n",
       "Edelberto Santos Dias                                                                              A2, A1   \n",
       "Edward José de Oliveira                                                                            B1, A2   \n",
       "Erika Michalsky Monteiro                                                                           A2, A1   \n",
       "Flora Satiko Kano                                                                              A1, A1, A1   \n",
       "Gabriel da Rocha Fernandes                                                             A3, A1, A4, A1, A1   \n",
       "Glaucia Fernandes Cota                                                 A1, B2, A1, B1, A1, A4, A1, A1, B1   \n",
       "Gustavo Fontes Paz                                                                                 A2, A1   \n",
       "Jaquelline Germano de Oliveira                                                                 A3, A2, B1   \n",
       "Jeronimo Conceição Ruiz                                                                        A2, A2, A3   \n",
       "José Dilermando Andrade Filho                                                                          B3   \n",
       "Lileia Gonçalves Diotaiuti                                                                 A2, B4, B2, A2   \n",
       "Lis Ribeiro do Valle Antonelli                                             A3, A1, A1, A1, A4, A1, A2, A1   \n",
       "Luciano Andrade Moreira                                                                    A1, A1, A2, A1   \n",
       "Luiz Carlos Júnior Alcantara                                            A3, A1, A2, A2, C, A2, A2, A1, A1   \n",
       "Luzia Helena Carvalho                                                                      A2, A1, A1, A1   \n",
       "Marcelo Antonio Pascoal Xavier                                          A1, A1, A2, A2, B2, C, A2, A1, B1   \n",
       "Marcelo Gustavo Lorenzo                                                                                A2   \n",
       "Marco Antônio da Silva Campos                                                                      B4, A1   \n",
       "Marina de Moraes Mourão                                                                                A1   \n",
       "Márcio Sobreira Silva Araújo                                   A2, A4, A3, B2, A1, A2, A1, A1, A4, A3, B1   \n",
       "Nagila Francinete Costa Secundino                                                  A4, A1, A1, A2, A1, A1   \n",
       "Olindo Assis Martins Filho             B2, A4, A1, A2, A1, B2, A3, A4, A1, A2, A1, A2, A2, A3, A1, A2, A4   \n",
       "Paloma Helena Fernandes Shimabukuro                                                            A1, A4, A4   \n",
       "Paulo Filemon Paolucci Pimenta                                     A1, A1, A2, A2, A1, A1, A1, A4, A1, A1   \n",
       "Paulo Marcos Zech Coelho                                                                A3, A1, B1, A3, C   \n",
       "Pedro Augusto Alves                                                                            B4, A1, A2   \n",
       "Rafaella Fortini Grenfell e Queiroz                                                                    A3   \n",
       "Ricardo Tostes Gazzinelli                                          A2, A1, Não encontrado, A1, A1, A1, A1   \n",
       "Rita de Cássia Moreira de Souza                                                                    B2, A2   \n",
       "Roberta Lima Caldeira                                                       Não encontrado, A2, B1, B3, C   \n",
       "Rodrigo Correa de Oliveira                                                             A4, A2, A3, A1, A1   \n",
       "Rodrigo Pedro Pinto Soares                                             B1, A4, A4, A4, A1, A2, A2, A2, A1   \n",
       "Roney Santos Coimbra                                                                   A2, A3, A1, A1, A1   \n",
       "Rosiane Aparecida da Silva Pereira                                                                 A1, A4   \n",
       "Rubens Lima do Monte Neto                                                              A3, A1, A2, A2, A1   \n",
       "Silvane Maria Fonseca Murta                                                        A4, A4, A1, A1, A4, B1   \n",
       "Soraya Torres Gaze Jangola                                                                         A2, B2   \n",
       "Taís Nóbrega de Sousa                                                                          A2, A1, A1   \n",
       "Tânia Maria de Almeida Alves                                                       A4, A4, B1, A4, A1, A1   \n",
       "Vanessa Peruhype Magalhães Pascoal                                                             B2, A1, A2   \n",
       "Álvaro Gil Araújo Ferreira                                                                             A1   \n",
       "Érica Alessandra Rocha Alves                                                                   A2, B4, A2   \n",
       "\n",
       "Ano                                   2019                                                                                             \\\n",
       "Autor                                                                                                                                   \n",
       "Alessandra Aparecida Guarneri                                                                                              A1, A2, A4   \n",
       "Alexandre de Magalhaes Vieira Machado                                                                                          A3, A1   \n",
       "Ana Lúcia Teles Rabello                                                                            A1, A4, A1, B1, A1, A1, A3, A1, A4   \n",
       "Andréa Teixeira de Carvalho                                    A4, A3, A2, A1, A1, A2, A2, A1, A2, A1, A2, A2, A2, A3, A2, A3, A1, A3   \n",
       "Antonio Mauro Rezende                                                                                          A1, A2, A3, A1, A2, A2   \n",
       "Carina Margonari de Souza                                                                                                          A3   \n",
       "Carlos Eduardo Calzavara Silva                                                                                                     A3   \n",
       "Caroline Furtado Junqueira                                                                                             Não encontrado   \n",
       "Celia Maria Ferreira Gontijo                                                                                   A4, A2, A4, A3, A1, A2   \n",
       "Cristiana Couto Garcia                                                                                                             A2   \n",
       "Cristiana Ferreira Alves de Brito                                                                       A3, A2, C, A1, A1, A2, A1, A4   \n",
       "Cristina Toscano Fonseca                                                                                                   A4, A2, A3   \n",
       "Daniel Moreira de Avelar                                                                                                   A4, A3, A1   \n",
       "Edelberto Santos Dias                                                                               B1, B1, Não encontrado, A1, A1, C   \n",
       "Edward José de Oliveira                                                                                    A2, A4, A4, A1, A1, B1, B1   \n",
       "Erika Michalsky Monteiro                                                                                                A1, A1, B1, C   \n",
       "Flora Satiko Kano                                                                                                              A2, A1   \n",
       "Gabriel da Rocha Fernandes                                                                     A1, Não encontrado, A2, Não encontrado   \n",
       "Glaucia Fernandes Cota                                                                                             B1, A1, A1, A1, B1   \n",
       "Gustavo Fontes Paz                                                                                                             A1, A2   \n",
       "Jaquelline Germano de Oliveira                                                                                     A3, Não encontrado   \n",
       "Jeronimo Conceição Ruiz                                                                                                            A3   \n",
       "José Dilermando Andrade Filho                                                                      A1, A2, Não encontrado, B4, B1, A1   \n",
       "Lileia Gonçalves Diotaiuti                                                                                         Não encontrado, B1   \n",
       "Lis Ribeiro do Valle Antonelli                                                                         A1, A2, A2, A1, A1, A2, A1, A1   \n",
       "Luciano Andrade Moreira                                                                                A2, A1, A1, A4, A1, A2, A2, A3   \n",
       "Luiz Carlos Júnior Alcantara                                         A1, C, A1, A4, A1, Não encontrado, A1, A1, C, A2, A1, B2, A2, A1   \n",
       "Luzia Helena Carvalho                                                                                                          A2, A1   \n",
       "Marcelo Antonio Pascoal Xavier                                                                                                          \n",
       "Marcelo Gustavo Lorenzo                                                                                                        A1, A2   \n",
       "Marco Antônio da Silva Campos                                                                                                           \n",
       "Marina de Moraes Mourão                                                                                        A2, A4, A4, A3, A2, A1   \n",
       "Márcio Sobreira Silva Araújo                                                                                                   A1, A1   \n",
       "Nagila Francinete Costa Secundino                                                                                          B1, A1, A1   \n",
       "Olindo Assis Martins Filho             A2, A1, A2, A2, A1, A2, A2, A1, A1, A2, A2, A2, A1, A3, A1, A2, A2, A2, B1, A3, A1, A3, A2, A3   \n",
       "Paloma Helena Fernandes Shimabukuro                                                                                                     \n",
       "Paulo Filemon Paolucci Pimenta                                                                                 A4, A1, A1, A1, B1, B1   \n",
       "Paulo Marcos Zech Coelho                                                                       A1, B1, A4, B2, A4, A1, A4, A3, B1, B1   \n",
       "Pedro Augusto Alves                                                                                                    A2, A1, B1, A3   \n",
       "Rafaella Fortini Grenfell e Queiroz                                                                                    A4, A1, B2, A1   \n",
       "Ricardo Tostes Gazzinelli                                                                                      A4, A4, A1, A1, A2, A1   \n",
       "Rita de Cássia Moreira de Souza                                                                                        A1, A2, B1, A1   \n",
       "Roberta Lima Caldeira                                                                                                          A4, A4   \n",
       "Rodrigo Correa de Oliveira                                                                     A2, A1, A2, A2, B1, A1, A1, A1, A3, A1   \n",
       "Rodrigo Pedro Pinto Soares                                                                         A1, A2, A4, A2, A2, A4, A1, A2, A4   \n",
       "Roney Santos Coimbra                                                                                                                    \n",
       "Rosiane Aparecida da Silva Pereira                                                                                         A4, A1, A3   \n",
       "Rubens Lima do Monte Neto                                                                                                          A4   \n",
       "Silvane Maria Fonseca Murta                                                                                A4, A2, B1, A4, A3, A4, A2   \n",
       "Soraya Torres Gaze Jangola                                                                                                              \n",
       "Taís Nóbrega de Sousa                                                                                                  A3, A1, A2, A2   \n",
       "Tânia Maria de Almeida Alves                                                                                               A1, A2, A4   \n",
       "Vanessa Peruhype Magalhães Pascoal                                                                                     A2, A2, A1, A2   \n",
       "Álvaro Gil Araújo Ferreira                                                                                                         A4   \n",
       "Érica Alessandra Rocha Alves                                                                                                            \n",
       "\n",
       "Ano                                   2020                                                                                                     \\\n",
       "Autor                                                                                                                                           \n",
       "Alessandra Aparecida Guarneri                                                                                              A1, A2, A2, A1, A1   \n",
       "Alexandre de Magalhaes Vieira Machado                                                                                                      A1   \n",
       "Ana Lúcia Teles Rabello                                                                                                    A2, A1, B1, A2, A1   \n",
       "Andréa Teixeira de Carvalho                            B1, A2, B2, A2, A2, A4, A2, A2, A4, A4, A2, A1, A2, A1, A1, A1, A1, A1, A2, A4, A1, A1   \n",
       "Antonio Mauro Rezende                                                                                      A2, A3, A2, B1, A1, A1, B1, A4, A2   \n",
       "Carina Margonari de Souza                                                                                                              A4, A4   \n",
       "Carlos Eduardo Calzavara Silva                                                                                                 A2, A1, A2, A2   \n",
       "Caroline Furtado Junqueira                                                                                                     A1, A1, A1, A1   \n",
       "Celia Maria Ferreira Gontijo                                                                                       A2, A2, A1, A4, A2, A1, A4   \n",
       "Cristiana Couto Garcia                                                                                                     A1, A2, A2, A4, A1   \n",
       "Cristiana Ferreira Alves de Brito                                                                                                  A1, A1, A1   \n",
       "Cristina Toscano Fonseca                                                                                                                   A2   \n",
       "Daniel Moreira de Avelar                                                                                                               A2, B1   \n",
       "Edelberto Santos Dias                                                                                                              A1, B1, B1   \n",
       "Edward José de Oliveira                                                                        A4, B2, A4, A1, B1, A1, A3, Não encontrado, A4   \n",
       "Erika Michalsky Monteiro                                                                                                           A1, B1, B1   \n",
       "Flora Satiko Kano                                                                                                                  A2, A1, A1   \n",
       "Gabriel da Rocha Fernandes                                                                 A1, A1, A1, Não encontrado, Não encontrado, A2, A2   \n",
       "Glaucia Fernandes Cota                                                                                         A2, A1, A2, A1, A1, A1, B1, A2   \n",
       "Gustavo Fontes Paz                                                                                                              A1, A2, C, A1   \n",
       "Jaquelline Germano de Oliveira                                                                                 A1, A2, A4, A2, Não encontrado   \n",
       "Jeronimo Conceição Ruiz                                                                                                    A3, A1, A1, A2, A1   \n",
       "José Dilermando Andrade Filho                                                                                              A1, A1, A1, A1, A4   \n",
       "Lileia Gonçalves Diotaiuti                                                                                                     A2, A1, B1, B2   \n",
       "Lis Ribeiro do Valle Antonelli                                                                                         A4, A2, A1, A1, A2, A1   \n",
       "Luciano Andrade Moreira                                                                                                A1, A1, A1, A2, A1, A4   \n",
       "Luiz Carlos Júnior Alcantara                        A1, A4, A4, A2, A1, A3, A3, A2, A1, A1, A1, A1, A1, A3, A3, A1, Não encontrado, A2, C, B1   \n",
       "Luzia Helena Carvalho                                                                                                          A1, A1, A1, A1   \n",
       "Marcelo Antonio Pascoal Xavier                                                                                 A4, A2, B2, B1, B3, A1, A1, A1   \n",
       "Marcelo Gustavo Lorenzo                                                                                                A2, A1, A1, A2, A2, A1   \n",
       "Marco Antônio da Silva Campos                                                                                                          A2, B1   \n",
       "Marina de Moraes Mourão                                                                                                        A2, A1, A2, A2   \n",
       "Márcio Sobreira Silva Araújo                                                                                                   A4, B2, A2, A2   \n",
       "Nagila Francinete Costa Secundino                                                                                      A1, A2, A1, A1, A1, A1   \n",
       "Olindo Assis Martins Filho             A3, A3, A2, A4, B2, A2, A2, A1, A2, A2, A4, A1, A1, A2, A2, A2, A4, A1, A1, A4, B2, A1, B3, A1, A3, A2   \n",
       "Paloma Helena Fernandes Shimabukuro                                                                                            B1, A1, A1, A1   \n",
       "Paulo Filemon Paolucci Pimenta                                                                                 B1, A1, A2, A1, A1, A1, A1, A1   \n",
       "Paulo Marcos Zech Coelho                                                                                                       A1, A4, A1, A4   \n",
       "Pedro Augusto Alves                                                                                                        A2, A1, A1, A1, A1   \n",
       "Rafaella Fortini Grenfell e Queiroz                                                                                                        A1   \n",
       "Ricardo Tostes Gazzinelli                                                                                  A1, A2, A1, A1, A1, A1, A1, A1, A2   \n",
       "Rita de Cássia Moreira de Souza                                                                                                        A1, B1   \n",
       "Roberta Lima Caldeira                                                                                                          A2, A4, A2, A2   \n",
       "Rodrigo Correa de Oliveira                                                                                                 A2, A1, A1, A3, A4   \n",
       "Rodrigo Pedro Pinto Soares                                                                         A2, A2, Não encontrado, A2, A2, A2, A4, A2   \n",
       "Roney Santos Coimbra                                                                                                                   A2, A1   \n",
       "Rosiane Aparecida da Silva Pereira                                                                                                              \n",
       "Rubens Lima do Monte Neto                                                                                                      A1, A1, A3, A4   \n",
       "Silvane Maria Fonseca Murta                                                                                A2, A4, A4, A4, A4, A2, A2, B1, A4   \n",
       "Soraya Torres Gaze Jangola                                                                                                                 A2   \n",
       "Taís Nóbrega de Sousa                                                                                                          A1, A1, A1, A2   \n",
       "Tânia Maria de Almeida Alves                                                                                       A4, Não encontrado, A4, A2   \n",
       "Vanessa Peruhype Magalhães Pascoal                                                                                     B2, A4, A1, A2, A4, A4   \n",
       "Álvaro Gil Araújo Ferreira                                                                                                             A1, A2   \n",
       "Érica Alessandra Rocha Alves                                                                                                       A2, A3, A1   \n",
       "\n",
       "Ano                                   2021                                                                                        \\\n",
       "Autor                                                                                                                              \n",
       "Alessandra Aparecida Guarneri                                                                                     A2, A1, A2, A1   \n",
       "Alexandre de Magalhaes Vieira Machado                                                                                     A1, A2   \n",
       "Ana Lúcia Teles Rabello                                                                                                   B1, A2   \n",
       "Andréa Teixeira de Carvalho                                                        A3, C, A2, B3, A1, A2, A3, A4, A1, A2, A3, A2   \n",
       "Antonio Mauro Rezende                                                                                  A2, A3, A3, A2, A2, A1, C   \n",
       "Carina Margonari de Souza                                                                                         A1, A4, A1, A1   \n",
       "Carlos Eduardo Calzavara Silva                                                                                                A1   \n",
       "Caroline Furtado Junqueira                                                                                            A1, A1, A3   \n",
       "Celia Maria Ferreira Gontijo                                                                                       A1, A1, A2, C   \n",
       "Cristiana Couto Garcia                                                                                                A4, A1, A4   \n",
       "Cristiana Ferreira Alves de Brito                                                                                 A2, A1, A2, A2   \n",
       "Cristina Toscano Fonseca                                                                                              A2, A2, A2   \n",
       "Daniel Moreira de Avelar                                                                                               A2, C, A1   \n",
       "Edelberto Santos Dias                                                                                                              \n",
       "Edward José de Oliveira                                                                                               A2, A3, A1   \n",
       "Erika Michalsky Monteiro                                                                                                           \n",
       "Flora Satiko Kano                                                                                                     A2, A2, A1   \n",
       "Gabriel da Rocha Fernandes                                                                                                A1, A3   \n",
       "Glaucia Fernandes Cota                                                    A3, B1, A3, B1, A1, A1, A1, A2, Não encontrado, B1, A2   \n",
       "Gustavo Fontes Paz                                                                                                        A1, A1   \n",
       "Jaquelline Germano de Oliveira                                                                                                     \n",
       "Jeronimo Conceição Ruiz                                                                                           A2, A2, A1, A2   \n",
       "José Dilermando Andrade Filho                                                                                 A2, A1, A4, A1, B1   \n",
       "Lileia Gonçalves Diotaiuti                                                                                            B1, A1, A1   \n",
       "Lis Ribeiro do Valle Antonelli                                                                    A1, A2, A1, A3, A1, A2, A1, A1   \n",
       "Luciano Andrade Moreira                                                                       A2, A3, A2, A1, A1, A1, A2, A2, A1   \n",
       "Luiz Carlos Júnior Alcantara                  A2, A1, A1, A2, A1, A1, A1, A3, A1, B1, B1, A1, A1, A2, A1, A3, A2, A1, B1, A2, A2   \n",
       "Luzia Helena Carvalho                                                                                         A2, A2, A2, A1, A2   \n",
       "Marcelo Antonio Pascoal Xavier                                                                            B2, A1, A1, B2, A2, A2   \n",
       "Marcelo Gustavo Lorenzo                                                                                       A1, A1, A2, A1, A1   \n",
       "Marco Antônio da Silva Campos                                                                             A2, A3, Não encontrado   \n",
       "Marina de Moraes Mourão                                                                                           B3, A1, A2, A4   \n",
       "Márcio Sobreira Silva Araújo                                                                                          A1, B2, A2   \n",
       "Nagila Francinete Costa Secundino                                                                         A2, A2, A1, A1, A2, A1   \n",
       "Olindo Assis Martins Filho             A2, B2, A2, A3, A3, A1, A4, A1, A3, A2, A2, A2, A2, A1, C, B3, A1, A4, A3, A2, A3, A2, A2   \n",
       "Paloma Helena Fernandes Shimabukuro                                                                                       A1, A2   \n",
       "Paulo Filemon Paolucci Pimenta                                                                            A2, A1, A2, A2, A1, A2   \n",
       "Paulo Marcos Zech Coelho                                                                                                  A3, A2   \n",
       "Pedro Augusto Alves                                                                                                       A3, A2   \n",
       "Rafaella Fortini Grenfell e Queiroz                                                                                   A1, A1, A4   \n",
       "Ricardo Tostes Gazzinelli                                                             A1, A3, A1, A2, A1, A1, A1, A1, A1, A2, A1   \n",
       "Rita de Cássia Moreira de Souza                                                                       B1, A4, A4, A1, A2, A1, A4   \n",
       "Roberta Lima Caldeira                                                                                                 A2, A1, A2   \n",
       "Rodrigo Correa de Oliveira                                                                                    A1, A1, A3, A3, A3   \n",
       "Rodrigo Pedro Pinto Soares                                                            A2, A2, B2, A1, A1, A1, B2, A1, A2, A3, A3   \n",
       "Roney Santos Coimbra                                                                                                      A2, A2   \n",
       "Rosiane Aparecida da Silva Pereira                                                                                                 \n",
       "Rubens Lima do Monte Neto                                                                                     A2, A1, A2, A1, A2   \n",
       "Silvane Maria Fonseca Murta                                                                               A2, A2, A2, A2, B2, B1   \n",
       "Soraya Torres Gaze Jangola                                                                                            B4, A1, A2   \n",
       "Taís Nóbrega de Sousa                                                                                             A2, A2, A2, A2   \n",
       "Tânia Maria de Almeida Alves                                                                                              A3, A3   \n",
       "Vanessa Peruhype Magalhães Pascoal                                                                            A3, B2, A2, A1, A3   \n",
       "Álvaro Gil Araújo Ferreira                                                                                            A2, A2, A1   \n",
       "Érica Alessandra Rocha Alves                                                                                          B4, A1, B4   \n",
       "\n",
       "Ano                                   2022                                                                                                                                                 \\\n",
       "Autor                                                                                                                                                                                       \n",
       "Alessandra Aparecida Guarneri                                                                                                                                                  A1, A2, A1   \n",
       "Alexandre de Magalhaes Vieira Machado                                                                                                                                                       \n",
       "Ana Lúcia Teles Rabello                                                                                                                                                            A1, A1   \n",
       "Andréa Teixeira de Carvalho                                                            A3, A2, A2, A1, A1, A3, A2, A3, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A1, A4, Não encontrado   \n",
       "Antonio Mauro Rezende                                                                                                                                                   A1, B1, A2, A1, C   \n",
       "Carina Margonari de Souza                                                                                                                                                              A2   \n",
       "Carlos Eduardo Calzavara Silva                                                                                                                         A2, A1, A4, A2, A3, Não encontrado   \n",
       "Caroline Furtado Junqueira                                                                                                                                                     A1, A1, B1   \n",
       "Celia Maria Ferreira Gontijo                                                                                                                                            A1, A2, A2, B1, C   \n",
       "Cristiana Couto Garcia                                                                                                                                     A2, A2, A1, A2, A3, A1, A1, A2   \n",
       "Cristiana Ferreira Alves de Brito                                                                                                                                  A2, A2, Não encontrado   \n",
       "Cristina Toscano Fonseca                                                                                                                                                       A2, A2, A2   \n",
       "Daniel Moreira de Avelar                                                                                                                                                       A2, A2, A1   \n",
       "Edelberto Santos Dias                                                                                                                                                                  A2   \n",
       "Edward José de Oliveira                                                                                                                                                B1, A2, A1, A2, A2   \n",
       "Erika Michalsky Monteiro                                                                                                                                                               A2   \n",
       "Flora Satiko Kano                                                                                                                                                                  A1, A1   \n",
       "Gabriel da Rocha Fernandes                                                                                     A2, A1, A2, A3, Não encontrado, A2, A1, Não encontrado, A1, A3, A1, A1, B2   \n",
       "Glaucia Fernandes Cota                                                                                                                                                 A2, A1, A1, A1, A2   \n",
       "Gustavo Fontes Paz                                                                                                                                                     A2, B1, A2, A2, A3   \n",
       "Jaquelline Germano de Oliveira                                                                                                                                     A3, A2, Não encontrado   \n",
       "Jeronimo Conceição Ruiz                                                                                                                                                                A2   \n",
       "José Dilermando Andrade Filho                                                                                                                            A2, A1, A1, A2, A2, C, A1, C, A1   \n",
       "Lileia Gonçalves Diotaiuti                                                                                                                                             B1, A3, A4, A4, A2   \n",
       "Lis Ribeiro do Valle Antonelli                                                                                                                 A3, A2, A2, A1, Não encontrado, A3, A2, A2   \n",
       "Luciano Andrade Moreira                                                                                                                                                    B1, A1, A3, A1   \n",
       "Luiz Carlos Júnior Alcantara               A1, B1, A2, B1, A1, A1, B4, A4, A1, A2, A2, A2, A2, A1, A3, A2, A2, A1, A1, B1, A1, A2, A1, Não encontrado, A2, A1, A2, A2, A2, Não encontrado   \n",
       "Luzia Helena Carvalho                                                                                                                                                          A1, A1, A2   \n",
       "Marcelo Antonio Pascoal Xavier                                                                                                                                                 A2, A2, A4   \n",
       "Marcelo Gustavo Lorenzo                                                                                                                                                            A1, A2   \n",
       "Marco Antônio da Silva Campos                                                                                                                                                          B1   \n",
       "Marina de Moraes Mourão                                                                                                                                                A1, A2, A2, A2, A2   \n",
       "Márcio Sobreira Silva Araújo                                                                                                                                   A2, A2, A2, A2, B2, A2, A3   \n",
       "Nagila Francinete Costa Secundino                                                                                                                                          A2, B1, A1, A2   \n",
       "Olindo Assis Martins Filho             A1, A2, A3, A2, A1, A3, A2, A2, A2, A4, A2, A4, A2, A2, A2, A3, A2, A3, A2, A2, Não encontrado, Não encontrado, A2, A3, A2, A2, A1, Não encontrado   \n",
       "Paloma Helena Fernandes Shimabukuro                                                                                                                               C, C, C, A4, A1, A1, A1   \n",
       "Paulo Filemon Paolucci Pimenta                                                                                                                         A3, A1, A2, A2, B1, Não encontrado   \n",
       "Paulo Marcos Zech Coelho                                                                                                                                                               B1   \n",
       "Pedro Augusto Alves                                                                                                A2, A2, A4, Não encontrado, A1, Não encontrado, A1, Não encontrado, A2   \n",
       "Rafaella Fortini Grenfell e Queiroz                                                                                     Não encontrado, C, A2, Não encontrado, A3, A2, Não encontrado, A4   \n",
       "Ricardo Tostes Gazzinelli                                                                                                          A2, A1, A4, A2, A2, A1, Não encontrado, A1, A1, A1, A3   \n",
       "Rita de Cássia Moreira de Souza                                                                                                                                    A2, A3, Não encontrado   \n",
       "Roberta Lima Caldeira                                                                                                                                                          A2, A2, A3   \n",
       "Rodrigo Correa de Oliveira                                                                                                                                                     A1, A3, A2   \n",
       "Rodrigo Pedro Pinto Soares                                                                                                                                                 A2, A4, B2, B2   \n",
       "Roney Santos Coimbra                                                                                                                                                                   A1   \n",
       "Rosiane Aparecida da Silva Pereira                                                                                                                                             A2, A2, A3   \n",
       "Rubens Lima do Monte Neto                                                                                                              A3, A4, B1, Não encontrado, A4, A4, A2, B2, A4, A1   \n",
       "Silvane Maria Fonseca Murta                                                                                                                                A4, A2, B1, A4, A2, A2, A1, A1   \n",
       "Soraya Torres Gaze Jangola                                                                                                                                                         A3, A4   \n",
       "Taís Nóbrega de Sousa                                                                                                                                                      A1, A2, A1, A1   \n",
       "Tânia Maria de Almeida Alves                                                                                                                                                   A4, A3, A3   \n",
       "Vanessa Peruhype Magalhães Pascoal                                                                                                                 A3, A2, A2, A2, A2, A2, A2, A4, A2, A2   \n",
       "Álvaro Gil Araújo Ferreira                                                                                                                                                             A3   \n",
       "Érica Alessandra Rocha Alves                                                                                                                                                           A3   \n",
       "\n",
       "Ano                                   2023                                                                                                         \\\n",
       "Autor                                                                                                                                               \n",
       "Alessandra Aparecida Guarneri                                                                                                      A1, A1, A1, A1   \n",
       "Alexandre de Magalhaes Vieira Machado                                                                                                               \n",
       "Ana Lúcia Teles Rabello                                                                                                                        B1   \n",
       "Andréa Teixeira de Carvalho                                             A4, A4, A1, A4, A3, A1, A2, A4, A2, A2, C, A2, Não encontrado, A1, A2, A4   \n",
       "Antonio Mauro Rezende                                                                                                      A2, A2, B1, A1, A3, A3   \n",
       "Carina Margonari de Souza                                                                                                                      A2   \n",
       "Carlos Eduardo Calzavara Silva                                                                                                     A1, A1, A3, A1   \n",
       "Caroline Furtado Junqueira                                                                                                     A1, A1, A1, A1, A1   \n",
       "Celia Maria Ferreira Gontijo                                                                                                       A3, A4, A2, A2   \n",
       "Cristiana Couto Garcia                                                                                         A2, A3, A1, A4, A4, A1, A2, A1, B2   \n",
       "Cristiana Ferreira Alves de Brito                                                                                      A2, Não encontrado, A2, B3   \n",
       "Cristina Toscano Fonseca                                                                                                       A4, A3, A2, A2, A1   \n",
       "Daniel Moreira de Avelar                                                                                                       A1, Não encontrado   \n",
       "Edelberto Santos Dias                                                                                                                  A2, A3, A3   \n",
       "Edward José de Oliveira                                                                                                        A4, A1, A1, A1, A2   \n",
       "Erika Michalsky Monteiro                                                                                                                   A3, A3   \n",
       "Flora Satiko Kano                                                                                                                              A2   \n",
       "Gabriel da Rocha Fernandes                                                                 A1, A2, A1, A1, A1, A1, B1, A4, A2, Não encontrado, A2   \n",
       "Glaucia Fernandes Cota                                                                                                             A1, B1, A1, A1   \n",
       "Gustavo Fontes Paz                                                                                                                             A3   \n",
       "Jaquelline Germano de Oliveira                                                                                                                      \n",
       "Jeronimo Conceição Ruiz                                                                                                                        A2   \n",
       "José Dilermando Andrade Filho                                                                                                      A2, A3, B1, A2   \n",
       "Lileia Gonçalves Diotaiuti                                                                                                                     A3   \n",
       "Lis Ribeiro do Valle Antonelli                                                                                                     A3, A2, A1, A4   \n",
       "Luciano Andrade Moreira                                                                                                                    A1, A1   \n",
       "Luiz Carlos Júnior Alcantara                                   A1, A1, A2, A2, A1, A2, B1, A2, A1, A2, A2, A3, A2, A2, A3, A3, A1, A1, A1, A2, A3   \n",
       "Luzia Helena Carvalho                                                                                                                          A2   \n",
       "Marcelo Antonio Pascoal Xavier                                                                                                      C, A1, A1, A3   \n",
       "Marcelo Gustavo Lorenzo                                                                                                                        A2   \n",
       "Marco Antônio da Silva Campos                                                                                              Não encontrado, B1, A3   \n",
       "Marina de Moraes Mourão                                                                                                                A2, A4, A2   \n",
       "Márcio Sobreira Silva Araújo                                                                                                                   A4   \n",
       "Nagila Francinete Costa Secundino                                                                                              A2, A1, A1, A2, A2   \n",
       "Olindo Assis Martins Filho             A4, A3, A3, A3, A3, A2, A1, A2, A4, A2, A2, A2, A4, Não encontrado, A2, A2, A1, A1, A3, A2, A2, B2, B1, B1   \n",
       "Paloma Helena Fernandes Shimabukuro                                                                                                A1, A1, A1, A4   \n",
       "Paulo Filemon Paolucci Pimenta                                                                                                 A2, A2, A2, A1, A2   \n",
       "Paulo Marcos Zech Coelho                                                                                                               B1, A1, A2   \n",
       "Pedro Augusto Alves                                                                                                            A3, A1, A3, A1, A2   \n",
       "Rafaella Fortini Grenfell e Queiroz                                                                                B1, A3, A2, A1, Não encontrado   \n",
       "Ricardo Tostes Gazzinelli                                                                                                  A1, A1, A1, A2, A1, A3   \n",
       "Rita de Cássia Moreira de Souza                                                                                                            B1, A1   \n",
       "Roberta Lima Caldeira                                                                                                  A1, A4, A2, B4, A4, A2, B1   \n",
       "Rodrigo Correa de Oliveira                                                                                                             A1, A1, A1   \n",
       "Rodrigo Pedro Pinto Soares                                                                                         A3, A3, A4, Não encontrado, A3   \n",
       "Roney Santos Coimbra                                                                                                       A2, A4, A1, A4, A2, A1   \n",
       "Rosiane Aparecida da Silva Pereira                                                                                                     A4, A3, A2   \n",
       "Rubens Lima do Monte Neto                                                                                                      A3, A3, A2, A4, A1   \n",
       "Silvane Maria Fonseca Murta                                                                                A4, A2, A1, Não encontrado, A2, A1, A2   \n",
       "Soraya Torres Gaze Jangola                                                                                                                     A1   \n",
       "Taís Nóbrega de Sousa                                                                                                      A2, Não encontrado, A2   \n",
       "Tânia Maria de Almeida Alves                                                                                                               B4, A1   \n",
       "Vanessa Peruhype Magalhães Pascoal                                                                                         A4, A4, A2, A3, A1, A2   \n",
       "Álvaro Gil Araújo Ferreira                                                                                                                 A1, A1   \n",
       "Érica Alessandra Rocha Alves                                                                                                           A1, A1, A1   \n",
       "\n",
       "Ano                                   2024                         \n",
       "Autor                                                              \n",
       "Alessandra Aparecida Guarneri                                  A1  \n",
       "Alexandre de Magalhaes Vieira Machado                              \n",
       "Ana Lúcia Teles Rabello                                            \n",
       "Andréa Teixeira de Carvalho                         A2, A1, C, A2  \n",
       "Antonio Mauro Rezende                                          B2  \n",
       "Carina Margonari de Souza                                          \n",
       "Carlos Eduardo Calzavara Silva                                 A1  \n",
       "Caroline Furtado Junqueira                                         \n",
       "Celia Maria Ferreira Gontijo                                       \n",
       "Cristiana Couto Garcia                                             \n",
       "Cristiana Ferreira Alves de Brito                                  \n",
       "Cristina Toscano Fonseca                                           \n",
       "Daniel Moreira de Avelar                                           \n",
       "Edelberto Santos Dias                                              \n",
       "Edward José de Oliveira                                            \n",
       "Erika Michalsky Monteiro                                           \n",
       "Flora Satiko Kano                                                  \n",
       "Gabriel da Rocha Fernandes             A3, A1, A1, A3, A2, A1, A1  \n",
       "Glaucia Fernandes Cota                         A1, A1, A1, A1, B1  \n",
       "Gustavo Fontes Paz                                                 \n",
       "Jaquelline Germano de Oliveira                                 A2  \n",
       "Jeronimo Conceição Ruiz                                            \n",
       "José Dilermando Andrade Filho                                      \n",
       "Lileia Gonçalves Diotaiuti                                         \n",
       "Lis Ribeiro do Valle Antonelli                                 A1  \n",
       "Luciano Andrade Moreira                                            \n",
       "Luiz Carlos Júnior Alcantara               A2, B2, A2, A1, A1, A2  \n",
       "Luzia Helena Carvalho                                              \n",
       "Marcelo Antonio Pascoal Xavier                             A4, A1  \n",
       "Marcelo Gustavo Lorenzo                                            \n",
       "Marco Antônio da Silva Campos                      Não encontrado  \n",
       "Marina de Moraes Mourão                            Não encontrado  \n",
       "Márcio Sobreira Silva Araújo                           A1, A4, A1  \n",
       "Nagila Francinete Costa Secundino                                  \n",
       "Olindo Assis Martins Filho              A2, A1, A1, C, A1, A4, A1  \n",
       "Paloma Helena Fernandes Shimabukuro                     B1, C, B2  \n",
       "Paulo Filemon Paolucci Pimenta                                     \n",
       "Paulo Marcos Zech Coelho                                       A2  \n",
       "Pedro Augusto Alves                    A1, Não encontrado, A2, A1  \n",
       "Rafaella Fortini Grenfell e Queiroz                            A2  \n",
       "Ricardo Tostes Gazzinelli                                          \n",
       "Rita de Cássia Moreira de Souza                                    \n",
       "Roberta Lima Caldeira                                          A2  \n",
       "Rodrigo Correa de Oliveira                                         \n",
       "Rodrigo Pedro Pinto Soares                             A4, A1, A4  \n",
       "Roney Santos Coimbra                                           A2  \n",
       "Rosiane Aparecida da Silva Pereira                             A1  \n",
       "Rubens Lima do Monte Neto                  A4, Não encontrado, A1  \n",
       "Silvane Maria Fonseca Murta                            A1, A1, A1  \n",
       "Soraya Torres Gaze Jangola                                         \n",
       "Taís Nóbrega de Sousa                                      A1, A1  \n",
       "Tânia Maria de Almeida Alves                                       \n",
       "Vanessa Peruhype Magalhães Pascoal                              C  \n",
       "Álvaro Gil Araújo Ferreira                                         \n",
       "Érica Alessandra Rocha Alves                                       "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atualizador.apurar_qualis_periodo(dict_list, ano_inicio, ano_final).fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57866783",
   "metadata": {},
   "source": [
    "### Avaliação da pontuação participação em artigos por anos do período ponderada pelo Qualis Periódicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ba17bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pontuacao = atualizador.apurar_pontos_periodo(dict_list, ano_inicio, ano_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b958e62",
   "metadata": {},
   "source": [
    "### Contagem da publicação com discentes\n",
    "\n",
    "- Busca por nomes de discentes nas publicações dos currículos dos docentes e \n",
    "- Calcular o percentual de publicações com participação de discente da lista fornecida pelo programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c08f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 discentes no período 2021-2024 informados pelos programa\n"
     ]
    }
   ],
   "source": [
    "discent_collab_counter = DiscentCollaborationCounter(dict_list)\n",
    "\n",
    "fonte_planilha = 'ppgcs_estudantes_2021-2024.xlsx'\n",
    "dados_discentes = pd.read_excel(os.path.join(LattesScraper.find_repo_root(),'_data','in_xls',fonte_planilha), header=1)\n",
    "lista_discentes = list(dados_discentes['Discente'].unique())\n",
    "print(f'{len(lista_discentes)} discentes no período 2021-2024 informados pelos programa')\n",
    "\n",
    "lista_normalizada_discentes=[]\n",
    "for i in lista_discentes:\n",
    "    lista_normalizada_discentes.append(discent_collab_counter.iniciais_nome(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583cd37f",
   "metadata": {},
   "source": [
    "### Extrair lista de coautorias para buscar participação discente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c7954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 discentes no período 2021-2024 informados pelos programa\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Ano</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "      <th>2024</th>\n",
       "      <th>Soma de Pontos</th>\n",
       "      <th>colab_disc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Autor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Olindo Assis Martins Filho</th>\n",
       "      <td>1505</td>\n",
       "      <td>1120</td>\n",
       "      <td>1850</td>\n",
       "      <td>1730</td>\n",
       "      <td>1485</td>\n",
       "      <td>1850</td>\n",
       "      <td>1465</td>\n",
       "      <td>480</td>\n",
       "      <td>11485</td>\n",
       "      <td>99.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luiz Carlos Júnior Alcantara</th>\n",
       "      <td>430</td>\n",
       "      <td>650</td>\n",
       "      <td>845</td>\n",
       "      <td>1300</td>\n",
       "      <td>1560</td>\n",
       "      <td>2025</td>\n",
       "      <td>1610</td>\n",
       "      <td>435</td>\n",
       "      <td>8855</td>\n",
       "      <td>98.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andréa Teixeira de Carvalho</th>\n",
       "      <td>1135</td>\n",
       "      <td>1115</td>\n",
       "      <td>1370</td>\n",
       "      <td>1555</td>\n",
       "      <td>730</td>\n",
       "      <td>1610</td>\n",
       "      <td>930</td>\n",
       "      <td>250</td>\n",
       "      <td>8695</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ricardo Tostes Gazzinelli</th>\n",
       "      <td>510</td>\n",
       "      <td>530</td>\n",
       "      <td>430</td>\n",
       "      <td>790</td>\n",
       "      <td>940</td>\n",
       "      <td>790</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>4490</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gabriel da Rocha Fernandes</th>\n",
       "      <td>920</td>\n",
       "      <td>370</td>\n",
       "      <td>170</td>\n",
       "      <td>430</td>\n",
       "      <td>150</td>\n",
       "      <td>825</td>\n",
       "      <td>750</td>\n",
       "      <td>560</td>\n",
       "      <td>4175</td>\n",
       "      <td>95.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glaucia Fernandes Cota</th>\n",
       "      <td>570</td>\n",
       "      <td>545</td>\n",
       "      <td>310</td>\n",
       "      <td>620</td>\n",
       "      <td>610</td>\n",
       "      <td>430</td>\n",
       "      <td>290</td>\n",
       "      <td>380</td>\n",
       "      <td>3755</td>\n",
       "      <td>98.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lis Ribeiro do Valle Antonelli</th>\n",
       "      <td>310</td>\n",
       "      <td>630</td>\n",
       "      <td>690</td>\n",
       "      <td>470</td>\n",
       "      <td>670</td>\n",
       "      <td>530</td>\n",
       "      <td>270</td>\n",
       "      <td>90</td>\n",
       "      <td>3660</td>\n",
       "      <td>98.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rodrigo Correa de Oliveira</th>\n",
       "      <td>1290</td>\n",
       "      <td>360</td>\n",
       "      <td>770</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>230</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "      <td>3640</td>\n",
       "      <td>98.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rodrigo Pedro Pinto Soares</th>\n",
       "      <td>540</td>\n",
       "      <td>560</td>\n",
       "      <td>620</td>\n",
       "      <td>520</td>\n",
       "      <td>750</td>\n",
       "      <td>150</td>\n",
       "      <td>220</td>\n",
       "      <td>170</td>\n",
       "      <td>3530</td>\n",
       "      <td>95.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paulo Filemon Paolucci Pimenta</th>\n",
       "      <td>320</td>\n",
       "      <td>830</td>\n",
       "      <td>350</td>\n",
       "      <td>640</td>\n",
       "      <td>500</td>\n",
       "      <td>330</td>\n",
       "      <td>410</td>\n",
       "      <td>0</td>\n",
       "      <td>3380</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luciano Andrade Moreira</th>\n",
       "      <td>560</td>\n",
       "      <td>350</td>\n",
       "      <td>610</td>\n",
       "      <td>480</td>\n",
       "      <td>740</td>\n",
       "      <td>260</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>3180</td>\n",
       "      <td>95.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Silvane Maria Fonseca Murta</th>\n",
       "      <td>340</td>\n",
       "      <td>320</td>\n",
       "      <td>360</td>\n",
       "      <td>460</td>\n",
       "      <td>355</td>\n",
       "      <td>520</td>\n",
       "      <td>460</td>\n",
       "      <td>270</td>\n",
       "      <td>3085</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antonio Mauro Rezende</th>\n",
       "      <td>230</td>\n",
       "      <td>360</td>\n",
       "      <td>480</td>\n",
       "      <td>560</td>\n",
       "      <td>450</td>\n",
       "      <td>280</td>\n",
       "      <td>390</td>\n",
       "      <td>15</td>\n",
       "      <td>2765</td>\n",
       "      <td>65.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Celia Maria Ferreira Gontijo</th>\n",
       "      <td>480</td>\n",
       "      <td>585</td>\n",
       "      <td>390</td>\n",
       "      <td>500</td>\n",
       "      <td>260</td>\n",
       "      <td>270</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>2745</td>\n",
       "      <td>94.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nagila Francinete Costa Secundino</th>\n",
       "      <td>320</td>\n",
       "      <td>480</td>\n",
       "      <td>200</td>\n",
       "      <td>530</td>\n",
       "      <td>510</td>\n",
       "      <td>270</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>2730</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanessa Peruhype Magalhães Pascoal</th>\n",
       "      <td>460</td>\n",
       "      <td>185</td>\n",
       "      <td>330</td>\n",
       "      <td>305</td>\n",
       "      <td>305</td>\n",
       "      <td>740</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "      <td>2715</td>\n",
       "      <td>94.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>José Dilermando Andrade Filho</th>\n",
       "      <td>670</td>\n",
       "      <td>10</td>\n",
       "      <td>285</td>\n",
       "      <td>400</td>\n",
       "      <td>320</td>\n",
       "      <td>600</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>2525</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cristiana Couto Garcia</th>\n",
       "      <td>310</td>\n",
       "      <td>240</td>\n",
       "      <td>80</td>\n",
       "      <td>380</td>\n",
       "      <td>170</td>\n",
       "      <td>650</td>\n",
       "      <td>585</td>\n",
       "      <td>0</td>\n",
       "      <td>2415</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cristiana Ferreira Alves de Brito</th>\n",
       "      <td>370</td>\n",
       "      <td>440</td>\n",
       "      <td>530</td>\n",
       "      <td>270</td>\n",
       "      <td>330</td>\n",
       "      <td>160</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>2270</td>\n",
       "      <td>98.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alessandra Aparecida Guarneri</th>\n",
       "      <td>350</td>\n",
       "      <td>220</td>\n",
       "      <td>210</td>\n",
       "      <td>430</td>\n",
       "      <td>340</td>\n",
       "      <td>260</td>\n",
       "      <td>360</td>\n",
       "      <td>90</td>\n",
       "      <td>2260</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marcelo Antonio Pascoal Xavier</th>\n",
       "      <td>225</td>\n",
       "      <td>545</td>\n",
       "      <td>0</td>\n",
       "      <td>435</td>\n",
       "      <td>370</td>\n",
       "      <td>200</td>\n",
       "      <td>240</td>\n",
       "      <td>130</td>\n",
       "      <td>2145</td>\n",
       "      <td>98.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Márcio Sobreira Silva Araújo</th>\n",
       "      <td>145</td>\n",
       "      <td>665</td>\n",
       "      <td>180</td>\n",
       "      <td>215</td>\n",
       "      <td>185</td>\n",
       "      <td>475</td>\n",
       "      <td>40</td>\n",
       "      <td>220</td>\n",
       "      <td>2125</td>\n",
       "      <td>97.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pedro Augusto Alves</th>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>250</td>\n",
       "      <td>440</td>\n",
       "      <td>140</td>\n",
       "      <td>460</td>\n",
       "      <td>380</td>\n",
       "      <td>260</td>\n",
       "      <td>2105</td>\n",
       "      <td>95.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rubens Lima do Monte Neto</th>\n",
       "      <td>80</td>\n",
       "      <td>400</td>\n",
       "      <td>40</td>\n",
       "      <td>280</td>\n",
       "      <td>420</td>\n",
       "      <td>425</td>\n",
       "      <td>330</td>\n",
       "      <td>130</td>\n",
       "      <td>2105</td>\n",
       "      <td>83.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marina de Moraes Mourão</th>\n",
       "      <td>460</td>\n",
       "      <td>90</td>\n",
       "      <td>390</td>\n",
       "      <td>330</td>\n",
       "      <td>220</td>\n",
       "      <td>410</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>2100</td>\n",
       "      <td>97.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taís Nóbrega de Sousa</th>\n",
       "      <td>160</td>\n",
       "      <td>260</td>\n",
       "      <td>310</td>\n",
       "      <td>350</td>\n",
       "      <td>320</td>\n",
       "      <td>350</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>2090</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ana Lúcia Teles Rabello</th>\n",
       "      <td>610</td>\n",
       "      <td>210</td>\n",
       "      <td>610</td>\n",
       "      <td>360</td>\n",
       "      <td>100</td>\n",
       "      <td>180</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2090</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edward José de Oliveira</th>\n",
       "      <td>110</td>\n",
       "      <td>100</td>\n",
       "      <td>380</td>\n",
       "      <td>395</td>\n",
       "      <td>230</td>\n",
       "      <td>350</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>98.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luzia Helena Carvalho</th>\n",
       "      <td>250</td>\n",
       "      <td>350</td>\n",
       "      <td>170</td>\n",
       "      <td>360</td>\n",
       "      <td>410</td>\n",
       "      <td>260</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1880</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roney Santos Coimbra</th>\n",
       "      <td>520</td>\n",
       "      <td>410</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>160</td>\n",
       "      <td>90</td>\n",
       "      <td>420</td>\n",
       "      <td>80</td>\n",
       "      <td>1850</td>\n",
       "      <td>88.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paulo Marcos Zech Coelho</th>\n",
       "      <td>375</td>\n",
       "      <td>230</td>\n",
       "      <td>435</td>\n",
       "      <td>260</td>\n",
       "      <td>140</td>\n",
       "      <td>20</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>1730</td>\n",
       "      <td>99.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gustavo Fontes Paz</th>\n",
       "      <td>550</td>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "      <td>260</td>\n",
       "      <td>180</td>\n",
       "      <td>320</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1710</td>\n",
       "      <td>97.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roberta Lima Caldeira</th>\n",
       "      <td>310</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>280</td>\n",
       "      <td>250</td>\n",
       "      <td>220</td>\n",
       "      <td>355</td>\n",
       "      <td>80</td>\n",
       "      <td>1685</td>\n",
       "      <td>91.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marcelo Gustavo Lorenzo</th>\n",
       "      <td>180</td>\n",
       "      <td>80</td>\n",
       "      <td>170</td>\n",
       "      <td>510</td>\n",
       "      <td>440</td>\n",
       "      <td>170</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1630</td>\n",
       "      <td>74.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jeronimo Conceição Ruiz</th>\n",
       "      <td>450</td>\n",
       "      <td>220</td>\n",
       "      <td>60</td>\n",
       "      <td>410</td>\n",
       "      <td>330</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1630</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caroline Furtado Junqueira</th>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>240</td>\n",
       "      <td>200</td>\n",
       "      <td>450</td>\n",
       "      <td>0</td>\n",
       "      <td>1590</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carlos Eduardo Calzavara Silva</th>\n",
       "      <td>20</td>\n",
       "      <td>290</td>\n",
       "      <td>60</td>\n",
       "      <td>330</td>\n",
       "      <td>90</td>\n",
       "      <td>350</td>\n",
       "      <td>330</td>\n",
       "      <td>90</td>\n",
       "      <td>1560</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cristina Toscano Fonseca</th>\n",
       "      <td>150</td>\n",
       "      <td>210</td>\n",
       "      <td>180</td>\n",
       "      <td>80</td>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>350</td>\n",
       "      <td>0</td>\n",
       "      <td>1450</td>\n",
       "      <td>89.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paloma Helena Fernandes Shimabukuro</th>\n",
       "      <td>130</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>290</td>\n",
       "      <td>170</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>35</td>\n",
       "      <td>1415</td>\n",
       "      <td>89.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rita de Cássia Moreira de Souza</th>\n",
       "      <td>270</td>\n",
       "      <td>95</td>\n",
       "      <td>280</td>\n",
       "      <td>110</td>\n",
       "      <td>400</td>\n",
       "      <td>140</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>1405</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flora Satiko Kano</th>\n",
       "      <td>180</td>\n",
       "      <td>270</td>\n",
       "      <td>170</td>\n",
       "      <td>260</td>\n",
       "      <td>250</td>\n",
       "      <td>180</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1390</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lileia Gonçalves Diotaiuti</th>\n",
       "      <td>400</td>\n",
       "      <td>180</td>\n",
       "      <td>20</td>\n",
       "      <td>205</td>\n",
       "      <td>200</td>\n",
       "      <td>240</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1305</td>\n",
       "      <td>99.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rafaella Fortini Grenfell e Queiroz</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>235</td>\n",
       "      <td>90</td>\n",
       "      <td>220</td>\n",
       "      <td>260</td>\n",
       "      <td>250</td>\n",
       "      <td>80</td>\n",
       "      <td>1195</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tânia Maria de Almeida Alves</th>\n",
       "      <td>120</td>\n",
       "      <td>320</td>\n",
       "      <td>210</td>\n",
       "      <td>160</td>\n",
       "      <td>120</td>\n",
       "      <td>160</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>1185</td>\n",
       "      <td>98.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Érica Alessandra Rocha Alves</th>\n",
       "      <td>140</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "      <td>965</td>\n",
       "      <td>88.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edelberto Santos Dias</th>\n",
       "      <td>100</td>\n",
       "      <td>170</td>\n",
       "      <td>220</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daniel Moreira de Avelar</th>\n",
       "      <td>20</td>\n",
       "      <td>75</td>\n",
       "      <td>190</td>\n",
       "      <td>100</td>\n",
       "      <td>170</td>\n",
       "      <td>250</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>895</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alexandre de Magalhaes Vieira Machado</th>\n",
       "      <td>380</td>\n",
       "      <td>85</td>\n",
       "      <td>150</td>\n",
       "      <td>90</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>875</td>\n",
       "      <td>94.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rosiane Aparecida da Silva Pereira</th>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>810</td>\n",
       "      <td>78.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Erika Michalsky Monteiro</th>\n",
       "      <td>100</td>\n",
       "      <td>170</td>\n",
       "      <td>200</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carina Margonari de Souza</th>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>310</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>790</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Álvaro Gil Araújo Ferreira</th>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>40</td>\n",
       "      <td>170</td>\n",
       "      <td>250</td>\n",
       "      <td>60</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>790</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaquelline Germano de Oliveira</th>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>730</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soraya Torres Gaze Jangola</th>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>175</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>65.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marco Antônio da Silva Campos</th>\n",
       "      <td>60</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>140</td>\n",
       "      <td>20</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>495</td>\n",
       "      <td>96.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ano                                    2017  2018  2019  2020  2021  2022  \\\n",
       "Autor                                                                       \n",
       "Olindo Assis Martins Filho             1505  1120  1850  1730  1485  1850   \n",
       "Luiz Carlos Júnior Alcantara            430   650   845  1300  1560  2025   \n",
       "Andréa Teixeira de Carvalho            1135  1115  1370  1555   730  1610   \n",
       "Ricardo Tostes Gazzinelli               510   530   430   790   940   790   \n",
       "Gabriel da Rocha Fernandes              920   370   170   430   150   825   \n",
       "Glaucia Fernandes Cota                  570   545   310   620   610   430   \n",
       "Lis Ribeiro do Valle Antonelli          310   630   690   470   670   530   \n",
       "Rodrigo Correa de Oliveira             1290   360   770   360   360   230   \n",
       "Rodrigo Pedro Pinto Soares              540   560   620   520   750   150   \n",
       "Paulo Filemon Paolucci Pimenta          320   830   350   640   500   330   \n",
       "Luciano Andrade Moreira                 560   350   610   480   740   260   \n",
       "Silvane Maria Fonseca Murta             340   320   360   460   355   520   \n",
       "Antonio Mauro Rezende                   230   360   480   560   450   280   \n",
       "Celia Maria Ferreira Gontijo            480   585   390   500   260   270   \n",
       "Nagila Francinete Costa Secundino       320   480   200   530   510   270   \n",
       "Vanessa Peruhype Magalhães Pascoal      460   185   330   305   305   740   \n",
       "José Dilermando Andrade Filho           670    10   285   400   320   600   \n",
       "Cristiana Couto Garcia                  310   240    80   380   170   650   \n",
       "Cristiana Ferreira Alves de Brito       370   440   530   270   330   160   \n",
       "Alessandra Aparecida Guarneri           350   220   210   430   340   260   \n",
       "Marcelo Antonio Pascoal Xavier          225   545     0   435   370   200   \n",
       "Márcio Sobreira Silva Araújo            145   665   180   215   185   475   \n",
       "Pedro Augusto Alves                       0   175   250   440   140   460   \n",
       "Rubens Lima do Monte Neto                80   400    40   280   420   425   \n",
       "Marina de Moraes Mourão                 460    90   390   330   220   410   \n",
       "Taís Nóbrega de Sousa                   160   260   310   350   320   350   \n",
       "Ana Lúcia Teles Rabello                 610   210   610   360   100   180   \n",
       "Edward José de Oliveira                 110   100   380   395   230   350   \n",
       "Luzia Helena Carvalho                   250   350   170   360   410   260   \n",
       "Roney Santos Coimbra                    520   410     0   170   160    90   \n",
       "Paulo Marcos Zech Coelho                375   230   435   260   140    20   \n",
       "Gustavo Fontes Paz                      550   170   170   260   180   320   \n",
       "Roberta Lima Caldeira                   310   110    80   280   250   220   \n",
       "Marcelo Gustavo Lorenzo                 180    80   170   510   440   170   \n",
       "Jeronimo Conceição Ruiz                 450   220    60   410   330    80   \n",
       "Caroline Furtado Junqueira              170   170     0   360   240   200   \n",
       "Carlos Eduardo Calzavara Silva           20   290    60   330    90   350   \n",
       "Cristina Toscano Fonseca                150   210   180    80   240   240   \n",
       "Paloma Helena Fernandes Shimabukuro     130   170     0   290   170   310   \n",
       "Rita de Cássia Moreira de Souza         270    95   280   110   400   140   \n",
       "Flora Satiko Kano                       180   270   170   260   250   180   \n",
       "Lileia Gonçalves Diotaiuti              400   180    20   205   200   240   \n",
       "Rafaella Fortini Grenfell e Queiroz       0    60   235    90   220   260   \n",
       "Tânia Maria de Almeida Alves            120   320   210   160   120   160   \n",
       "Érica Alessandra Rocha Alves            140   165     0   230   100    60   \n",
       "Edelberto Santos Dias                   100   170   220   130     0    80   \n",
       "Daniel Moreira de Avelar                 20    75   190   100   170   250   \n",
       "Alexandre de Magalhaes Vieira Machado   380    85   150    90   170     0   \n",
       "Rosiane Aparecida da Silva Pereira        0   130   190     0     0   220   \n",
       "Erika Michalsky Monteiro                100   170   200   130     0    80   \n",
       "Carina Margonari de Souza                80   100    60    80   310    80   \n",
       "Álvaro Gil Araújo Ferreira                0    90    40   170   250    60   \n",
       "Jaquelline Germano de Oliveira            0   160    60   290     0   140   \n",
       "Soraya Torres Gaze Jangola                0    95     0    80   175   100   \n",
       "Marco Antônio da Silva Campos            60    95     0   100   140    20   \n",
       "\n",
       "Ano                                    2023  2024  Soma de Pontos  colab_disc  \n",
       "Autor                                                                          \n",
       "Olindo Assis Martins Filho             1465  480   11485            99.15      \n",
       "Luiz Carlos Júnior Alcantara           1610  435    8855            98.46      \n",
       "Andréa Teixeira de Carvalho             930  250    8695           100.00      \n",
       "Ricardo Tostes Gazzinelli               500    0    4490           100.00      \n",
       "Gabriel da Rocha Fernandes              750  560    4175            95.18      \n",
       "Glaucia Fernandes Cota                  290  380    3755            98.78      \n",
       "Lis Ribeiro do Valle Antonelli          270   90    3660            98.81      \n",
       "Rodrigo Correa de Oliveira              270    0    3640            98.34      \n",
       "Rodrigo Pedro Pinto Soares              220  170    3530            95.50      \n",
       "Paulo Filemon Paolucci Pimenta          410    0    3380           100.00      \n",
       "Luciano Andrade Moreira                 180    0    3180            95.56      \n",
       "Silvane Maria Fonseca Murta             460  270    3085           100.00      \n",
       "Antonio Mauro Rezende                   390   15    2765            65.57      \n",
       "Celia Maria Ferreira Gontijo            260    0    2745            94.39      \n",
       "Nagila Francinete Costa Secundino       420    0    2730           100.00      \n",
       "Vanessa Peruhype Magalhães Pascoal      390    0    2715            94.87      \n",
       "José Dilermando Andrade Filho           240    0    2525           100.00      \n",
       "Cristiana Couto Garcia                  585    0    2415           100.00      \n",
       "Cristiana Ferreira Alves de Brito       170    0    2270            98.77      \n",
       "Alessandra Aparecida Guarneri           360   90    2260           100.00      \n",
       "Marcelo Antonio Pascoal Xavier          240  130    2145            98.11      \n",
       "Márcio Sobreira Silva Araújo             40  220    2125            97.56      \n",
       "Pedro Augusto Alves                     380  260    2105            95.24      \n",
       "Rubens Lima do Monte Neto               330  130    2105            83.64      \n",
       "Marina de Moraes Mourão                 200    0    2100            97.92      \n",
       "Taís Nóbrega de Sousa                   160  180    2090           100.00      \n",
       "Ana Lúcia Teles Rabello                  20    0    2090           100.00      \n",
       "Edward José de Oliveira                 390    0    1955            98.39      \n",
       "Luzia Helena Carvalho                    80    0    1880           100.00      \n",
       "Roney Santos Coimbra                    420   80    1850            88.89      \n",
       "Paulo Marcos Zech Coelho                190   80    1730            99.58      \n",
       "Gustavo Fontes Paz                       60    0    1710            97.44      \n",
       "Roberta Lima Caldeira                   355   80    1685            91.36      \n",
       "Marcelo Gustavo Lorenzo                  80    0    1630            74.67      \n",
       "Jeronimo Conceição Ruiz                  80    0    1630           100.00      \n",
       "Caroline Furtado Junqueira              450    0    1590            30.00      \n",
       "Carlos Eduardo Calzavara Silva          330   90    1560           100.00      \n",
       "Cristina Toscano Fonseca                350    0    1450            89.47      \n",
       "Paloma Helena Fernandes Shimabukuro     310   35    1415            89.13      \n",
       "Rita de Cássia Moreira de Souza         110    0    1405           100.00      \n",
       "Flora Satiko Kano                        80    0    1390           100.00      \n",
       "Lileia Gonçalves Diotaiuti               60    0    1305            99.35      \n",
       "Rafaella Fortini Grenfell e Queiroz     250   80    1195           100.00      \n",
       "Tânia Maria de Almeida Alves             95    0    1185            98.94      \n",
       "Érica Alessandra Rocha Alves            270    0     965            88.24      \n",
       "Edelberto Santos Dias                   200    0     900           100.00      \n",
       "Daniel Moreira de Avelar                 90    0     895           100.00      \n",
       "Alexandre de Magalhaes Vieira Machado     0    0     875            94.12      \n",
       "Rosiane Aparecida da Silva Pereira      180   90     810            78.26      \n",
       "Erika Michalsky Monteiro                120    0     800           100.00      \n",
       "Carina Margonari de Souza                80    0     790           100.00      \n",
       "Álvaro Gil Araújo Ferreira              180    0     790           100.00      \n",
       "Jaquelline Germano de Oliveira            0   80     730           100.00      \n",
       "Soraya Torres Gaze Jangola               90    0     540            65.52      \n",
       "Marco Antônio da Silva Campos            80    0     495            96.15      "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discent_collab_counter = DiscentCollaborationCounter(dict_list)\n",
    "colaboracoes, percentuais = discent_collab_counter.get_articles_coauthorings(dict_list)\n",
    "df_pontuacao['colab_disc'] = percentuais\n",
    "filepath = os.path.join(os.path.join(\"./\",\"_data\",\"powerbi\",'ppgcs_prodpubl_2017_2024.xlsx'))\n",
    "df_pontuacao.to_excel(filepath)\n",
    "df_pontuacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5acca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69102242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pesquisadores=[]\n",
    "# for artigo in colabs:\n",
    "#     for pesquisador, coautores in artigo.items():\n",
    "#         for nome_coautor in coautores:\n",
    "#             for nome_discente in lista_normalizada_discentes:\n",
    "#                 similaridade_sobrenome, similaridade_iniciais = discent_collab_counter.similar_index(nome_discente, nome_coautor)\n",
    "#                 if similaridade_sobrenome > 0.87 and similaridade_iniciais > 0.8:\n",
    "#                     print(f'{pesquisador:40} {nome_discente:20} {nome_coautor:25} | {similaridade_sobrenome:.6f} | {similaridade_iniciais:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da8026",
   "metadata": {},
   "source": [
    "## Montar Grafo e analisar redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ab57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j.exceptions import Neo4jError\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s', filename='logs/persister.log')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Neo4jPersister:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._uri = uri\n",
    "        self._user = user\n",
    "        self._password = password\n",
    "        self._driver = GraphDatabase.driver(self._uri, auth=(self._user, self._password))\n",
    "        self.logger = logger\n",
    "\n",
    "        # Devem ser persistidos como \n",
    "        self.tipos = ['Identificação',\n",
    "                      'Idiomas',\n",
    "                      'Formação',\n",
    "                      'Atuação Profissional',\n",
    "                      'Linhas de Pesquisa',\n",
    "                      'Áreas',\n",
    "                      'Produções',\n",
    "                      'ProjetosPesquisa',\n",
    "                      'ProjetosExtensão',\n",
    "                      'ProjetosDesenvolvimento',\n",
    "                      'ProjetosOutros',\n",
    "                      'Bancas',\n",
    "                      'Orientações',\n",
    "                      ]\n",
    "\n",
    "        self.subtipos= ['Acadêmica',\n",
    "                        'Pos-Doc',\n",
    "                        'Complementar',\n",
    "                        'Artigos completos publicados em periódicos',\n",
    "                        'Resumos publicados em anais de congressos',\n",
    "                        'Apresentações de Trabalho',\n",
    "                        'Outras produções bibliográficas',\n",
    "                        'Entrevistas, mesas redondas, programas e comentários na mídia',\n",
    "                        'Concurso público',\n",
    "                        'Outras participações',\n",
    "                        'Livros publicados/organizados ou edições',\n",
    "                        'Capítulos de livros publicados',\n",
    "                        'Resumos expandidos publicados em anais de congressos',\n",
    "                        'Resumos publicados em anais de congressos (artigos)',\n",
    "                        'Trabalhos técnicos',\n",
    "                        'Demais trabalhos',\n",
    "                        'Mestrado',\n",
    "                        'Teses de doutorado',\n",
    "                        'Qualificações de Doutorado',\n",
    "                        'Qualificações de Mestrado',\n",
    "                        'Monografias de cursos de aperfeiçoamento/especialização',\n",
    "                        'Trabalhos de conclusão de curso de graduação',\n",
    "                        'Orientações e supervisões concluídas',\n",
    "                        'Citações',\n",
    "                        'Trabalhos completos publicados em anais de congressos',\n",
    "                        'Produtos tecnológicos',\n",
    "                        'Artigos  aceitos para publicação',\n",
    "                        'Assessoria e consultoria',\n",
    "                        'Programas de computador sem registro',\n",
    "                        'Professor titular',\n",
    "                        'Avaliação de cursos',\n",
    "                        'Orientações e supervisões em andamento',\n",
    "                        'Processos ou técnicas',\n",
    "                        'Outras produções artísticas/culturais',\n",
    "                        'Textos em jornais de notícias/revistas',\n",
    "                        'Redes sociais, websites e blogs',\n",
    "                        'Artes Visuais'            \n",
    "                        ]\n",
    "\n",
    "        self.propriedades = ['Nome',\n",
    "                             'ID Lattes',\n",
    "                             'Última atualização',\n",
    "                             ]\n",
    "    # def run(self, query, **parameters):\n",
    "    #     with self.driver.session() as session:\n",
    "    #         return session.run(query, **parameters)\n",
    "        \n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "    \n",
    "    def persistir_revistas_da_planilha(self):\n",
    "        \"\"\"\n",
    "        Persiste dados de revistas a partir da planilha 'classificações_publicadas_todas_as_areas_avaliacao1672761192111.xlsx' no Neo4j.\n",
    "\n",
    "        Args:\n",
    "            session: Objeto de sessão do Neo4j.\n",
    "        \"\"\"\n",
    "        # Leitura da planilha\n",
    "        dados_qualis = pd.read_excel(os.path.join(LattesScraper.find_repo_root(),'data','classificações_publicadas_todas_as_areas_avaliacao1672761192111.xlsx'))\n",
    "\n",
    "        # Extração e persistência de dados de revista\n",
    "        with self._driver.session() as session:\n",
    "            for index, row in dados_qualis.iterrows():\n",
    "                issn = row['ISSN'].replace('-','')\n",
    "                nome_revista = row['Título']\n",
    "                area_avaliacao = row['Área de Avaliação']\n",
    "                estrato = row['Estrato']\n",
    "\n",
    "                # Verificação de existência da revista\n",
    "                revista_node = session.run(\"\"\"\n",
    "                    MATCH (j:Revista {issn: $issn})\n",
    "                    RETURN j\n",
    "                \"\"\", issn=issn).single()\n",
    "\n",
    "                if not revista_node:\n",
    "                    # Criação da revista se não existir\n",
    "                    session.run(\"\"\"\n",
    "                        CREATE (j:Revista {issn: $issn, nome_revista: $nome_revista, area_avaliacao: $area_avaliacao, estrato: $estrato})\n",
    "                    \"\"\", nome_revista=nome_revista, issn=issn, area_avaliacao=area_avaliacao,  estrato=estrato)\n",
    "\n",
    "    # Testes Ok! \n",
    "    def persist_pessoa_nodes(self, dict_list):\n",
    "        query_pessoa = \"\"\"\n",
    "        MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "        ON CREATE SET p.nome = $nome, p.ultima_atualizacao = $ultima_atualizacao\n",
    "        ON MATCH SET p.nome = $nome, p.ultima_atualizacao = $ultima_atualizacao\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self._driver.session() as session:\n",
    "                for item in dict_list:\n",
    "                    identificacao = item.get('Identificação')\n",
    "                    nome = identificacao.get('Nome')\n",
    "                    id_lattes = identificacao.get('ID Lattes')\n",
    "                    ultima_atualizacao = identificacao.get('Última atualização')\n",
    "                    if nome:\n",
    "                        session.run(query_pessoa, id_lattes=id_lattes, nome=nome, ultima_atualizacao=ultima_atualizacao)\n",
    "        except Exception as e:\n",
    "            self.logger.error('Erro ao criar node \"Pesquisador\": {}'.format(e))\n",
    "\n",
    "    # Testes Ok!         \n",
    "    def persist_pesquisador_grande_area_relationships(self, dict_list):\n",
    "        query_rel_pessoa_grande_area = \"\"\"\n",
    "        MATCH (p:Pesquisador {id_lattes: $id_lattes})\n",
    "        MATCH (ga:GrandeArea {nome: $grande_area_nome})\n",
    "        MERGE (p)-[:ATUA_EM]->(ga)\n",
    "        \"\"\"\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for item in dict_list:\n",
    "                identificacao = item.get('Identificação')\n",
    "                id_lattes = identificacao.get('ID Lattes')\n",
    "                areas = item.get('Áreas').values()\n",
    "                for area_string in areas:\n",
    "                    grande_area_nome, _, _ = self.extract_area_info(area_string)\n",
    "                    if grande_area_nome:\n",
    "                        session.run(query_rel_pessoa_grande_area, id_lattes=id_lattes, grande_area_nome=grande_area_nome)\n",
    "\n",
    "    # Testes Ok! \n",
    "    def persist_areas_nodes(self, dict_list):\n",
    "        query_grande_area = \"\"\"\n",
    "        MERGE (ga:GrandeArea {nome: $nome})\n",
    "        \"\"\"\n",
    "        query_area = \"\"\"\n",
    "        MATCH (ga:GrandeArea {nome: $grande_area_nome})\n",
    "        MERGE (a:Area {nome: $nome}) ON CREATE SET a:Area\n",
    "        MERGE (ga)-[:CONTEM]->(a)\n",
    "        \"\"\"\n",
    "        query_subarea = \"\"\"\n",
    "        MATCH (a:Area {nome: $area_nome})\n",
    "        MERGE (sa:Subarea {nome: $nome}) ON CREATE SET sa:Subarea\n",
    "        MERGE (a)-[:CONTEM]->(sa)\n",
    "        \"\"\"\n",
    "        query_rel_pessoa_grande_area = \"\"\"\n",
    "        MATCH (p:Pesquisador {id_lattes: $id_lattes})\n",
    "        MATCH (ga:GrandeArea {nome: $grande_area_nome})\n",
    "        MERGE (p)-[:ATUA_EM]->(ga)    \n",
    "        \"\"\"\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for item in dict_list:\n",
    "                areas = item.get('Áreas').values()\n",
    "                for area_string in areas:\n",
    "                    grande_area_nome, area_nome, subarea_nome = self.extract_area_info(area_string)\n",
    "                    \n",
    "                    # Verificar se o nome não está vazio\n",
    "                    if grande_area_nome:\n",
    "                        session.run(query_grande_area, nome=grande_area_nome)\n",
    "                    if area_nome:\n",
    "                        session.run(query_area, grande_area_nome=grande_area_nome, nome=area_nome)\n",
    "                    if subarea_nome:\n",
    "                        session.run(query_subarea, area_nome=area_nome, nome=subarea_nome)\n",
    "                    \n",
    "                    # Adicionar relacionamento Pesquisador - GrandeÁrea\n",
    "                    id_lattes = item['Identificação']['ID Lattes']\n",
    "                    if grande_area_nome:\n",
    "                        session.run(query_rel_pessoa_grande_area, id_lattes=id_lattes, grande_area_nome=grande_area_nome)\n",
    "\n",
    "    # Testes Ok! \n",
    "    @staticmethod\n",
    "    def extract_area_info(area_string):\n",
    "        # Extraindo os nomes de GrandeÁrea, Área e Subárea da string\n",
    "        try:\n",
    "            grande_area_nome = area_string.split('/')[0].strip().split(': ')[1]\n",
    "        except:\n",
    "            grande_area_nome = ''\n",
    "        try:\n",
    "            area_nome = area_string.split('/')[1].strip().split(': ')[1]\n",
    "        except:\n",
    "            area_nome = ''\n",
    "        try:\n",
    "            subarea_nome = area_string.split('/')[2].strip().split(': ')[1]\n",
    "        except:\n",
    "            subarea_nome = ''\n",
    "        return grande_area_nome, area_nome, subarea_nome\n",
    "\n",
    "    ## PRODUÇÕES\n",
    "    def persist_producoes_pesquisador(self, dict_list):\n",
    "        with self._driver.session() as session:\n",
    "            for pesq in dict_list:\n",
    "                identificacao = pesq.get('Identificação')\n",
    "                id_lattes = identificacao.get('ID Lattes')\n",
    "                producoes = pesq.get('Produções')\n",
    "\n",
    "                if not isinstance(producoes, dict):\n",
    "                    print(f\"Erro!! Dicionário da seção 'Produções' não encontrado para {id_lattes}\")\n",
    "                    continue\n",
    "\n",
    "                for chave_producao, valores_producao in producoes.items():\n",
    "                    print(f'{chave_producao} | {valores_producao}')\n",
    "                    if chave_producao == 'Artigos completos publicados em periódicos':\n",
    "                        # self.persistir_artigos_completos(session, id_lattes, valores_producao)\n",
    "                        self.persistir_artigos_revistas(session, id_lattes, valores_producao)\n",
    "\n",
    "    def _get_or_create_node(self, session, label, properties):\n",
    "        properties = [x.rstrip('.') for x in properties]\n",
    "        node = session.run(\"MATCH (n: {label}) WHERE {properties} RETURN n\", {\"label\": label, \"properties\": properties}).single()\n",
    "\n",
    "        if not node:\n",
    "            node = session.run(\"CREATE (n: {label} {properties}) RETURN n\", {\"label\": label, \"properties\": properties}).single()[\"n\"]\n",
    "            self._node_created_count += 1\n",
    "\n",
    "        return node\n",
    "\n",
    "    def persist_tipo_producao(self, session, id_lattes, tipo_producao):\n",
    "        query_create_node = \"\"\"\n",
    "        MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "        MERGE (t:TipoProducao {nome: $tipo_producao})\n",
    "        MERGE (p)-[:PRODUZ]->(t)\n",
    "        \"\"\"\n",
    "        result = session.run(query_create_node, id_lattes=id_lattes, tipo_producao=tipo_producao)\n",
    "        summary = result.consume()\n",
    "        return summary.counters.nodes_created, summary.counters.nodes_deleted, summary.counters.relationships_created, summary.counters.relationships_deleted\n",
    "\n",
    "    def persist_subtipo_producao(self, session, id_lattes, tipo_producao, subtipo_producao, dados_producao):\n",
    "        def checar_e_serializar(dados):\n",
    "            \"\"\" Verifica e serializa dicionários recursivamente \"\"\"\n",
    "            if isinstance(dados, dict):\n",
    "                for chave, valor in dados.items():\n",
    "                    if isinstance(valor, dict):\n",
    "                        dados[chave] = json.dumps(valor)\n",
    "                    # Checagem adicional para outros tipos inválidos, se necessário\n",
    "            return dados\n",
    "        # Serialização recursiva do dicionário\n",
    "        dados_producao = checar_e_serializar(dados_producao)\n",
    "\n",
    "        query_create_node = \"\"\"\n",
    "        MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "        MERGE (t:TipoProducao {nome: $tipo_producao})\n",
    "        MERGE (s:SubtipoProducao {nome: $subtipo_producao})\n",
    "        MERGE (p)-[PRODUZ:]->(t)-[:DO_TIPO]->(s)\n",
    "        \"\"\"\n",
    "\n",
    "        if subtipo_producao in [\"ArtigoCompleto\", \"ResumoCongresso\", \"ApresentacaoTrabalho\", \"OutrasProducoesBibliograficas\"]:\n",
    "            query_create_node += \"\"\"\n",
    "            MERGE (o:Ocorrencia {tipo: $subtipo_producao, dados: $dados})\n",
    "            MERGE (s)-[:OCORRENCIA]->(o)\n",
    "            \"\"\"\n",
    "\n",
    "        result = session.run(query_create_node, id_lattes=id_lattes, tipo_producao=tipo_producao, \n",
    "                            subtipo_producao=subtipo_producao, dados=dados_producao)\n",
    "\n",
    "        # Obtendo as informações de contadores\n",
    "        summary = result.consume()\n",
    "\n",
    "        return summary.counters.nodes_created, summary.counters.nodes_deleted, summary.counters.relationships_created, summary.counters.relationships_deleted\n",
    "\n",
    "    def persistir_artigos_completos(self, session, id_lattes, dados):\n",
    "        created_nodes = 0\n",
    "        updated_nodes = 0\n",
    "        created_relations = 0\n",
    "        updated_relations = 0\n",
    "        \n",
    "        for dados_artigo in dados:\n",
    "            dados_artigo['dados'] = json.dumps(dados_artigo)  # Conversão para JSON da estrutura completa\n",
    "            query_create_node_artigo = \"\"\"\n",
    "                MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "                CREATE (a:ArtigoPublicado {\n",
    "                    ano: $ano,\n",
    "                    fator_impacto_jcr: $fator_impacto_jcr,\n",
    "                    ISSN: $ISSN,\n",
    "                    titulo: $titulo,\n",
    "                    revista: $revista,\n",
    "                    autores: $autores,\n",
    "                    Qualis: $Qualis,\n",
    "                    DOI: $DOI,\n",
    "                    dados: $dados_artigo\n",
    "                })\n",
    "                CREATE (p)-[:PRODUZ]->(a)\n",
    "\n",
    "                MERGE (j:Revista {nome: $revista, issn: $ISSN})\n",
    "                CREATE (a)-[:PUBLICADO_EM]->(j)\n",
    "            \"\"\"\n",
    "\n",
    "            result_artigo = session.run(query_create_node_artigo, \n",
    "                                id_lattes=id_lattes, \n",
    "                                ano=dados_artigo['ano'],\n",
    "                                fator_impacto_jcr=dados_artigo['fator_impacto_jcr'],\n",
    "                                ISSN=dados_artigo['ISSN'],\n",
    "                                titulo=dados_artigo['titulo'],\n",
    "                                revista=dados_artigo['revista'],\n",
    "                                autores=dados_artigo['autores'],\n",
    "                                Qualis=dados_artigo['Qualis'],\n",
    "                                DOI=dados_artigo['DOI'],\n",
    "                                dados_artigo=dados_artigo\n",
    "                                )\n",
    "            summary_artigo = result_artigo.consume()\n",
    "            created_nodes += summary_artigo.counters.nodes_created\n",
    "            updated_nodes += summary_artigo.counters.nodes_deleted\n",
    "            created_relations += summary_artigo.counters.relationships_created\n",
    "            updated_relations += summary_artigo.counters.relationships_deleted\n",
    "\n",
    "        return created_nodes, updated_nodes, created_relations, updated_relations\n",
    "\n",
    "\n",
    "    def persistir_artigos_completos(self, session, id_lattes, dados):\n",
    "        created_nodes = 0\n",
    "        updated_nodes = 0\n",
    "        created_relations = 0\n",
    "        updated_relations = 0\n",
    "                \n",
    "        for dados_artigo in dados:\n",
    "            ano = dados_artigo['ano']\n",
    "            impact_jcr = dados_artigo['fator_impacto_jcr']\n",
    "            issn = dados_artigo['ISSN']\n",
    "            titulo = dados_artigo['titulo']\n",
    "            revista = dados_artigo['revista']\n",
    "            autores = dados_artigo['autores']\n",
    "            qualis = dados_artigo['Qualis']\n",
    "            doi = dados_artigo['DOI']\n",
    "\n",
    "            query_create_node_artigo = \"\"\"\n",
    "                MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "                CREATE (a:ArtigoPublicado {ano: $ano, impact_jcr: $impact_jcr, issn: $issn, titulo: $titulo, revista: $revista, autores: $autores, qualis: $qualis, doi: $doi})\n",
    "                CREATE (p)-[:PRODUZ]->(a)\n",
    "                MERGE (j:Revista {nome: $revista, issn: $issn})\n",
    "                CREATE (a)-[:PUBLICADO_EM]->(j)\n",
    "            \"\"\"\n",
    "            print(query_create_node_artigo)\n",
    "            result_artigo = session.run(query_create_node_artigo, \n",
    "                                id_lattes=id_lattes, \n",
    "                                ano=ano,\n",
    "                                impact_jcr=impact_jcr,\n",
    "                                issn=issn,\n",
    "                                titulo=titulo,\n",
    "                                revista=revista,\n",
    "                                autores=autores,\n",
    "                                qualis=qualis,\n",
    "                                doi=doi,\n",
    "                                )\n",
    "            summary_artigo = result_artigo.consume()\n",
    "            created_nodes += summary_artigo.counters.nodes_created\n",
    "            updated_nodes += summary_artigo.counters.nodes_deleted\n",
    "            created_relations += summary_artigo.counters.relationships_created\n",
    "            updated_relations += summary_artigo.counters.relationships_deleted\n",
    "\n",
    "        return created_nodes, updated_nodes, created_relations, updated_relations\n",
    "\n",
    "    ## Agrupando nós de artigos por revista\n",
    "    # def persistir_artigos_revistas(self, session, id_lattes, dados):\n",
    "    #     created_nodes = 0\n",
    "    #     updated_nodes = 0\n",
    "    #     created_relations = 0\n",
    "    #     updated_relations = 0\n",
    "\n",
    "    #     for dados_artigo in dados:\n",
    "    #         ano = dados_artigo['ano']\n",
    "    #         impact_jcr = dados_artigo['fator_impacto_jcr']\n",
    "    #         issn = dados_artigo['ISSN']\n",
    "    #         titulo = dados_artigo['titulo']\n",
    "    #         revista = dados_artigo['revista']\n",
    "    #         autores = dados_artigo['autores']\n",
    "    #         qualis = dados_artigo['Qualis']\n",
    "    #         doi = dados_artigo['DOI']\n",
    "            \n",
    "    #         query_create_node_artigo = \"\"\"\n",
    "    #             MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "    #             CREATE (a:ArtigoPublicado {ano: $ano, impact_jcr: $impact_jcr, issn: $issn, titulo: $titulo, revista: $revista, autores: $autores, qualis: $qualis, doi: $doi})\n",
    "    #             CREATE (p)-[:PRODUZ]->(a)\n",
    "    #             MERGE (j:Revista {nome: $revista, issn: $issn})\n",
    "    #             CREATE (a)-[:PUBLICADO_EM]->(j)\n",
    "    #         \"\"\"\n",
    "\n",
    "    #         with session.begin_transaction() as tx:\n",
    "    #             print(\"DEBUG: ISSN da revista:\", issn)\n",
    "    #             # Verificação de existência da revista\n",
    "    #             revista_node = tx.run(\"\"\"\n",
    "    #                 MATCH (j:Revista {issn: $issn})\n",
    "    #                 RETURN j\n",
    "    #             \"\"\", issn=issn).single()\n",
    "\n",
    "    #             if not revista_node:\n",
    "    #                 # Revista não encontrada, crie-a\n",
    "    #                 tx.run(\"\"\"\n",
    "    #                     CREATE (j:Revista {nome: $revista, issn: $issn})\n",
    "    #                 \"\"\", revista=revista, issn=issn)\n",
    "\n",
    "    #                 revista_node = tx.run(\"\"\"\n",
    "    #                     MATCH (j:Revista {issn: $issn})\n",
    "    #                     RETURN j\n",
    "    #                 \"\"\", issn=issn).single()\n",
    "\n",
    "    #             if revista_node is not None:\n",
    "    #                 print(\"DEBUG: Propriedades do nó Revista:\", revista_node.items())  \n",
    "    #                 node_revista = revista_node[0]  # Extrair o objeto Node\n",
    "    #                 print(f\"revista_node[0] {node_revista}\")\n",
    "    #                 issn = node_revista['issn']  # Acessar a propriedade 'issn'\n",
    "    #                 print(f\"node_revista['issn'] {issn}\")\n",
    "    #                 revista_nome = node_revista['nome_revista']\n",
    "    #                 revista_area_avaliacao = node_revista['area_avaliacao']\n",
    "    #                 revista_estrato = node_revista['estrato']\n",
    "    #             else:\n",
    "    #                 print(\"Erro: O retorno para a revista com ISSN\", issn, \"é None.\")\n",
    "\n",
    "    #             # Criação do nó do artigo\n",
    "    #             tx.run(query_create_node_artigo, \n",
    "    #                 id_lattes=id_lattes, \n",
    "    #                 ano=ano,\n",
    "    #                 impact_jcr=impact_jcr,\n",
    "    #                 issn=issn,\n",
    "    #                 titulo=titulo,\n",
    "    #                 revista=revista,\n",
    "    #                 autores=autores,\n",
    "    #                 qualis=qualis,\n",
    "    #                 doi=doi)\n",
    "\n",
    "    #             # Criação do relacionamento PUBLICADO_EM\n",
    "    #             tx.run(\"\"\"\n",
    "    #                 MATCH (a:ArtigoPublicado {doi: $doi}), (j:Revista {nome_revista: $revista_nome, issn: $issn, area_avaliacao: $revista_area_avaliacao, estrato: $revista_estrato})\n",
    "    #                 CREATE (a)-[:PUBLICADO_EM]->(j)\n",
    "    #             \"\"\", doi=doi, revista_nome=revista_nome, issn=issn, revista_area_avaliacao=revista_area_avaliacao, revista_estrato=revista_estrato)\n",
    "\n",
    "    #             tx.commit()\n",
    "\n",
    "    #         # Atualização dos contadores\n",
    "    #         created_nodes += tx.run(\"MATCH (n) WHERE n:ArtigoPublicado RETURN count(n)\").single()[0]\n",
    "    #         updated_nodes += tx.run(\"MATCH (n) WHERE n:ArtigoPublicado SET n.updated_at = datetime() RETURN count(n)\").single()[0]\n",
    "    #         created_relations += tx.run(\"MATCH (r) WHERE r:PUBLICADO_EM RETURN count(r)\").single()[0]\n",
    "    #         updated_relations += tx.run(\"MATCH (r) WHERE r:PUBLICADO_EM SET r.updated_at = datetime() RETURN count(r)\").single()[0]\n",
    "\n",
    "    #     return created_nodes, updated_nodes, created_relations, updated_relations\n",
    "\n",
    "    def buscar_revista_por_issn(session, issn):\n",
    "        query = \"\"\"\n",
    "            MATCH (revista:Revista {issn: $issn})\n",
    "            RETURN revista\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            result = session.run(query, issn=issn)\n",
    "            return result.single()\n",
    "        except Neo4jError as e:\n",
    "            print(f\"Erro Neo4j ao buscar a revista por ISSN: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def persistir_artigos_revistas(self, session, id_lattes, dados):\n",
    "        \"\"\"\n",
    "        Função para persistir os dados de artigos completos publicados em periódicos.\n",
    "\n",
    "        Args:\n",
    "            session (neo4j.Session): Sessão Neo4j.\n",
    "            id_lattes (str): ID do Lattes do pesquisador.\n",
    "            dados (dict): Dicionário contendo os dados dos artigos.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        for artigo in dados:\n",
    "            # Extraindo informações do artigo\n",
    "            ano = artigo['ano']\n",
    "            impact_jcr = artigo['fator_impacto_jcr']\n",
    "            issn = artigo['ISSN']\n",
    "            titulo = artigo['titulo']\n",
    "            revista = artigo['revista']\n",
    "            autores = artigo['autores']\n",
    "            data_issn = artigo['data_issn']\n",
    "            doi = artigo['DOI']\n",
    "            qualis = artigo['Qualis']\n",
    "\n",
    "            query_create_node_artigo = \"\"\"\n",
    "                MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "                CREATE (a:ArtigoPublicado {ano: $ano, impact_jcr: $impact_jcr, issn: $issn, titulo: $titulo, revista: $revista, autores: $autores, qualis: $qualis, doi: $doi})\n",
    "                CREATE (p)-[:PRODUZ]->(a)\n",
    "                MERGE (j:Revista {nome: $revista, issn: $issn})\n",
    "                CREATE (a)-[:PUBLICADO_EM]->(j)\n",
    "            \"\"\"\n",
    "            \n",
    "            # Buscando o nó da revista\n",
    "            revista_node = self.buscar_revista_por_issn(session, issn)\n",
    "\n",
    "            # Criando o nó do artigo\n",
    "            with session.begin_transaction() as tx:\n",
    "                tx.run(query_create_node_artigo,\n",
    "                    id_lattes=id_lattes,\n",
    "                    ano=ano,\n",
    "                    impact_jcr=impact_jcr,\n",
    "                    issn=issn,\n",
    "                    titulo=titulo,\n",
    "                    revista=revista,\n",
    "                    autores=autores,\n",
    "                    data_issn=data_issn,\n",
    "                    doi=doi,\n",
    "                    qualis=qualis\n",
    "                    )\n",
    "\n",
    "                # Criando o relacionamento PUBLICADO_EM\n",
    "                if revista_node is not None:\n",
    "                    node_revista = revista_node[0][1]\n",
    "                    if node_revista is not None:\n",
    "                        revista_nome = node_revista['nome_revista']\n",
    "                        revista_issn = node_revista['issn']\n",
    "                        revista_area_avaliacao = node_revista['area_avaliacao']\n",
    "                        revista_estrato = node_revista['estrato']\n",
    "                    else:\n",
    "                        print(\"Erro: O nó da revista não foi encontrado para o ISSN\", issn)\n",
    "                        # Lógica de tratamento de erro (opcional)\n",
    "                else:\n",
    "                    print(\"Erro: O retorno para a revista com ISSN\", issn, \"é None.\")\n",
    "\n",
    "                if revista_nome:\n",
    "                    tx.run(\"\"\"\n",
    "                        MATCH (a:ArtigoPublicado {doi: $doi}), (j:Revista {nome_revista: $revista_nome, issn: $revista_issn, area_avaliacao: $revista_area_avaliacao, estrato: $revista_estrato})\n",
    "                        CREATE (a)-[:PUBLICADO_EM]->(j)\n",
    "                    \"\"\", doi=doi, revista_nome=revista_nome, revista_issn=revista_issn, revista_area_avaliacao=revista_area_avaliacao, revista_estrato=revista_estrato)\n",
    "\n",
    "                tx.commit()\n",
    "\n",
    "        # Atualização dos contadores\n",
    "        with session.begin_transaction() as tx:\n",
    "            created_nodes += tx.run(\"MATCH (n) WHERE n:ArtigoPublicado RETURN count(n)\").single()[0]\n",
    "            updated_nodes += tx.run(\"MATCH (n) WHERE n:ArtigoPublicado SET n.updated_at = datetime() RETURN count(n)\").single()[0]\n",
    "            created_relations += tx.run(\"MATCH (r) WHERE r:PUBLICADO_EM RETURN count(r)\").single()[0]\n",
    "\n",
    "\n",
    "\n",
    "    def persistir_resumos_congressos(self, session, id_lattes, dados):\n",
    "        query_create_node = \"\"\"\n",
    "        MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "        MERGE (r:ResumoCongresso {titulo: $titulo, ano: $ano, evento: $evento, autores: $autores, data_issn: $data_issn, doi: $doi})\n",
    "        MERGE (p)-[:PRODUZ]->(r)\n",
    "        \"\"\"\n",
    "        result = session.run(query_create_node, id_lattes=id_lattes, **dados)\n",
    "        # Obtendo as informações de contadores\n",
    "        summary = result.consume()\n",
    "        created_nodes = summary.counters.nodes_created\n",
    "        updated_nodes = summary.counters.nodes_deleted  \n",
    "        created_relations = summary.counters.relationships_created\n",
    "        updated_relations = summary.counters.relationships_deleted \n",
    "\n",
    "        return created_nodes, updated_nodes, created_relations, updated_relations\n",
    "\n",
    "    def persistir_apresentacoes_trabalho(self, session, id_lattes, dados):\n",
    "        query_create_node = \"\"\"\n",
    "        MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "        MERGE (a:ApresentacaoTrabalho {\n",
    "            titulo: $titulo,\n",
    "            ano: $ano,\n",
    "            evento: $evento,\n",
    "            autores: $autores\n",
    "        })\n",
    "        MERGE (p)-[:PRODUZ]->(a)\n",
    "        \"\"\"\n",
    "        result = session.run(query_create_node, id_lattes=id_lattes, **dados)\n",
    "\n",
    "        # Obtendo as informações de contadores\n",
    "        summary = result.consume()\n",
    "        created_nodes = summary.counters.nodes_created\n",
    "        updated_nodes = summary.counters.nodes_deleted  \n",
    "        created_relations = summary.counters.relationships_created\n",
    "        updated_relations = summary.counters.relationships_deleted \n",
    "\n",
    "        return created_nodes, updated_nodes, created_relations, updated_relations\n",
    "\n",
    "    def persistir_outras_producoes_bibliograficas(self, session, id_lattes, dados):\n",
    "        query_create_node = \"\"\"\n",
    "        MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "        MERGE (o:OutrasProducoesBibliograficas {\n",
    "            titulo: $titulo,\n",
    "            ano: $ano,\n",
    "            autores: $autores,\n",
    "            doi: $doi\n",
    "        })\n",
    "        MERGE (p)-[:PRODUZ]->(o)\n",
    "        \"\"\"\n",
    "        result = session.run(query_create_node, id_lattes=id_lattes, **dados)\n",
    "\n",
    "        # Obtendo as informações de contadores\n",
    "        summary = result.consume()\n",
    "        created_nodes = summary.counters.nodes_created\n",
    "        updated_nodes = summary.counters.nodes_deleted  \n",
    "        created_relations = summary.counters.relationships_created\n",
    "        updated_relations = summary.counters.relationships_deleted \n",
    "\n",
    "        return created_nodes, updated_nodes, created_relations, updated_relations\n",
    "\n",
    "    def persistir_orientacoes_concluidas(self, session, id_lattes, dados):\n",
    "        query_create_node = \"\"\"\n",
    "        MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "        MERGE (o:OrientacaoConcluida {\n",
    "            tipo: $tipo,\n",
    "            titulo: $titulo,\n",
    "            ano: $ano,\n",
    "            autor: $autor,\n",
    "            instituicao: $instituicao\n",
    "        })\n",
    "        MERGE (p)-[:ORIENTA]->(o)\n",
    "        \"\"\"\n",
    "        result = session.run(query_create_node, id_lattes=id_lattes, **dados)\n",
    "\n",
    "        # Obtendo as informações de contadores\n",
    "        summary = result.consume()\n",
    "        created_nodes = summary.counters.nodes_created\n",
    "        updated_nodes = summary.counters.nodes_deleted  \n",
    "        created_relations = summary.counters.relationships_created\n",
    "        updated_relations = summary.counters.relationships_deleted \n",
    "\n",
    "        return created_nodes, updated_nodes, created_relations, updated_relations\n",
    "\n",
    "    def persistir_participacoes_bancas(self, session, id_lattes, dados):\n",
    "        query_create_node = \"\"\"\n",
    "        MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "        MERGE (b:Banca {\n",
    "            tipo: $tipo,\n",
    "            titulo: $titulo,\n",
    "            ano: $ano,\n",
    "            instituicao: $instituicao\n",
    "        })\n",
    "        MERGE (p)-[:PARTICIPA_BANCA]->(b)\n",
    "        \"\"\"\n",
    "        result = session.run(query_create_node, id_lattes=id_lattes, **dados)\n",
    "\n",
    "        # Obtendo as informações de contadores\n",
    "        summary = result.consume()\n",
    "        created_nodes = summary.counters.nodes_created\n",
    "        updated_nodes = summary.counters.nodes_deleted  \n",
    "        created_relations = summary.counters.relationships_created\n",
    "        updated_relations = summary.counters.relationships_deleted \n",
    "\n",
    "        return created_nodes, updated_nodes, created_relations, updated_relations\n",
    "\n",
    "    def persistir_projetos_pesquisa(self, session, id_lattes, dados):\n",
    "        query_create_node = \"\"\"\n",
    "        MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "        MERGE (pr:ProjetoPesquisa {\n",
    "            titulo: $titulo,\n",
    "            ano_inicio: $ano_inicio,\n",
    "            ano_fim: $ano_fim,\n",
    "            agencia_financiadora: $agencia_financiadora,\n",
    "            valor_financiamento: $valor_financiamento\n",
    "        })\n",
    "        MERGE (p)-[:COORDENA]->(pr)\n",
    "        \"\"\"\n",
    "        result = session.run(query_create_node, id_lattes=id_lattes, dados=dados)\n",
    "\n",
    "        # Obtendo as informações de contadores\n",
    "        summary = result.consume()\n",
    "        created_nodes = summary.counters.nodes_created\n",
    "        updated_nodes = summary.counters.nodes_deleted  \n",
    "        created_relations = summary.counters.relationships_created\n",
    "        updated_relations = summary.counters.relationships_deleted \n",
    "\n",
    "        return created_nodes, updated_nodes, created_relations, updated_relations\n",
    "\n",
    "    def persistir_premios_distincoes(self, session, id_lattes, dados):\n",
    "        query_create_node = \"\"\"\n",
    "        MERGE (p:Pesquisador {id_lattes: $id_lattes})\n",
    "        MERGE (pd:PremioDistincao {\n",
    "            titulo: $titulo,\n",
    "            ano: $ano,\n",
    "            instituicao: $instituicao,\n",
    "        })\n",
    "        MERGE (p)-[:RECEBE]->(pd)\n",
    "        \"\"\"\n",
    "        result = session.run(query_create_node, id_lattes=id_lattes, **dados)\n",
    "\n",
    "        # Obtendo as informações de contadores\n",
    "        summary = result.consume()\n",
    "        created_nodes = summary.counters.nodes_created\n",
    "        updated_nodes = summary.counters.nodes_deleted  \n",
    "        created_relations = summary.counters.relationships_created\n",
    "        updated_relations = summary.counters.relationships_deleted \n",
    "\n",
    "        return created_nodes, updated_nodes, created_relations, updated_relations\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "persister = Neo4jPersister(uri, user, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36721f73",
   "metadata": {},
   "source": [
    "### Persistir dados no Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a822f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_list\n",
    "filename = 'dict_list.json'\n",
    "json_data, formatted_creation_date, formatted_modification_date, time_count, unit = jfm.load_from_json(os.path.join(folder_data_input,filename))\n",
    "print(f\"\\n{len(dict_list)} currículos carregados na lista de dicionários '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "persister.persistir_revistas_da_planilha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "persister.persist_areas_nodes(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b26275",
   "metadata": {},
   "outputs": [],
   "source": [
    "persister.persist_pessoa_nodes(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a42d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "persister.persist_pesquisador_grande_area_relationships(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "persister.persist_producoes_pesquisador(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfee57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jDataPersister:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._uri = uri\n",
    "        self._user = user\n",
    "        self._password = password\n",
    "        self._driver = GraphDatabase.driver(self._uri, auth=(self._user, self._password))\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        self._node_created_count = 0\n",
    "        self._node_updated_count = 0\n",
    "        self._node_deleted_count = 0\n",
    "\n",
    "        self._relationship_created_count = 0\n",
    "        self._relationship_updated_count = 0\n",
    "        self._relationship_deleted_count = 0\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def persist_data_from_json(self, json_data):\n",
    "        with self._driver.session() as session:\n",
    "            self._persist_data(session, json_data)\n",
    "\n",
    "        self.logger.info(\"Total nodes created: %d\", self._node_created_count)\n",
    "        self.logger.info(\"Total nodes updated: %d\", self._node_updated_count)\n",
    "        self.logger.info(\"Total nodes deleted: %d\", self._node_deleted_count)\n",
    "\n",
    "        self.logger.info(\"Total relationships created: %d\", self._relationship_created_count)\n",
    "        self.logger.info(\"Total relationships updated: %d\", self._relationship_updated_count)\n",
    "        self.logger.info(\"Total relationships deleted: %d\", self._relationship_deleted_count)\n",
    "\n",
    "    def _persist_data(self, session, data):\n",
    "        if isinstance(data, dict):  # Check if it's a dictionary\n",
    "            for key, value in data.items():\n",
    "                if key == 'Identificação':\n",
    "                    self._handle_identificacao(session, value)\n",
    "                elif key == 'Atuação Profissional':\n",
    "                    self._handle_atuacao_profissional(session, value)\n",
    "                elif key == 'Atuação Profissional':\n",
    "                    self._handle_atuacao_profissional(session, value)                    \n",
    "                elif key == 'Produções':\n",
    "                    self._handle_producoes(session, value)\n",
    "                elif isinstance(value, list) and not value:\n",
    "                    # Ignore empty lists\n",
    "                    pass\n",
    "                elif isinstance(value, dict):\n",
    "                    if 'JCR2' in value:\n",
    "                        # Ignore the 'JCR2' subdictionary\n",
    "                        del value['JCR2']\n",
    "\n",
    "                    self._persist_data(session, value)\n",
    "\n",
    "        elif isinstance(data, list):  # Check if it's a list\n",
    "            for item in data:\n",
    "                self._persist_data(session, item)  # Recurse on list items\n",
    "\n",
    "        else:\n",
    "            # Handle other data types (strings, numbers, etc.) or raise an error\n",
    "            self.logger.warning(\"Unexpected data type: %s\", type(data))\n",
    "\n",
    "\n",
    "    def _handle_identificacao(self, session, data):\n",
    "        for item in data:\n",
    "            if item['campo'] == 'ID Lattes':\n",
    "                node_id = item['valor']\n",
    "                node = self._get_or_create_node(session, 'Pesquisador', {'ID Lattes': node_id})\n",
    "                self._persist_other_properties(session, node, data)\n",
    "            else:\n",
    "                node = self._get_or_create_node(session, 'Pesquisador', {data.items()})\n",
    "\n",
    "    def _handle_atuacao_profissional(self, session, data):\n",
    "        for item in data:\n",
    "            node = self._create_node(session, 'AtuaçãoProfissional', item)\n",
    "\n",
    "    def _handle_producoes(self, session, data):\n",
    "        for production_type, production_data in data.items():\n",
    "            if production_type == 'Artigos completos publicados em periódicos':\n",
    "                for article in production_data:\n",
    "                    node = self._create_node(session, production_type, article)\n",
    "            elif production_type.startswith('1.'):\n",
    "                for index, article in enumerate(production_data):\n",
    "                    node = self._create_node(session, production_type, article)\n",
    "                    node['OrdemCronológica'] = index + 1\n",
    "\n",
    "    def _get_or_create_node(self, session, label, properties):\n",
    "        node = session.run(\"MATCH (n: {label}) WHERE {properties} RETURN n\", {\"label\": label, \"properties\": properties}).single()\n",
    "\n",
    "        if not node:\n",
    "            node = session.run(\"CREATE (n: {label} {properties}) RETURN n\", {\"label\": label, \"properties\": properties}).single()[\"n\"]\n",
    "            self._node_created_count += 1\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _create_node(self, session, label, properties):\n",
    "        node = session.run(\"CREATE (n: {label} {properties}) RETURN n\", {\"label\": label, \"properties\": properties}).single()[\"n\"]\n",
    "        self._node_created_count += 1\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _persist_other_properties(self, session, node, data):\n",
    "        for key, value in data.items():\n",
    "            if key not in ['campo', 'valor']:\n",
    "                node[key] = value  \n",
    "\n",
    "    def _create_relationship(self, session, start_node, relationship_type, end_node, properties={}):\n",
    "        session.run(\"MATCH (a), (b) WHERE ID(a) = {start_node_id} AND ID(b) = {end_node_id} CREATE (a)-[r:{type} {props}]->(b) RETURN r\", \n",
    "                    {\"start_node_id\": id(start_node), \"end_node_id\": id(end_node), \"type\": relationship_type, \"props\": properties})\n",
    "        self._relationship_created_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99598e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tipos=[]\n",
    "subtipos=[]\n",
    "for dict_pesq in dict_list:\n",
    "    # Avaliando elementos da lista de dicionários do arquivo JSON de entrada\n",
    "    if isinstance(dict_pesq, dict):\n",
    "        for key1,val1 in dict_pesq.items():\n",
    "            if key1 not in tipos:\n",
    "                tipos.append(key1)            \n",
    "            print(f'N01: Elementos armazenados em dicionário:')\n",
    "            print(f'     {key2:25} | Tipo dos valores: {type(val2)}')\n",
    "            print(f'       Conteúdo disponível nos valores do dicionário de Nível 01:')\n",
    "            # Avaliando filhos de primeiro nível na hierarquia (Seções)\n",
    "            if isinstance(val1, list):\n",
    "                print(f'N01: Elementos armazenados em lista:')\n",
    "                print(f'     Conteúdo disponível nos valores:')\n",
    "                print(f'       {[x for x in val1]}')\n",
    "            elif isinstance(val1, dict):\n",
    "                for key2,val2 in val1.items():\n",
    "                    if key2 not in subtipos:\n",
    "                        subtipos.append(key2)\n",
    "                    print(f'       Chave: {key2:25} | Tipo dos valores: {type(val2)}')\n",
    "                    print(f'         Conteúdo disponível nos valores do dicionário de Nível 02:')\n",
    "                    # Avaliando filhos de segundo nível na hierarquia (Tipos de Seções)\n",
    "                    if isinstance(val2, dict):\n",
    "                        for key3,val3 in val2.items():\n",
    "                            if key3 not in tipos:\n",
    "                                print(f'         Chave: {key3:25} | Tipo dos valores: {type(val3)}')\n",
    "                                print(f'          Conteúdo disponível nos valores do dicionário de Nível 03:')\n",
    "                                # Avaliando filhos de terceiro nível na hierarquia (Ocorrências de Tipos de Seções)\n",
    "                                if isinstance(val3, dict):\n",
    "                                    print(f'               Chave: {key3:25} | Tipo dos valores: {type(val3)}')\n",
    "                                    print(f'                 Conteúdo disponível nos valores do dicionário de Nível 04:')\n",
    "                                    # print(val3)\n",
    "            else:\n",
    "                print(f'N01: Elementos armazenados em {type(val1)}')\n",
    "    else:\n",
    "        print('ERRO NA ESTRUTURA DO JSON!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Neo4jDataPersister instance\n",
    "# data_persister = Neo4jDataPersister('neo4j://localhost:7687', 'neo4j', 'password')\n",
    "# filename = 'dict_list.json'\n",
    "# json_data, formatted_creation_date, formatted_modification_date, time_count, unit = jfm.load_from_json(os.path.join(folder_data_input,filename))\n",
    "# print(f\"\\n{len(dict_list)} currículos carregados na lista de dicionários '{filename}'\")\n",
    "\n",
    "# # Persist the data to Neo4j\n",
    "# data_persister.persist_data_from_json(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pesq in json_data:\n",
    "    print(len(pesq.get('Produções').keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a77e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_tipo_instancias_arvore(estrutura, nivel=1, identacao=\"\"):\n",
    "    \"\"\"\n",
    "    Função recursiva que avalia o tipo e a quantidade de instâncias em cada nível da estrutura, exibindo-a em formato de árvore.\n",
    "\n",
    "    Args:\n",
    "        estrutura (list|dict): A estrutura a ser avaliada.\n",
    "        nivel (int): Nível atual da recurssão (inicia em 1).\n",
    "        identacao (str): String de indentação para cada nível (inicia vazia).\n",
    "\n",
    "    Returns:\n",
    "        None: A função imprime os resultados na tela e não retorna nada.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(estrutura, list):\n",
    "        print(f\"{identacao}N{nivel}. Lista: {len(estrutura)} elementos\")\n",
    "        for item in estrutura:\n",
    "            avaliar_tipo_instancias_arvore(item, nivel + 1, identacao + \"    \")\n",
    "\n",
    "    elif isinstance(estrutura, dict):\n",
    "        print(f\"{identacao}N{nivel}. Mapa: {estrutura.keys()}\")\n",
    "        for chave, valor in estrutura.items():\n",
    "            print(f\"{identacao}  {chave}:\")\n",
    "            avaliar_tipo_instancias_arvore(valor, nivel + 1, identacao + \"    \")\n",
    "\n",
    "    elif isinstance(estrutura, str):\n",
    "        print(f\"{identacao}N{nivel}. String: {estrutura}\")\n",
    "\n",
    "for dict_pesq in dict_list[:3]:\n",
    "    avaliar_tipo_instancias_arvore(dict_pesq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc6c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dict_pesq in dict_list:\n",
    "    if isinstance(dict_pesq, list):\n",
    "        print(f'L1. Lista: {dict_pesq}')\n",
    "    elif isinstance(dict_pesq, dict):\n",
    "        print(f'L1. Mapa: {dict_pesq.keys()}')\n",
    "        for tipo in dict_pesq.values():\n",
    "            if isinstance(tipo, list):\n",
    "                print(f'    L2. Lista: {tipo}')\n",
    "            elif isinstance(tipo, dict):\n",
    "                print(f'    L2. Mapa: {tipo}')\n",
    "                for subtipo in tipo.values():\n",
    "                    if isinstance(subtipo, list):\n",
    "                        print(f'        L3. Lista: {len(subtipo)} elementos')\n",
    "                        for subsubtipo in subtipo:\n",
    "                            if isinstance(subsubtipo, list):\n",
    "                                print(f'            L4. Lista: {subsubtipo}')\n",
    "                            elif isinstance(subsubtipo, dict):\n",
    "                                print(f'            L4. Mapa: {subsubtipo.keys()}')\n",
    "                                for subsubsubtipo in subsubtipo.values():\n",
    "                                    if isinstance(subsubsubtipo, list):\n",
    "                                        print(f'                L5. Lista: {subsubsubtipo}')\n",
    "                                    elif isinstance(subsubsubtipo, dict):\n",
    "                                        print(f'                L5. Mapa: {subsubsubtipo.keys()}')\n",
    "                                    elif isinstance(subsubsubtipo, str):\n",
    "                                        print(f'                L5. String: {subsubsubtipo}')                                  \n",
    "                            elif isinstance(subsubtipo, str):\n",
    "                                print(f'            L4. String: {subsubtipo}')     \n",
    "                    elif isinstance(subtipo, dict):\n",
    "                        print(f'        L3. Mapa: {subtipo.keys()}')\n",
    "                        for subsubtipo in subtipo.values():\n",
    "                            if isinstance(subsubtipo, list):\n",
    "                                print(f'            L4. Lista: {subsubtipo}')\n",
    "                            elif isinstance(subsubtipo, dict):\n",
    "                                print(f'            L4. Mapa: {subsubtipo.keys()}')\n",
    "                            elif isinstance(subsubtipo, str):\n",
    "                                print(f'            L4. String: {subsubtipo}')                             \n",
    "                    elif isinstance(tipo, str):\n",
    "                        print(f'        L3. String: {subtipo}')                \n",
    "            elif isinstance(tipo, str):\n",
    "                print(f'    L2. String: {tipo}')\n",
    "    elif isinstance(dict_pesq, str):\n",
    "        print(f'L1. String: {dict_pesq}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a81a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list[20:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a5d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict_list[20].get('Identificação').items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict_list[5].get('Idiomas'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e49f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(x.get('Descrição') for x in dict_list[5].get('Linhas de Pesquisa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict_list[5].get('Áreas').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tipos=[]\n",
    "for pesq in dict_list:\n",
    "    for t, s in pesq.get('Produções').items():\n",
    "        if t not in tipos:\n",
    "            tipos.append(t)\n",
    "            print(t, type(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict_list[1].get('Produções').get('Artigos completos publicados em periódicos')[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict_list[5].get('Atuação Profissional'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict_list[5].get('Formação').items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e1f72",
   "metadata": {},
   "source": [
    "## Classe para gerar dados para grafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neo4j import GraphDatabase\n",
    "# import json\n",
    "\n",
    "# class Neo4jConnection:\n",
    "#     def __init__(self, uri, user, password):\n",
    "#         self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "#     def close(self):\n",
    "#         self._driver.close()\n",
    "\n",
    "#     def persist_data(self, data):\n",
    "#         with self._driver.session() as session:\n",
    "#             for person_data in data:\n",
    "#                 person_id = person_data['Identificação']['ID Lattes']\n",
    "#                 prepared_data, relationships = self._prepare_data(person_data, person_id)\n",
    "#                 session.write_transaction(self._create_person_node, person_id, prepared_data)\n",
    "#                 self._create_relationships(session, person_id, relationships)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _create_person_node(tx, person_id, person_data):\n",
    "#         tx.run(\n",
    "#             \"MERGE (p:Person {id_lattes: $person_id}) \"\n",
    "#             \"SET p += $person_data\",\n",
    "#             person_id=person_id,\n",
    "#             person_data=person_data\n",
    "#         )\n",
    "\n",
    "#     def _prepare_data(self, data, person_id):\n",
    "#         prepared_data = {}\n",
    "#         relationships = []\n",
    "#         for key, value in data.items():\n",
    "#             if isinstance(value, dict):\n",
    "#                 label = key.capitalize().replace(' ', '_')\n",
    "#                 prepared_data.update({f'{label}_{k}': v for k, v in value.items()})\n",
    "#                 sub_prepared_data, sub_relationships = self._prepare_data(value, person_id)\n",
    "#                 prepared_data.update(sub_prepared_data)\n",
    "#                 relationships.extend(sub_relationships)\n",
    "#                 relationships.append((label, {'label': 'Person', 'person_id': person_id}))\n",
    "#             elif isinstance(value, list):\n",
    "#                 for idx, item in enumerate(value):\n",
    "#                     if isinstance(item, dict):\n",
    "#                         nested_label = f'{key}_{idx}'\n",
    "#                         nested_prepared_data, nested_relationships = self._prepare_data(item, person_id)\n",
    "#                         prepared_data.update({f'{nested_label}_{k}': v for k, v in item.items()})\n",
    "#                         prepared_data.update(nested_prepared_data)\n",
    "#                         relationships.extend(nested_relationships)\n",
    "#                         relationships.append((nested_label, {'label': key.capitalize().replace(' ', '_'), 'person_id': person_id}))\n",
    "#                     elif isinstance(item, str):\n",
    "#                         # Tratar strings\n",
    "#                         if \":\" in item:\n",
    "#                             label, value = item.split(\":\", 1)\n",
    "#                             prepared_data[f'{key}_{idx}'] = value.strip()\n",
    "#                             relationships.append((label.strip(), {'label': key.capitalize().replace(' ', '_'), 'person_id': person_id}))\n",
    "#                         else:\n",
    "#                             prepared_data[f'{key}_{idx}'] = item.strip()\n",
    "#         return prepared_data, relationships\n",
    "\n",
    "#     def _create_relationships(self, session, person_id, relationships):\n",
    "#         for label, parent_id in relationships:\n",
    "#             query = (\n",
    "#                 f\"MATCH (p:Person {{id_lattes: '{person_id}'}}) \"\n",
    "#                 f\"MATCH (n:{label} {{id_lattes: '{person_id}_{label}'}}) \"\n",
    "#                 f\"MERGE (p)-[:HAS_{label}]->(n)\"\n",
    "#             )\n",
    "#             if parent_id:\n",
    "#                 query += (\n",
    "#                     f\" MERGE (parent:{parent_id['label']} {{id_lattes: '{parent_id['person_id']}'}}) \"\n",
    "#                     f\"MERGE (parent)-[:HAS_{label}]->(n)\"\n",
    "#                 )\n",
    "#             session.run(query)\n",
    "\n",
    "# # Dados de conexão com o banco de dados Neo4j\n",
    "# uri = \"bolt://localhost:7687\"\n",
    "# user = \"neo4j\"\n",
    "# password = \"password\"\n",
    "\n",
    "# # Conecta-se ao banco de dados Neo4j e persiste os dados\n",
    "# connection = Neo4jConnection(uri, user, password)\n",
    "# connection.persist_data(dict_list)\n",
    "# connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85580969",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "import ast\n",
    "\n",
    "class GraphPreparer:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._uri = uri\n",
    "        self._user = user\n",
    "        self._password = password\n",
    "        self._driver = None\n",
    "\n",
    "    def connect(self):\n",
    "        self._driver = GraphDatabase.driver(self._uri, auth=(self._user, self._password))\n",
    "\n",
    "    def prepare_data_from_file(self, file_path):\n",
    "        nodes = []\n",
    "        relationships = []\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        for person_data in data:\n",
    "            print(person_data.keys())\n",
    "            person_id = person_data['Identificação'].get('ID Lattes')\n",
    "            nodes.append(('Pesquisador', {'id_lattes': person_id}))\n",
    "            \n",
    "            for language in person_data.get('Idiomas', []):\n",
    "                language_name = language['Idioma']\n",
    "                proficiency = language['Proficiência']\n",
    "                nodes.append(('Idioma', {'nome_idioma': language_name, 'proficiencia': proficiency}))\n",
    "                relationships.append(('Pesquisador', {'id_lattes': person_id}, 'FALA', {'nome_idioma': language_name}))\n",
    "            \n",
    "            for academic_info in person_data.get('Formação', {}).get('Acadêmica', []):\n",
    "                description = academic_info['Descrição']\n",
    "                nodes.append(('Curso', {'descricao': description}))\n",
    "                relationships.append(('Pesquisador', {'id_lattes': person_id}, 'REALIZOU', {'descricao': description}))\n",
    "                \n",
    "            for professional_info in person_data.get('Atuação Profissional', []):\n",
    "                institution = professional_info['Instituição']\n",
    "                description = professional_info['Descrição']\n",
    "                relationships.append(('Pesquisador', {'id_lattes': person_id}, 'ATUA', {'instituicao': institution, 'descricao': description}))\n",
    "\n",
    "        return nodes, relationships\n",
    "\n",
    "    def persist_data(self, data_for_neo4j):\n",
    "        try:\n",
    "            with self._driver.session() as session:\n",
    "                for data in data_for_neo4j:\n",
    "                    if data[0] == 'Node':\n",
    "                        label, properties = data[1:]\n",
    "                        session.write_transaction(self._create_node, label, **properties)\n",
    "                    elif data[0] == 'Relationship':\n",
    "                        start_label, rel_type, end_label = data[1:]\n",
    "                        session.write_transaction(self._create_relationship, start_label, rel_type, end_label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error persisting data: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_node(tx, label, **properties):\n",
    "        query = f\"CREATE (n:{label} $props)\"\n",
    "        tx.run(query, props=properties)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_relationship(tx, start_label, rel_type, end_label):\n",
    "        query = \"MATCH (a:{start_label}), (b:{end_label}) \" \\\n",
    "                \"CREATE (a)-[r:{rel_type}]->(b)\"\n",
    "        tx.run(query, start_label=start_label, rel_type=rel_type, end_label=end_label)\n",
    "\n",
    "    def close(self):\n",
    "        if self._driver is not None:\n",
    "            self._driver.close()\n",
    "\n",
    "# Exemplo de utilização:\n",
    "graph_preparer = GraphPreparer(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "graph_preparer.connect()\n",
    "\n",
    "filepath = os.path.join(folder_data_input,'dict_list.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list[0]['Idioma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ecf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ebece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, relationships = graph_preparer.prepare_data_from_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78acb05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_neo4j = nodes + relationships\n",
    "graph_preparer.persist_data(data_for_neo4j)\n",
    "\n",
    "graph_preparer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1823211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from datetime import datetime\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "graph_preparer = GraphPreparer(uri, user, password) # Instanciar classe GraphPreparer\n",
    "graph_preparer.connect() # Conectar ao banco de dados Neo4j\n",
    "graph_preparer.process_data(dict_list) # Processar e persistir dados\n",
    "\n",
    "# Transformar os dados para o formato adequado do Neo4j\n",
    "dados_para_neo4j = graph_preparer.transform_data_for_neo4j(dict_list)\n",
    "graph_preparer.persist_nodes_and_relationships(dados_para_neo4j) # PersistiR nós e relações no Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e803731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52fd544",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_para_neo4j[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def42ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "# Agora você pode inserir os dados no Neo4j utilizando sua biblioteca de acesso ao banco de dados Neo4j, por exemplo:\n",
    "neo4j_driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "with neo4j_driver.session() as session:\n",
    "    for item in data_for_neo4j:\n",
    "        label, properties = item\n",
    "        cypher_query = f\"CREATE (:{label} $properties)\"\n",
    "        session.run(cypher_query, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Função para criar nós e relações\n",
    "# def create_nodes_and_relationships(tx, nodes_and_relationships):\n",
    "#     for item in nodes_and_relationships:\n",
    "#         if len(item) == 3:  # Se for uma relação\n",
    "#             origin_node, rel_type, dest_node = item\n",
    "#             tx.run(\n",
    "#                 f\"MATCH (a), (b) WHERE a.name = $origin_node AND b.name = $dest_node \"\n",
    "#                 \"CREATE (a)-[r:\" + rel_type + \"]->(b)\",\n",
    "#                 origin_node=origin_node, dest_node=dest_node\n",
    "#             )\n",
    "\n",
    "# # Conectando ao banco de dados Neo4j e executando a transação\n",
    "# with GraphDatabase.driver(uri, auth=(user, password)) as driver:\n",
    "#     with driver.session() as session:\n",
    "#         session.write_transaction(create_nodes_and_relationships, relacoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class Neo4jGraphGenerator:\n",
    "    def __init__(self, dict_list):\n",
    "        self.dict_list = dict_list\n",
    "        self.graph_data = {'nodes': [], 'relationships': []}\n",
    "\n",
    "    def generate_graph_json(self):\n",
    "        for item in self.dict_list:\n",
    "            id_lattes = item.get('Identificação', {}).get('ID Lattes')\n",
    "            pesquisador = item.get('Identificação', {}).get('Nome')\n",
    "            \n",
    "            # Adicionar nó de pessoa (Person)\n",
    "            person_node = {'id': id_lattes, 'label': 'Person', 'name': pesquisador}\n",
    "            self.graph_data['nodes'].append(person_node)\n",
    "\n",
    "            areas = item.get('Áreas', {})\n",
    "            for area in areas.values():\n",
    "                # Adicionar nó de Grande Área (GrandeArea)\n",
    "                grande_area_node = {'id': area['Grande Área'], 'label': 'GrandeArea', 'description': area['Grande Área']}\n",
    "                self.graph_data['nodes'].append(grande_area_node)\n",
    "                self.graph_data['relationships'].append({'source': id_lattes, 'target': area['Grande Área'], 'type': 'BELONGS_TO'})\n",
    "                \n",
    "                for subarea in area.get('Subáreas', []):\n",
    "                    # Adicionar nó de Área (Area)\n",
    "                    area_node = {'id': subarea['Área'], 'label': 'Area', 'description': subarea['Área']}\n",
    "                    self.graph_data['nodes'].append(area_node)\n",
    "                    self.graph_data['relationships'].append({'source': area['Grande Área'], 'target': subarea['Área'], 'type': 'BELONGS_TO'})\n",
    "                    \n",
    "                    # Adicionar nó de Subárea (Subarea)\n",
    "                    subarea_node = {'id': subarea['Subárea'], 'label': 'Subarea', 'description': subarea['Subárea']}\n",
    "                    self.graph_data['nodes'].append(subarea_node)\n",
    "                    self.graph_data['relationships'].append({'source': subarea['Área'], 'target': subarea['Subárea'], 'type': 'BELONGS_TO'})\n",
    "\n",
    "        return json.dumps(self.graph_data, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da358b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Neo4jGraphGenerator(dict_list)\n",
    "generator.generate_graph_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03290d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class AreasHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "        \n",
    "    def consult_areas_atuacao(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"MATCH (p:Person {name: $name}) RETURN p.`Áreas de atuação` as areas_atuacao\", name=name)\n",
    "            record = result.single()\n",
    "            if record:\n",
    "                return record['areas_atuacao']\n",
    "            return None\n",
    "\n",
    "    def debug_and_convert(self, areas_str):\n",
    "        try:\n",
    "            return json.loads(areas_str)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to deserialize JSON string: {areas_str}\")\n",
    "            return None\n",
    "\n",
    "    def extract_subarea(self, area_detail):\n",
    "        # Extract the 'Subárea' content from the area detail\n",
    "        match = re.search(r'Subárea: ([^/]+)', area_detail)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return None\n",
    "\n",
    "    def extract_areas(self, area_detail):\n",
    "        # Extract the 'Grande Área', 'Área', and 'Subárea' contents from the area detail\n",
    "        grande_area_match = re.search(r'Grande área: ([^/]+)', area_detail)\n",
    "        area_match = re.search(r'Área: ([^/]+)', area_detail)\n",
    "        subarea_match = re.search(r'Subárea: ([^/]+)', area_detail)\n",
    "        \n",
    "        grande_area = grande_area_match.group(1).strip() if grande_area_match else None\n",
    "        area = area_match.group(1).strip() if area_match else None\n",
    "        subarea = subarea_match.group(1).strip() if subarea_match else None\n",
    "        \n",
    "        return grande_area, area, subarea\n",
    "\n",
    "    def create_areas_relations(self, name):\n",
    "        # Get 'Áreas de atuação' properties\n",
    "        areas_properties = self.consult_areas_atuacao(name)\n",
    "\n",
    "        # Convert the serialized JSON strings back into dictionaries\n",
    "        try:\n",
    "            deserialized_areas_properties = self.debug_and_convert(areas_properties)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deserializing 'Áreas de atuação' properties: {e}\")\n",
    "            return\n",
    "\n",
    "        successful_areas_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for _, area_detail in deserialized_areas_properties.items():\n",
    "                try:\n",
    "                    # Extracting Grande Área, Área, and Subárea from the details\n",
    "                    match = re.match(r'Grande área: (.*?) / Área: (.*?) / Subárea: (.*?)(?:/Especialidade: (.*?))?\\.?$', area_detail)\n",
    "                    if not match:\n",
    "                        print(f\"Unexpected format for 'Áreas de atuação' detail: {area_detail}\")\n",
    "                        continue\n",
    "                    grande_area, area, subarea = match.groups()[:3]\n",
    "\n",
    "                    # Creating or merging nodes for Subárea, Área, and Grande Área\n",
    "                    session.run(\"MERGE (s:Subárea {name: $subarea})\", subarea=subarea)\n",
    "                    session.run(\"MERGE (a:Área {name: $area})\", area=area)\n",
    "                    session.run(\"MERGE (ga:GrandeÁrea {name: $grande_area})\", grande_area=grande_area)\n",
    "\n",
    "                    # Creating or merging relationships. Using MERGE ensures no duplicate relationships are created.\n",
    "                    session.run(\"MATCH (p:Person {name: $name}), (s:Subárea {name: $subarea}) MERGE (p)-[r:ATUA_EM]->(s)\", name=name, subarea=subarea)\n",
    "                    session.run(\"MATCH (ga:GrandeÁrea {name: $grande_area}), (a:Área {name: $area}) MERGE (ga)-[r:CONTÉM_ÁREA]->(a)\", grande_area=grande_area, area=area)\n",
    "                    session.run(\"MATCH (a:Área {name: $area}), (s:Subárea {name: $subarea}) MERGE (a)-[r:CONTEM_SUBÁREA]->(s)\", area=area, subarea=subarea)\n",
    "\n",
    "                    successful_areas_creations += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing 'Áreas de atuação' detail '{area_detail}': {e}\")\n",
    "\n",
    "            # Inform the user about areas\n",
    "            print(f\"{successful_areas_creations} 'Áreas de atuação' relations successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fdc77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "handleareas = AreasHandler(uri, user, password)\n",
    "name = 'Antonio Marcos Aires Barbosa'\n",
    "handleareas.consult_areas_atuacao(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffefb3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectsHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def consult_data_by_property(self, name, property_name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(f\"MATCH (p:Person {{name: $name}}) RETURN p.`{property_name}` as data\", name=name)\n",
    "            record = result.single()\n",
    "            return record['data'] if record else None\n",
    "\n",
    "    def create_projects_relations(self, name):\n",
    "        successful_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            # Process 'Atuação Profissional' data\n",
    "            professional_data = self.consult_data_by_property(name, 'Atuação Profissional')\n",
    "            if professional_data:\n",
    "                for institution_name, _ in json.loads(professional_data).items():\n",
    "                    session.run(\"MERGE (i:Instituição {name: $institution_name})\", institution_name=institution_name)\n",
    "                    print(f\"Institution node created/merged for: {institution_name}\")\n",
    "\n",
    "                    session.run(\"MATCH (p:Person {name: $name}), (i:Instituição {name: $institution_name}) MERGE (p)-[:TEM]->(i)\", name=name, institution_name=institution_name)\n",
    "                    print(f\"Relationship established between {name} and {institution_name}.\")\n",
    "\n",
    "            # Process other dynamic nodes\n",
    "            key_labels_to_check = ['Linhas de pesquisa', 'Projetos de pesquisa', 'Projetos de extensão', 'Projetos de desenvolvimento']\n",
    "            for key in key_labels_to_check:\n",
    "                formatted_key = f\"`{key}`\"  # Wrap the key with backticks\n",
    "                project_data = self.consult_data_by_property(name, key)\n",
    "                if project_data:\n",
    "                    for project_time, project_name in json.loads(project_data).items():\n",
    "                        if project_name:  # to avoid empty names\n",
    "                            session.run(f\"MERGE (p:{formatted_key} {{name: $project_name}})\", project_name=project_name)\n",
    "                            print(f\"{key} node created/merged for: {project_name}\")\n",
    "\n",
    "                            session.run(f\"MATCH (a:Person {{name: $name}}), (p:{formatted_key} {{name: $project_name}}) MERGE (a)-[:TEM]->(p)\", name=name, project_name=project_name)\n",
    "                            print(f\"Relationship established between {name} and {project_name} ({key}).\")\n",
    "                            successful_creations += 1\n",
    "                else:\n",
    "                    print(f\"'{key}' data not found for {name}\")\n",
    "\n",
    "        print(f\"{successful_creations} projetos atualizados com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc0fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class ArticleHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def fetch_person_productions(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"MATCH (p:Person {name: $name}) RETURN p.Produções as produções\", name=name)\n",
    "            record = result.single()\n",
    "            return record['produções'] if record else None\n",
    "\n",
    "    def extract_article_info(self, input_str):\n",
    "        # Encontre todas as abreviaturas de iniciais em maiúsculas e seus índices\n",
    "        abbreviations = [(match.group(), match.start()) for match in re.finditer(r'\\b[A-Z]\\.', input_str)]\n",
    "\n",
    "        # Encontre a posição da maior ocorrência de abreviaturas de iniciais, se houver\n",
    "        if abbreviations:\n",
    "            max_abbr_position = max(abbreviations, key=lambda x: x[1])\n",
    "\n",
    "            # Encontre a primeira ocorrência de '. ' ou ' . ' após a maior ocorrência de abreviaturas de iniciais\n",
    "            first_separator_candidates = [\n",
    "                input_str.find('. ', max_abbr_position[1] + 3),\n",
    "                input_str.find(' . ', max_abbr_position[1] + 3),\n",
    "                input_str.find('.. ')\n",
    "            ]\n",
    "            first_separator_candidates = [pos for pos in first_separator_candidates if pos != -1]\n",
    "\n",
    "            if first_separator_candidates:\n",
    "                first_separator = min(first_separator_candidates)\n",
    "\n",
    "                # Encontre a primeira ocorrência de '. ' após o primeiro separador\n",
    "                second_separator = input_str.find('. ', first_separator + 2)\n",
    "\n",
    "                # Encontre a primeira ocorrência de ', ' após o segundo separador\n",
    "                third_separator = input_str.find(', ', second_separator + 2)\n",
    "            else:\n",
    "                first_separator = second_separator = third_separator = -1\n",
    "        else:\n",
    "            first_separator = second_separator = third_separator = -1\n",
    "\n",
    "        # Defina o padrão para encontrar \"p.\" e o conteúdo até a próxima vírgula\n",
    "        pages_match = re.search(r' p\\.\\s*(.*?),', input_str)\n",
    "        pages = pages_match.group(1) if pages_match else \"\"\n",
    "\n",
    "        # Defina o padrão para encontrar \"v.\" e o conteúdo até a próxima vírgula\n",
    "        volume_match = re.search(r' v\\.\\s*(.*?),', input_str)\n",
    "        volume = volume_match.group(1) if volume_match else \"\"\n",
    "\n",
    "        # Encontre a primeira ocorrência de um ano de quatro dígitos seguido de ponto final após o terceiro separador\n",
    "        year_match = re.search(r' \\d{4}\\.', input_str[third_separator + 2:])\n",
    "        year = year_match.group().strip('.').strip() if year_match else \"\"\n",
    "\n",
    "        # Extraia os dados com base nas posições dos separadores\n",
    "        authors = input_str[:first_separator].strip()\n",
    "        title = input_str[first_separator + 2:second_separator].strip()\n",
    "        journal = input_str[second_separator + 2:third_separator].strip()\n",
    "\n",
    "        # Verifique se a lista de autores e o título não estão vazios\n",
    "        if not authors or not title:\n",
    "            return None  # Retorna None para indicar falha\n",
    "\n",
    "        # Crie um dicionário com os dados extraídos\n",
    "        article_info = {\n",
    "            \"authors\": authors,\n",
    "            \"title\": title,\n",
    "            \"original_title\": journal,\n",
    "            \"pages\": pages,\n",
    "            \"volume\": volume,\n",
    "            \"year\": year\n",
    "        }\n",
    "\n",
    "        return article_info\n",
    "    \n",
    "    def deserialize_and_create_nodes(self, name):\n",
    "        print(f\"Fetching 'Produções' data for {name}...\")\n",
    "        productions_data = self.fetch_person_productions(name)\n",
    "        \n",
    "        if not productions_data:\n",
    "            print(f\"'Produções' data not found or empty for {name}.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Attempting to deserialize 'Produções' data for {name}...\")\n",
    "        try:\n",
    "            productions_data = json.loads(productions_data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to deserialize 'Produções' data for {name}: {e}\")\n",
    "            return\n",
    "\n",
    "        successful_articles = 0\n",
    "        unsuccessful_articles = []\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            print(f\"Processing 'Produção bibliográfica' for {name}...\")\n",
    "            bibliographic_production = productions_data.get(\"Produção bibliográfica\", {})\n",
    "            \n",
    "            if isinstance(bibliographic_production, str):\n",
    "                print(f\"Attempting to deserialize 'Produção bibliográfica' for {name}...\")\n",
    "                try:\n",
    "                    bibliographic_production = json.loads(bibliographic_production)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Failed to deserialize 'Produção bibliográfica' for {name}: {e}\")\n",
    "                    return\n",
    "\n",
    "            articles = json.loads(bibliographic_production.get(\"Artigos completos publicados em periódicos\", \"{}\"))\n",
    "\n",
    "            for _, article_str in articles.items():\n",
    "                article_details = self.extract_article_info(article_str)\n",
    "\n",
    "                # Vamos imprimir os detalhes de cada artigo e verificar se os autores estão presentes.\n",
    "                print(f\"Original Article: {article_str}\")\n",
    "                print(f\"Extracted Details: {article_details}\")\n",
    "\n",
    "                if article_details:\n",
    "                    article_details[\"title\"] = article_details[\"title\"].strip()\n",
    "                    article_details[\"original_title\"] = article_details[\"original_title\"].strip()\n",
    "\n",
    "                    session.run(f\"MERGE (a:Artigo {{title: $title}}) SET a += $details\", title=article_details[\"title\"], details=article_details)\n",
    "                    session.run(f\"MATCH (p:Person {{name: $name}}), (a:Artigo {{title: $title}}) MERGE (p)-[:PUBLICOU]->(a)\", name=name, title=article_details[\"title\"])\n",
    "                    successful_articles += 1\n",
    "                else:\n",
    "                    unsuccessful_articles.append(article_str)\n",
    "\n",
    "        print(f\"Processed {successful_articles} articles successfully for {name}.\")\n",
    "\n",
    "        if unsuccessful_articles:\n",
    "            print(\"Failed to process the following articles:\")\n",
    "            for article in unsuccessful_articles:\n",
    "                print(article)\n",
    "\n",
    "    def process_articles(self, name):\n",
    "        self.deserialize_and_create_nodes(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4009b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class JcrHandler:\n",
    "#     def __init__(self, uri, user, password):\n",
    "#         self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "#     def close(self):\n",
    "#         self._driver.close()\n",
    "\n",
    "#     def _consultar_propriedades_jcr(self, tx, name):\n",
    "#         query = (\n",
    "#             \"MATCH (p:Person {name: $name})\"\n",
    "#             \"RETURN p.JCR AS jcr\"\n",
    "#         )\n",
    "#         result = tx.run(query, name=name)\n",
    "#         return [record[\"jcr\"] for record in result]\n",
    "   \n",
    "#     ## Versão para usar com criação de nós secundários retorna JSON\n",
    "#     def consultar_propriedades_jcr(self, name):\n",
    "#         with self._driver.session() as session:\n",
    "#             query = (\n",
    "#                 \"MATCH (p:Person {name: $name})\"\n",
    "#                 \"RETURN p.JCR AS jcr\"\n",
    "#             )\n",
    "#             result = session.run(query, name=name)\n",
    "#             jcr_data = result.single()[\"jcr\"]\n",
    "#             jcr_properties_list = json.loads(jcr_data)\n",
    "#             return jcr_properties_list\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _convert_list_to_dict(lst):\n",
    "#         \"\"\"\n",
    "#         Converts a list into a dictionary with indices as keys.\n",
    "        \n",
    "#         Parameters:\n",
    "#         - lst: list, input list to be transformed.\n",
    "        \n",
    "#         Returns:\n",
    "#         - dict: Transformed dictionary.\n",
    "#         \"\"\"\n",
    "#         return {str(i): item for i, item in enumerate(lst)}\n",
    "    \n",
    "#     def create_person_with_jcr(self, name, jcr_properties):\n",
    "#         with self._driver.session() as session:\n",
    "#             session.write_transaction(self._create_person_with_jcr, name, jcr_properties)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _create_person_with_jcr(tx, name, jcr_properties):\n",
    "#         # Cria o nó Person\n",
    "#         person_query = (\n",
    "#             \"CREATE (p:Person {name: $name}) \"\n",
    "#             \"RETURN p\"\n",
    "#         )\n",
    "#         person_result = tx.run(person_query, name=name)\n",
    "#         person_node = person_result.single()[0]\n",
    "\n",
    "#         # Cria os nós secundários para cada valor único de data-issn\n",
    "#         data_issn_values = set(prop.get(\"data-issn\") for prop in jcr_properties)\n",
    "#         for data_issn in data_issn_values:\n",
    "#             if data_issn:\n",
    "#                 secondary_node_query = (\n",
    "#                     \"CREATE (s:SecondaryNode {data_issn: $data_issn}) \"\n",
    "#                     \"RETURN s\"\n",
    "#                 )\n",
    "#                 tx.run(secondary_node_query, data_issn=data_issn)\n",
    "\n",
    "#                 # Cria a relação entre o nó Person e o nó secundário\n",
    "#                 relation_query = (\n",
    "#                     \"MATCH (p:Person {name: $name}), (s:SecondaryNode {data_issn: $data_issn}) \"\n",
    "#                     \"CREATE (p)-[:HAS_JCR]->(s)\"\n",
    "#                 )\n",
    "#                 tx.run(relation_query, name=name, data_issn=data_issn)\n",
    "\n",
    "#     def createJournalsNodes(self, name):\n",
    "#         # Get JCR properties\n",
    "#         jcr_properties = self.consultar_propriedades_jcr(name)\n",
    "\n",
    "#         # Convert the serialized JSON strings back into dictionaries\n",
    "#         deserialized_jcr_properties = [json.loads(prop) for prop in jcr_properties.values()]\n",
    "\n",
    "#         # Inform the user about the total number of JCR property entries\n",
    "#         total_entries = len(deserialized_jcr_properties)\n",
    "#         print(f\"Read {total_entries} entries from JCR properties of Person '{name}'.\")\n",
    "\n",
    "#         # Extract relevant journal properties and their count\n",
    "#         journal_counts = Counter(prop.get(\"data-issn\") for prop in deserialized_jcr_properties)\n",
    "        \n",
    "#         # Number of unique ISSNs\n",
    "#         unique_issns = len(journal_counts)\n",
    "#         print(f\"Identified {unique_issns} unique ISSN values.\")\n",
    "\n",
    "#         null_count = journal_counts.pop(None, 0)  # Remove None (null) ISSN and get its count\n",
    "#         null_count += journal_counts.pop(\"NULL\", 0)  # Also account for \"NULL\" as a string\n",
    "\n",
    "#         # Counters for journals\n",
    "#         successful_journal_creations = 0\n",
    "\n",
    "#         with self._driver.session() as session:\n",
    "#             for data_issn, count in journal_counts.items():\n",
    "#                 if data_issn and data_issn != \"NULL\":\n",
    "#                     representative_entry = next(prop for prop in deserialized_jcr_properties if prop.get(\"data-issn\") == data_issn)\n",
    "#                     journal_name = representative_entry.get(\"original_title\")\n",
    "#                     fator_impacto = representative_entry.get(\"impact-factor\")\n",
    "#                     jcr_year = representative_entry.get(\"jcr-year\")\n",
    "\n",
    "#                     # Create or merge the Journal node\n",
    "#                     journal_node_query = (\n",
    "#                         \"MERGE (j:Revistas {ISSN: $data_issn}) \"\n",
    "#                         \"ON CREATE SET j.name = $journal_name, j.FatorImpacto = $impact_factor, j.JCRYear = $jcr_year \"  # Corrected this line\n",
    "#                         \"RETURN j\"\n",
    "#                     )\n",
    "#                     session.run(journal_node_query, data_issn=data_issn, journal_name=journal_name, impact_factor=fator_impacto, jcr_year=jcr_year)  # And this line\n",
    "\n",
    "#                     # Create or update the \"PUBLICOU_EM\" relationship\n",
    "#                     relation_query = (\n",
    "#                         \"MATCH (p:Person {name: $name}), (j:Revistas {ISSN: $data_issn}) \"  # corrected this line\n",
    "#                         \"MERGE (p)-[r:PUBLICOU_EM]->(j) \"\n",
    "#                         \"ON CREATE SET r.QuantidadePublicações = $count \"\n",
    "#                         \"ON MATCH SET r.QuantidadePublicações = r.QuantidadePublicações + $count\"\n",
    "#                     )\n",
    "#                     session.run(relation_query, name=name, data_issn=data_issn, count=count)\n",
    "                    \n",
    "#                     successful_journal_creations += 1\n",
    "                \n",
    "#                 if null_count:\n",
    "#                     # For example, to print the count:\n",
    "#                     pass\n",
    "        \n",
    "#         # Inform the user about journals\n",
    "#         print(f\"{successful_journal_creations} Revistas adicionadas com sucesso.\")\n",
    "#         print(f\"{null_count} Revistas não foram criadas por terem valor NULL de ISSN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import urllib.parse\n",
    "import json\n",
    "import re\n",
    "\n",
    "class AdvisorHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(input_data):\n",
    "        if input_data is None:\n",
    "            return None\n",
    "        if isinstance(input_data, dict):\n",
    "            for key, value in input_data.items():\n",
    "                if isinstance(value, dict):  \n",
    "                    input_data[key] = json.dumps(AdvisorHandler.convert_to_primitives(value), ensure_ascii=False)\n",
    "                else:\n",
    "                    input_data[key] = AdvisorHandler.convert_to_primitives(value)\n",
    "            return input_data\n",
    "        elif isinstance(input_data, list):\n",
    "            return [AdvisorHandler.convert_to_primitives(item) for item in input_data]\n",
    "        elif isinstance(input_data, str):\n",
    "            if 'http://' in input_data or 'https://' in input_data:\n",
    "                parts = input_data.split(\" \")\n",
    "                new_parts = [urllib.parse.quote(part) if part.startswith(('http://', 'https://')) else part for part in parts]\n",
    "                return \" \".join(new_parts)\n",
    "            return input_data\n",
    "        elif isinstance(input_data, (int, float, bool)):\n",
    "            return input_data\n",
    "        else:\n",
    "            return str(input_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return AdvisorHandler.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise\n",
    "\n",
    "    def consult_orientacoes(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            query = (\n",
    "                \"MATCH (p:Person {name: $name})\"\n",
    "                \"RETURN p.Orientações AS orientacoes\"\n",
    "            )\n",
    "            result = session.run(query, name=name)\n",
    "            orient_data = result.single()[\"orientacoes\"]\n",
    "            if orient_data is None:\n",
    "                raise ValueError(f\"No data found for 'Orientações' attribute for Person '{name}'\")\n",
    "            orient_properties_list = json.loads(orient_data)\n",
    "            return orient_properties_list\n",
    "\n",
    "    def create_advisor_relations(self, name):\n",
    "        # Get Orientações properties\n",
    "        orient_properties = self.consult_orientacoes(name)\n",
    "\n",
    "        # Convert the serialized JSON strings back into dictionaries\n",
    "        try:\n",
    "            deserialized_orient_properties = self.debug_and_convert(orient_properties)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deserializing Orientações properties: {e}\")\n",
    "            return\n",
    "\n",
    "        # Advisory relationship mapping\n",
    "        advisory_types = {\n",
    "            \"Dissertação de mestrado\": \"ORIENTOU_MESTRADO\",\n",
    "            \"Tese de doutorado\": \"ORIENTOU_DOUTORADO\",\n",
    "            \"Trabalho de conclusão de curso de graduação\": \"ORIENTOU_GRADUAÇÃO\"\n",
    "        }\n",
    "\n",
    "        successful_advisory_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for orientacao_category, advisories in deserialized_orient_properties.items():\n",
    "                if isinstance(advisories, str):\n",
    "                    try:\n",
    "                        advisories = json.loads(advisories)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Failed to deserialize JSON string in 'Orientações' for category '{orientacao_category}': {advisories}\")\n",
    "                        continue\n",
    "\n",
    "                if not isinstance(advisories, dict):\n",
    "                    print(f\"Unexpected data type in 'Orientações' for category '{orientacao_category}': {advisories}\")\n",
    "                    continue\n",
    "\n",
    "                for advisory_type, relationships in advisories.items():\n",
    "                    relation_label = advisory_types.get(advisory_type)\n",
    "                    if not relation_label:\n",
    "                        continue  # skip if the advisory type is not one of the specified ones\n",
    "\n",
    "                    for _, detail in json.loads(relationships).items():\n",
    "                        try:\n",
    "                            student_name = detail.split(\".\")[0]\n",
    "                            title = detail.split(\".\")[1]\n",
    "                            \n",
    "                            # Extract the year from the detail string\n",
    "                            year_match = re.search(r'(\\d{4})', detail)\n",
    "                            year = year_match.group(1) if year_match else None\n",
    "\n",
    "                            # Create or merge the Orientações node\n",
    "                            node_query = (\n",
    "                                \"MERGE (a:Orientações {Title: $title}) \"\n",
    "                                \"ON CREATE SET a.StudentName = $student_name, a.Tipo = $advisory_type, a.Year = $year \"\n",
    "                                \"ON MATCH SET a.Tipo = $advisory_type, a.Year = $year \"  # Ensure the 'Tipo' and 'Year' are always updated\n",
    "                                \"RETURN a\"\n",
    "                            )\n",
    "                            session.run(node_query, title=title, student_name=student_name, advisory_type=advisory_type, year=year)\n",
    "\n",
    "                            # Create or update the advisory relationship\n",
    "                            relation_query = (\n",
    "                                f\"MATCH (p:Person {{name: $name}}), (a:Orientações {{Title: $title}}) \"\n",
    "                                f\"MERGE (p)-[r:{relation_label}]->(a) \"\n",
    "                            )\n",
    "                            session.run(relation_query, name=name, title=title)\n",
    "\n",
    "                            successful_advisory_creations += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing advisory '{detail}': {e}\")\n",
    "\n",
    "        # Inform the user about advisories\n",
    "        print(f\"{successful_advisory_creations} orientações atualizadas com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "class DataRemovalHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        \"\"\"\n",
    "        Inicializa a classe DataRemovalHandler com informações de conexão ao banco de dados Neo4j.\n",
    "\n",
    "        Parâmetros:\n",
    "        - uri (str): URI de conexão ao Neo4j.\n",
    "        - user (str): Nome de usuário para autenticação.\n",
    "        - password (str): Senha para autenticação.\n",
    "        \"\"\"\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Fecha a conexão com o banco de dados Neo4j.\n",
    "        \"\"\"\n",
    "        self._driver.close()\n",
    "\n",
    "    def delete_nodes_by_label(self, label):\n",
    "        \"\"\"\n",
    "        Deleta todos os nós associados a um label específico no Neo4j.\n",
    "\n",
    "        Parâmetro:\n",
    "        - label (str): O label dos nós a serem deletados.\n",
    "\n",
    "        Retorna:\n",
    "        - int: O número de nós deletados.\n",
    "        \"\"\"\n",
    "        with self._driver.session() as session:\n",
    "            # Esta consulta combina com todos os nós do label especificado e os deleta\n",
    "            result = session.run(f\"MATCH (n:{label}) DETACH DELETE n RETURN count(n) as deleted_count\")\n",
    "            deleted_count = result.single()[\"deleted_count\"]\n",
    "            return deleted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e663506",
   "metadata": {},
   "source": [
    "# Interação com dados do e-Lattes\n",
    "\n",
    "- Fazer análise com lista dos nomes no e-lattes e baixar os arquivos gerados\n",
    "- Salvar na pasta _data/in_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090dde8d-29f4-4736-b400-beb9627220af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Grupos de Funções antigas para refatorar nas classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install levenshtein\n",
    "# !pip3 install editdistance\n",
    "# !pip3 install seaborn\n",
    "# !pip3 install pyjarowinkler\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from string import Formatter\n",
    "from datetime import date, timedelta\n",
    "from datetime import datetime as dt\n",
    "from unidecode import unidecode\n",
    "from plotly.subplots import make_subplots\n",
    "from pyjarowinkler.distance import get_jaro_distance\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "\n",
    "class PreparadorDePublicacoes:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.colunas = ['idLattes', 'nome', 'tipo', 'titulo_do_capitulo', 'idioma', 'titulo_do_livro', 'ano', 'doi', 'pais_de_publicacao', 'isbn', \n",
    "        'nome_da_editora', 'numero_da_edicao_revisao', 'organizadores', 'paginas', 'autores', 'autores-endogeno', 'autores-endogeno-nome', 'tags', \n",
    "        'Hash', 'tipo_producao', 'natureza', 'titulo', 'nome_do_evento', 'ano_do_trabalho', 'pais_do_evento', 'cidade_do_evento', 'classificacao', \n",
    "        'periodico', 'volume', 'issn', 'estrato_qualis', 'editora', 'numero_de_paginas', 'numero_de_volumes']\n",
    "\n",
    "    def extrair_dados(self, registro, tipo_producao):\n",
    "        linha = {coluna: None for coluna in self.colunas}\n",
    "        linha['tipo_producao'] = tipo_producao  # Define o tipo de produção com base na chave do dicionário\n",
    "        \n",
    "        # Mapear diretamente os campos do registro para a linha, assegurando que todos os campos desejados sejam extraídos\n",
    "        for campo in ['titulo', 'idioma', 'periodico', 'ano', 'volume', 'issn', 'estrato_qualis', 'pais_de_publicacao', 'paginas', 'doi']:\n",
    "            linha[campo] = registro.get(campo, '')\n",
    "\n",
    "        # Tratar os autores como uma string concatenada se eles existirem\n",
    "        linha['autores'] = '; '.join(registro.get('autores', []))\n",
    "\n",
    "        # Tratar os campos 'autores-endogeno' e 'autores-endogeno-nome'\n",
    "        if 'autores-endogeno' in registro and registro['autores-endogeno']:\n",
    "            id_endogeno = registro['autores-endogeno'][0]\n",
    "            linha['idLattes'] = id_endogeno\n",
    "            linha['nome'] = registro['autores-endogeno-nome'][0].get(id_endogeno, None)\n",
    "        \n",
    "        # Adicionar os outros campos conforme necessário aqui\n",
    "        # Por exemplo, tratamento de 'tags', 'Hash', etc., quando necessário\n",
    "\n",
    "        return linha\n",
    "\n",
    "    def processar_publicacoes(self):\n",
    "        linhas = []\n",
    "        # Itera diretamente sobre cada registro em self.data\n",
    "        for registro in self.data:\n",
    "            linha = self.extrair_dados(registro, registro.get('tipo_producao', 'Desconhecido'))\n",
    "            linhas.append(linha)\n",
    "        return linhas\n",
    "\n",
    "    def extract_zips(self, pathzip):\n",
    "        destination = os.path.join(os.getcwd(), '_data', 'in_json')\n",
    "        if not os.path.exists(destination):\n",
    "            os.makedirs(destination)\n",
    "            print(f\"Criada pasta para armazenar dados descompactados: {destination}\")\n",
    "        else:\n",
    "            print(f\"Descompactando arquivos para: {destination}\")\n",
    "\n",
    "        with zipfile.ZipFile(pathzip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(destination)\n",
    "        \n",
    "        print(\"Descompactação concluída...\")\n",
    "        return destination  # Retorna o caminho onde os arquivos foram descompactados\n",
    "\n",
    "    def find_and_merge_publication_json_files(self, pathjson):\n",
    "        all_data = []\n",
    "        for filename in os.listdir(pathjson):\n",
    "            if 'publication.json' in filename:\n",
    "                print(f\"Extraindo dados do arquivo '{filename}'...\")\n",
    "                with open(os.path.join(pathjson, filename), 'r', encoding='utf-8') as file:\n",
    "                    file_data = json.load(file)\n",
    "                    for tipo_producao in file_data:  # Para cada tipo de produção no arquivo\n",
    "                        for ano in file_data[tipo_producao]:  # Para cada ano dentro de um tipo de produção\n",
    "                            for registro in file_data[tipo_producao][ano]:  # Itera sobre cada registro\n",
    "                                # Adicionar ou atualizar o campo 'tipo_producao' em cada registro\n",
    "                                registro_atualizado = registro.copy()  # Fazer cópia para evitar modificar o original\n",
    "                                registro_atualizado['tipo_producao'] = tipo_producao  # Atualizar ou adicionar o campo 'tipo_producao'\n",
    "                                all_data.append(registro_atualizado)  # Adicionar o registro atualizado à lista\n",
    "\n",
    "        # Salva a lista unificada em um novo arquivo JSON\n",
    "        unified_json_path = os.path.join(pathjson, 'unified_pub.json')\n",
    "        with open(unified_json_path, 'w', encoding='utf-8') as unified_file:\n",
    "            json.dump(all_data, unified_file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"Arquivo unificado criado em: {unified_json_path}\")\n",
    "\n",
    "        # Atualiza self.data com os dados unidos\n",
    "        self.data = all_data\n",
    "\n",
    "    def merge_publication_json_files(self, pathjson):\n",
    "        \"\"\"\n",
    "        This function receives a path to a JSON folder, accesses the folder's contents, searches for files\n",
    "        containing 'publication.json' in their filename, merges their contents and saves the resulting\n",
    "        data as a CSV file in destination folder.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        # Looping through the files in the given path\n",
    "        for filename in os.listdir(pathjson):\n",
    "            if 'publication.json' in filename:\n",
    "                print(f\"Extraindo dados do arquivo {filename}...\")\n",
    "                # Opening the file and appending its data to the list\n",
    "                with open(os.path.join(pathjson, filename), 'r', encoding='utf-8') as file:\n",
    "                    data.append(json.load(file))\n",
    "\n",
    "        # Creating the output directory if it doesn't exist\n",
    "        destination = os.path.join(os.getcwd(), '_data','powerbi')\n",
    "        if not os.path.exists(destination):\n",
    "            os.mkdir(destination)\n",
    "\n",
    "        # Writing the merged data to a CSV file\n",
    "        print(f\"Criando arquivo CSV...\")\n",
    "        with open(os.path.join(destination, 'publication.csv'), 'w', encoding='utf-8', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            # Writing the header based on the keys of the first item in the list\n",
    "            writer.writerow(data[0].keys())\n",
    "            # Looping through the items in the list and writing them as rows in the CSV file\n",
    "            for item in data:\n",
    "                writer.writerow(item.values())\n",
    "\n",
    "    def exportar_para_csv(self, nome_arquivo='publicacoes.csv'):\n",
    "        linhas = self.processar_publicacoes()\n",
    "        df = pd.DataFrame(linhas, columns=self.colunas)\n",
    "        filepathcsv = os.path.join(\"./\",\"_data\",\"powerbi\",nome_arquivo)\n",
    "        df.to_csv(filepathcsv, index=False)\n",
    "        print(f'Arquivo criado com sucesso em {filepathcsv}')\n",
    "\n",
    "    def ler_csv_dados(self, pathdata,filename):\n",
    "        filepath = os.path.join(pathdata,filename)\n",
    "        df_pub=pd.read_csv(filepath)\n",
    "        tipos = df_pub['tipo_producao'].value_counts()\n",
    "        qualis=df_pub['estrato_qualis'].value_counts()  \n",
    "        quantidade_nan = df_pub['estrato_qualis'].isna().sum()\n",
    "        tipos_qualis = qualis.count()\n",
    "        percentual_nan=np.round(quantidade_nan/tipos.sum()*100,1)\n",
    "        print(f\"\\n{tipos.sum()} linhas no total, distribuídas nos seguintes tipos de produção:\")\n",
    "        print(f\"{quantidade_nan} linhas ({percentual_nan}% do total de linhas) com NaN no campo estrato_qualis\")\n",
    "        print(tipos)\n",
    "        ano_min=df_pub['ano'].min()\n",
    "        ano_max=df_pub['ano'].max()\n",
    "        print(f\"\\n{tipos.get('PERIODICO', 0)} publicações em periódicos, no período de {int(ano_min)} a {int(ano_max)}\")\n",
    "        percentual=np.round(qualis.sum()/tipos.get('PERIODICO', 0)*100,1)\n",
    "        print(f\"{qualis.sum()} publicações ({percentual}%) classificadas no qualis periodicos conforme estratificação:\")\n",
    "        quantidade_nan_periodico = df_pub[df_pub['tipo_producao'] == 'PERIODICO']['estrato_qualis'].isna().sum()\n",
    "        percentual_nan_periodico = np.round(quantidade_nan_periodico/tipos.get('PERIODICO', 0)*100,1)\n",
    "        print(f\"{quantidade_nan_periodico} publicações em periódicos ({percentual_nan_periodico}%) com valor 'NaN' no estrato_qualis\\n\")\n",
    "        print(qualis)\n",
    "        return df_pub\n",
    "\n",
    "    def ler_xls_dados(self, pathdata,filename):\n",
    "        filepath = os.path.join(pathdata,filename)\n",
    "        df_pub=pd.read_excel(filepath)\n",
    "        tipos = df_pub['tipo_producao'].value_counts()\n",
    "        qualis=df_pub['estrato_qualis'].value_counts()\n",
    "        tipos_qualis = qualis.count()\n",
    "        print(f\"\\n{tipos.sum()} linhas no total, distribuídas nos seguintes tipos de produção:\")\n",
    "        print(tipos)\n",
    "        ano_min=df_pub['ano'].min()\n",
    "        ano_max=df_pub['ano'].max()\n",
    "        print(f\"\\n{tipos.get('PERIODICO', 0)} publicações em periódicos, no período de {int(ano_min)} a {int(ano_max)}\")\n",
    "        percentual=np.round(qualis.sum()/tipos.get('PERIODICO', 0)*100,1)\n",
    "        print(f\"{qualis.sum()} publicações ({percentual}%) classificadas no qualis periodicos conforme estratificação:\")\n",
    "        print(qualis)\n",
    "        return df_pub\n",
    "\n",
    "    def sigla_producao(self, tipo_producao, separador=\"\"):\n",
    "        \"\"\"\n",
    "        Função para converter os valores da coluna 'tipo_producao' em siglas, desconsiderando preposições,\n",
    "        fazendo split de 'tipo_producao' em palavras também pelo caractere \"_\".\n",
    "        \n",
    "        Parâmetros:\n",
    "        - tipo_producao: O valor da coluna 'tipo_producao'.\n",
    "        - separador: O caractere para separar as iniciais. Padrão é \"_\".\n",
    "        \n",
    "        Retorna:\n",
    "        - Uma string que é a sigla formada pelas iniciais das palavras em 'tipo_producao', desconsiderando preposições.\n",
    "        \"\"\"\n",
    "        # Lista de preposições para serem ignoradas\n",
    "        preposicoes = ['e', 'de', 'do', 'da', 'dos', 'das', 'em', 'na', 'no', 'nas', 'nos', 'por', 'para', 'com']\n",
    "\n",
    "        # Mapeamento de exemplo conforme as siglas fornecidas\n",
    "        mapeamento_siglas = {\n",
    "            \"DTPB\": 1, #DEMAIS_TIPOS_DE_PRODUCAO_BIBLIOGRAFICA \n",
    "            \"TJ\": 2, #TEXTO_EM_JORNAIS\n",
    "            \"E\": 3, #EVENTO\n",
    "            \"CL\": 4, #CAPITULO_DE_LIVRO\n",
    "            \"L\": 5, #LIVRO\n",
    "            \"AA\": 6, #ARTIGO_ACEITO\n",
    "            \"P\": 7, #PERIODICO\n",
    "        }\n",
    "\n",
    "        # Fazendo split por espaço e por underscore\n",
    "        palavras = tipo_producao.replace('_', ' ').replace('-', ' ').split()\n",
    "\n",
    "        # Inicializar a lista de iniciais\n",
    "        iniciais = []\n",
    "\n",
    "        for palavra in palavras:\n",
    "            # Verificar se a palavra não é uma preposição e não está vazia\n",
    "            if palavra.lower() not in preposicoes and palavra:\n",
    "                # Adicionar a inicial em maiúsculo à lista de iniciais\n",
    "                iniciais.append(palavra[0].upper())\n",
    "\n",
    "        # Juntar as iniciais para formar a sigla\n",
    "        sigla = \"\".join(iniciais)\n",
    "\n",
    "        # Aplicar o mapeamento para encontrar o número correspondente à sigla\n",
    "        numero = mapeamento_siglas.get(sigla, \"\")\n",
    "\n",
    "        # Retornar a string formatada com o número e a sigla\n",
    "        return f\"{numero}-{sigla}\"  \n",
    "\n",
    "    def sigla_qualis(self, estrato_qualis, separador=\"\"):\n",
    "        \"\"\"\n",
    "        Função para converter os valores da coluna 'estrato_qualis' em siglas com números de ordenação,\n",
    "        \n",
    "        Parâmetros:\n",
    "        - estrato_qualis: O valor da coluna 'estrato_qualis'.\n",
    "        - separador: O caractere para separar as iniciais. Padrão é \"_\".\n",
    "        \n",
    "        Retorna:\n",
    "        - Uma string que é a sigla formada pelas iniciais das palavras em 'estrato_qualis' antecedida por seu número de ordenação.\n",
    "        \"\"\"\n",
    "\n",
    "        # Mapeamento de exemplo conforme as siglas fornecidas\n",
    "        mapeamento_siglas = {\n",
    "            \"C\": 1,\n",
    "            \"B4\": 2,\n",
    "            \"B3\": 3,\n",
    "            \"B2\": 4,\n",
    "            \"B1\": 5,\n",
    "            \"B2\": 6,\n",
    "            \"B1\": 7,\n",
    "            \"A4\": 8,\n",
    "            \"A3\": 9,\n",
    "            \"A2\": 10,\n",
    "            \"A1\": 11,\n",
    "        }\n",
    "\n",
    "        # Aplicar o mapeamento para encontrar o número correspondente à sigla\n",
    "        numero = mapeamento_siglas.get(estrato_qualis, \"\")\n",
    "\n",
    "        # Retornar a string formatada com o número e a sigla\n",
    "        return f\"{numero}-{estrato_qualis}\"\n",
    "\n",
    "    def agrupar_publicacoes(self, filename):\n",
    "        # Caminho para o arquivo CSV\n",
    "        pathfilename = os.path.join(os.getcwd(), '_data', 'powerbi', filename)\n",
    "        df = pd.read_csv(pathfilename)\n",
    "\n",
    "        # Use .loc para modificar o DataFrame diretamente e evitar o aviso\n",
    "        df.loc[:, 'sigla_producao'] = df['tipo_producao'].apply(lambda x: self.sigla_producao(x))\n",
    "        df_publicacoes = df[['idLattes','ano','sigla_producao']]\n",
    "\n",
    "        # Agrupar por 'ano', 'natureza' e orientador e contar as ocorrências\n",
    "        contagem_publicacoes = df_publicacoes.groupby(['ano','sigla_producao','idLattes']).size().reset_index(name='contagem')\n",
    "\n",
    "        # Retornar a contagem de orientações por tipo e por ano\n",
    "        return contagem_publicacoes\n",
    "\n",
    "    def agrupar_qualis(self, filename):\n",
    "        # Caminho para o arquivo CSV\n",
    "        pathfilename = os.path.join(os.getcwd(), '_data', 'powerbi', filename)\n",
    "        df = pd.read_csv(pathfilename)\n",
    "\n",
    "        # # Use .loc para modificar o DataFrame diretamente e evitar o aviso\n",
    "        df.loc[:, 'estrato_qualis'] = df['estrato_qualis'].apply(lambda x: self.sigla_qualis(x))\n",
    "        df_publicacoes = df[['idLattes','ano','estrato_qualis']]\n",
    "\n",
    "        # Remover todas as linhas que contêm pelo menos um valor NaN\n",
    "        df_publicacoes_limpo = df_publicacoes.dropna()\n",
    "\n",
    "        # Agrupar por 'ano', 'natureza' e orientador e contar as ocorrências\n",
    "        contagem_qualis = df_publicacoes_limpo.groupby(['ano','estrato_qualis','idLattes']).size().reset_index(name='contagem')\n",
    "\n",
    "        # Retornar a contagem de orientações por tipo e por ano\n",
    "        return contagem_qualis\n",
    "\n",
    "    def plotar_producoes_barras_empilhadas(self, filename):\n",
    "        contagem_publicacoes = self.agrupar_publicacoes(filename)\n",
    "        fig = make_subplots(rows=1, cols=1)\n",
    "        anos = sorted(contagem_publicacoes['ano'].unique())\n",
    "\n",
    "        mapeamento_siglas_para_numeros = {\n",
    "            \"1-DTPB\": 1, # Demais tipos de produção bibliográfica\n",
    "            \"2-TJ\": 2, # Texto em jornais\n",
    "            \"3-E\": 3, # Evento\n",
    "            \"4-CL\": 4, # Capítulo de Livro\n",
    "            \"5-L\": 5, # Livro\n",
    "            \"6-AA\": 6, # Artigo Aceito\n",
    "            \"7-P\": 7, # Periódico\n",
    "        }\n",
    "\n",
    "        naturezas_ordenadas = sorted(contagem_publicacoes['sigla_producao'].unique(), key=lambda x: mapeamento_siglas_para_numeros.get(x, 999))\n",
    "\n",
    "        for natureza in naturezas_ordenadas:\n",
    "            contagem_por_ano = []\n",
    "            labels_por_ano = [] # Para armazenar os rótulos de dados\n",
    "            for ano in anos:\n",
    "                contagem = contagem_publicacoes[(contagem_publicacoes['ano'] == ano) & (contagem_publicacoes['sigla_producao'] == natureza)]['contagem'].sum()\n",
    "                if mapeamento_siglas_para_numeros.get(natureza, 999) in [1, 2, 3, 4]:\n",
    "                    contagem = -contagem\n",
    "                contagem_por_ano.append(contagem)\n",
    "                labels_por_ano.append(str(contagem)) # Convertendo a contagem em string para usar como rótulo\n",
    "            \n",
    "            # Adicionar a barra ao gráfico com rótulos de dados\n",
    "            fig.add_trace(go.Bar(x=anos, y=contagem_por_ano, name=natureza, text=labels_por_ano, textposition='auto'))\n",
    "\n",
    "        # Atualizar o layout para permitir barras empilhadas, ajustar o eixo Y para mostrar valores negativos, e garantir que todos os anos sejam mostrados no eixo X\n",
    "        fig.update_layout(\n",
    "            barmode='relative',\n",
    "            title_text='Contagem de Produções por Tipo e Ano (Produção de divulgação plotadas abaixo do eixo X)',\n",
    "            xaxis_title=\"Ano\",\n",
    "            yaxis_title=\"Quantidade de Produções Biliográficas\",\n",
    "            yaxis=dict(zeroline=True, zerolinewidth=2, zerolinecolor='black'),\n",
    "        )        \n",
    "        # Garantir que todos os anos apareçam na barra de rótulos do eixo X\n",
    "        fig.update_xaxes(tickmode='array', tickvals=anos)\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    def plotar_qualis_barras_empilhadas(self, filename):\n",
    "        contagem_publicacoes = self.agrupar_qualis(filename)\n",
    "        contagem_publicacoes.replace(\"-nan\", np.nan, inplace=True)\n",
    "        contagem_publicacoes.dropna(inplace=True)\n",
    "        fig = make_subplots(rows=1, cols=1)\n",
    "        anos = sorted(contagem_publicacoes['ano'].unique())\n",
    "\n",
    "        mapeamento_siglas_para_numeros = {\n",
    "            \"1-C\": 1, \"2-B4\": 2, \"3-B3\": 3, \"4-B2\": 4, \"5-B1\": 5,\n",
    "            \"6-B2\": 6, \"7-B1\": 7, \"8-A4\": 8, \"9-A3\": 9, \"10-A2\": 10, \"11-A1\": 11,\n",
    "        }\n",
    "        naturezas_ordenadas = sorted(contagem_publicacoes['estrato_qualis'].unique(), key=lambda x: mapeamento_siglas_para_numeros.get(x, 999))\n",
    "        \n",
    "        # Obter a paleta de cores 'Greens'\n",
    "        cores = px.colors.sequential.Greens\n",
    "        # Certificar que temos cores suficientes para todas as barras, repetindo a paleta se necessário\n",
    "        num_barras = len(naturezas_ordenadas)\n",
    "        cores_repetidas = cores * (num_barras // len(cores) + 1)\n",
    "        \n",
    "        for index, natureza in enumerate(naturezas_ordenadas):\n",
    "            contagem_por_ano = []\n",
    "            for ano in anos:\n",
    "                contagem = contagem_publicacoes[(contagem_publicacoes['ano'] == ano) & (contagem_publicacoes['estrato_qualis'] == natureza)]['contagem'].sum()\n",
    "                if mapeamento_siglas_para_numeros.get(natureza, 999) in [1, 2, 3, 4, 5, 6, 7]:\n",
    "                    contagem = -contagem\n",
    "                contagem_por_ano.append(contagem)\n",
    "            \n",
    "            # Usar uma cor da paleta para cada barra\n",
    "            fig.add_trace(go.Bar(x=anos, y=contagem_por_ano, name=natureza, marker_color=cores_repetidas[index]))\n",
    "\n",
    "        fig.update_layout(\n",
    "            barmode='relative',\n",
    "            title_text='Contagem de Produções por Estrato Qualis e Ano (Qualis abaixo de B plotado do eixo X)',\n",
    "            xaxis_title=\"Ano\",\n",
    "            yaxis_title=\"Quantidade de Publicações\",\n",
    "            yaxis=dict(zeroline=True, zerolinewidth=2, zerolinecolor='black'),\n",
    "        )\n",
    "        # Garantir que todos os anos apareçam na barra de rótulos do eixo X\n",
    "        fig.update_xaxes(tickmode='array', tickvals=anos)        \n",
    "        fig.show()\n",
    "\n",
    "class PreparadorDeOrientacoes():\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.colunas = ['id_lattes_orientadores', 'tipo_orientacao', 'natureza', 'titulo', 'idioma', 'ano', 'id_lattes_aluno', 'nome_aluno', 'instituicao', 'pais', 'curso', 'codigo_do_curso', 'bolsa', 'agencia_financiadora', 'codigo_agencia_financiadora', 'nome_orientadores', 'tags', 'Hash']\n",
    "        self.pathjson = os.path.join('_data','in_json')\n",
    "        # self.filename = '863.advise.json'\n",
    "\n",
    "        # Abrir o arquivo e carregando o JSON\n",
    "    def open_file(self, filename):\n",
    "        with open(os.path.join(self.pathjson, filename), 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        linhas_achatadas = self.extrair_orientacoes(data)\n",
    "        df = pd.DataFrame(linhas_achatadas)\n",
    "        df.to_csv('orientacoes_achatadas.csv', index=False)\n",
    "        return df\n",
    "    \n",
    "    def agrupar_orientacoes(self, filename):\n",
    "        # Caminho para o arquivo JSON\n",
    "        pathfilename = os.path.join('_data', 'in_json', filename)\n",
    "        # dict_orientacoes = pd.read_json(pathfilename)\n",
    "\n",
    "        # Carregar o JSON e aplicar a função\n",
    "        with open(pathfilename, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        linhas_achatadas = self.extrair_orientacoes(data)\n",
    "        df = pd.DataFrame(linhas_achatadas)\n",
    "\n",
    "        # Salvar os dados achatados em um CSV\n",
    "        df.to_csv('orientacoes_achatadas.csv', index=False)\n",
    "\n",
    "        # Use .loc para modificar o DataFrame diretamente e evitar o aviso\n",
    "        df.loc[:, 'sigla_natureza'] = df['natureza'].apply(lambda x: self.sigla_natureza(x))\n",
    "        df_orientacoes = df[['id_lattes_orientadores','ano','sigla_natureza']]\n",
    "\n",
    "        # Agrupar por 'ano', 'natureza' e orientador e contar as ocorrências\n",
    "        contagem_orientacoes = df_orientacoes.groupby(['ano','sigla_natureza','id_lattes_orientadores']).size().reset_index(name='contagem')\n",
    "\n",
    "        # Retornar a contagem de orientações por tipo e por ano\n",
    "        return contagem_orientacoes\n",
    "\n",
    "    # Prosseguir com a definição e uso da função para extrair as orientações\n",
    "    def extrair_orientacoes(self, json_data):\n",
    "        colunas = self.colunas\n",
    "        linhas = []\n",
    "\n",
    "        # Iterar sobre cada tipo de orientação e ano\n",
    "        for tipo_orientacao, anos in json_data.items():\n",
    "            for ano, orientacoes in anos.items():\n",
    "                for orientacao in orientacoes:\n",
    "                    # Preparar um dicionário para cada linha de acordo com as colunas definidas\n",
    "                    linha = {}\n",
    "                    # Adicionar dados específicos da orientação\n",
    "                    for campo in orientacao:\n",
    "                        if campo in colunas:\n",
    "                            linha[campo] = orientacao[campo]\n",
    "                    # Tratar 'id_lattes_orientadores' como uma lista, juntar os IDs com vírgula se houver mais de um\n",
    "                    linha['id_lattes_orientadores'] = ', '.join(orientacao.get('id_lattes_orientadores', []))\n",
    "                    # Adicionar o tipo de orientação como uma coluna\n",
    "                    linha['tipo_orientacao'] = tipo_orientacao\n",
    "                    # Adicionar o ano, garantindo que sobreponha qualquer valor de 'ano' nos dados individuais\n",
    "                    linha['ano'] = ano\n",
    "                    linhas.append(linha)\n",
    "\n",
    "        return linhas\n",
    "\n",
    "    def sigla_natureza(self, natureza, separador=\"\"):\n",
    "        \"\"\"\n",
    "        Função para converter os valores da coluna 'natureza' em siglas, desconsiderando preposições,\n",
    "        fazendo split de 'natureza' em palavras também pelo caractere \"_\".\n",
    "        \n",
    "        Parâmetros:\n",
    "        - natureza: O valor da coluna 'natureza'.\n",
    "        - separador: O caractere para separar as iniciais. Padrão é \"_\".\n",
    "        \n",
    "        Retorna:\n",
    "        - Uma string que é a sigla formada pelas iniciais das palavras em 'natureza', desconsiderando preposições.\n",
    "        \"\"\"\n",
    "        # Lista de preposições para serem ignoradas\n",
    "        preposicoes = ['e', 'de', 'do', 'da', 'dos', 'das', 'em', 'na', 'no', 'nas', 'nos', 'por', 'para', 'com']\n",
    "\n",
    "        # Mapeamento de exemplo conforme as siglas fornecidas\n",
    "        mapeamento_siglas = {\n",
    "            \"OON\": 1, #Orientações de Outra Natureza\n",
    "            \"IC\": 2, #Iniciação Científica\n",
    "            \"MCCAE\": 3, #Monografia de Conclusão Curso de Atualização ou Especialização\n",
    "            \"SPD\": 4, #\n",
    "            \"TCCG\": 5, #Trabalho de Conclusão de Curso de Graduação\n",
    "            \"DM\": 6, #Dissertação de Mestrado\n",
    "            \"TD\": 7, #Tese de Doutorado\n",
    "        }\n",
    "\n",
    "        # Fazendo split por espaço e por underscore\n",
    "        palavras = natureza.replace('_', ' ').replace('-', ' ').split()\n",
    "\n",
    "        # Inicializar a lista de iniciais\n",
    "        iniciais = []\n",
    "\n",
    "        for palavra in palavras:\n",
    "            # Verificar se a palavra não é uma preposição e não está vazia\n",
    "            if palavra.lower() not in preposicoes and palavra:\n",
    "                # Adicionar a inicial em maiúsculo à lista de iniciais\n",
    "                iniciais.append(palavra[0].upper())\n",
    "\n",
    "        # Juntar as iniciais para formar a sigla\n",
    "        sigla = \"\".join(iniciais)\n",
    "\n",
    "        # Aplicar o mapeamento para encontrar o número correspondente à sigla\n",
    "        numero = mapeamento_siglas.get(sigla, \"Desconhecido\")\n",
    "\n",
    "        # Retornar a string formatada com o número e a sigla\n",
    "        return f\"{numero}-{sigla}\"     \n",
    "\n",
    "    def plotar_orientacoes_barras_agrupadas(self, filename):\n",
    "        # montar dataframe com agrupamento de orientações por tipo por ano no programa\n",
    "        contagem_orientacoes = self.agrupar_orientacoes(filename)\n",
    "\n",
    "        # Criar uma figura com subplots\n",
    "        fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "        # Encontrar anos e naturezas únicos para as orientações\n",
    "        anos = sorted(contagem_orientacoes['ano'].unique())\n",
    "\n",
    "        # Mapeamento de exemplo conforme as siglas fornecidas\n",
    "        mapeamento_siglas = {\n",
    "            \"OON\": 1,\n",
    "            \"IC\": 2,\n",
    "            \"MCCAE\": 3,\n",
    "            \"SPD\": 4,\n",
    "            \"TCCG\": 5,\n",
    "            \"DM\": 6,\n",
    "            \"TD\": 7,\n",
    "        }\n",
    "\n",
    "        # Ordenar as naturezas com base nos números associados em mapeamento_siglas\n",
    "        mapeamento_siglas_para_numeros = {sigla: int(sigla.split('-')[0]) for sigla in contagem_orientacoes['sigla_natureza'].unique()}\n",
    "        naturezas_ordenadas = sorted(contagem_orientacoes['sigla_natureza'].unique(), key=lambda x: mapeamento_siglas_para_numeros.get(x, 999))\n",
    "\n",
    "        # Criar uma barra para cada tipo de orientação em cada ano\n",
    "        for natureza in naturezas_ordenadas:\n",
    "            contagem_por_ano = []\n",
    "            for ano in anos:\n",
    "                # Somar as contagens para cada ano e natureza\n",
    "                contagem = contagem_orientacoes[(contagem_orientacoes['ano'] == ano) & (contagem_orientacoes['sigla_natureza'] == natureza)]['contagem'].sum()\n",
    "                contagem_por_ano.append(contagem)\n",
    "            \n",
    "            # Adicionar a barra ao gráfico\n",
    "            fig.add_trace(go.Bar(x=anos, y=contagem_por_ano, name=natureza))\n",
    "\n",
    "        # Atualizar o layout para permitir barras agrupadas\n",
    "        fig.update_layout(barmode='group', title_text='Contagem de Orientações no Programa por Tipo e Ano', xaxis_title=\"Ano\", yaxis_title=\"Quantidade de Orientações\")\n",
    "\n",
    "        # Mostrar o gráfico\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    def plotar_orientacoes_barras_empilhadas(self, filename):\n",
    "        # montar dataframe com agrupamento de orientações por tipo por ano no programa\n",
    "        contagem_orientacoes = self.agrupar_orientacoes(filename)\n",
    "        \n",
    "        # Criar uma figura com subplots\n",
    "        fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "        # Encontrar anos únicos para as orientações\n",
    "        anos = sorted(contagem_orientacoes['ano'].unique())\n",
    "\n",
    "        # Mapeamento para siglas de tipos de orientações\n",
    "        mapeamento_siglas = {\n",
    "            \"OON\": 1,\n",
    "            \"IC\": 2,\n",
    "            \"MCCAE\": 3,\n",
    "            \"SPD\": 4,\n",
    "            \"TCCG\": 5,\n",
    "            \"DM\": 6,\n",
    "            \"TD\": 7,\n",
    "        }\n",
    "\n",
    "        # Mapeamento para ordenar siglas por duração/complexidade\n",
    "        mapeamento_siglas_para_numeros = {\n",
    "            \"1-OON\": 1,\n",
    "            \"2-IC\": 2,\n",
    "            \"3-MCCAE\": 3,\n",
    "            \"4-SPD\": 4,\n",
    "            \"5-TCCG\": 5,\n",
    "            \"6-DM\": 6,\n",
    "            \"7-TD\": 7,\n",
    "        }\n",
    "\n",
    "        # Ordenar as naturezas baseando-se no número extraído da sigla\n",
    "        naturezas_ordenadas = sorted(contagem_orientacoes['sigla_natureza'].unique(), key=lambda x: mapeamento_siglas_para_numeros.get(x, 999))\n",
    "\n",
    "        # Criar uma barra para cada tipo de orientação em cada ano, seguindo a ordem definida\n",
    "        for natureza in naturezas_ordenadas:\n",
    "            contagem_por_ano = []\n",
    "            for ano in anos:\n",
    "                # Somar as contagens para cada ano e natureza\n",
    "                contagem = contagem_orientacoes[(contagem_orientacoes['ano'] == ano) & (contagem_orientacoes['sigla_natureza'] == natureza)]['contagem'].sum()\n",
    "                # Se a sigla estiver associada ao número 1 ou 2, tornar a contagem negativa\n",
    "                if mapeamento_siglas_para_numeros.get(natureza, 999) in [1, 2, 3, 4]:\n",
    "                    contagem = -contagem\n",
    "                contagem_por_ano.append(contagem)\n",
    "            \n",
    "            # Adicionar a barra ao gráfico\n",
    "            fig.add_trace(go.Bar(x=anos, y=contagem_por_ano, name=natureza))\n",
    "\n",
    "        # Atualizar o layout para permitir barras empilhadas e ajustar o eixo Y para mostrar valores negativos\n",
    "        fig.update_layout(\n",
    "            barmode='relative',  # Usar 'relative' para empilhar incluindo valores negativos\n",
    "            title_text='Contagem de Orientações por Tipo e Ano (Orientações de curta duração plotadas abaixo do eixo X)',\n",
    "            xaxis_title=\"Ano\",\n",
    "            yaxis_title=\"Quantidade de Orientações\",\n",
    "            yaxis=dict(zeroline=True, zerolinewidth=2, zerolinecolor='black'),  # Destacar a linha zero\n",
    "        )\n",
    "\n",
    "        # Mostrar o gráfico\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8007a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "print(f\"     Pasta corrente: {path}\")\n",
    "pathjson = os.path.join(path,'_data','in_json')\n",
    "print(f\"Pasta arquivos JSON: {pathjson}\")\n",
    "try:\n",
    "    pathdata = os.path.join(path,'_data','powerbi')\n",
    "    print(f\" Pasta de dados CSV: {pathdata}\")\n",
    "except:\n",
    "    print('Pasta de dados ainda não existe.')\n",
    "\n",
    "print(\"\\nConteúdo da pasta JSON:\")\n",
    "list(os.listdir(pathjson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Descompactar arquivo zipado na pasta JSON\n",
    "prep_pub = PreparadorDePublicacoes()\n",
    "pathfilezip = '_data/in_zip/890.files.zip'\n",
    "destination = prep_pub.extract_zips(pathfilezip)\n",
    "\n",
    "## Unir todos aquivos de publicações (caso haja mais de um com final publication.json na pasta serão mesclados):\n",
    "prep_pub.find_and_merge_publication_json_files(pathjson)\n",
    "\n",
    "## Mapear arquivo de dados para PowerBI a partir do JSON unificado\n",
    "linhas = prep_pub.processar_publicacoes()\n",
    "prep_pub.exportar_para_csv()\n",
    "\n",
    "## Ler e montar análise exploratória\n",
    "filename='publicacoes.csv'\n",
    "df_pub = prep_pub.ler_csv_dados(pathdata,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61df53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='890.advise.json'\n",
    "pathjson = os.path.join('_data','in_json')\n",
    "pathfilename = os.path.join(pathjson,filename)\n",
    "print(pathfilename)\n",
    "dict_orientacoes = pd.read_json(pathfilename)\n",
    "print(f'{len(dict_orientacoes):02} dicionários com dados de orientações')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_orientacoes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c4685",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_ort = PreparadorDeOrientacoes()\n",
    "prep_ort.plotar_orientacoes_barras_agrupadas(filename)\n",
    "prep_ort.plotar_orientacoes_barras_empilhadas(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathcsv = os.path.join('_data','powerbi')\n",
    "os.listdir(pathcsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be258ab8",
   "metadata": {},
   "source": [
    "# Demais passos\n",
    "\n",
    "Calcular índice de publicação em conjunto com alunos do programa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1574aac1",
   "metadata": {},
   "source": [
    "- Levantar nome dos discentes do programa\n",
    "- Levantar os nomes de autores nas publicações de docentes\n",
    "- Identificar por similadidade as publicações onde constam nome de alunos do programa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb00d652",
   "metadata": {},
   "source": [
    "    Padronizar nomes de autores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94552058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_nomes(linha_texto):\n",
    "    '''\n",
    "    Retira erros e sujeira da string formada pela lista de nomes de autor dos artigos retirada do Lattes\n",
    "     Recebe: Uma string com os nomes de autor\n",
    "    Retorna: Uma string com os nomes de autor, removidas preposições, acentos, e demais erros pontuais\n",
    "    '''\n",
    "    import unicodedata\n",
    "    import re\n",
    "    # print('               Analisando:',linha_texto)\n",
    "    string = linha_texto.replace('Network for Genomic Surveillance in South Africa;Network for Genomic Surveillance in South Africa (NGS-SA);10.1002/jmv.27190;','')\n",
    "    string = string.replace('Autores: ','').replace('(Org)','').replace('(Org.)','').replace('et. al.','').replace('et al','').replace('(Org).','').replace('.','').replace('\\'','')\n",
    "    string = string.replace(',,,',',').replace(',,',',').replace(';',', ').replace('-',' ').replace('S?', 'SA').replace('S?', 'SA').replace('ARA?JO', 'ARAUJO').replace('FL?VIO','FLAVIO').replace('F?BIO','FABIO').replace('VIT?RIO','VITORIO')\n",
    "    string = re.sub(r'[0-9]+', '', string)\n",
    "    partes_string = string.split(' ')\n",
    "\n",
    "    ## Retirar partes de nomes caso sejam preposições\n",
    "    preposicoes = ['da', 'de', 'do', 'das', 'dos', ' e ']\n",
    "    string = ' '.join(x for x in partes_string if x.lower() not in preposicoes)\n",
    "\n",
    "    ## Retirar símbolos não unicode, como acentuação gráfica e cedilha\n",
    "    string = ''.join(ch for ch in unicodedata.normalize('NFKD', string) if not unicodedata.combining(ch))\n",
    "    \n",
    "    ## Retirar iniciais juntas, separando-as com espaço em branco\n",
    "    letras_duasconsnts = re.compile(r'^[B-DF-HJ-NP-TV-XZ-r]{2}$')            # Duas Letras consoantes maiúsculas juntas do início ao final da string\n",
    "    letras_tresconsnts = re.compile(r'^[B-DF-HJ-NP-TV-XZ-r]{3}$')            # Três Letras consoantes maiúsculas juntas do início ao final da string\n",
    "    letras_duasconsntsvirg = re.compile(r'^[B-DF-HJ-NP-TV-XZ-r]{2},$')       # Duas Letras consoantes maiúsculas juntas do início ao final da string, seguidas de vírgula\n",
    "    letras_tresconsntsvirg = re.compile(r'^[B-DF-HJ-NP-TV-XZ-r]{3},$')       # Três Letras consoantes maiúsculas juntas do início ao final da string, seguidas de vírgula\n",
    "    \n",
    "    partes_nome=[]\n",
    "    for j in string.split(' '):\n",
    "        div_ltrduasconsnts = letras_duasconsnts.findall(j)\n",
    "        div_ltrtriplicadas = letras_tresconsnts.findall(j)\n",
    "        div_ltrduasconsntsvirg = letras_duasconsntsvirg.findall(j)\n",
    "        div_ltrtresconsntsvirg = letras_tresconsntsvirg.findall(j)\n",
    "        if div_ltrduasconsnts or div_ltrtriplicadas:\n",
    "            iniciais_separadas = ' '.join(x for x in j)\n",
    "            partes_nome.append(iniciais_separadas)\n",
    "        elif div_ltrduasconsntsvirg or div_ltrtresconsntsvirg:\n",
    "            iniciais_separadas = ' '.join(x for x in j[:-1])\n",
    "            partes_nome.append(iniciais_separadas+',')\n",
    "        else:\n",
    "            partes_nome.append(j)\n",
    "    string = ' '.join(x for x in partes_nome).strip()\n",
    "    return string\n",
    "\n",
    "\n",
    "\n",
    "def padronizar_nome(linha_texto):\n",
    "    '''\n",
    "    Procura sobrenomes e abreviaturas e monta nome completo\n",
    "     Recebe: String com todos os sobrenomes e nomes, abreviados ou não\n",
    "    Retorna: Nome completo no formato padronizado em SOBRENOME AGNOME, Partes de nomes\n",
    "      Autor: Marcos Aires (Mar.2022)\n",
    "    '''\n",
    "    import unicodedata\n",
    "    import re\n",
    "    # print('               Analisando:',linha_texto)\n",
    "    partes_string = linha_texto.split(' ')\n",
    "\n",
    "    ## Retirar partes de nomes caso sejam preposições\n",
    "    preposicoes = ['da', 'de', 'do', 'das', 'dos', ' e ']\n",
    "    string = ' '.join(x for x in partes_string if x.lower() not in preposicoes)\n",
    "\n",
    "    ## Retirar símbolos não unicode, como acentuação gráfica e cedilha\n",
    "    string = ''.join(ch for ch in unicodedata.normalize('NFKD', string) if not unicodedata.combining(ch))\n",
    "    \n",
    "    ## Expressões regulares para encontrar padrões de divisão de nomes de autores\n",
    "    sobrenome_inicio   = re.compile(r'^[A-ZÀ-ú-a-z]+,')                  # Sequência de letras maiúsculas no início da string\n",
    "    sobrenome_composto = re.compile(r'^[A-ZÀ-ú-a-z]+[ ][A-ZÀ-ú-a-z]+,')  # Duas sequências de letras no início da string, separadas por espaço, seguidas por vírgula\n",
    "    letra_abrevponto   = re.compile(r'^[A-Z][.]')                        # Uma letra maiúscula no início da string, seguida por ponto\n",
    "    letra_abrevespaco  = re.compile(r'^[A-Z][ ]')                        # Uma letra maiúscula no início da string, seguida por espaço\n",
    "    letras_dobradas    = re.compile(r'[A-Z]{2}')                         # Duas letras maiúsculas juntas\n",
    "    letras_dobradasini = re.compile(r'[A-Z]{2}[ ]')                      # Duas letras maiúsculas juntas, seguidas por espaço\n",
    "    letras_dobradasfim = re.compile(r'[ ][A-Z]{2}')                      # Duas letras maiúsculas juntas, precedidas por espaço\n",
    "\n",
    "    ## Expressões regulares para encontrar iniciais juntas, separando-as com espaço em branco\n",
    "    letras_duasconsnts = re.compile(r'^[B-DF-HJ-NP-TV-XZ-r]{2}$')            # Duas Letras consoantes maiúsculas juntas do início ao final da string\n",
    "    letras_tresconsnts = re.compile(r'^[B-DF-HJ-NP-TV-XZ-r]{3}$')            # Três Letras consoantes maiúsculas juntas do início ao final da string\n",
    "    letras_duasconsntsvirg = re.compile(r'^[B-DF-HJ-NP-TV-XZ-r]{2},$')       # Duas Letras consoantes maiúsculas juntas do início ao final da string, seguidas de vírgula\n",
    "    letras_tresconsntsvirg = re.compile(r'^[B-DF-HJ-NP-TV-XZ-r]{3},$')       # Três Letras consoantes maiúsculas juntas do início ao final da string, seguidas de vírgula\n",
    "\n",
    "    ## Agnomes e preprosições a tratar, agnomes vão em maiúsculas para sobrenome e preposições vão em minúsculas para restante das partes de nomes\n",
    "    nomes=[]\n",
    "    agnomes       = ['NETO','JUNIOR','FILHO','SEGUNDO','TERCEIRO','SOBRINHO']\n",
    "    preposicoes   = ['de','da','do','das','dos', ' e ']\n",
    "    nome_completo = ''\n",
    "    \n",
    "    ## Ajustar lista de termos, identificar sobrenomes compostos e ajustar sobrenome com ou sem presença de vírgula\n",
    "    div_sobrenome   = sobrenome_inicio.findall(string)\n",
    "    div_sbrcomposto = sobrenome_composto.findall(string)\n",
    "    \n",
    "    # print('-'*100)\n",
    "    # print('                 Recebido:',string)\n",
    "    \n",
    "    ## Caso haja vírgulas na string, tratar sobrenomes e sobrenomes compostos\n",
    "    if div_sobrenome != [] or div_sbrcomposto != []:\n",
    "        # print('CASO_01: Há víruglas na string')\n",
    "        div = string.split(', ')\n",
    "        sobrenome     = div[0].strip().upper()\n",
    "        try:\n",
    "            div_espaco    = div[1].split(' ')\n",
    "        except:\n",
    "            div_espaco    = ['']\n",
    "        primeiro      = div_espaco[0].strip('.').strip()\n",
    "        \n",
    "        # print('     Dividir por vírgulas:',div)\n",
    "        # print('      Primeira DivVirgula:',sobrenome)\n",
    "        # print('Segunda DivVrg/DivEspaços:',div_espaco)\n",
    "        # print('      Primeira DivEspaços:',primeiro)\n",
    "               \n",
    "        # Caso primeiro nome seja somente duas letras maiúsculas juntas, trata-se de duas iniciais\n",
    "        if len(primeiro)==2 or letras_tresconsnts.findall(primeiro) or letras_duasconsnts.findall(primeiro):\n",
    "            # print('CASO_01.a: Há duas letras ou três letras consoantes juntas, são iniciais')\n",
    "            primeiro_nome=primeiro[0].strip()\n",
    "            # print('          C01.a1_PrimNome:',primeiro_nome)\n",
    "            nomes.append(primeiro[1].strip().upper())\n",
    "            try:\n",
    "                nomes.append(primeiro[2].strip().upper())\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                nomes.append(primeiro[3].strip().upper())\n",
    "            except:\n",
    "                pass            \n",
    "        else:\n",
    "            # print('CASO_01.b: Primeiro nome maior que 2 caracteres')\n",
    "            primeiro_nome = div_espaco[0].strip().title()\n",
    "            # print('          C01.a2_PrimNome:',primeiro_nome)\n",
    "        \n",
    "        ## Montagem da lista de partes de nomes do meio\n",
    "        for nome in div_espaco:\n",
    "            # print('CASO_01.c: Para cada parte de nome da divisão por espaços após divisão por vírgula')\n",
    "            if nome not in nomes and nome.lower()!=primeiro_nome.lower() and nome.lower() not in primeiro_nome.lower() and nome!=sobrenome:   \n",
    "                # print('CASO_01.c1: Se o nome não está nem como primeiro nome, nem sobrenomes')\n",
    "                # print(nome, len(nome))\n",
    "                \n",
    "                ## Avaliar se é abreviatura seguida de ponto e remover o ponto\n",
    "                if len(nome)<=2 and nome.lower() not in preposicoes:\n",
    "                    # print('    C01.c1.1_Nome<=02:',nome)\n",
    "                    for inicial in nome:\n",
    "                        # print(inicial)\n",
    "                        if inicial not in nomes and inicial not in primeiro_nome:\n",
    "                            nomes.append(inicial.replace('.','').strip().title())\n",
    "                elif len(nome)==3 and nome.lower() not in preposicoes:\n",
    "                        # print('    C01.c1.2_Nome==03:',nome)\n",
    "                        for inicial in nome:\n",
    "                            if inicial not in nomes and inicial not in primeiro_nome:\n",
    "                                nomes.append(inicial.replace('.','').strip().title())\n",
    "                else:\n",
    "                    if nome not in nomes and nome!=primeiro_nome and nome!=sobrenome and nome!='':\n",
    "                        if nome.lower() in preposicoes:\n",
    "                            nomes.append(nome.replace('.','').strip().lower())\n",
    "                        else:\n",
    "                            nomes.append(nome.replace('.','').strip().title())\n",
    "                        # print(nome,'|',primeiro_nome)\n",
    "                        \n",
    "        ## Caso haja sobrenome composto que não esteja nos agnomes considerar somente primeira parte como sobrenome\n",
    "        if div_sbrcomposto !=[] and sobrenome.split(' ')[1] not in agnomes and sobrenome.split(' ')[0].lower() not in preposicoes:\n",
    "            # print('CASO_01.d: Sobrenome composto sem agnomes')\n",
    "            # print(div_sbrcomposto)\n",
    "            # print('Sobrenome composto:',sobrenome)\n",
    "            \n",
    "            nomes.append(sobrenome.split(' ')[1].title())\n",
    "            sobrenome = sobrenome.split(' ')[0].upper().strip()\n",
    "            # print('Sobrenome:',sobrenome)\n",
    "            \n",
    "            for i in nomes:\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('    Nomes:',nomes)\n",
    "        \n",
    "        ## Caso haja preposição como agnome desconsiderar e passar para final dos nomes\n",
    "        if div_sbrcomposto !=[] and sobrenome.split(' ')[0].lower() in preposicoes:\n",
    "            # print('CASO_01.e: Preposição no Sobrenome passar para o final dos nomes')\n",
    "            # print('   div_sbrcomposto:', div_sbrcomposto)\n",
    "            # print('Sobrenome composto:',div_sbrcomposto)\n",
    "            \n",
    "            nomes.append(div_sbrcomposto[0].split(' ')[0].lower())\n",
    "            # print('    Nomes:',nomes)\n",
    "            sobrenome = div_sbrcomposto[0].split(' ')[1].upper().strip(',').strip()\n",
    "            # print('Sobrenome:',sobrenome)\n",
    "            \n",
    "            for i in nomes:\n",
    "                # print('CASO_01.e1: Para cada nome avaliar se o sobrenome está na lista')\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('  Nomes:',nomes)\n",
    "        \n",
    "        # print('Ao final do Caso 01')\n",
    "        # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('           Lista de nomes:',nomes, len(nomes),'nomes')\n",
    "        \n",
    "    ## Caso não haja vírgulas na string considera sobrenome o último nome da string dividida com espaço vazio\n",
    "    else:\n",
    "        # print('CASO_02: Não há víruglas na string')\n",
    "        try:\n",
    "            div = string.split(' ')\n",
    "            # print('      Divisões por espaço:',div)\n",
    "            \n",
    "            if div[-1] in agnomes: # nome final é um agnome\n",
    "                sobrenome     = div[-2].upper().strip()+' '+div[-1].upper().strip()\n",
    "                for i in div[1:-2]:\n",
    "                    if i not in sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.title().strip())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.lower().strip())\n",
    "            else:\n",
    "                if len(div[-1]) > 2:\n",
    "                    sobrenome     = div[-1].upper().strip()\n",
    "                    primeiro_nome = div[1].title().strip()\n",
    "                    for i in div[1:-1]:\n",
    "                        if i != sobrenome and i not in preposicoes:\n",
    "                            nomes.append(i.title().strip())\n",
    "                        if i in preposicoes:\n",
    "                            nomes.append(i.lower().strip())\n",
    "                else:\n",
    "                    sobrenome     = div[-2].upper().strip()\n",
    "                    for i in div[-1]:\n",
    "                        nomes.append(i.title())\n",
    "                    primeiro_nome = nomes[0].title().strip()\n",
    "                    for i in div[1:-1]:\n",
    "                        if i != sobrenome and i not in preposicoes:\n",
    "                            nomes.append(i.title().strip())\n",
    "                        if i in preposicoes:\n",
    "                            nomes.append(i.lower().strip())\n",
    "        except:\n",
    "            sobrenome = div[-1].upper().strip()\n",
    "            for i in div[1:-1]:\n",
    "                    if i != sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.title().strip())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.lower().strip())\n",
    "            \n",
    "        if sobrenome.lower() != div[0].lower().strip():\n",
    "            primeiro_nome=div[0].title().strip()\n",
    "        else:\n",
    "            primeiro_nome=''\n",
    "        \n",
    "        # print('Ao final do Caso 02')\n",
    "        # print('    Sobrenome sem vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome sem vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('Nomes do meio sem vírgula:',nomes, len(nomes),'nomes')\n",
    "    \n",
    "    ## Encontrar e tratar como abreviaturas termos com apenas uma ou duas letras iniciais juntas, com ou sem ponto\n",
    "    for j in nomes:\n",
    "        # print('CASO_03: Avaliar cada nome armazenado na variável nomes')\n",
    "        # Procura padrões com expressões regulares na string\n",
    "        div_sobrenome      = sobrenome_inicio.findall(j)\n",
    "        div_sbrcomposto    = sobrenome_composto.findall(j)\n",
    "        div_abrevponto     = letra_abrevponto.findall(j)\n",
    "        div_abrevespaco    = letra_abrevespaco.findall(j)\n",
    "        div_ltrdobradasini = letras_dobradasini.findall(j)\n",
    "        div_ltrdobradasfim = letras_dobradasfim.findall(j)\n",
    "        div_ltrdobradas    = letras_dobradas.findall(j)\n",
    "        div_ltrduasconsnts = letras_duasconsnts.findall(j)\n",
    "        div_ltrtriplicadas = letras_tresconsnts.findall(j)\n",
    "        tamanho=len(j)\n",
    "        # print('\\n', div_ltrdobradasini, div_ltrdobradasfim, tamanho, 'em:',j,len(j))\n",
    "        \n",
    "        ## Caso houver abreviatura com uma letra em maiúscula nos nomes\n",
    "        if div_abrevponto !=[] or tamanho==1:\n",
    "            # print('CASO_03.1: Há abreviaturas uma letra maiúscula nos nomes')\n",
    "            nome = j.replace('.','').strip()\n",
    "            if nome not in nomes and nome != sobrenome and nome != primeiro_nome:\n",
    "                # print('CASO_03.1a: Há abreviaturas uma letra maiúscula nos nomes')\n",
    "                nomes.append(nome.upper())\n",
    "        \n",
    "        ## Caso houver duas inicias juntas em maiúsculas\n",
    "        elif div_ltrdobradasini !=[] or div_ltrdobradasfim !=[] or div_ltrdobradas !=[] :\n",
    "            # print('CASO_03.2: Há abreviaturas uma letra maiúscula nos nomes')\n",
    "            for letra in j:\n",
    "                # print('CASO_03.2a: Avaliar cada inicial do nome')\n",
    "                if letra not in nomes and letra != sobrenome and letra != primeiro_nome:\n",
    "                    # print('CASO_03.2a.1: Se não estiver adicionar inicial aos nomes')\n",
    "                    nomes.append(letra.upper())\n",
    "        \n",
    "        # Caso haja agnomes ao sobrenome\n",
    "        elif sobrenome in agnomes:\n",
    "            # print('CASO_03.3: Há agnomes nos sobrenomes')\n",
    "            sobrenome = nomes[-1].upper()+' '+sobrenome\n",
    "            # print(sobrenome.split(' '))\n",
    "            # print('Sobrenome composto:',sobrenome)\n",
    "            for i in nomes:\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('Nomes do meio:',nomes)\n",
    "            \n",
    "        else:\n",
    "            # print('CASO_03.4: Não há agnomes nos sobrenomes')\n",
    "            if j not in nomes and j not in sobrenome and j != primeiro_nome:\n",
    "                if len(nomes) == 1:\n",
    "                    nomes.append(j.upper())\n",
    "                elif 1 < len(nomes) <= 3:\n",
    "                    nomes.append(j.lower())\n",
    "                else:\n",
    "                    nomes.append(j.title())\n",
    "         \n",
    "        # print('Ao final do Caso 03')\n",
    "        # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('Nomes do meio com vírgula:',nomes, len(nomes),'nomes')\n",
    "        \n",
    "    nomes_meio=' '.join([str for str in nomes]).strip()\n",
    "    # print('        Qte nomes do meio:',nomes,len(nomes))\n",
    "    \n",
    "    if primeiro_nome.lower() == sobrenome.lower():\n",
    "        # print('CASO_04: Primeiro nome é igual ao sobrenome')\n",
    "        try:\n",
    "            primeiro_nome=nomes_meio.split(' ')[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            nomes_meio.remove(sobrenome)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        # print('Ao final do caso 04')\n",
    "        # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('Nomes do meio com vírgula:',nomes, len(nomes),'nomes')\n",
    "    \n",
    "    ## Caso sobrenome seja só de 1 letra passá-lo para nomes e considerar o próximo nome como sobrenome\n",
    "    for i in range(len(div)):\n",
    "        if len(sobrenome)==1 or sobrenome.lower() in preposicoes:\n",
    "            # print('CASO_05: Mudar sobrenomes até o adequado')\n",
    "            div    = string.split(', ')\n",
    "            # print('Divisão por vírgulas:',div)\n",
    "            avaliar0       = div[0].split(' ')[0].strip()\n",
    "            if 1< len(avaliar0) < 3:\n",
    "                # print('CASO_05.1: 1 < Sobrenome < 3 fica em minúsculas')\n",
    "                sbrn0          = avaliar0.lower()\n",
    "            else:\n",
    "                # print('CASO_05.2: Sobrenome de tamanho 1 ou maior que 3 fica em maiúsculas')\n",
    "                sbrn0          = avaliar0.title()\n",
    "            # print('sbrn0:',sbrn0, len(sbrn0))\n",
    "            \n",
    "            try:\n",
    "                avaliar1=div[0].split(' ')[1].strip()\n",
    "                # print('avaliar0',avaliar0)\n",
    "                # print('avaliar1',avaliar1)\n",
    "                if 1 < len(avaliar1) <=3:\n",
    "                    sbrn1     = avaliar1.lower()\n",
    "                else:\n",
    "                    sbrn1     = avaliar1.title()\n",
    "                # print('sbrn1:',sbrn1, len(sbrn1))\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if div != []:\n",
    "                # print('CASO_05.3: Caso haja divisão por vírgulas na string')\n",
    "                try:\n",
    "                    div_espaco     = div[1].split(' ')\n",
    "                except:\n",
    "                    div_espaco     = div[0].split(' ')\n",
    "                sobrenome      = div_espaco[0].strip().upper()\n",
    "                try:\n",
    "                    primeiro_nome  = div_espaco[1].title().strip()\n",
    "                except:\n",
    "                    primeiro_nome  = div_espaco[0].title().strip()\n",
    "                if len(sbrn0) == 1:\n",
    "                    # print('CASO_05.3a: Avalia primeiro sobrenome de tamanho 1')\n",
    "                    # print('Vai pros nomes:',str(sbrn0).title())\n",
    "                    nomes_meio = nomes_meio+str(' '+sbrn0.title())\n",
    "                    # print('   NomesMeio:',nomes_meio)\n",
    "\n",
    "                elif 1 < len(sbrn0) <= 3:\n",
    "                    # print('CASO_05.3b: Avalia primeiro sobrenome 1< tamanho <=3')\n",
    "                    # print('Vão pros nomes sbrn0:',sbrn0, 'e sbrn1:',sbrn1)\n",
    "\n",
    "                    div_tresconsoantes = letras_tresconsnts.findall(sobrenome)\n",
    "                    if div_tresconsoantes != []:\n",
    "                        # print('CASO_05.4: Três consoantes como sobrenome')\n",
    "                        for letra in sobrenome:\n",
    "                            nomes.append(letra)\n",
    "\n",
    "                        if len(sobrenome) >2:\n",
    "                            sobrenome=nomes[0]\n",
    "                        else:\n",
    "                            sobrenome=nomes[1]\n",
    "                        nomes.remove(sobrenome)\n",
    "                        primeiro_nome=nomes[0]\n",
    "                        nomes_meio=' '.join([str for str in nomes[1:]]).strip()\n",
    "                        nome_completo=sobrenome.upper()+', '+nomes_meio                \n",
    "                    \n",
    "                    try:                       \n",
    "                        # print(' 05.3b    Lista de Nomes:',nomes_meio)\n",
    "                        nomes_meio=nomes_meio.replace(sbrn0,'')\n",
    "                        # print(' 05.3b ReplaceSobrenome0:',nomes_meio)\n",
    "                        nomes_meio=nomes_meio.replace(sbrn1,'')\n",
    "                        # print(' 05.3b ReplaceSobrenome1:',nomes_meio)\n",
    "                    except Exception as e:\n",
    "                        # print('   Erro ReplaceSobrenome:',e)\n",
    "                        pass\n",
    "                    try:\n",
    "                        nomes_meio.replace(primeiro_nome.title(),'')\n",
    "                        nomes_meio.replace(primeiro_nome.lower(),'')\n",
    "                        nomes_meio.replace(primeiro_nome,'')\n",
    "                        # print(' 05.3b Replace PrimNome:',nomes_meio)\n",
    "                    except Exception as e:\n",
    "                        print('Erro no try PrimeiroNome:',e)\n",
    "                        pass\n",
    "                    nomes_meio = nomes_meio.replace(sobrenome,'')\n",
    "                    try:\n",
    "                        for n,i in enumerate(avaliar1):\n",
    "                            nomes.append(i.upper())\n",
    "                            sbrn1     = avaliar1[0]\n",
    "                        else:\n",
    "                            sbrn1     = avaliar1.title()\n",
    "                        # print('sbrn1:',sbrn1, len(sbrn1))\n",
    "                        nomes_meio = nomes_meio+str(' '+sbrn0)+str(' '+sbrn1)\n",
    "                    except:\n",
    "                        nomes_meio = nomes_meio+str(' '+sbrn0)\n",
    "                    nomes      = nomes_meio.strip().strip(',').split(' ')\n",
    "                    # print(' 05.3b NomesMeio:',nomes_meio)\n",
    "                    # print(' 05.3b     Nomes:',nome)\n",
    "\n",
    "                else:\n",
    "                    # print('CASO_05.3c: Avalia primeiro sobrenome >3')\n",
    "                    nomes_meio = nomes_meio+str(' '+div[0].strip().title())\n",
    "                    nomes      = nomes_meio.strip().split(' ')\n",
    "                    # print(' 05.3c NomesMeio:',nomes_meio)\n",
    "                    # print(' 05.3c     Nomes:',nomes)\n",
    "\n",
    "                nomes_meio=nomes_meio.replace(sobrenome,'').replace(',','').strip()\n",
    "                nomes_meio=nomes_meio.replace(primeiro_nome,'').strip()\n",
    "\n",
    "            # print('Ao final do caso 05')\n",
    "            # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "            # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "            # print('Nomes do meio com vírgula:',nomes, len(nomes),'nomes')\n",
    "    \n",
    "    if sobrenome != '' and primeiro_nome !='':\n",
    "        nome_completo=sobrenome.upper().replace(',','')+', '+primeiro_nome.replace(',','')+' '+nomes_meio.replace(sobrenome,'').replace(',','')\n",
    "    elif sobrenome != '':\n",
    "        nome_completo=sobrenome.upper().replace(',','')+', '+nomes_meio.replace(sobrenome,'').replace(',','')\n",
    "    else:\n",
    "        nome_completo=sobrenome.upper()\n",
    "    \n",
    "#     print('Após ajustes finais')\n",
    "#     print('     Sobrenome:',sobrenome)\n",
    "#     print(' Primeiro Nome:',primeiro_nome)\n",
    "#     print('         Nomes:',nomes)\n",
    "#     print('     NomesMeio:',nomes_meio)        \n",
    "        \n",
    "#     print('                Resultado:',nome_completo)\n",
    "    \n",
    "    return nome_completo.strip()\n",
    "\n",
    "\n",
    "\n",
    "def padronizar_titulo(titulo_bruto):\n",
    "    '''\n",
    "    Retira acentos, expressão (Org.) e espaços vazios do título da publicação\n",
    "    Autor: Marcos Aires (Fev.2022)\n",
    "    '''\n",
    "    import re\n",
    "    import unicodedata\n",
    "    \n",
    "    ## Retirar caracteres não unicode\n",
    "    string = ''.join(ch for ch in unicodedata.normalize('NFKD', titulo_bruto) if not unicodedata.combining(ch))\n",
    "    string = string.replace('(Org)','').replace('(Org.)','').replace('(Org).','').replace('.',' ').replace('-',' ').replace('\\'',' ').lower()\n",
    "    \n",
    "    substitui_iniciais=[('rl,', 'r l,'), ('hs,', 'h s,'), ('gc,', 'g c,'), ('sf,', 's f,'), ('fo,', 'f o,'), (' oa,', ' o a,'), ('mss,', 'm s s,'), \n",
    "                        ('lagares, ma', 'lagares, m a'), ('cota, gf', 'cota, g f'), ('cota gf,', 'cota, g f,'), ('diotaiut,', 'diotaiuti,'), ('grenfell, r f q','queiroz, rafaella fortini grenfell')]\n",
    "    for i in substitui_iniciais:\n",
    "        string = string.replace(i[0], i[1])\n",
    "    \n",
    "    ## Retirar preposições\n",
    "    preposicoes = ['da', 'de', 'do', 'das', 'dos']\n",
    "    partes_string = string.split(' ')\n",
    "    string = ' '.join(x for x in partes_string if x.lower() not in preposicoes)\n",
    "    \n",
    "    titulo_padronizado = string.strip().strip('\"')\n",
    "    \n",
    "    return titulo_padronizado\n",
    "\n",
    "\n",
    "\n",
    "def iniciais_nome(linha_texto):\n",
    "    '''\n",
    "    Função para retornar sobrenome+iniciais das partes de nome, na forma: SOBRENOME, X Y Z\n",
    "     Recebe: String com nome\n",
    "    Retorna: Tupla com nome e sua versão padronizada em sobrenome+agnome em maiúsculas, seguida de vírgula e das iniciais das demais partes de nome\n",
    "      Autor: Marcos Aires (Mar.2022)\n",
    "    '''\n",
    "    import unicodedata\n",
    "    import re\n",
    "    # print('               Analisando:',linha_texto)\n",
    "    \n",
    "    ## Retirar caracteres não unicode\n",
    "    string = ''.join(ch for ch in unicodedata.normalize('NFKD', linha_texto) if not unicodedata.combining(ch))\n",
    "    string = string.replace('(Org)','').replace('(Org.)','').replace('(Org).','').replace('.','')\n",
    "    \n",
    "    ## Retirar preposições\n",
    "    preposicoes   = ['da','de','do','das','dos']\n",
    "    partes_string = string.split(' ')\n",
    "    string = ' '.join(x for x in partes_string if x.lower() not in preposicoes)\n",
    "        \n",
    "    ## Expressões regulares para encontrar padrões de divisão de nomes de autores\n",
    "    sobrenome_inicio   = re.compile(r'^[A-ZÀ-ú-a-z]+,')                 # Sequência de letras maiúsculas no início da string\n",
    "    sobrenome_composto = re.compile(r'^[A-ZÀ-ú-a-z]+[ ][A-ZÀ-ú-a-z]+,') # Duas sequências de letras no início da string, separadas por espaço, seguidas por vírgula\n",
    "    letra_abrevponto   = re.compile(r'^[A-Z][.]')                       # Uma letra maiúscula no início da string, seguida por ponto\n",
    "    letra_abrevespaco  = re.compile(r'^[A-Z][ ]')                       # Uma letra maiúscula no início da string, seguida por espaço\n",
    "    letras_dobradas    = re.compile(r'[A-Z]{2}')                        # Duas letras maiúsculas juntas no início da string, seguida por espaço\n",
    "    letras_dobradasini = re.compile(r'[A-Z]{2}[ ]')                     # Duas letras maiúsculas juntas no início da string, seguida por espaço\n",
    "    letras_dobradasfim = re.compile(r'[ ][A-Z]{2}')                     # Duas letras maiúsculas juntas no final da string, precedida por espaço\n",
    "        \n",
    "    nomes=[]\n",
    "    agnomes       = ['NETO','JUNIOR','FILHO','SEGUNDO','TERCEIRO', 'SOBRINHO']\n",
    "    nome_completo = ''\n",
    "    \n",
    "    ## Ajustar lista de termos, identificar sobrenomes compostos e ajustar sobrenome com ou sem presença de vírgula\n",
    "    div_sobrenome      = sobrenome_inicio.findall(string)\n",
    "    div_sbrcomposto    = sobrenome_composto.findall(string)\n",
    "    \n",
    "    ## Caso haja vírgulas na string, tratar sobrenomes e sobrenomes compostos\n",
    "    if div_sobrenome != [] or div_sbrcomposto != []:\n",
    "        div   = string.split(', ')\n",
    "        sobrenome     = div[0].strip().upper()\n",
    "        try:\n",
    "            div_espaco    = div[1].split(' ')\n",
    "        except:\n",
    "            div_espaco  = ['']\n",
    "        primeiro      = div_espaco[0].strip('.')\n",
    "        \n",
    "        ## Caso primeiro nome sejam somente duas letras maiúsculas juntas, trata-se de duas iniciais\n",
    "        if len(primeiro)==2:\n",
    "            primeiro_nome=primeiro[0].strip()\n",
    "            nomes.append(primeiro[1].strip())\n",
    "        else:\n",
    "            primeiro_nome = div_espaco[0].strip().title()\n",
    "        \n",
    "        ## Montagem da lista de nomes do meio\n",
    "        for nome in div_espaco:\n",
    "            if nome not in nomes and nome.lower()!=primeiro_nome.lower() and nome.lower() not in primeiro_nome.lower() and nome!=sobrenome:   \n",
    "                # print(nome, len(nome))\n",
    "                \n",
    "                ## Avaliar se é abreviatura seguida de ponto e remover o ponto\n",
    "                if len(nome)<=2 and nome.lower() not in preposicoes:\n",
    "                    for inicial in nome:\n",
    "                        # print(inicial)\n",
    "                        if inicial not in nomes and inicial not in primeiro_nome:\n",
    "                            nomes.append(inicial.replace('.','').strip().title())\n",
    "                else:\n",
    "                    if nome not in nomes and nome!=primeiro_nome and nome!=sobrenome and nome!='':\n",
    "                        if nome.lower() in preposicoes:\n",
    "                            nomes.append(nome.replace('.','').strip().lower())\n",
    "                        else:\n",
    "                            nomes.append(nome.replace('.','').strip().title())\n",
    "                        # print(nome,'|',primeiro_nome)\n",
    "                        \n",
    "        ## Caso haja sobrenome composto que não esteja nos agnomes considerar somente primeira parte como sobrenome\n",
    "        if div_sbrcomposto !=[] and sobrenome.split(' ')[1] not in agnomes:\n",
    "            # print(div_sbrcomposto)\n",
    "            # print('Sobrenome composto:',sobrenome)\n",
    "            nomes.append(sobrenome.split(' ')[1].title())\n",
    "            sobrenome = sobrenome.split(' ')[0].upper()\n",
    "            # print('Sobrenome:',sobrenome.split(' '))\n",
    "            for i in nomes:\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('Nomes do meio:',nomes)\n",
    "        \n",
    "        # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('Nomes do meio com vírgula:',nomes, len(nomes),'nomes')\n",
    "        \n",
    "    ## Caso não haja vírgulas na string considera sobrenome o último nome da string dividida com espaço vazio\n",
    "    else:\n",
    "        try:\n",
    "            div       = string.split(' ')\n",
    "            if div[-2] in agnomes:\n",
    "                sobrenome = div[-2].upper()+' '+div[-1].strip().upper()\n",
    "                for i in nomes[1:-2]:\n",
    "                    if i not in sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.strip().title())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.strip().lower())\n",
    "            else:\n",
    "                sobrenome = div[-1].strip().upper()\n",
    "                for i in div[1:-1]:\n",
    "                    if i not in sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.strip().title())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.strip().lower())\n",
    "        except:\n",
    "            sobrenome = div[-1].strip().upper()\n",
    "            for i in div[1:-1]:\n",
    "                    if i not in sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.strip().title())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.strip().lower())\n",
    "            \n",
    "        if sobrenome.lower() != div[0].strip().lower():\n",
    "            primeiro_nome=div[0].strip().title()\n",
    "        else:\n",
    "            primeiro_nome=''\n",
    "        \n",
    "        # print('    Sobrenome sem vírgula:',sobrenome)\n",
    "        # print('Primeiro nome sem vírgula:',primeiro_nome)\n",
    "        # print('Nomes do meio sem vírgula:',nomes)\n",
    "    \n",
    "    # Encontrar e tratar como abreviaturas termos com apenas uma ou duas letras iniciais juntas, com ou sem ponto\n",
    "    for j in nomes:\n",
    "        # Procura padrões com expressões regulares na string\n",
    "        div_sobrenome      = sobrenome_inicio.findall(j)\n",
    "        div_sbrcomposto    = sobrenome_composto.findall(j)\n",
    "        div_abrevponto     = letra_abrevponto.findall(j)\n",
    "        div_abrevespaco    = letra_abrevespaco.findall(j)\n",
    "        div_ltrdobradasini = letras_dobradasini.findall(j)\n",
    "        div_ltrdobradasfim = letras_dobradasfim.findall(j)\n",
    "        div_ltrdobradas    = letras_dobradas.findall(j)\n",
    "        tamanho=len(j)\n",
    "        # print('\\n', div_ltrdobradasini, div_ltrdobradasfim, tamanho, 'em:',j,len(j))\n",
    "        \n",
    "        #caso houver abreviatura com uma letra em maiúscula nos nomes\n",
    "        if div_abrevponto !=[] or tamanho==1:\n",
    "            cada_nome = j.replace('.','').strip()\n",
    "            if cada_nome not in nomes and cada_nome != sobrenome and nome != primeiro_nome:\n",
    "                nomes.append(cada_nome)\n",
    "        \n",
    "        #caso houver duas inicias juntas em maiúsculas\n",
    "        elif div_ltrdobradasini !=[] or div_ltrdobradasfim !=[] or div_ltrdobradas !=[] :\n",
    "            for letra in j:\n",
    "                if letra not in nomes and letra != sobrenome and letra != primeiro_nome:\n",
    "                    nomes.append(letra)\n",
    "        \n",
    "        #caso haja agnomes ao sobrenome\n",
    "        elif sobrenome in agnomes:\n",
    "            sobrenome = nomes[-1].upper()+' '+sobrenome\n",
    "            # print(sobrenome.split(' '))\n",
    "            # print('Sobrenome composto:',sobrenome)\n",
    "            for i in nomes:\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('Nomes do meio:',nomes)\n",
    "            \n",
    "        else:\n",
    "            if j not in nomes and j not in sobrenome and j != primeiro_nome:\n",
    "                nomes.append(j)\n",
    "    \n",
    "    nomes_meio=' '.join([str[0] for str in nomes]).strip()\n",
    "    # print('Qte nomes do meio',len(nomes),nomes)\n",
    "    if sobrenome != '' and primeiro_nome !='':\n",
    "        sobrenome_iniciais = sobrenome+', '+primeiro_nome[0]+' '+nomes_meio\n",
    "    elif sobrenome != '':\n",
    "        sobrenome_iniciais = sobrenome\n",
    "    \n",
    "    return sobrenome_iniciais.strip()\n",
    "\n",
    "\n",
    "## Agregar aprendizado supervisionado humano à medida que forem sendo identificados erros na situação atual\n",
    "lista_extra = [\n",
    "                # ('ALBUQUERQUE, Adriano B', 'ALBUQUERQUE, Adriano Bessa'),\n",
    "                # ('ALBUQUERQUE, Adriano', 'ALBUQUERQUE, Adriano Bessa'),\n",
    "                # ('COELHO, Andre L V', 'COELHO, Andre Luis Vasconcelos'),\n",
    "                # ('DUARTE, Joao B F', 'DUARTE, Joao Batista Furlan'),\n",
    "                # ('FILHO, Raimir H','HOLANDA FILHO, Raimir'),\n",
    "                # ('FILHO, Raimir','HOLANDA FILHO, Raimir'),\n",
    "                # ('FORMIGO, A','FORMICO, Maria Andreia Rodrigues'),\n",
    "                # ('FORMICO, A','FORMICO, Maria Andreia Rodrigues'),\n",
    "                # ('FURLAN, J B D', 'FURLAN, Joao Batista Duarte'),\n",
    "                # ('FURTADO, Elizabeth', 'FURTADO, Maria Elizabeth Sucupira'),\n",
    "                # ('FURTADO, Elizabeth S', 'FURTADO, Maria Elizabeth Sucupira'),\n",
    "                # ('FURTADO, Elizabeth Sucupira','FURTADO, Maria Elizabeth Sucupira'),\n",
    "                # ('FURTADO, M E S', 'FURTADO, Maria Elizabeth Sucupira'),\n",
    "                # ('FURTADO, Vasco', 'FURTADO, Joao Jose Vasco Peixoto'),\n",
    "                # ('FURTADO, J P', 'FURTADO, Joao Jose Vasco Peixoto'),\n",
    "                # ('FURTADO, J V P', 'FURTADO, Joao Jose Vasco Peixoto'),\n",
    "                # ('FURTADO, Vasco', 'FURTADO, Joao Jose Vasco Peixoto'),\n",
    "                # ('FURTADO, Elizabeth','FURTADO, Maria Elizabeth Sucupira'),\n",
    "                # ('HOLANDA, Raimir', 'HOLANDA FILHO, Raimir'),\n",
    "                # ('LEITE, G S', 'LEITE, Gleidson Sobreira'),\n",
    "                # ('PEQUENO, T H C', 'PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                # ('PEQUENO, Tarcisio','PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                # ('PEQUENO, Tarcisio Cavalcante', 'PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                # ('PINHEIRO, Placido R', 'PINHEIRO, Placido Rogerio'),\n",
    "                # ('PINHEIRO, Vladia', 'PINHEIRO, Vladia Celia Monteiro'),\n",
    "                # ('RODRIGUES, M A F', 'RODRIGUES, Maria Andreia Formico'),\n",
    "                # ('RODRIGUES, Andreia', 'RODRIGUES, Maria Andreia Formico'),\n",
    "                # ('JOAO, Batista F Duarte,', 'FURLAN, Joao Batista Duarte'),\n",
    "                # ('MACEDO, Antonio Roberto M de', 'MACEDO, Antonio Roberto Menescal de'),\n",
    "                # ('MACEDO, D V', 'MACEDO, Daniel Valente'),\n",
    "                # ('MENDONCA, Nabor C', 'MENDONCA, Nabor das Chagas'),\n",
    "                # ('PEQUENO, Tarcisio', 'PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                # ('PEQUENO, Tarcisio H', 'PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                # ('PINHEIRO, Mirian C D', 'PINHEIRO, Miriam Caliope Dantas'),\n",
    "                # ('PINHEIRO, Mirian Caliope Dantas', 'PINHEIRO, Miriam Caliope Dantas'),\n",
    "                # ('PINHEIRO, P G C D', 'PINHEIRO, Pedro Gabriel Caliope Dantas'),\n",
    "                # ('PINHEIRO, Pedro G C', 'PINHEIRO, Pedro Gabriel Caliope Dantas'),\n",
    "                # ('PINHEIRO, Placido R', 'PINHEIRO, Placido Rogerio'),\n",
    "                # ('PINHEIRO, Vladia', 'PINHEIRO, Vladia Celia Monteiro'),\n",
    "                # ('ROGERIO, Placido Pinheiro', 'PINHEIRO, Placido Rogerio'),\n",
    "                # ('REBOUCRAS FILHO, Pedro', 'REBOUCAS FILHO, Pedro Pedrosa'),\n",
    "                # ('SAMPAIO, A', 'SAMPAIO, Americo Tadeu Falcone'),\n",
    "                # ('SAMPAIO, Americo', 'SAMPAIO, Americo Tadeu Falcone'),\n",
    "                # ('SAMPAIO, Americo Falcone', 'SAMPAIO, Americo Tadeu Falcone'),\n",
    "                # ('SUCUPIRA, Elizabeth Furtado','FURTADO, Maria Elizabeth Sucupira'),\n",
    "                ]\n",
    "\n",
    "\n",
    "def converter_lista_set(lista):\n",
    "    set1 = set(lista)\n",
    "    return set1\n",
    "\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    '''\n",
    "    Recebe dois conjuntos como entradas e retorna a similaridade Jaccard entre eles. \n",
    "    1. calcula a interseção dos dois conjuntos usando a função de interseção e, \n",
    "    2. calcula a união dos dois conjuntos usando a função de união. \n",
    "    3. retorna a razão entre o comprimento da interseção e o comprimento da união, que é a similaridade de Jaccard.\n",
    "    '''\n",
    "    intersection = set1.intersection(set2)\n",
    "    union        = set1.union(set2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "\n",
    "def similares(lista_autores, lista_grupo, limite_jarowinkler, distancia_levenshtein, lista_extra):\n",
    "    \"\"\"Função para aplicar padronização no nome de autor da lista de pesquisadores e buscar similaridade na lista de coautores\n",
    "     Recebe: Lista de pesquisadores do grupo em análise gerada pela lista de nomes dos coautores das publicações em análise\n",
    "    Utiliza: get_jaro_distance(), editdistance()\n",
    "    Retorna: Lista de autores com fusão de nomes cuja similaridade esteja dentro dos limites definidos nesta função\n",
    "      Autor: Marcos Aires (Fev.2022)\n",
    "      \n",
    "    Refazer: Inserir crítica de, mantendo sequência ordem alfabética, retornar no final nome mais extenso em caso de similaridade;\n",
    "    \"\"\"\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    from IPython.display import clear_output\n",
    "    import editdistance\n",
    "    import numpy as np\n",
    "    import time\n",
    "    \n",
    "    t0=time.time()\n",
    "    \n",
    "    # limite_jarowinkler=0.85\n",
    "    # distancia_levenshtein=6\n",
    "    similares_jwl=[]\n",
    "    similares_regras=[]\n",
    "    similares=[]\n",
    "    tempos=[]\n",
    "    \n",
    "    count=0\n",
    "    t1=time.time()\n",
    "    for i in lista_autores:\n",
    "        count+=1\n",
    "        if count > 0:\n",
    "            tp=time.time()-t1\n",
    "            tmed=tp/count*2\n",
    "            tempos.append(tp)\n",
    "        # print(\"Analisar similaridades com: \", nome_padronizado)\n",
    "        \n",
    "        count1=0\n",
    "        for nome in lista_autores:\n",
    "            if count1 > 0:\n",
    "                resta=len(lista_autores)-count\n",
    "                print(f'Analisando {count1:3}/{len(lista_autores)} resta analisar {resta:3} nomes. Previsão de término em {np.round(tmed*resta/60,1)} minutos')\n",
    "            else:\n",
    "                print(f'Analisando {count1:3}/{len(lista_autores)} resta analisar {len(lista_autores)-count1} nomes.')\n",
    "            \n",
    "            t2=time.time()\n",
    "            count1+=1            \n",
    "\n",
    "            try:\n",
    "                similaridade_jarowinkler = get_jaro_distance(i, nome)\n",
    "                print(f'{i:40} | {nome:40} | Jaro-Winkler: {np.round(similaridade_jarowinkler,2):4} Levenshtein: {editdistance.eval(i, nome)}')\n",
    "                similaridade_levenshtein = editdistance.eval(i, nome)\n",
    "\n",
    "                # inferir similaridade para nomes que estejam acima do limite ponderado definido, mas não idênticos e não muito distantes em edição\n",
    "                if  similaridade_jarowinkler > limite_jarowinkler and similaridade_jarowinkler!=1 and similaridade_levenshtein < distancia_levenshtein:\n",
    "                    # Crítica no nome mais extenso como destino no par (origem, destino)\n",
    "                    \n",
    "                    similares_jwl.append((i,nome))\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            clear_output(wait=True)\n",
    "    \n",
    "    # Conjunto de regras de validação de similaridade\n",
    "    # Monta uma lista de nomes a serem retirados antes de montar a lista de troca\n",
    "    trocar=[]\n",
    "    retirar=[]\n",
    "    for i in similares_jwl:\n",
    "        sobrenome_i = i[0].split(',')[0]\n",
    "        sobrenome_j = i[1].split(',')[0]\n",
    "\n",
    "        try:\n",
    "            iniciais_i  = iniciais_nome(i[0]).split(',')[1].strip()\n",
    "        except:\n",
    "            iniciais_i  = ''\n",
    "\n",
    "        try:\n",
    "            iniciais_j  = iniciais_nome(i[1]).split(',')[1].strip()\n",
    "        except:\n",
    "            iniciais_j  = ''\n",
    "\n",
    "        try:\n",
    "            primnome_i = i[0].split(',')[1].strip().split(' ')[0].strip()\n",
    "        except:\n",
    "            primnome_i = ''\n",
    "\n",
    "        try:\n",
    "            primnome_j = i[1].split(',')[1].strip().split(' ')[0].strip()\n",
    "        except:\n",
    "            primnome_j = ''    \n",
    "\n",
    "        try:\n",
    "            inicial_i = i[0].split(',')[1].strip()[0]\n",
    "        except:\n",
    "            inicial_i = ''\n",
    "\n",
    "        try:\n",
    "            resto_i   = i[0].split(',')[1].strip().split(' ')[0][1:]\n",
    "        except:\n",
    "            resto_i   = ''\n",
    "\n",
    "        try:\n",
    "            inicial_j = i[1].split(',')[1].strip()[0]\n",
    "        except:\n",
    "            inicial_j = ''\n",
    "\n",
    "        try:\n",
    "            resto_j   = i[1].split(',')[1].strip().split(' ')[0][1:]\n",
    "        except:\n",
    "            resto_j = ''\n",
    "\n",
    "        # Se a distância de edição entre os sobrenomes\n",
    "        if editdistance.eval(sobrenome_i, sobrenome_j) > 2 or inicial_i!=inicial_j:\n",
    "            retirar.append(i)\n",
    "        else:\n",
    "            if primnome_i!=primnome_j and len(primnome_i)>1:\n",
    "                retirar.append(i)\n",
    "            if primnome_i!=primnome_j and len(primnome_i)>1 and len(primnome_j)>1:\n",
    "                retirar.append(i)\n",
    "            if resto_i!=resto_j and resto_i!='':\n",
    "                retirar.append(i)\n",
    "            if len(i[1]) < len(i[0]):\n",
    "                retirar.append(i)\n",
    "            if len(iniciais_i) != len(iniciais_j):\n",
    "                retirar.append(i)\n",
    "\n",
    "    for i in similares_jwl:\n",
    "        if i not in retirar:\n",
    "            trocar.append(i)\n",
    "\n",
    "        if iniciais_nome(i[0]) in iniciais_nome(i[1]) and len(i[0]) < len(i[1]):\n",
    "            trocar.append(i)\n",
    "\n",
    "        if iniciais_nome(i[0]) == iniciais_nome(i[1]) and len(i[0]) < len(i[1]):\n",
    "             trocar.append(i)\n",
    "    \n",
    "    trocar=trocar+lista_extra\n",
    "    trocar.sort()\n",
    "    \n",
    "    return trocar\n",
    "\n",
    "\n",
    "\n",
    "def extrair_variantes(df_dadosgrupo):\n",
    "    ''' Utiliza campo de Nome em Citações do currículo como filtro para obter variantes do nome de cada membro\n",
    "     Recebe: Dataframe com os dados brutos do grupo de pesquisa agrupados; lista de nomes de pesquisadores de interesse\n",
    "    Retorna: Lista de tuplas com pares a serem trocados da variante pelo nome padronizado na forma (origem, destino)\n",
    "    '''\n",
    "    filtro1   = 'Nome'\n",
    "    lista_nomes = df_dadosgrupo[(df_dadosgrupo.ROTULOS == filtro1)]['CONTEUDOS'].values\n",
    "\n",
    "    variantes=[]\n",
    "    filtro='Nome em citações bibliográficas'\n",
    "    variantes=df_dadosgrupo[(df_dadosgrupo.ROTULOS == filtro)]['CONTEUDOS'].to_list()\n",
    "\n",
    "    trocar=[]\n",
    "    for j in range(len(variantes)):\n",
    "        padrao_destino = padronizar_nome(lista_nomes[j])\n",
    "        trocar.append((lista_nomes[j], padrao_destino))\n",
    "        for k in variantes[j]:\n",
    "            padrao_origem = padronizar_nome(k)\n",
    "            trocar.append((k, padrao_destino))\n",
    "            trocar.append((padrao_origem, padrao_destino))\n",
    "    \n",
    "    return trocar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbfc46",
   "metadata": {},
   "source": [
    "    Apurar colaborações docente/discente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d78c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter_lista_set(lista):\n",
    "    set1 = set(lista)\n",
    "    return set1\n",
    "\n",
    "def montardf_producao(lista_csv):\n",
    "    print(f'{len(lista_csv):02} nomes a extrair')\n",
    "    df_public=pd.DataFrame()\n",
    "    for nome_csv in lista_csv:\n",
    "        if 'colaboradores' in nome_csv.lower():\n",
    "            tipo='colaboradores'\n",
    "        else:\n",
    "            tipo='permanentes'\n",
    "        \n",
    "        df_pub = pd.read_csv(pathcsv+nome_csv)\n",
    "        print(len(df_pub.index))\n",
    "        print(df_pub.keys())\n",
    "\n",
    "        pat='\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
    "        df_temp1 = df_pub.Data.str.split(pat=pat,expand=True)\n",
    "        df_temp1.columns = (['TITULO','RevAut'])\n",
    "\n",
    "        pat1='\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
    "        df_temp2 = df_temp1.RevAut.str.split(pat=pat1,expand=True)\n",
    "        df_temp2.columns = (['REVISTA','AUTORES'])\n",
    "\n",
    "        df_temp0 = df_pub.drop(['Data'], axis=1)\n",
    "        df_pub=df_temp0.merge(df_temp1['TITULO'],left_index=True,right_index=True)\n",
    "        df_pub=df_pub.merge(df_temp2,left_index=True,right_index=True)\n",
    "        try:\n",
    "            df_pub.drop(['Issn','Natureza'], axis=1, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_pub.drop(['Tipo','Idioma'], axis=1, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        df_public = pd.concat([df_public, df_pub], ignore_index=True)\n",
    "        print(len(df_public.index))\n",
    "        print(df_public.keys())\n",
    "        \n",
    "    ## Extrai o período com base nos dados\n",
    "    inicio = min(df_public['Ano'])\n",
    "    final  = max(df_public['Ano'])\n",
    "    \n",
    "    total_artigos=len(df_public.index)\n",
    "    # print(f'{total_artigos:4} publicações de artigos de docentes do programa no período de {inicio} a {final} carregadas...') \n",
    "    \n",
    "    return df_public\n",
    "\n",
    "def ler_nomesdocentes():    \n",
    "    print(pathcsv)\n",
    "    import os, sys\n",
    "\n",
    "    lista_csv=[]\n",
    "    dirs = os.listdir(pathcsv)\n",
    "    for file in dirs:\n",
    "        if 'nomes_docentes' in file:\n",
    "            lista_csv.append(file)\n",
    "    lista_csv.sort()\n",
    "    \n",
    "    for i in lista_csv:\n",
    "        print(i)\n",
    "\n",
    "    return lista_csv\n",
    "\n",
    "\n",
    "## ler arquivo com as orientações para gerar a lista de discentes\n",
    "def ler_lista_orientacoes():\n",
    "    try:\n",
    "        l1='lista_orientadores-discentes.csv'\n",
    "        df_orientacoes = pd.read_csv(pathcsv+l1, delimiter=';', header=None)\n",
    "        \n",
    "        lista_orientadores = df_orientacoes.iloc[:,0].unique()\n",
    "        lista_discentes    = df_orientacoes.iloc[:,1].unique()\n",
    "        print(f'{len(lista_orientadores):4} orientadores, com {len(lista_discentes)} discentes encontrados')\n",
    "    except Exception as e:\n",
    "        print('Erro ao gerar lista de orientações:')\n",
    "        print(e)\n",
    "        return df_orientacoes\n",
    "        \n",
    "    return lista_orientadores, lista_discentes \n",
    "\n",
    "\n",
    "## montar um dataframe com nome dos discentes de cada orientador\n",
    "def montardf_orientacoes():\n",
    "    try:\n",
    "        l1='lista_orientadores-discentes.csv'\n",
    "        df_orientacoes = pd.read_csv(pathcsv+l1, delimiter=';', header=None)\n",
    "        df_orientacoes.columns=['ORIENTADOR','DISCENTE']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Erro ao dividir dataframe de orientações:')\n",
    "        print(e)\n",
    "        return\n",
    "        \n",
    "    return df_orientacoes\n",
    "\n",
    "\n",
    "\n",
    "def montardf_docentes_permanentes_colaboradores():\n",
    "    try:\n",
    "        l1='lista_docentes_colaboradores.csv'\n",
    "        l2='lista_docentes_permanentes.csv'\n",
    "        df_docclbr = pd.read_csv(os.path.join(pathcsv,l1), header=None)\n",
    "        df_docperm = pd.read_csv(os.path.join(pathcsv,l2), header=None)\n",
    "        df_docentes = pd.concat([df_docperm, df_docclbr], ignore_index=True)\n",
    "        print(f'{len(df_docentes.index):4} docentes permanentes e colaboradores encontrados')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return df_docentes\n",
    "\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    '''\n",
    "    Recebe dois conjuntos (sets) como entradas e retorna a similaridade Jaccard entre eles e avalia: \n",
    "    1. calcula a interseção dos dois conjuntos usando a função de interseção \n",
    "    2. calcula a união dos dois conjuntos usando a função de união \n",
    "    3. retorna a razão entre o comprimento da interseção e o comprimento da união, que é a similaridade de Jaccard.\n",
    "    '''\n",
    "    intersection = set1.intersection(set2)\n",
    "    union        = set1.union(set2)\n",
    "    return len(intersection) / len(union)\n",
    "    \n",
    "def montardf_docentes(lista_nomes1=False, lista_nomes2=False):\n",
    "    # print(lista_nomes1)\n",
    "    # print(lista_nomes2)\n",
    "\n",
    "    ## Montagem do dataframe de participação docente\n",
    "    if (lista_nomes1 and lista_nomes2) != False:\n",
    "        ## Criar dataframe com os nomes do grupo de docentes permanentes\n",
    "        file_path = os.path.join(pathcsv,'lista_docentes_permanentes.csv')\n",
    "        df_docentes_permanentes   = pd.read_csv(file_path, header=None)\n",
    "        # df_docentes_permanentes['GRUPO']='Permanente'\n",
    "        # try:\n",
    "        #     df_docentes_permanentes.drop(columns=([1,2]), inplace=True)\n",
    "        # except:\n",
    "        #     pass\n",
    "        # df_docentes_permanentes.columns = ['DOCENTE','GRUPO']\n",
    "        df_docentes_permanentes.columns = ['DOCENTE','IDLATES','PROGRAMA','GRUPO']\n",
    "\n",
    "        ## Criar dataframe com os nomes do grupo de docentes colaboradores\n",
    "        file_path = os.path.join(pathcsv,'lista_docentes_colaboradores.csv')\n",
    "        df_docentes_colaboradores = pd.read_csv(file_path, header=None)\n",
    "        # df_docentes_colaboradores['GRUPO']='Colaborador'\n",
    "        # try:\n",
    "        #     df_docentes_colaboradores.drop(columns=([1,2]), inplace=True)\n",
    "        # except:\n",
    "        #     pass\n",
    "        df_docentes_colaboradores.columns = ['DOCENTE','IDLATES','PROGRAMA','GRUPO']\n",
    "\n",
    "        ## Criar um dataframe único com todos grupos de docentes juntos\n",
    "        df_docentes = pd.concat([df_docentes_permanentes, df_docentes_colaboradores]).reset_index(drop=True)\n",
    "        return df_docentes\n",
    "\n",
    "    elif 'permanentes' in lista_nomes1.lower():\n",
    "        ## Criar dataframe com os nomes do grupo de docentes permanentes\n",
    "        file_path = os.path.join(pathcsv,lista_nomes1)       \n",
    "        df_docentes_permanentes   = pd.read_csv(file_path, header=None)\n",
    "        # df_docentes_permanentes['GRUPO']='Permanente'\n",
    "        # try:\n",
    "        #     df_docentes_permanentes.drop(columns=([1,2]), inplace=True)\n",
    "        # except:\n",
    "        #     pass\n",
    "        df_docentes_permanentes.columns = ['DOCENTE','IDLATES','PROGRAMA','GRUPO']\n",
    "        return df_docentes_permanentes\n",
    "\n",
    "    elif 'colaboradores' in lista_nomes1.lower():\n",
    "        ## Criar dataframe com os nomes do grupo de docentes colaboradores\n",
    "        file_path = os.path.join(pathcsv,lista_nomes1)\n",
    "        df_docentes_colaboradores = pd.read_csv(file_path, header=None)\n",
    "        # df_docentes_colaboradores['GRUPO']='Colaborador'\n",
    "        # try:\n",
    "        #     df_docentes_colaboradores.drop(columns=([1,2]), inplace=True)\n",
    "        # except:\n",
    "        #     pass\n",
    "        df_docentes_colaboradores.columns = ['DOCENTE','IDLATES','PROGRAMA','GRUPO']\n",
    "        return df_docentes_colaboradores\n",
    "    else:\n",
    "        print('Erro ao montar dataframe de docentes, verifique os nomes de arquivo.')\n",
    "        return\n",
    "        \n",
    "def montar_listas(lista_csv, csv_permanentes=None, csv_colaboradores=None):\n",
    "    if csv_permanentes == None and csv_colaboradores == None:\n",
    "        csv_permanentes   = 'lista_docentes_permanentes.csv'\n",
    "        csv_colaboradores = 'lista_docentes_colaboradores.csv'\n",
    "        print(f'\\nNomes de docentes não informados, utilizando caminho e nomes padrão:')\n",
    "        print(f'     Docentes   permanentes de {pathcsv}{csv_permanentes}')\n",
    "        print(f'     Docentes colaboradores de {pathcsv}{csv_colaboradores}')\n",
    "        try:\n",
    "            file_path = os.path.join(pathcsv,csv_permanentes)\n",
    "            df_docperm = pd.read_csv(file_path, header=None)\n",
    "            file_path = os.path.join(pathcsv,csv_colaboradores)\n",
    "            df_docclbr = pd.read_csv(file_path, header=None)\n",
    "            lista_docentes = pd.concat([df_docperm, df_docclbr], ignore_index=True)[0].values\n",
    "            print(f'{len(lista_docentes):4} docentes permanentes e colaboradores encontrados')\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao ler listas de docentes, verificar se os arquivos CSV estão na pasta {pathcsv}')\n",
    "            print(e)\n",
    "    elif 'permanentes' in csv_permanentes.lower():\n",
    "        print(f'\\nArquivo docentes   permanentes informado: {csv_permanentes}')\n",
    "        try:\n",
    "            file_path = os.path.join(pathcsv,csv_permanentes)\n",
    "            lista_docentes = pd.read_csv(file_path, header=None)[0].values\n",
    "            print(f'{len(lista_docentes)} docentes permanentes encontrados')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    elif 'colaboradores' in csv_permanentes.lower():\n",
    "        print(f'\\nArquivo docentes colaboradores informado: {csv_colaboradores}')\n",
    "        try:\n",
    "            file_path = os.path.join(pathcsv,csv_permanentes)\n",
    "            lista_docentes = pd.read_csv(file_path, header=None)[0].values\n",
    "            print(f'{len(lista_docentes)} docentes colaboradores encontrados')\n",
    "        except Exception as e:\n",
    "            print(e)    \n",
    "    else:\n",
    "        print('Erro ao ler listas de docentes, verificar listas')\n",
    "\n",
    "    df_prod   = montardf_producao(lista_csv)\n",
    "    \n",
    "    ## Montar a lista de autores com limpar_nomes remove caracteres, preposições e separa iniciais com espaço\n",
    "    lista_listas = df_prod['AUTORES'].tolist()\n",
    "    lista_autores_artigos = []\n",
    "    for i in lista_listas:\n",
    "        lista_autores_artigos.append(limpar_nomes(i))\n",
    "    \n",
    "    ## Ler nomes de discentes e orientadores\n",
    "    lista_orientadores, lista_discentes = ler_lista_orientacoes()\n",
    "    \n",
    "    return lista_autores_artigos, lista_docentes, lista_orientadores, lista_discentes\n",
    "\n",
    "def montardf_participacao_docente_discente(df_prod, dic_nomes_docentes, dic_nomes_discentes):\n",
    "    ## Montar dataframe de participação docente\n",
    "    df_participacao_docente = pd.DataFrame(dic_nomes_docentes).T\n",
    "    df_participacao_docente.columns = ['DOCENTE','INDICES_ARTIGOS']\n",
    "    df_participacao_docente['AUTORIAS'] = [len(x) for x in df_participacao_docente['INDICES_ARTIGOS']]\n",
    "\n",
    "    ## Montar dataframe de participação discente\n",
    "    df_participacao_discente = pd.DataFrame(dic_nomes_discentes).T\n",
    "    df_participacao_discente.columns = ['DISCENTE','INDICES_ARTIGOS']\n",
    "    df_participacao_discente['AUTORIAS'] = [len(x) for x in df_participacao_discente['INDICES_ARTIGOS']]\n",
    "\n",
    "    ## Criar lista com os artigos onde foi encontado nome de algum docente\n",
    "    artigos_com_docentes=[]\n",
    "    for m in df_participacao_docente['INDICES_ARTIGOS']:\n",
    "        for n in m:\n",
    "            if n not in artigos_com_docentes:\n",
    "                artigos_com_docentes.append(n)\n",
    "    artigos_com_docentes.sort()\n",
    "                \n",
    "    ## Criar lista com os artigos onde foi encontado nome de algum discente\n",
    "    artigos_com_discentes=[]\n",
    "    for m in df_participacao_discente['INDICES_ARTIGOS']:\n",
    "        for n in m:\n",
    "            if n not in artigos_com_discentes:\n",
    "                artigos_com_discentes.append(n)\n",
    "    artigos_com_discentes.sort()\n",
    "                \n",
    "        \n",
    "    ## Criar lista com os artigos onde NÃO foi encontado nome de discente\n",
    "    lista_semparticipacaodiscente=[]\n",
    "    for i in range(len(df_prod.index)):\n",
    "        if i not in artigos_com_discentes and i not in lista_semparticipacaodiscente:\n",
    "            lista_semparticipacaodiscente.append(i)\n",
    "            \n",
    "    ## Criar lista com os artigos onde NÃO foi encontado nome de docente\n",
    "    lista_semparticipacaodocente=[]\n",
    "    for i in range(len(df_prod.index)):\n",
    "        if i not in artigos_com_docentes and i not in lista_semparticipacaodocente:\n",
    "            lista_semparticipacaodocente.append(i)\n",
    "\n",
    "    ## Apresentar resultados das buscas por nomes de autores docentes e discentes\n",
    "    lista_titulos = pd.Series(df_prod['TITULO'].values).unique().tolist()\n",
    "    print(len(lista_titulos),'títulos únicos de artigo encontrados')\n",
    "    pdoc = np.round(100*len(artigos_com_docentes)/len(df_prod.index),2)\n",
    "    spdoc = np.round(100*len(lista_semparticipacaodocente)/len(df_prod.index),2)\n",
    "    print(f'{len(artigos_com_docentes):4} ({pdoc:6}%) artigos com nome de  docente encontrado, faltando {len(lista_semparticipacaodocente):3} ({spdoc:5}%)')\n",
    "\n",
    "    pdis = np.round(100*len(artigos_com_discentes)/len(df_prod.index),2)\n",
    "    spdis = np.round(100*len(lista_semparticipacaodiscente)/len(df_prod.index),2)\n",
    "    print(f'{len(artigos_com_discentes):4} ({pdis:6}%) artigos com nome de discente encontrado, faltando {len(lista_semparticipacaodiscente):3} ({spdis:5}%)')\n",
    "\n",
    "    return df_participacao_docente, df_participacao_discente, lista_semparticipacaodocente, lista_semparticipacaodiscente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ca80e",
   "metadata": {},
   "source": [
    "    Ler arquivos de dados do disco local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea93d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ler arquivos de publicação de artigos na pasta de arquivos CSV\n",
    "def ler_artigostodosperiodos():    \n",
    "    print(pathcsv)\n",
    "    import os, sys\n",
    "\n",
    "    lista_csv=[]\n",
    "    dirs = os.listdir(pathcsv)\n",
    "    for file in dirs:\n",
    "        if 'Artigos' in file:\n",
    "            lista_csv.append(file)\n",
    "    lista_csv.sort()\n",
    "    \n",
    "    for i in lista_csv:\n",
    "        print(i)\n",
    "\n",
    "    return lista_csv\n",
    "\n",
    "\n",
    "\n",
    "## ler arquivos de publicação de de período determinado\n",
    "def ler_csvptg(inicio=False, final=False, tipo=False, grupo=False):    \n",
    "    import os, sys\n",
    "\n",
    "    print(pathcsv)\n",
    "    lista_csv=[]\n",
    "    dirs = os.listdir(pathcsv)\n",
    "    for file in dirs:\n",
    "        if (str(inicio) or str(final)) == False:\n",
    "            if tipo.lower() == False:\n",
    "                if grupo.lower() == False:\n",
    "                    lista_csv.append(file)\n",
    "\n",
    "        elif (str(inicio) and str(final)) in file.lower():\n",
    "            if unidecode(tipo).lower() in unidecode(file).lower():\n",
    "                if unidecode(grupo).lower() in unidecode(file).lower():\n",
    "                    lista_csv.append(file)\n",
    "    lista_csv.sort()\n",
    "    \n",
    "    for i in lista_csv:\n",
    "        print(i)\n",
    "\n",
    "    return lista_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f505b4",
   "metadata": {},
   "source": [
    "### Funções para avaliar a PCD e Pontuação de Impacto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eef1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def montardf_impacto_docente_discente(df_prod, dic_nomes_docentes, dic_nomes_discentes, inicio, final):\n",
    "    artigos_com_discentes=[]\n",
    "    artigos_com_docentes=[]\n",
    "    lista_semparticipacaodiscente=[]\n",
    "    lista_semparticipacaodocente=[]\n",
    "\n",
    "    try:\n",
    "        ## Montar dataframe de participação docente\n",
    "        df_impacto_docente = pd.DataFrame(dic_nomes_docentes).T\n",
    "        df_impacto_docente.columns = ['DOCENTE','INDICES_ARTIGOS','EXTRATOS_QUALIS']\n",
    "        df_impacto_docente['AUTORIAS'] = [len(x) for x in df_impacto_docente['INDICES_ARTIGOS']]\n",
    "\n",
    "        ## Criar lista com os artigos onde foi encontado nome de algum docente\n",
    "        for m in df_impacto_docente['INDICES_ARTIGOS']:\n",
    "            for n in m:\n",
    "                if n not in artigos_com_docentes:\n",
    "                    artigos_com_docentes.append(n)\n",
    "        artigos_com_docentes.sort()\n",
    "\n",
    "        ## Criar lista com os artigos onde NÃO foi encontado nome de docente\n",
    "        for i in range(len(df_prod.index)):\n",
    "            if i not in artigos_com_docentes and i not in lista_semparticipacaodocente:\n",
    "                lista_semparticipacaodocente.append(i)\n",
    "    except:\n",
    "        df_impacto_docente = pd.DataFrame()\n",
    "        print('Não foi possível encontrar nenhuma ocorrência dos nomes dos docentes com este conjunto de dados')\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        ## Montar dataframe de participação discente\n",
    "        df_impacto_discente = pd.DataFrame(dic_nomes_discentes).T\n",
    "        df_impacto_discente.columns = ['DISCENTE','INDICES_ARTIGOS','EXTRATOS_QUALIS']\n",
    "        df_impacto_discente['AUTORIAS'] = [len(x) for x in df_impacto_discente['INDICES_ARTIGOS']]\n",
    "\n",
    "        ## Criar lista com os artigos onde foi encontado nome de algum discente\n",
    "        for m in df_impacto_discente['INDICES_ARTIGOS']:\n",
    "            for n in m:\n",
    "                if n not in artigos_com_discentes:\n",
    "                    artigos_com_discentes.append(n)\n",
    "        artigos_com_discentes.sort()\n",
    "    except:\n",
    "        df_impacto_discente = pd.DataFrame()\n",
    "        print('Não foi possível encontrar nenhuma ocorrência dos nomes dos discentes com este conjunto de dados')\n",
    "        pass\n",
    "                \n",
    "    ## Criar lista com os artigos onde NÃO foi encontado nome de discente\n",
    "    for i in range(len(df_prod.index)):\n",
    "        if i not in artigos_com_discentes and i not in lista_semparticipacaodiscente:\n",
    "            lista_semparticipacaodiscente.append(i)\n",
    "\n",
    "    ## Apresentar resultados das buscas por nomes de autores docentes e discentes\n",
    "    lista_titulos = pd.Series(df_prod['TITULO'].values).unique().tolist()\n",
    "    print(len(lista_titulos),'títulos únicos de artigo encontrados')\n",
    "    pdoc  = np.round(100*len(artigos_com_docentes)/len(df_prod.index),2)\n",
    "    spdoc = np.round(100*len(lista_semparticipacaodocente)/len(df_prod.index),2)\n",
    "    print(f'{len(artigos_com_docentes):4} ({pdoc:6}%) artigos com nome de  docente encontrado, faltando {len(lista_semparticipacaodocente):3} ({spdoc:5}%)')\n",
    "\n",
    "    pdis = np.round(100*len(artigos_com_discentes)/len(df_prod.index),2)\n",
    "    spdis = np.round(100*len(lista_semparticipacaodiscente)/len(df_prod.index),2)\n",
    "    print(f'{len(artigos_com_discentes):4} ({pdis:6}%) artigos com nome de discente encontrado, faltando {len(lista_semparticipacaodiscente):3} ({spdis:5}%)')\n",
    "\n",
    "    ## A1=100, A2=80, B1=60, B2=40, B3=20, B4=10, B5=2\n",
    "    soma_impacto=[]\n",
    "    for linha in df_impacto_docente['EXTRATOS_QUALIS']:\n",
    "        impacto=0\n",
    "        for extrato in linha:\n",
    "            if extrato == 'A1':\n",
    "                impacto+=100\n",
    "            elif extrato == 'A2':\n",
    "                impacto+=80\n",
    "            elif extrato == 'B1':\n",
    "                impacto+=60\n",
    "            elif extrato == 'B2':\n",
    "                impacto+=40\n",
    "            elif extrato == 'B3':\n",
    "                impacto+=20\n",
    "            elif extrato == 'B4':\n",
    "                impacto+=10\n",
    "            elif extrato == 'B5':\n",
    "                impacto+=2\n",
    "            elif extrato == 'C':\n",
    "                impacto+=0\n",
    "            elif extrato == 'nan':\n",
    "                impacto+=0\n",
    "        soma_impacto.append(impacto)\n",
    "\n",
    "    df_impacto_docente['SOMA_IMPACTO'] = soma_impacto\n",
    "    qte_anos = (final-inicio+1)\n",
    "    df_impacto_docente['ANOS'] = qte_anos\n",
    "    df_impacto_docente['IMPACTO_MEDIO_ANUAL'] = np.round(df_impacto_docente['SOMA_IMPACTO']/df_impacto_docente['ANOS'],1)\n",
    "\n",
    "    return df_impacto_docente, df_impacto_discente\n",
    "\n",
    "\n",
    "\n",
    "def apurar_pcd_impacto(df_docentes, df_impacto_docente, df_impacto_discente, meta_pcd=50.0, meta_impacto=150.0):\n",
    "    print(f'Total de nomes de docentes em análise: {len(df_docentes.index)}')\n",
    "    print(f'Total de nomes de docentes  encontrados nos artigos: {len(df_impacto_docente.index)}')\n",
    "    print(f'Total de nomes de discentes encontrados nos artigos: {len(df_impacto_discente.index)}')\n",
    "\n",
    "    df_docentes_pcd_impacto = df_docentes\n",
    "    ## Montar lista com os índices do dataframe de artigos onde foram achados nomes de discentes na lista de autores\n",
    "    lista_participacao_discente = []\n",
    "    for artigos_discentes in df_impacto_discente['INDICES_ARTIGOS']:\n",
    "        for indice in artigos_discentes:\n",
    "            lista_participacao_discente.append(indice)\n",
    "\n",
    "    ## Contar a quantidade de participações de discentes que ocorrem no dataframe de produção docente\n",
    "    qte_colab_discente=[]\n",
    "    for docente,artigos_docente in zip(df_impacto_docente['DOCENTE'], df_impacto_docente['INDICES_ARTIGOS']):\n",
    "        qte=0\n",
    "        for indice in artigos_docente:\n",
    "            if indice in lista_participacao_discente:\n",
    "                qte+=1\n",
    "            \n",
    "        qte_colab_discente.append(qte)\n",
    "\n",
    "    df_docentes_pcd_impacto['PUBLICAÇÕES']  = df_impacto_docente['AUTORIAS']\n",
    "    df_docentes_pcd_impacto['COM_DISCENTE'] = qte_colab_discente\n",
    "    df_docentes_pcd_impacto['PCD'] = np.round(100*(df_docentes_pcd_impacto['COM_DISCENTE']/df_impacto_docente['AUTORIAS']),1)\n",
    "    df_docentes_pcd_impacto['IMPACTO'] = df_impacto_docente['SOMA_IMPACTO']\n",
    "    df_docentes_pcd_impacto['IMPACTO_MEDIO_ANUAL'] = df_impacto_docente['IMPACTO_MEDIO_ANUAL']\n",
    "\n",
    "    ## Definir a meta de produção conjunta com discente e apurar o resultado\n",
    "    # meta_pcd=50.0\n",
    "    apuracao_pcd     = df_docentes_pcd_impacto.groupby([df_docentes_pcd_impacto.index,'PCD'])\n",
    "    df_abaixo_pcd    = apuracao_pcd.filter(lambda x: x['PCD'] < meta_pcd)\n",
    "    df_atingiram_pcd = apuracao_pcd.filter(lambda x: x['PCD'] >= meta_pcd)\n",
    "\n",
    "    total_docentes         = len(df_docentes_pcd_impacto.index)\n",
    "\n",
    "    contagem_abaixometa    = len(df_abaixo_pcd.index)\n",
    "    contagem_atingindometa = len(df_atingiram_pcd.index)\n",
    "    indicador_pcd = np.round(100*contagem_atingindometa/total_docentes,1)\n",
    "    print(f'{indicador_pcd}% dos docentes {contagem_atingindometa}/{total_docentes} atingem a meta de {meta_pcd}% publicação com discente')\n",
    "\n",
    "    ## Definir a meta de impacto por pesquisador e para o grupo\n",
    "    # meta_impacto=150\n",
    "    apuracao_impacto     = df_docentes_pcd_impacto.groupby([df_docentes_pcd_impacto.index,'IMPACTO'])\n",
    "    df_abaixo_impacto    = apuracao_impacto.filter(lambda x: x['IMPACTO_MEDIO_ANUAL'] < meta_impacto)\n",
    "    df_atingiram_impacto = apuracao_impacto.filter(lambda x: x['IMPACTO_MEDIO_ANUAL'] >= meta_impacto)\n",
    "\n",
    "    contagem_abaixometa_impacto    = len(df_abaixo_impacto.index)\n",
    "    contagem_atingindometa_impacto = len(df_atingiram_impacto.index)\n",
    "    indicador_impacto = np.round(100*contagem_atingindometa_impacto/total_docentes,1)\n",
    "    print(f'{indicador_impacto}% dos docentes {contagem_atingindometa_impacto}/{total_docentes} atingem a meta de {meta_impacto} pontos de impacto médio por ano das publicações')\n",
    "\n",
    "    return df_docentes_pcd_impacto, apuracao_pcd, df_abaixo_pcd, df_atingiram_pcd, indicador_pcd, indicador_impacto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cda89d",
   "metadata": {},
   "source": [
    "### Funções para plotagem: \n",
    "    \n",
    "    Plotar gráficos de percentual de participação discente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib.offsetbox import (TextArea, DrawingArea, OffsetImage, AnnotationBbox)\n",
    "from matplotlib.cbook import get_sample_data\n",
    "plt.rcParams['font.size']      = 12\n",
    "# plt.rcParams[\"figure.figsize\"] = (15,9)\n",
    "\n",
    "\n",
    "\n",
    "def plotar_pcd(df, grupo=False, inicio=False, final=False):\n",
    "    N    = len(df.index)\n",
    "    percentual = (df['PCD'].values.round(1))\n",
    "\n",
    "    ind   = np.arange(N) # the x locations for the groups\n",
    "    width = 0.75         # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "    # criar figura\n",
    "    fig, ax = plt.subplots(figsize=(25,10))\n",
    "\n",
    "    # plotar barras verticais com cores condicionais se abaixo do valor da variável para aceitação\n",
    "    par = 50 \n",
    "    cor = ['yellow' if (x < par) else 'green' for x in percentual]\n",
    "    p1  = ax.bar(ind, percentual, width, \n",
    "                #  yerr=dsvpad, \n",
    "                 error_kw=dict(lw=0.3, capsize=2, capthick=1),\n",
    "                 label='Percentual de publicação com discente', color=cor)\n",
    "\n",
    "    # plotar os rótulos e título\n",
    "    ax.axhline(par, color='red', linewidth=3, linestyle='dotted')\n",
    "    ax.axhline(70, color='gray', linewidth=3, linestyle='dotted')\n",
    "    if (grupo and inicio and final) != False:\n",
    "        ax.set_title(f'Apuração do percentual de publicação de docentes {grupo.upper()} com discente no período de {inicio} a {final}')\n",
    "    elif grupo == False:\n",
    "        ax.set_title(f'Apuração do percentual de publicação de docentes com discente no período de {inicio} a {final}')\n",
    "    else:\n",
    "        ax.set_title(f'Apuração do percentual de publicação com discente')\n",
    "\n",
    "    ax.set_ylabel('Percentual de artigos publicados com discente')\n",
    "    ax.set_xticks(ind)\n",
    "    \n",
    "    labels_pos=np.arange(1,N+1)\n",
    "    ax.set_xticklabels(labels_pos)\n",
    "\n",
    "    # Label with label_type 'center' instead of the default 'edge'\n",
    "    ax.bar_label(p1, label_type='center')\n",
    "    # ax.bar_label(p1, dsvpad)\n",
    "    # ax.set_yticks(range(0,100))\n",
    "    \n",
    "    # respectivo domínio de cada questão no AGREE II\n",
    "    grupos = ['01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01',\n",
    "              '02','02','02','02','02','02','02','02','02','02','02','02','02','02','02','02',]\n",
    "    \n",
    "    # calcular limites de retângulos dos domínios\n",
    "    lista_dominios   = pd.Series(grupos)\n",
    "    largura_dominios = lista_dominios.value_counts().sort_index().values   \n",
    "    altura = 100\n",
    "    rets=[]   \n",
    "    \n",
    "    for i in range(1,len(largura_dominios)+1):\n",
    "        ret = patches.Rectangle((-0.5,0),\n",
    "                                np.sum(largura_dominios[:i]), altura,\n",
    "#                                 linestyle='dashdot',\n",
    "                                linewidth=2,\n",
    "                                edgecolor='b',\n",
    "                                fill = False)\n",
    "        rets.append(ret)\n",
    "    \n",
    "    # plotar os retângulos das dimensões na área do gráfico\n",
    "    for i in rets:\n",
    "        ax.add_patch(i)\n",
    "        \n",
    "    # plotar legenda, comentar para excluir\n",
    "    # ax.legend(bbox_to_anchor=(0.75,-0.05), ncol=2)\n",
    "    \n",
    "    campo='04'\n",
    "    # savefig_respostas(campo)\n",
    "    \n",
    "    plt.show()\n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "def plotar_medias_impacto(df, grupo=False, inicio=False, final=False):\n",
    "    N      = len(df.index)\n",
    "    pontos = (df['IMPACTO_MEDIO_ANUAL'].values.round(1))\n",
    "\n",
    "    ind   = np.arange(N) # the x locations for the groups\n",
    "    width = 0.75         # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "    # criar figura\n",
    "    fig, ax = plt.subplots(figsize=(25,10))\n",
    "\n",
    "    # plotar barras verticais com cores condicionais se abaixo do valor da variável para aceitação\n",
    "    par = 150 \n",
    "    cor = ['yellow' if (x < par) else 'green' for x in pontos]\n",
    "    p1  = ax.bar(ind, pontos, width, \n",
    "                #  yerr=dsvpad, \n",
    "                 error_kw=dict(lw=0.3, capsize=2, capthick=1),\n",
    "                 label='Pontuação em impacto das publicações de docentes', color=cor)\n",
    "\n",
    "    # plotar os rótulos e título\n",
    "    ax.axhline(par, color='red', linewidth=3, linestyle='dotted')\n",
    "    if (grupo and inicio and final) != False:\n",
    "        ax.set_title(f'Apuração da pontuação de impacto das publicações de docentes {grupo.upper()} no período de {inicio} a {final}')\n",
    "    elif (inicio and final) != False:\n",
    "        ax.set_title(f'Apuração da pontuação de impacto das publicações de docentes no período de {inicio} a {final}')\n",
    "    else:\n",
    "        ax.set_title(f'Apuração da pontuação de impacto médio (total do impacto acumulado / quantidade de anos do período) das publicações de docentes')\n",
    "\n",
    "    ax.set_ylabel('Pontuação ponderada pelo Qualis dos artigos publicados')\n",
    "    ax.set_xticks(ind)\n",
    "    \n",
    "    labels_pos=np.arange(1,N+1)\n",
    "    ax.set_xticklabels(labels_pos)\n",
    "\n",
    "    # Label with label_type 'center' instead of the default 'edge'\n",
    "    ax.bar_label(p1, label_type='center')\n",
    "    # ax.bar_label(p1, dsvpad)\n",
    "    vr_maximo = int(max(pontos)+50)\n",
    "    # ax.set_yticks(range(0,vr_maximo))\n",
    "    \n",
    "    # respectivo domínio de cada questão no AGREE II\n",
    "    grupos = ['01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01','01',\n",
    "              '02','02','02','02','02','02','02','02','02','02','02','02','02','02','02','02',]\n",
    "    \n",
    "    # calcular limites de retângulos dos domínios\n",
    "    lista_dominios   = pd.Series(grupos)\n",
    "    largura_dominios = lista_dominios.value_counts().sort_index().values   \n",
    "    altura = max(pontos)+50\n",
    "    rets=[]   \n",
    "    \n",
    "    for i in range(1,len(largura_dominios)+1):\n",
    "        ret = patches.Rectangle((-0.5,0),\n",
    "                                np.sum(largura_dominios[:i]), altura,\n",
    "#                                 linestyle='dashdot',\n",
    "                                linewidth=2,\n",
    "                                edgecolor='b',\n",
    "                                fill = False)\n",
    "        rets.append(ret)\n",
    "    \n",
    "    # plotar os retângulos das dimensões na área do gráfico\n",
    "    for i in rets:\n",
    "        ax.add_patch(i)\n",
    "        \n",
    "    # plotar legenda, comentar para excluir\n",
    "    # ax.legend(bbox_to_anchor=(0.75,-0.05), ncol=2)\n",
    "    \n",
    "    campo='04'\n",
    "    # savefig_respostas(campo)\n",
    "    \n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa03cb",
   "metadata": {},
   "source": [
    "## Apurar participação discente e impacto artigos dos docentes\n",
    "### Todos os períodos e grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(pathcsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea408ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_nomes_docentes_permanentes   = 'lista_docentes_permanentes.csv'\n",
    "lista_nomes_docentes_colaboradores = 'lista_docentes_colaboradores.csv'\n",
    "df_docentes = montardf_docentes(lista_nomes_docentes_permanentes, lista_nomes_docentes_colaboradores)\n",
    "\n",
    "## Escolha dos arquivos que alimentarão a análise da produção\n",
    "lista_csv = ler_artigostodosperiodos()\n",
    "lista_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2eb3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c09d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathfilename = os.path.join(pathcsv,'lista_docentes.csv')\n",
    "lista_csv = pd.read_csv(pathfilename,header=None)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prod   = montardf_producao(lista_csv)\n",
    "\n",
    "## Mostrar quantitativos lidos\n",
    "print(f'\\nCarregado dataframe com {len(df_prod.index)} linhas:')\n",
    "lista_titulos = pd.Series(df_prod['TITULO'].values).unique().tolist()\n",
    "print(f'{len(lista_titulos):4} artigos distintos publicados no período')\n",
    "lista_revistas = pd.Series(df_prod['REVISTA'].values).unique().tolist()\n",
    "print(f'{len(lista_revistas):4} revistas distintas utilizadas no período')\n",
    "\n",
    "## Ler nomes de docentes, discentes e papel de orientador\n",
    "lista_autores_artigos, lista_docentes, lista_orientadores, lista_discentes = montar_listas(lista_csv)\n",
    "qte_docentes=len(lista_docentes)\n",
    "qte_discentes=len(lista_discentes)\n",
    "total_iteracoes=(qte_docentes+qte_discentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e7f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e2c31",
   "metadata": {},
   "source": [
    "    Organizar lista de autores e iniciais de nomes\n",
    "    PROBLEMA: não está quebrando no número de artigos 1115, mas sim em apenas 2 e última vazia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f515360",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Montar a lista de autores a partir da lista de strings limpa com nomes de autores\n",
    "def organizar_nomes(str_autores_artigo):\n",
    "    erros_organizar  = []\n",
    "    lista_organizada = []\n",
    "    partes_nomes = str_autores_artigo.split(', ')\n",
    "    n       = len(partes_nomes)\n",
    "    pares   = range(0,n+1,2)\n",
    "    impares = range(1,n+1,2)\n",
    "    try:\n",
    "        for i,j in zip(pares,impares):\n",
    "            nome=[]\n",
    "            try:\n",
    "                nomes_ordenado = str(partes_nomes[j].lower()+' '+partes_nomes[i].lower()).replace('  ',' ').strip()\n",
    "            except:\n",
    "                if len(pares) > len(impares):\n",
    "                    nomes_ordenado = str(partes_nomes[j].lower()).replace('  ',' ').strip()\n",
    "                else:\n",
    "                    nomes_ordenado = str(partes_nomes[i].lower()).replace('  ',' ').strip()\n",
    "            lista_organizada.append(nomes_ordenado)\n",
    "    \n",
    "    except Exception as e1:\n",
    "        print('Erro ao montar listas de nomes de autor:',e1)\n",
    "        erros_organizar.append((i,str_autores_artigo))\n",
    "\n",
    "    return lista_organizada, erros_organizar\n",
    "\n",
    "## Quebrar um nome em seu sobrenome seguido das partes de nome\n",
    "def quebrar_partesnomes(nome):\n",
    "    padrao     = padronizar_nome(nome).lower()\n",
    "    sobrenome  = padrao.split(',')[0].strip()\n",
    "    restonomes = padrao.split(',')[1].strip().split(' ')\n",
    "    try:\n",
    "        partenome1 = restonomes[0].strip()\n",
    "    except:\n",
    "        partenome1 = np.NaN\n",
    "    try:\n",
    "        partenome2 = restonomes[1].strip()\n",
    "    except:\n",
    "        partenome2 = np.NaN\n",
    "    try:\n",
    "        partenome3 = restonomes[2].strip()\n",
    "    except:\n",
    "        partenome3 = np.NaN        \n",
    "    # print(f'{sobrenome:15} | {partenome1:1} | {partenome2:1} | {partenome3}')\n",
    "    \n",
    "    return sobrenome, partenome1, partenome2, partenome3\n",
    "\n",
    "\n",
    "## Quebrar um nome em seu sobrenome seguido as iniciais das partes de nome\n",
    "def quebrar_iniciais(nome):\n",
    "    padrao = iniciais_nome(nome).lower()\n",
    "    sobrenome  = padrao.split(',')[0].strip()\n",
    "    restonomes = padrao.split(',')[1].strip().split(' ')\n",
    "\n",
    "    try:\n",
    "        partenome1 = restonomes[0].strip()\n",
    "    except:\n",
    "        partenome1 = np.NaN\n",
    "    try:\n",
    "        partenome2 = restonomes[1].strip()\n",
    "    except:\n",
    "        partenome2 = np.NaN\n",
    "    try:\n",
    "        partenome3 = restonomes[2].strip()\n",
    "    except:\n",
    "        partenome3 = np.NaN        \n",
    "    # print(f'{sobrenome:15} | {partenome1:1} | {partenome2:1} | {partenome3}')\n",
    "    \n",
    "    return sobrenome, partenome1, partenome2, partenome3\n",
    "\n",
    "\n",
    "## compilar padrão regular expression para buscar ocorência de duas das quatro partes de nome dentro de janela de no máximo 2 palavras de distância\n",
    "def compilar_partes(sobrenome, partenome1, partenome2, partenome3):\n",
    "    return re.compile(r'\\b({0}|{1}|{2}|{3})(?:\\W+\\w+){{0,2}}?\\W+({1}&{2}|{2}&{3}|{0}&{1}|{0}&{3})\\b'.format(sobrenome, partenome1, partenome2, partenome3), flags=re.IGNORECASE)\n",
    "\n",
    "def compilar_iniciais(sobrenome, inicial1, inicial2, inicial3):\n",
    "    return re.compile(r'\\b({0})(?:\\W+\\w+){{0,1}}?\\W+({1}|{2}|{3})\\b'.format(sobrenome, inicial1, inicial2, inicial3), flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73ffe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_dicionarios(df_prod, lista_docentes, lista_discentes, inicio = min(df_prod['Ano']), final  = max(df_prod['Ano'])):\n",
    "    qte_docentes     = len(lista_docentes)\n",
    "    qte_discentes    = len(lista_discentes)\n",
    "    total_iteracoes  = (qte_docentes+qte_discentes)\n",
    "    \n",
    "    ## Monta uma lista de strings com nomes dos autores de cada artigo na forma extraída pelo e-lattes\n",
    "    lista_nomes_autores = [limpar_nomes(x) for x in df_prod['AUTORES'].values]\n",
    "    qte_artigos         = len(lista_nomes_autores)\n",
    "        \n",
    "    ## Define os limites para considerar duas strings com nomes similares entre si\n",
    "    limite_jaro_nome        = 0.88\n",
    "    limite_jaro_iniciais    = 0.75\n",
    "    limite_jaccard_iniciais = 0.32\n",
    "\n",
    "    ## Função para calcular similaridade dos nomes de docentes com cada nome de autor da lista de autores de cada artigo\n",
    "    t1 = time.time()\n",
    "    dic_nomes_docentes  = {}\n",
    "    dic_nomes_discentes = {}    \n",
    "    erros=[]\n",
    "    rot1='Comparando nome do docente'\n",
    "    rot2='Sobren/Iniciais docen'\n",
    "    rot3='Sobrenome/Iniciais autor'\n",
    "    rot4='I.Doc'\n",
    "    rot5='I.Aut'\n",
    "    rot6='Jaro-Winkler'\n",
    "    rot7='Jaccard'\n",
    "    for m,docente in enumerate(lista_docentes):\n",
    "        achados_docentes     = []\n",
    "        lista_indice_docente = []\n",
    "        lista_qualis_docente = []\n",
    "        docentes_naoencontrados = []          \n",
    "        contagem=m+1\n",
    "        # clear_output(wait=True)       \n",
    "        try:\n",
    "            nome_docente_padronizado = padronizar_nome(docente).lower()\n",
    "            iniciais_nome_docente    = iniciais_nome(docente).lower()\n",
    "            string_iniciais_docente  = ' '.join(x.strip() for x in iniciais_nome_docente.split(',')[1:])\n",
    "            set_iniciais_docente     = converter_lista_set([x.strip() for x in iniciais_nome_docente.split(',')[1:]])                \n",
    "            print(f'\\nCálculo das similaridades autores-docentes (nome/iniciais/Jaccard):')\n",
    "            print(f'{rot1:^40} |{rot2:^20}| {rot3:^25}|{rot4:^5}|{rot5:^5}|{rot6:^15}|{rot7:^10}')\n",
    "            ## Monta a lista de autores com a divisão da string organizada padronizada no formato: {sobrenome, iniciais de nomes}\n",
    "            lst_autores_artigo = []\n",
    "            lista_nomes_autores = [limpar_nomes(x) for x in df_prod['AUTORES'].values]\n",
    "            for n,nomes_autor in enumerate(lista_nomes_autores):\n",
    "                autores,erros = organizar_nomes(nomes_autor)\n",
    "                for o,autor in enumerate(autores):\n",
    "                    qte_autores_artigo = len(autores)\n",
    "                    # print(f'{o+1:2}/{qte_autores_artigo:2} nomes de autor do artigo em análise, restando {qte_autores_artigo-o-1:2}')\n",
    "                    nome_autor_padronizado = padronizar_nome(autor).lower()\n",
    "                    try:    \n",
    "                        iniciais_nome_autor    = iniciais_nome(autor).lower()\n",
    "                        string_iniciais_autor  = ' '.join(x.strip() for x in iniciais_nome_autor.split(',')[1:])\n",
    "                        set_iniciais_autor     = converter_lista_set([x.strip() for x in iniciais_nome_autor.split(',')[1:]])\n",
    "                        # time.sleep(1)\n",
    "                        ## Cálculos de similaridades entre os nomes do autor e do docente\n",
    "                        if iniciais_nome_docente and iniciais_nome_autor != '':\n",
    "                            similaridade_nome     = get_jaro_distance(iniciais_nome_docente, iniciais_nome_autor)\n",
    "                            similaridade_iniciais = get_jaro_distance(string_iniciais_docente, string_iniciais_autor)\n",
    "                            similaridade_jaccard  = np.round(jaccard_similarity(set_iniciais_docente, set_iniciais_autor),2)\n",
    "                            print(f'\\nCálculo das similaridades autores-discentes (nome/iniciais/Jaccard):')\n",
    "                            print(f'{padronizar_nome(docente):^40} | {iniciais_nome_docente:<20}|{iniciais_nome_autor:<20} {similaridade_nome:^5}|{string_iniciais_docente:^5}|{string_iniciais_autor:^5}|{similaridade_iniciais:^15}|{similaridade_jaccard:^10}')\n",
    "                            clear_output(wait=True)\n",
    "                            if similaridade_nome > limite_jaro_nome and similaridade_iniciais > limite_jaro_iniciais and similaridade_jaccard > limite_jaccard_iniciais:\n",
    "                                if n not in lista_indice_docente:\n",
    "                                    print(f'Similaridade encontrada no artigo {n+1}/{qte_artigos}, docente {m+1}/{qte_docentes}')\n",
    "                                    print(f'{rot1:^40} |{rot2:^20}| {rot3:^25}|{rot4:^5}|{rot5:^5}|{rot6:^15}|{rot7:^10}')\n",
    "                                    print(f'{padronizar_nome(docente):^40} | {iniciais_nome_docente:<20}|{iniciais_nome_autor:<20} {similaridade_nome:^5}|{string_iniciais_docente:^5}|{string_iniciais_autor:^5}|{similaridade_iniciais:^15}|{similaridade_jaccard:^10}')\n",
    "                                    achados_docentes.append(docente)\n",
    "                                    lista_indice_docente.append(n)\n",
    "                                    # print(len(lista_indice_docente))\n",
    "                                    extrato = df_prod['Qualis'][n]\n",
    "                                    lista_qualis_docente.append(extrato)\n",
    "                                    clear_output(wait=True)                                    \n",
    "                    except Exception as e1:\n",
    "                        # print(f'Erro na etapa 1 de gerar_dicionarios, ao tratar iniciais do nome de autor/docente da linha  {n}/{o}/{qte_docentes}/{qte_artigos}')\n",
    "                        # print(e1)\n",
    "                        similaridade_iniciais = np.NaN\n",
    "                        similaridade_jaccard  = np.NaN\n",
    "                        erros.append(('e1_similaridadesdocente',m,n,o,e1))                    \n",
    "                lst_autores_artigo.append(iniciais_nome_autor)\n",
    "            # print(f'Lista organizada de nomes de autores: {len(lst_autores_artigo)} listas de autores de artigos publicados no período')\n",
    "\n",
    "            ## Ao final da leitura todos artigos para cada docente, criar o dicionário de docentes quando docente tenha aparecido na linha de autores\n",
    "            if lista_indice_docente != []:\n",
    "                dic_nomes_docentes[m] = (docente, lista_indice_docente, lista_qualis_docente)\n",
    "            else:\n",
    "                docentes_naoencontrados.append(docente)\n",
    "\n",
    "            tdec=time.time()-t1\n",
    "            # tres=tdec/(m+1)*((qte_docentes-m)+qte_discentes)\n",
    "            # print(f'Analisadas{m+1:4}/{total_iteracoes} iterações em {horas(tdec)}, restando {total_iteracoes-m}. Busca{m+1:4}/{qte_docentes:<4}docente: {docente.title():50}')\n",
    "            # print(f'Nome do docente foi encontrado em {len(lista_indice_docente):2} artigos')\n",
    "        except Exception as e2:\n",
    "            # print(f'Erro na etapa 2 de gerar_dicionarios, ao calcular similaridades de autor/docente  da linha {n}/{o}/{qte_docentes}/{qte_artigos}')\n",
    "            # print(e2)\n",
    "            erros.append(('e2_padronizardocentes',m,n,o,e2)) \n",
    "\n",
    "    ## Função para calcular similaridade dos nomes de discentes com cada nome de autor da lista de autores de cada artigo\n",
    "    for p,discente in enumerate(lista_discentes):\n",
    "        achados_discentes     = []\n",
    "        lista_indice_discente = []\n",
    "        lista_qualis_discente = []\n",
    "        discentes_naoencontrados = []\n",
    "        contagem=m+p+1\n",
    "        # clear_output(wait=True)       \n",
    "        try:\n",
    "            nome_discente_padronizado = padronizar_nome(discente).lower()\n",
    "            iniciais_nome_discente    = iniciais_nome(discente).lower()\n",
    "            string_iniciais_discente  = ' '.join(x.strip() for x in iniciais_nome_discente.split(',')[1:])\n",
    "            set_iniciais_discente     = converter_lista_set([x.strip() for x in iniciais_nome_discente.split(',')[1:]])\n",
    "            rot2='Sobren/Iniciais disce'\n",
    "            rot4='I.Dis'            \n",
    "            ## Monta a lista de autores com a divisão da string organizada padronizada no formato: {sobrenome, iniciais de nomes}\n",
    "            lst_autores_artigo = []\n",
    "            lista_nomes_autores = [limpar_nomes(x) for x in df_prod['AUTORES'].values]\n",
    "            for n,nomes_autor in enumerate(lista_nomes_autores):\n",
    "                # clear_output(wait=True)\n",
    "                # print(f'Procurando {nome_discente_padronizado.title()} em {n+1:2}/{qte_artigos:<2} artigos, restando {qte_artigos-n-1:<2}')\n",
    "                autores,erros = organizar_nomes(nomes_autor)\n",
    "                for q,autor in enumerate(autores):\n",
    "                    qte_autores_artigo = len(autores)\n",
    "                    nome_autor_padronizado = padronizar_nome(autor).lower()\n",
    "                    # print(f'{q+1:2}/{qte_autores_artigo:2} nomes de autor do artigo em análise, restando {qte_autores_artigo-q-1:2}')\n",
    "                    nome_autor_padronizado = padronizar_nome(autor).lower()\n",
    "                    try:    \n",
    "                        iniciais_nome_autor    = iniciais_nome(autor).lower()\n",
    "                        string_iniciais_autor  = ' '.join(x.strip() for x in iniciais_nome_autor.split(',')[1:])\n",
    "                        set_iniciais_autor     = converter_lista_set([x.strip() for x in iniciais_nome_autor.split(',')[1:]])   \n",
    "                        ## Cálculos de similaridades entre os nomes do autor e do discente\n",
    "                        if iniciais_nome_discente and iniciais_nome_autor != '':\n",
    "                            similaridade_nome     = get_jaro_distance(iniciais_nome_discente, iniciais_nome_autor)\n",
    "                            similaridade_iniciais = get_jaro_distance(string_iniciais_discente, string_iniciais_autor)\n",
    "                            similaridade_jaccard  = np.round(jaccard_similarity(set_iniciais_discente, set_iniciais_autor),2)\n",
    "                            print(f'{padronizar_nome(discente):^40} | {iniciais_nome_discente:<20}|{iniciais_nome_autor:<20} {similaridade_nome:^5}|{string_iniciais_docente:^5}|{string_iniciais_autor:^5}|{similaridade_iniciais:^15}|{similaridade_jaccard:^10}')                            \n",
    "                            if similaridade_nome > limite_jaro_nome and similaridade_iniciais > limite_jaro_iniciais and similaridade_jaccard > limite_jaccard_iniciais:\n",
    "                                if n not in lista_indice_discente:\n",
    "                                    print(f'Similaridade encontrada no artigo {n+1}/{qte_artigos}, discente {p+1}/{qte_discentes}')\n",
    "                                    print(f'{rot1:^40} |{rot2:^20}| {rot3:^25}|{rot4:^5}|{rot5:^5}|{rot6:^15}|{rot7:^10}')\n",
    "                                    print(f'{padronizar_nome(discente):^40} | {iniciais_nome_discente:<20}|{iniciais_nome_autor:<20} {similaridade_nome:^5}|{string_iniciais_discente:^5}|{string_iniciais_autor:^5}|{similaridade_iniciais:^15}|{similaridade_jaccard:^10}')\n",
    "                                    achados_discentes.append(discente)\n",
    "                                    lista_indice_discente.append(n)\n",
    "                                    # print(len(lista_indice_discente))\n",
    "                                    extrato = df_prod['Qualis'][n]\n",
    "                                    lista_qualis_discente.append(extrato)\n",
    "                                    clear_output(wait=True)\n",
    "                    except Exception as e3:\n",
    "                        similaridade_iniciais = np.NaN\n",
    "                        similaridade_jaccard  = np.NaN\n",
    "                        # print(f'Erro na etapa 3 de gerar_dicionarios, ao calcular similaridades de nomes de autor/discente da linha {n}/{o}/{qte_discentes}/{qte_artigos}')\n",
    "                        # print(e3)\n",
    "                        erros.append(('e3_buscadiscentes',p,n,q,e3))\n",
    "        \n",
    "            ## Ao final da leitura todos artigos para cada discente, criar o dicionário de discente quando docente tenha aparecido na linha de autores\n",
    "            if lista_indice_discente != []:\n",
    "                dic_nomes_discentes[o] = (discente, lista_indice_discente, lista_qualis_discente)\n",
    "            else:\n",
    "                discentes_naoencontrados.append(discente)            \n",
    "            \n",
    "            tdec=time.time()-t1\n",
    "            # tres=tdec/(o+p+1)*((qte_discentes-p)+qte_discentes)\n",
    "            # print(f'Analisadas{contagem+1:4}/{total_iteracoes} iterações em {horas(tdec)}, restando {horas(tres)} para iterar {total_iteracoes-contagem-1}. Busca{p+1:4}/{qte_discentes:<4}discente: {discente.title():50}')\n",
    "            # print(f'Nome do discente foi encontrado em {len(lista_indice_discente):2} artigos')   \n",
    "        except Exception as e4:\n",
    "            # print('Erro na etapa 4 de gerar_dicionarios, ao finalizar montagem dos dicionários:',e4)\n",
    "            erros.append(('e4_padronizardiscente',m,n,e4))\n",
    "\n",
    "    return dic_nomes_docentes, dic_nomes_discentes, docentes_naoencontrados, discentes_naoencontrados, erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770168bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_nomes_docentes, dic_nomes_discentes, docentes_naoencontrados, discentes_naoencontrados, erros = gerar_dicionarios(df_prod, lista_docentes, lista_discentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b0da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic_nomes_docentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f9ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docentes_naoencontrados),'total de nomes de  docentes não encontrados nos artigos')\n",
    "print(len(discentes_naoencontrados),'total de nomes de discentes não encontrados nos artigos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedd039",
   "metadata": {},
   "outputs": [],
   "source": [
    "discentes_naoencontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(erros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac927dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "erros[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdffe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_artigos_problemas = []\n",
    "# lista_autores_problemas = []\n",
    "# for etapa,docente,i,discente,artigo in erros:\n",
    "#     if i not in lista_artigos_problemas:\n",
    "#         lista_artigos_problemas.append(i)\n",
    "#         lista_autores_problemas.append(df_prod['AUTORES'][i])\n",
    "\n",
    "# df_artigos_problemas=pd.DataFrame(lista_artigos_problemas).reset_index(drop=True)\n",
    "# df_artigos_problemas['LISTA_AUTORES'] = lista_autores_problemas\n",
    "# df_artigos_problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d13905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Lista ordenada alfabeticamente pelos sobrenomes de docentes\n",
    "# lista_docentes_sobrenome=[]\n",
    "# for i in lista_docentes:\n",
    "#     lista_docentes_sobrenome.append(iniciais_nome(i))\n",
    "    \n",
    "# lista_docentes_sobrenome.sort()\n",
    "# for j in lista_docentes_sobrenome:\n",
    "#     print(f'{j.lower()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a71542",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = 2017\n",
    "final  = 2022\n",
    "df_impacto_docente, df_impacto_discente = montardf_impacto_docente_discente(df_prod, dic_nomes_docentes, dic_nomes_discentes, inicio, final)\n",
    "df_impacto_docente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d83d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docentes = montardf_docentes(lista_nomes_docentes_permanentes, lista_nomes_docentes_colaboradores)\n",
    "df_docentes_pcd_impacto, apuracao_pcd, df_abaixo_pcd, df_atingiram_pcd, indicador_pcd, indicador_impacto = apurar_pcd_impacto(df_docentes, df_impacto_docente, df_impacto_discente, meta_pcd=50.0, meta_impacto=150.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36af03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotar_pcd(df_docentes_pcd_impacto)\n",
    "plotar_medias_impacto(df_docentes_pcd_impacto, grupo=False, inicio=False, final=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd992d",
   "metadata": {},
   "source": [
    "### Funções avaliação PCD e Impacto Médio Anual: \n",
    "    Avaliar indicadores PCD e Somatório do Fator de Impacto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf0beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_completo(inicio, final, tipo, grupo, lista_nomes_docentes):\n",
    "    ## Ler arquivos de dados\n",
    "    lista_csv   = ler_csvptg(inicio, final, tipo, grupo)\n",
    "    df_public   = montardf_producao(lista_csv)\n",
    "    df_docentes = montardf_docentes(lista_nomes_docentes)\n",
    "    lista_autores_artigos, lista_docentes, lista_orientadores, lista_discentes = montar_listas(lista_csv, lista_nomes_docentes)\n",
    "\n",
    "    ## Avaliação da Publicação Conjunta com Discentes (PCD) e do Impacto Médio Anual (IMA)\n",
    "    dic_nomes_docentes, dic_nomes_discentes, docentes_naoencontrados, discentes_naoencontrados, erros = gerar_dicionarios(df_public, lista_docentes, lista_discentes, inicio, final)\n",
    "    df_impacto_docente, df_impacto_discente = montardf_impacto_docente_discente(df_public, dic_nomes_docentes, dic_nomes_discentes, inicio, final)\n",
    "\n",
    "    ## Gerar gráfico de apuração de impacto médio anual por docente\n",
    "    df_docentes_pcd_impacto, apuracao_pcd, df_abaixo_pcd, df_atingiram_pcd, indicador_pcd, indicador_impacto = apurar_pcd_impacto(df_docentes, df_impacto_docente, df_impacto_discente)\n",
    "    plotar_pcd(df_docentes_pcd_impacto, grupo, inicio, final)\n",
    "    plotar_medias_impacto(df_docentes_pcd_impacto, grupo, inicio, final)\n",
    "\n",
    "    return df_public, df_impacto_docente, df_impacto_discente, df_docentes_pcd_impacto, indicador_pcd, indicador_impacto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85feab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ace99be1",
   "metadata": {},
   "source": [
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486c9c2",
   "metadata": {},
   "source": [
    "### Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a,b='',''\n",
    "# get_jaro_distance(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbcfc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b='1','0'\n",
    "get_jaro_distance(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fb0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_nomes_autores = [limpar_nomes(x) for x in df_prod['AUTORES'].values]\n",
    "print(len(lista_nomes_autores))\n",
    "print(lista_nomes_autores[0])\n",
    "\n",
    "## Verifica se a divisão em nomes de autor é par (sobrenome separado de nomes por vírgula)\n",
    "for i in lista_nomes_autores[:14]:\n",
    "    qte_nomes_autor = len(i.split(','))\n",
    "    if qte_nomes_autor/2 != qte_nomes_autor//2:\n",
    "        print(qte_nomes_autor,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ed58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m,docente in enumerate(lista_docentes):\n",
    "    nome_docente_padronizado = padronizar_nome(docente).lower()\n",
    "    iniciais_nome_docente    = iniciais_nome(docente).lower()\n",
    "    print(iniciais_nome_docente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4278bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ## Monta a lista de autores com a divisão da string de nomes acima por: sobrenome, nomes\n",
    "    # lst_autores_artigo = []\n",
    "    # lista_nomes_autores = [limpar_nomes(x) for x in df_prod['AUTORES'].values]\n",
    "    # for n,nomes_autor in enumerate(lista_nomes_autores):\n",
    "    #     autores,erros = organizar_nomes(nomes_autor)\n",
    "    #     # print(autores,'\\n')\n",
    "    #     for o,autor in enumerate(autores):\n",
    "    #         # print(autor)\n",
    "    #         # print(f'{o+1:2}/{qte_autores_artigo:2} nomes de autor do artigo em análise, restando {qte_autores_artigo-o-1:2}')\n",
    "    #         sobrenome_iniciais_autor = iniciais_nome(autor).lower()\n",
    "    #         string_iniciais_autor    = ' '.join(x.strip() for x in sobrenome_iniciais_autor.split(',')[1:])\n",
    "    #         set_iniciais_autor       = converter_lista_set([x.strip() for x in sobrenome_iniciais_autor.split(',')[1:]])\n",
    "    #         print(f'{sobrenome_iniciais_autor:20}|{string_iniciais_autor:^9}|{set_iniciais_autor}')\n",
    "    #     lst_autores_artigo.append(sobrenome_iniciais_autor)\n",
    "    # print(f'Lista organizada de nomes de autores: {len(lst_autores_artigo)} listas de autores de artigos publicados no período')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qte_docentes     = len(lista_docentes)\n",
    "qte_discentes    = len(lista_discentes)\n",
    "total_iteracoes  = (qte_docentes+qte_discentes)\n",
    "\n",
    "## Monta uma lista de strings com nomes dos autores de cada artigo\n",
    "lista_nomes_autores = [limpar_nomes(x) for x in df_prod['AUTORES'].values]\n",
    "qte_artigos         = len(lista_nomes_autores)\n",
    "\n",
    "## Monta a lista de autores com a divisão da string de nomes acima por: sobrenome, nomes\n",
    "lst_autores_artigo = []\n",
    "for cada_lista_autores in lista_nomes_autores:\n",
    "    lista_organizada, erros_organizar = organizar_nomes(cada_lista_autores)\n",
    "    lst_autores_artigo.append(lista_organizada)\n",
    "print(f'{len(lst_autores_artigo)} listas de autores de {qte_artigos} artigos publicados no período')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_nomes_autores[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_autores_artigo[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e752b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a='a b c de oliveira'\n",
    "padronizar_nome(a)\n",
    "iniciais_nome(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c009e",
   "metadata": {},
   "source": [
    "# Apuração segmentada por períodos e grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ff296",
   "metadata": {},
   "outputs": [],
   "source": [
    "evolucao_pcd=[]\n",
    "evolucao_impacto=[]\n",
    "tipo_analise=[]\n",
    "grupo_analise=[]\n",
    "periodo_analise=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d437b0",
   "metadata": {},
   "source": [
    "## Quadriênio 2017-2020 de Docentes Permanentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7569f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_nomes_docentes_permanentes   = 'lista_docentes_permanentes.csv'\n",
    "lista_nomes_docentes_colaboradores = 'lista_docentes_colaboradores.csv'\n",
    "df_docentes = montardf_docentes(lista_nomes_docentes_permanentes, lista_nomes_docentes_colaboradores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621055ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = 2017\n",
    "final  = 2020\n",
    "tipo   = 'artigos'\n",
    "grupo  = 'permanentes'\n",
    "lista_nomes_docentes = 'lista_docentes_permanentes.csv'\n",
    "\n",
    "df_public, df_impacto_docente, df_impacto_discente, df_docentespcd, indicador_pcd, indicador_impacto = avaliar_completo(inicio, final, tipo, grupo, lista_nomes_docentes)\n",
    "evolucao_pcd.append(indicador_pcd)\n",
    "evolucao_impacto.append(indicador_impacto)\n",
    "tipo_analise.append(tipo)\n",
    "grupo_analise.append(grupo)\n",
    "periodo_analise.append([inicio,final])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc16d3f",
   "metadata": {},
   "source": [
    "## .\n",
    "## Quadriênio 2017-2020 de Docentes Colaboradores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb5d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = 2017\n",
    "final  = 2020\n",
    "tipo   = 'artigos'\n",
    "grupo  = 'colaboradores'\n",
    "lista_nomes_docentes = 'lista_docentes_colaboradores.csv'\n",
    "\n",
    "df_public, df_impacto_docente, df_impacto_discente, df_docentespcd, indicador_pcd, indicador_impacto = avaliar_completo(inicio, final, tipo, grupo, lista_nomes_docentes)\n",
    "evolucao_pcd.append(indicador_pcd)\n",
    "evolucao_impacto.append(indicador_impacto)\n",
    "tipo_analise.append(tipo)\n",
    "grupo_analise.append(grupo)\n",
    "periodo_analise.append([inicio,final])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33f22aa",
   "metadata": {},
   "source": [
    "# .\n",
    "# .\n",
    "# .\n",
    "# Avaliação de meio termo biênio [2021-2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd6823",
   "metadata": {},
   "source": [
    "## Biênio 2021-2022 de Docentes Permanentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = 2021\n",
    "final  = 2022\n",
    "tipo   = 'artigos'\n",
    "grupo  = 'permanentes'\n",
    "lista_nomes_docentes = 'lista_docentes_permanentes.csv'\n",
    "# lista_nomes_docentes = 'lista_docentes_colaboradores.csv'\n",
    "\n",
    "df_public, df_impacto_docente, df_impacto_discente, df_docentespcd, indicador_pcd, indicador_impacto = avaliar_completo(inicio, final, tipo, grupo, lista_nomes_docentes)\n",
    "evolucao_pcd.append(indicador_pcd)\n",
    "evolucao_impacto.append(indicador_impacto)\n",
    "tipo_analise.append(tipo)\n",
    "grupo_analise.append(grupo)\n",
    "periodo_analise.append([inicio,final])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f19cf",
   "metadata": {},
   "source": [
    "## .\n",
    "## .\n",
    "## Biênio 2021-2022 de Docentes Colaboradores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = 2021\n",
    "final  = 2022\n",
    "tipo   = 'artigos'\n",
    "grupo  = 'colaboradores'\n",
    "# lista_nomes_docentes = 'lista_docentes_permanentes.csv'\n",
    "lista_nomes_docentes = 'lista_docentes_colaboradores.csv'\n",
    "\n",
    "df_public, df_impacto_docente, df_impacto_discente, df_docentespcd, indicador_pcd, indicador_impacto = avaliar_completo(inicio, final, tipo, grupo, lista_nomes_docentes)\n",
    "evolucao_pcd.append(indicador_pcd)\n",
    "evolucao_impacto.append(indicador_impacto)\n",
    "tipo_analise.append(tipo)\n",
    "grupo_analise.append(grupo)\n",
    "periodo_analise.append([inicio,final])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6a47c",
   "metadata": {},
   "source": [
    "# Evolução de indicadores de gestão do programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31656a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indicadores = pd.DataFrame({\n",
    "    'TIPO': pd.Series(tipo_analise),\n",
    "    'GRUPO': pd.Series(grupo_analise),\n",
    "    'PERIODOS': pd.Series(periodo_analise),\n",
    "    'META_PCD_50%': pd.Series(evolucao_pcd),\n",
    "    'META_IMPACTO_150ANO': pd.Series(evolucao_impacto),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indicadores.sort_values(by=['GRUPO'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7fd64a",
   "metadata": {},
   "source": [
    "# Conferência em detalhes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = 2021\n",
    "final  = 2022\n",
    "tipo   = 'artigos'\n",
    "grupo  = 'permanentes'\n",
    "lista_nomes_docentes = 'lista_docentes_permanentes.csv'\n",
    "# lista_nomes_docentes = 'lista_docentes_colaboradores.csv'\n",
    "\n",
    "df_public, df_impacto_docente, df_impacto_discente, df_docentespcd, indicador_pcd, indicador_impacto = avaliar_completo(inicio, final, tipo, grupo, lista_nomes_docentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f942da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_impacto_docente[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ccbd6a",
   "metadata": {},
   "source": [
    "## Conferência dos achados de nomes de discentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_partes_achadas(lista_discentes, lista_autores, verbose=False):\n",
    "    partes_achadas=[]\n",
    "    discentes_achados=[]\n",
    "    erros=[]\n",
    "    if verbose == True:\n",
    "        print(f'{padronizar_titulo(lista_autores).lower()}')\n",
    "    try:\n",
    "        ## Buscar pelas partes de nomes de autor em cada linha de autores de artigo\n",
    "        for nome in lista_discentes:\n",
    "            sobrenome, partenome1, partenome2, partenome3 = quebrar_partesnomes(nome)\n",
    "            if verbose == True:\n",
    "                print(f'{sobrenome}, {partenome1} {partenome2} {partenome3}')\n",
    "            strbusca_partes = compilar_partes(sobrenome, partenome1, partenome2, partenome3)\n",
    "            try:\n",
    "                achados_partes = re.search(strbusca_partes, padronizar_titulo(lista_autores).lower())\n",
    "                if achados_partes.span() !=None:\n",
    "                    partes_achadas.append(achados_partes.groups())\n",
    "                    discentes_achados.append(nome)\n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                pass\n",
    "\n",
    "        ## Buscar pelas iniciais de partes de nomes de autor, que seguem um sobrenome em cada linha de autores de artigo\n",
    "        for nome in lista_discentes:\n",
    "            sobrenome, inicial1, inicial2, inicial3 = quebrar_iniciais(nome)\n",
    "            if verbose == True:\n",
    "                print(f'{sobrenome}, {inicial1} {inicial2} {inicial3}')\n",
    "            strbusca_iniciais = compilar_iniciais(sobrenome, inicial1, inicial2, inicial3)\n",
    "            try:\n",
    "                achados_iniciais = re.search(strbusca_iniciais, padronizar_titulo(lista_autores).lower())\n",
    "                if achados_iniciais.span() !=None and achados_iniciais.groups() not in discentes_achados:\n",
    "                    partes_achadas.append(achados_iniciais.groups())\n",
    "                    discentes_achados.append(nome)\n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        # print(f'Erro ao buscar partes de nome; {e}')\n",
    "        print(e)\n",
    "    \n",
    "    return partes_achadas, discentes_achados\n",
    "\n",
    "def listar_achados(docente):\n",
    "    lista_csv = ler_artigostodosperiodos()\n",
    "    lista_autores_artigos, lista_docentes, lista_orientadores, lista_discentes = montar_listas(lista_csv)\n",
    "\n",
    "    df_filtrado = df_impacto_docente[(df_impacto_docente.DOCENTE==docente)]\n",
    "\n",
    "    lista_indices = df_filtrado['INDICES_ARTIGOS'].values.tolist()[0]\n",
    "    n=100\n",
    "    print('-'*n)\n",
    "    print(f'\\nDocente: {docente} | {len(lista_indices)} Publicações identificadas para no período [{inicio} a {final}]')\n",
    "    print()\n",
    "    for indice in lista_indices:\n",
    "        print('-'*n)\n",
    "        print(f'Índice da publicação: {indice}')\n",
    "        print(df_public.iloc[indice].values[:3])\n",
    "        lista_autores = padronizar_titulo(df_public.iloc[indice].values[4])\n",
    "        partes_achadas, discentes_achados = mostrar_partes_achadas(lista_discentes, lista_autores, verbose=False)\n",
    "        print()\n",
    "        print(lista_autores)\n",
    "        print('\\nPartes de nomes de alunos encontrados na lista de autores:')\n",
    "        print(partes_achadas)\n",
    "        print('\\nNomes de alunos considerados como encontrados na lista de autores:')\n",
    "        print(discentes_achados)\n",
    "\n",
    "def separar_iniciais(nome):\n",
    "    import re\n",
    "    letras_duasconsnts = re.compile(r'^[B-DF-HJ-NP-TV-XZ-r]{2}$')            # Duas Letras consoantes maiúsculas juntas do início ao final da string\n",
    "    letras_tresconsnts = re.compile(r'^[B-DF-HJ-NP-TV-XZ-r]{3}$')            # Três Letras consoantes maiúsculas juntas do início ao final da string\n",
    "    letras_duasconsntsvirg = re.compile(r'^[B-DF-HJ-NP-TV-XZ-r]{2},$')       # Duas Letras consoantes maiúsculas juntas do início ao final da string, seguidas de vírgula\n",
    "    letras_tresconsntsvirg = re.compile(r'^[B-DF-HJ-NP-TV-XZ-r]{3},$')       # Três Letras consoantes maiúsculas juntas do início ao final da string, seguidas de vírgula\n",
    "    partes_nome=[]\n",
    "    for j in nome.split(' '):\n",
    "        div_ltrduasconsnts = letras_duasconsnts.findall(j)\n",
    "        div_ltrtriplicadas = letras_tresconsnts.findall(j)\n",
    "        div_ltrduasconsntsvirg = letras_duasconsntsvirg.findall(j)\n",
    "        div_ltrtresconsntsvirg = letras_tresconsntsvirg.findall(j)\n",
    "        if div_ltrduasconsnts or div_ltrtriplicadas:\n",
    "            iniciais_separadas = ' '.join(x for x in j)\n",
    "            partes_nome.append(iniciais_separadas)\n",
    "        elif div_ltrduasconsntsvirg or div_ltrtresconsntsvirg:\n",
    "            iniciais_separadas = ' '.join(x for x in j[:-1])\n",
    "            partes_nome.append(iniciais_separadas+',')\n",
    "        else:\n",
    "            partes_nome.append(j)\n",
    "    nome_separado = ' '.join(x for x in partes_nome).strip()\n",
    "    return nome_separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1021acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "docente = 'Olindo Assis Martins Filho'\n",
    "listar_achados(docente)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dee2749",
   "metadata": {},
   "source": [
    "# Outras funcionalidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374848af",
   "metadata": {},
   "source": [
    "    Formatar tempo (h:min:seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72114631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempo(start, end):\n",
    "    t=end-start\n",
    "\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "\n",
    "def horas(segundos): \n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(segundos)) \n",
    "\n",
    "\n",
    "def dias_horas_minutos(td):\n",
    "    x = (td.days, td.seconds//3600, (td.seconds//60)%60, td.seconds)\n",
    "    return x #(days, hrs, mins, seconds)\n",
    "\n",
    "\n",
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    \n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    \n",
    "    return f.format(fmt, **values)\n",
    "\n",
    "\n",
    "# print (timedelta(days=365, hours=8, minutes=15))\n",
    "# print (\"   Hoje é: \" + str(date.today()))\n",
    "# print (\"Agora são: \" + str(datetime.now()))\n",
    "# print (\"Um ano no futuro estaremos em:\" + str(dt.today() + timedelta(days=365)))\n",
    "# hoje = date.today()\n",
    "# print(hoje)\n",
    "# hora = dt.now()\n",
    "# print(hora)\n",
    "# dias_ano = date(hoje.year, 1, 1)\n",
    "# if dias_ano < hoje:\n",
    "#     print (\"Decoridos %d dias do ano\" % ((hoje - dias_ano).days))\n",
    "    \n",
    "# from datetime import datetime\n",
    "# now= datetime.now() #get the current date and time\n",
    "\n",
    "# #%c - local date and time, %x-local's date, %X- local's time\n",
    "# print(now.strftime(\"%c\"))\n",
    "# print(now.strftime(\"%x\"))\n",
    "# print(now.strftime(\"%X\"))\n",
    "\n",
    "# ##### Time Formatting ####\n",
    "# #%I/%H - 12/24 Hour, %M - minute, %S - second, %p - local's AM/PM\n",
    "# print(now.strftime(\"%I:%M:%S %p\")) # 12-Hour:Minute:Second:AM\n",
    "# print(now.strftime(\"%H:%M\")) # 24-Hour:Minute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.spacy3': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "197d8f220c237ea2269a397bcc5571d13b6ee9614ab9ae87aa2d290b25dc373b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
