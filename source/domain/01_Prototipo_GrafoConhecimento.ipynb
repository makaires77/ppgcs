{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Interesse de pesquisa</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O termo \"inovação tecnológica\", no sentido moderno, começou a ser mais amplamente discutido e compreendido no início do século XX. Joseph Schumpeter, um economista austríaco, foi uma das figuras-chave que destacou a inovação tecnológica como um motor para o crescimento econômico. Após a Primeira Guerra Mundial, pensadores como Thorstein Veblen e Herbert Hoover também enfatizaram a importância da inovação tecnológica para a segurança nacional e competitividade industrial. Esse conceito se expandiu particularmente após a Segunda Guerra Mundial, quando a inovação tecnológica começou a ser vista como crucial para a prosperidade industrial e a segurança militar, especialmente nos Estados Unidos. fonte: Encyclopedia.com sobre Inovação Tecnológica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bases filosóficas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grandes filósofos durante toda história tem se oposto ao totalitarismo. Pelo menos a partir da visão Aristotélica as formas degeneradas de governar já são apontadas. Dentre as chamadas formas degeneradas, a tirania pode ser entendida como a pior forma de sistema político, sendo o resultado da maior degeneração do poder centralizado em uma única pessoa (como em uma monarquia absolutista). Na tirania o tirano governa sem consultar a ninguém e, geralmente, sua tomada de poder é ilegítima, e a lei não tem papel algum. Na oligarquia, onde o poder de decisão do governo é constituído por um pequeno número de privilegiados, há uma subclassificação quanto ao número dos donos do poder, como por exemplo, na Politirania onde os oligarcas governam hereditariamente e na riqueza, mas respeitando mais a lei. Ou nas oligarquias com uma maior percentagem de oligarcas, quando passa-se da hereditariedade à nomeação dos amigos do governo, independentemente da linhagem sanguínea destes, dentre outros tipos de oligarquia. \n",
    "\n",
    "Tomas de Aquino foi um filósofo de referência no papel da moral como orientação maior para a sociedade. Mais recentemente, Karl Popper foi um severo crítico ao totalitarismo e utopias demagógicas. São perceptíveis interseções e diálogos filosóficos entre as ideias centrais dessas filosofias, especialmente em epistemologia e ética, têm sido tema de discussão acadêmica. Por exemplo, na dissertação de David Gregory Broderick, intitulada \"Objetividade: Tomás de Aquino e Karl Popper\", explora-se a relação entre as noções de objetividade nas obras de Aquino e Popper. A dissertação sugere que, embora haja diferenças terminológicas, históricas e de interesses entre Aquino e Popper, também existem paralelos significativos em suas abordagens sobre objetividade e o crescimento do conhecimento.\n",
    "\n",
    "Além disso, na discussão sobre as raízes éticas da epistemologia de Popper, explorada pelo Grupo Ciencia, Razón y Fe (CRYF) da Universidad de Navarra, destaca-se a complementaridade das posições de Aquino e Popper. Esta análise sugere que a forte defesa de Popper do realismo, da verdade objetiva e da metodologia para o crescimento do conhecimento conjectural pode ser vista como complementar, ou até mesmo fundamentada, nos princípios éticos e metafísicos delineados por Aquino.\n",
    "\n",
    "Essas análises sugerem que, embora Popper possa não ter citado explicitamente Aquino em suas principais obras, as bases filosóficas de seus pensamentos, especialmente em relação à objetividade, ética e crescimento do conhecimento, têm grandes paralelos e áreas de convergência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, tal como acontece com a sua defesa das eleições numa democracia, o argumento de Popper a favor da engenharia social fragmentada baseia-se principalmente na sua compatibilidade com o método de tentativa e erro das ciências naturais: uma teoria é proposta e testada, erros na teoria são detectados e eliminado, e uma teoria nova e melhorada emerge, reiniciando o ciclo. Através da engenharia gradual, o processo de progresso social é, portanto, paralelo ao progresso científico. Na verdade, Popper diz que a engenharia social fragmentada é a única abordagem à política pública que pode ser genuinamente científica: \"Isto - e nenhum planeamento utópico ou profecia histórica - significaria a introdução do método científico na política, uma vez que todo o segredo do método científico é uma disposição para aprender com os erros\" ( Open Society Vol 1., 163)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodologia da Pesquisa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma contribuição seminal para abordagem falseável da ciência é a descrita por Karl Popper em sua obra \"A Lógica da Pesquisa Científica\", publicada originalmente em 1934. É nesta obra que Popper argumenta que a ciência deve adotar uma metodologia baseada na falseabilidade. Segundo ele, nenhuma quantidade de experimentos pode provar uma teoria, mas um único experimento ou observação reproduzível pode refutá-la. Esta abordagem destaca a importância da capacidade de uma teoria ser testada e potencialmente falsificada, em vez de apenas verificada. Popper diferencia as teorias científicas das pseudociências e da metafísica, salientando que as teorias científicas devem ser testáveis e passíveis de refutação, enquanto pseudociências e metafísicas não permitem essa possibilidade. Popper enfatiza que a ciência deve estar em constante evolução, com as hipóteses sendo submetidas a testes contínuos para acompanhar o desenvolvimento da ciência e das tecnologias (POPPER, 1934).\n",
    "\n",
    "A gênese de uma boa questão de pesquisa também é abordada por outros autores notáveis no campo da metodologia científica. Antonio Carlos Gil descreve que a elaboração de projetos de pesquisa deve ser guiada pela apresentação clara e acessível dos elementos necessários para a pesquisa, além da organização de conhecimentos dispersos. Gil enfatiza a natureza prática da pesquisa, abordando a importância de esclarecer os procedimentos para a elaboração de projetos em diversos tipos de pesquisa. (GIL, 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 01: Redigir uma boa questão de pesquisa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como chegar à uma boa questão de pesquisa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns dos critérios essenciais para avaliar a qualidade de uma questão de pesquisa, e como integrar ou considerar cada um deles no processo de formulação:\n",
    "\n",
    "<b>Clareza e Especificidade</b>: Uma boa questão de pesquisa deve ser clara e específica, evitando ambiguidades. Isto pode ser parcialmente garantido pelo processamento de linguagem natural, mas também requer revisão humana para garantir que a questão seja compreensível e precisamente focada.\n",
    "\n",
    "<b>Relevância Acadêmica ou Científica</b>: A questão deve ser relevante para o campo de estudo e contribuir de alguma forma para o conhecimento existente. Isso geralmente requer uma compreensão do contexto acadêmico e das pesquisas atuais, o que pode ser além do escopo da automação completa.\n",
    "\n",
    "<b>Viabilidade</b>: A pergunta deve ser algo que pode ser realisticamente respondido através de métodos de pesquisa disponíveis. Este aspecto pode ser parcialmente verificado por meio de regras heurísticas programadas, mas frequentemente requer avaliação humana, especialmente para julgar a disponibilidade de dados ou recursos de pesquisa.\n",
    "\n",
    "<b>Originalidade</b>: Uma boa questão de pesquisa deve oferecer novas perspectivas ou abordar lacunas existentes na literatura. A originalidade pode ser difícil de avaliar automaticamente, mas técnicas avançadas de NLP, como análise semântica e comparação com bancos de dados de literatura existente, podem ajudar.\n",
    "\n",
    "<b>Importância Prática ou Teórica</b>: A questão deve ter alguma importância prática ou contribuir para a compreensão teórica de um tópico. Isso geralmente exige conhecimento especializado na área de estudo para avaliar.\n",
    "\n",
    "<b>Estruturação Adequada</b>: A pergunta deve ser estruturada de forma a facilitar uma abordagem de pesquisa clara. Isso inclui a utilização de uma formulação que se alinhe com métodos de pesquisa qualitativos ou quantitativos, conforme apropriado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como Integrar os critérios da questão de pesquisa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerar uma boa questão de pesquisa depende de uma combinação de processamento automatizado, conhecimento especializado e revisão humana. A automação pode fornecer uma base útil, mas a supervisão e o julgamento humanos são cruciais para garantir a qualidade final da pergunta de pesquisa. As seguintes estratégias são utilizadas em nossa solução para chegar a uma boa questão de pesquisa:\n",
    "\n",
    "<b>Automatização com Revisão Humana</b>: Uma abordagem prática é usar a automação para gerar uma primeira versão da pergunta, que é então revisada e refinada por pesquisadores humanos (pesquisador principal, equipe de pesquisa e orientador). A automação garante que certos critérios básicos sejam atendidos (como clareza e estruturação), enquanto a revisão humana aborda aspectos mais sutis, como relevância, viabilidade e originalidade.\n",
    "\n",
    "<b>Feedback Interativo</b>: Incorporar um sistema de feedback no processo, onde o usuário pode refinar suas ideias iniciais ou ajustar a pergunta gerada, pode ajudar a melhorar a qualidade da questão de pesquisa final.\n",
    "\n",
    "<b>Integração de vários Bancos de Dados de Literatura</b>: Integrar um banco de dados de literatura existente pode ajudar a avaliar a originalidade e a relevância da pergunta, comparando-a com pesquisas já publicadas.\n",
    "\n",
    "<b>Educação do Usuário</b>: Fornecer orientações e exemplos de boas perguntas de pesquisa aos usuários pode ajudá-los a formular ideias iniciais mais eficazes, levando a melhores resultados na formulação automática.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bases de dados de artigos em Open Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao focar a pesquisa em fontes que oferecem conteúdos completos em Open Access, há bases de dados e repositórios acadêmicos importantes que se destacam em relevância e quantidade de conteúdos:\n",
    "\n",
    "<b>Directory of Open Access Journals (DOAJ)</b>: O DOAJ é um diretório online que indexa e fornece acesso a periódicos de alta qualidade, todos de acesso livre e revisados por pares. É uma excelente fonte para pesquisar artigos em uma ampla gama de disciplinas. O DOAJ oferece uma API para acessar seu índice de periódicos e artigos. A documentação e detalhes sobre a API estão disponíveis em DOAJ API.\n",
    "\n",
    "<b>PubMed Central</b>: Operado pela Biblioteca Nacional de Medicina dos EUA, o PubMed Central é um repositório gratuito de artigos de ciências biomédicas e ciências da vida. Embora seu foco seja mais na área da saúde, ele pode ter artigos relevantes sobre inovação tecnológica no contexto da saúde. A PubMed Central oferece uma API chamada Entrez Programming Utilities (E-utilities) para interagir com a base de dados. Mais informações podem ser encontradas em Entrez Programming Utilities Help.\n",
    "\n",
    "<b>arXiv</b>: O arXiv é um repositório de preprints em campos como física, matemática, ciência da computação, biologia quantitativa, finanças quantitativas e estatística. É uma boa fonte para literatura mais técnica e teórica sobre inovação tecnológica. O arXiv fornece uma API para acesso aos seus preprints. Informações detalhadas e documentação sobre a API estão disponíveis em arXiv API.\n",
    "\n",
    "<b>OpenAIRE</b>: Uma infraestrutura que promove a descoberta e o acesso a publicações científicas europeias de acesso livre. É especialmente útil para pesquisas que envolvem colaborações europeias ou focam em políticas e práticas de inovação na Europa. OpenAIRE oferece uma API para acessar seu repositório. Você pode encontrar mais informações sobre como usar esta API em OpenAIRE API.\n",
    "\n",
    "<b>Google Scholar</b>: Apesar de não ser exclusivamente dedicado ao Open Access, o Google Scholar pode ser utilizado para localizar artigos de acesso livre. Ele indexa uma variedade de fontes acadêmicas e muitas vezes inclui links para versões de acesso livre dos artigos. O Google Scholar não oferece uma API oficial para acesso programático.\n",
    "\n",
    "<b>CORE</b>: Agregador que permite o acesso a milhões de artigos de acesso livre. Ele reúne conteúdo de repositórios e periódicos de todo o mundo, sendo uma excelente ferramenta para uma pesquisa abrangente. CORE oferece uma API que permite acessar seu vasto repositório de artigos de acesso livre. A documentação da API pode ser encontrada em CORE API.\n",
    "\n",
    "<b>ScienceOpen</b>: Plataforma de pesquisa e publicação que oferece acesso a mais de 60 milhões de artigos e registros de pesquisa em todas as áreas. Não há informações disponíveis sobre uma API pública para o ScienceOpen.\n",
    "\n",
    "<b>SSRN (Social Science Research Network)</b>: Especializado em ciências sociais, o SSRN é um repositório de preprints que abrange uma ampla gama de áreas, incluindo economia, direito e gestão corporativa, onde você pode encontrar trabalhos relacionados à gestão da inovação. O SSRN não fornece uma API pública para acesso programático aos seus conteúdos.\n",
    "\n",
    "\n",
    "O tipo de busca mais comum é por palavras-chave, usadas para descobrir e para refinar a pesquisa e localizar artigos relevantes sobre os temas de interesse, no nosso caso, a gestão da inovação tecnológica em organizações públicas e privadas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editoras científicas com políticas de Open Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Várias editoras científicas proeminentes oferecem conteúdos em Open Access, proporcionando acesso livre a uma vasta gama de pesquisas acadêmicas. Algumas das principais editoras incluem:\n",
    "\n",
    "<b>Springer Nature</b>: Com mais de 600 periódicos totalmente em acesso aberto e mais de 1000 livros em acesso aberto, a Springer Nature é uma das pioneiras no campo da pesquisa aberta. Oferece uma variedade de opções de publicação em acesso aberto, mantendo rigorosos processos de revisão por pares e editoriais.\n",
    "\n",
    "<b>Oxford University Press (OUP)</b>: A OUP publica mais de 120 periódicos totalmente em acesso aberto e mais de 250 livros em acesso aberto, abrangendo uma ampla gama de disciplinas. Muitos de seus periódicos são classificados como \"diamond OA\", o que significa que não há taxas de processamento de artigos para autores ou leitores.\n",
    "\n",
    "<b>Frontiers</b>: Reconhecida como uma editora líder em Acesso Aberto e plataforma de Ciência Aberta, a Frontiers é muito citada, com mais de um bilhão de visualizações e downloads e 1.6 milhão de citações em seus artigos acessíveis gratuitamente. Ela é ativa em áreas como neurociências, psiquiatria, fisiologia, medicina clínica, ciências naturais e engenharia.\n",
    "\n",
    "<b>Wiley</b>: A Wiley oferece mais de 150 periódicos revisados por pares em acesso aberto, abrangendo diversas disciplinas de pesquisa. Seus periódicos em acesso aberto estão disponíveis para leitura, download e compartilhamento gratuitamente através da Wiley Online Library e do PubMed Central.\n",
    "\n",
    "<b>Public Library of Science (PLOS)</b>: Fundada como uma organização sem fins lucrativos, a PLOS tem como objetivo catalisar o movimento de acesso aberto. Publica periódicos em diversas áreas, incluindo medicina e ciências da vida.\n",
    "\n",
    "<b>Hindawi</b>: Inicialmente uma editora de periódicos por assinatura, a Hindawi fez a transição para um modelo de publicação em acesso aberto entre 2004 e 2007. Ela publica periódicos em ciência e engenharia, ciências sociais e filosofia.\n",
    "\n",
    "<b>MDPI AG</b>: Uma empresa suíça com presença global, a MDPI AG publica periódicos em ciência e engenharia, ciências sociais e filosofia.\n",
    "\n",
    "<b>Informa PLC</b>: Uma das maiores editoras nas humanidades e ciências sociais, a Informa publica em acesso aberto sob quatro selos: Taylor & Francis Open, Dove Medical Press, Cogent OA e Routledge Open.\n",
    "\n",
    "Estas editoras são conhecidas não apenas pela qualidade e diversidade de suas publicações, mas também por suas contribuições significativas para o movimento de acesso aberto na comunidade acadêmica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagramas de blocos da aplicação completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mermaid.live/\n",
    "\n",
    "graph LR\n",
    "    subgraph Entrar e Pré-processar Dados\n",
    "        A[Currículos Pesquisadores\\nda ICT]\n",
    "        B[Entidades do\\nCEIS]\n",
    "        C[Produtos Estratégicos\\nCEIS]\n",
    "        D[Normalizar,\\nLimpar e\\nUnificar Termos]\n",
    "    end\n",
    "\n",
    "    subgraph Gerar Embeddings em GPU\n",
    "        H[Embedding Multilingue\\n Sentence Transformers]\n",
    "        I[Codificar\\nCompetências]\n",
    "        J[Codificar\\nÁreas de Pesquisa]\n",
    "    end\n",
    "\n",
    "    subgraph Construir e Analisar Grafos em GPU/CPU\n",
    "        K[Adicionar\\nNós e Arestas]\n",
    "        L[Grafo Multiplex\\nMacroprocessos PDI]\n",
    "    end\n",
    "\n",
    "    subgraph Modelo de Aprendizagem\\nNão-Supervisionada em Grafos    \n",
    "        M[Detectar\\nComunidades]\n",
    "        N[Analisar\\nSimilaridade]\n",
    "        O[Identificar Lacunas\\nao maximizar \\nModularidade]\n",
    "    end\n",
    "\n",
    "    subgraph Recomendar e Visualizar\n",
    "        P[Recomendar\\nAlinhamento\\nCompetências/Produtos]\n",
    "        Q[Visualizar\\nResultados]\n",
    "    end\n",
    "\n",
    "    A --> D\n",
    "    B --> D\n",
    "    C --> D\n",
    "    D --> H\n",
    "    H --> I\n",
    "    H --> J\n",
    "    I --> K\n",
    "    J --> K\n",
    "    N <--> M\n",
    "    L --> M\n",
    "    L --> N    \n",
    "    M --> O\n",
    "    N --> O\n",
    "    K --> L\n",
    "    O --> P\n",
    "    P --> Q"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://mermaid.live/\n",
    "\n",
    "mindmap\n",
    "  root((Modelagem do Mundo Real em Grafos para Aplicações em Saúde))\n",
    "    Grafos Simples\n",
    "      Aplicações\n",
    "        Interações Medicamento-Medicamento (Efeitos Adversos)\n",
    "        Co-ocorrência de Doenças (Comorbidades)\n",
    "      Limitações\n",
    "        Não modela a direção ou força das relações\n",
    "    Grafos Direcionados\n",
    "      Aplicações\n",
    "        Propagação de Doenças Infecciosas (Transmissão)\n",
    "        Vias Metabólicas (Reações Bioquímicas)\n",
    "      Limitações\n",
    "        Não modela múltiplas relações entre os mesmos elementos\n",
    "    Grafos Ponderados\n",
    "      Aplicações\n",
    "        Redes de Interação Proteína-Proteína (Afinidade de Ligação)\n",
    "        Redes de Coexpressão Gênica (Correlação)\n",
    "      Limitações\n",
    "        Não modela diferentes tipos de relações simultaneamente\n",
    "    Grafos Multicamadas (Multiplex)\n",
    "      Aplicações\n",
    "        Redes de Interação Fármaco-Alvo-Doença (Múltiplos Mecanismos)\n",
    "        Redes de Microbioma Humano (Diferentes Nichos Tecnológicos)\n",
    "      Limitações\n",
    "        Aumenta a complexidade da análise\n",
    "    Hipergrafos\n",
    "      Aplicações\n",
    "        Análise de Dados de Saúde Multimodais (Prontuários, Imagens, Genética)\n",
    "        Modelagem de Fatores de Risco Complexos para Doenças Multifatoriais\n",
    "      Limitações\n",
    "        Dificuldade de visualização e análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIMPAR CONDA TOTALMENTE PARA LIBERAR ESPAÇO EM DISCO, USAR COM CAUTELA PODE APAGAR AMBIENTES INATIVOS\n",
    "# import subprocess\n",
    "\n",
    "# # Executa o comando e captura a saída\n",
    "# process = subprocess.Popen(['conda', 'clean', '--all'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# # Lê a saída e verifica se há uma linha indicando arquivos a serem removidos\n",
    "# for line in process.stdout:\n",
    "#     print(line, end='')  # Imprime a saída na célula do notebook\n",
    "#     if 'tarball(s)' in line:  # Procura pela linha com a informação dos arquivos\n",
    "#         # Solicita a confirmação do usuário\n",
    "#         confirmacao = input(\"Deseja prosseguir com a limpeza? (s/n): \")\n",
    "#         if confirmacao.lower() == 's':\n",
    "#             process.stdin.write('y\\n')  # Envia 'y' para confirmar\n",
    "#         else:\n",
    "#             process.stdin.write('n\\n')  # Envia 'n' para cancelar\n",
    "#         process.stdin.flush()\n",
    "\n",
    "# # Aguarda o término do processo e imprime o resultado\n",
    "# stdout, stderr = process.communicate()\n",
    "# print(stdout)\n",
    "# print(stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Criar ambiente virtual e instalar pacotes</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Procedimento normal de instalação em Windows\n",
    "## Criar novo ambiente virtual a partir de janela do jupyter notebook\n",
    "# !conda create -n pytorch_env python=3.11\n",
    "# !conda activate pytorch_env\n",
    "\n",
    "### Configurar as Variáveis de Ambiente em Windows:\n",
    "\n",
    "## Para configurar ambiente do conda corretamente no seu sistema. O caminho para o executável do conda (conda.exe) deve estar incluído na variável de ambiente PATH.\n",
    "\n",
    "# No Painel de Controle do Windows:\n",
    "# Abra o menu Iniciar e procure por \"variáveis de ambiente\".\n",
    "# Clique em \"Editar as variáveis de ambiente do sistema\".\n",
    "# Na aba \"Avançado\", clique em \"Variáveis de Ambiente\".\n",
    "\n",
    "## Em \"Variáveis do sistema\", encontre a variável \"Path\" e clique em \"Editar\".\n",
    "# Adicione os seguintes caminhos, se ainda não estiverem presentes (substitua \"seu_usuario\" pelo seu nome de usuário):\n",
    "# C:\\Users\\seu_usuario\\anaconda3\n",
    "# C:\\Users\\seu_usuario\\anaconda3\\Scripts\n",
    "# C:\\Users\\seu_usuario\\anaconda3\\Library\\bin\n",
    "\n",
    "# !conda init\n",
    "\n",
    "## A partir da janela do terminal\n",
    "# Atualizar o anaconda para sua versão mais recente\n",
    "# conda update -n base -c defaults conda\n",
    "# conda update -n base -c defaults conda --force-reinstall\n",
    "\n",
    "# Se precisar de uma versão específica basta determincar, por exemplo para 24.5.0\n",
    "# conda install conda=24.5.0\n",
    "\n",
    "# Para interagir diretamente no PowerShell é preciso ter permissão para isso, verificar no terminal com\n",
    "# Get-ExecutionPolicy\n",
    "\n",
    "# Se a política for Restricted, altere-a para RemoteSigned ou Unrestricted com o seguinte comando:\n",
    "# Set-ExecutionPolicy RemoteSigned -Scope CurrentUser\n",
    "\n",
    "# Usando direto no terminal para ativar o ambiente\n",
    "# conda activate pytorch_env\n",
    "# conda deactivate pytorch_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se o compulador CUDA está instalado\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda config --show channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intalar o RAPIDS de acordo com o mais atualizado em https://docs.rapids.ai/install ou https://docs.rapids.ai/install#wsl2\n",
    "# %conda create -n rapids-24.08 -c rapidsai-nightly -c conda-forge -c nvidia rapids=24.08 python=3.11 cuda-version=12.5\n",
    "# !conda install -c rapidsai -c conda-forge -c nvidia cuda-version=12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instalar e verificar intalação do RAPIDS NVIDIA\n",
    "# %conda install rapids\n",
    "!conda list rapids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instalações pontuais de pacotes extra requirements.txt\n",
    "# !python.exe -m pip install --upgrade pip\n",
    "# !pip install -r requirements.txt\n",
    "# !pip install requests beautifulsoup4 selenium pandas gitpython\n",
    "\n",
    "### Complementar instalação de modelos multilingues no SpaCy\n",
    "## Baixar modelos de de linguagem específica do SpaCy\n",
    "# !python -m spacy download pt_core_news_lg\n",
    "\n",
    "## Verificar modelos e versões instaladas no SpaCy\n",
    "# !python -m spacy validate\n",
    "\n",
    "# %pip install --upgrade ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "print(cudf.Series([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cudf\n",
    "# import cuml\n",
    "# import cugraph\n",
    "# # ... (use os módulos para realizar alguma operação)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Configurar drivers CUDA e PyTorch local</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalar no sistema operacional PyTorch e CUDA Toolkit\n",
    "\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "\n",
    "O cuDNN é uma biblioteca de rotinas otimizadas para Deep Learning, e é essencial para o PyTorch aproveitar ao máximo o potencial da sua GPU. \n",
    "\n",
    "Instalar a biblioteca cuDNN clicando no botão \"Download cuDNN Library\" de acordo com seu sistema operacional Windows ou Linux, Arquitetura x86_64, Versão Tarball e CUDA Version 12 \n",
    "\n",
    "https://developer.nvidia.com/cudnn\n",
    "\n",
    "Ou fazer o download com o comando:\n",
    "\n",
    "    wget https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/windows-x86_64/cudnn-windows-x86_64-9.2.1.18_cuda12-archive.zip\n",
    "\n",
    "Consultar compatibilidades em: \n",
    "\n",
    "https://docs.nvidia.com/deeplearning/cudnn/latest/reference/support-matrix.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar o cuDNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar o cuDNN: Acesse a página de download do cuDNN no site da NVIDIA: https://developer.nvidia.com/cudnn\n",
    "\n",
    "1. Criar Conta (se necessário): Você precisará de uma conta de desenvolvedor da NVIDIA para fazer o download. Crie uma conta gratuitamente, se ainda não tiver uma.\n",
    "\n",
    "2. Escolher a versão do cuDNN que é compatível com o seu CUDA Toolkit 12.3. A página de download da NVIDIA fornecerá as opções corretas.\n",
    "\n",
    "3. Baixar o arquivo zip do cuDNN e extraia-o em um local de sua preferência.\n",
    "\n",
    "4. Copiar os seguintes arquivos da pasta extraída para a pasta de instalação do CUDA Toolkit:\n",
    "\n",
    "DLL:\n",
    "\n",
    "    Copie cudnn_cnn_infer64_8.dll (ou o nome do arquivo DLL correspondente à sua versão do cuDNN) para C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\bin.\n",
    "\n",
    "Lib:\n",
    "\n",
    "    Copie cudnn_adv_infer64_8.lib (ou arquivo .lib correspondente) para C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\lib\\x64.\n",
    "    Copie cudnn_adv_train64_8.lib (ou arquivo .lib correspondente) para C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\lib\\x64.\n",
    "    Copie cudnn_cnn_infer64_8.lib (ou arquivo .lib correspondente) para C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\lib\\x64.\n",
    "    Copie cudnn_cnn_train64_8.lib (ou arquivo .lib correspondente) para C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\lib\\x64.\n",
    "\n",
    "Include:\n",
    "    Copie cudnn.h para C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\include.\n",
    "\n",
    "5. Verificar a Instalação:\n",
    "\n",
    "Após copiar os arquivos, você pode verificar se o cuDNN foi instalado corretamente executando o seguinte código Python:\n",
    "\n",
    "    import torch\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(torch.backends.cudnn.version())\n",
    "    else:\n",
    "        print(\"CUDA não está disponível.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.backends.cudnn.version())\n",
    "else:\n",
    "    print(\"CUDA não está disponível.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Deve retornar True se o CUDA estiver disponível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Em caso de necessidae de upgrade ou reinstalação do PyTorch\n",
    "# !pip3 install --upgrade torchvision\n",
    "\n",
    "### Em caso de conflitos entre pacotes que impedem instalação\n",
    "## Exemplo: Instalar xformers com pip pode dar conflito com a bilioteca TBB\n",
    "!pip show xformers\n",
    "!pip show TBB\n",
    "# !pip uninstall TBB # Se não funcionar deletar manualmente a biblioteca será reinstalada com xformers\n",
    "# !pip install xformers\n",
    "\n",
    "### Outras resoluções de conflito entre pacotes\n",
    "## 1. Limpar cache e atualizar versão do pip\n",
    "# !pip cache purge\n",
    "# !python.exe -m pip install --upgrade pip\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "## 2. Em caso de conflitos persistentes forçar atualização ou reinstalação de pacotes\n",
    "# !pip install --upgrade bottleneck\n",
    "# !python.exe -m pip install --upgrade --force-reinstall omegaconf\n",
    "\n",
    "## 3. Instalar xformers com conda pode gerenciar melhor conflitos\n",
    "# !conda config --add channels pytorch\n",
    "# !conda update -n base -c defaults conda\n",
    "# !conda install -c conda-forge xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copiar arquivos contidos no arquivo zip do cuDNN baixado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da pasta bin do cuDNN:\n",
    "\n",
    "    cudnn_adv_infer64_9.dll\n",
    "    cudnn_cnn_infer64_9.dll\n",
    "\n",
    "Copiar para:\n",
    "\n",
    "    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\bin\n",
    "\n",
    "Da pasta include do cuDNN:\n",
    "\n",
    "    cudnn.h\n",
    "\n",
    "Copiar para:\n",
    "\n",
    "    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\include\n",
    "\n",
    "Da pasta lib/x64 do cuDNN:\n",
    "\n",
    "    Copie todos os arquivos .lib da pasta x64 do cuDNN para a pasta lib/x64 da sua instalação do CUDA:\n",
    "\n",
    "Copiar para:\n",
    "\n",
    "    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\lib\\x64\n",
    "\n",
    "Substitua v12.3 pela versão do CUDA que você tem instalada, se for diferente.\n",
    "\n",
    "Se a pasta lib/x64 do CUDA já contiver arquivos com o mesmo nome, você pode substituí-los pelos arquivos do cuDNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# cuda_bin_dir = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\bin\"\n",
    "# cuda_inc_dir = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\include\"\n",
    "# cuda_lib_dir = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\lib\\x64\"\n",
    "\n",
    "# arquivos_na_pasta = os.listdir(cuda_inc_dir)\n",
    "\n",
    "# for i in arquivos_na_pasta:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instalar e testar biblioteca de gráficos Altair e Vega Datasets\n",
    "# !pip3 install altair vega_datasets\n",
    "# !pip3 list\n",
    "# import altair as alt\n",
    "# from vega_datasets import data\n",
    "\n",
    "# # Criar um gráfico de exemplo\n",
    "# chart = alt.Chart(data.cars()).mark_point().encode(\n",
    "#     x='Horsepower',\n",
    "#     y='Miles_per_Gallon',\n",
    "#     tooltip=['Horsepower', 'Miles_per_Gallon']\n",
    "# ).interactive()\n",
    "\n",
    "# chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Princípios de otimização da execução</b>\n",
    "\n",
    "As operações predominantes na conversão de embeddings dependem do modelo e da biblioteca utilizada, mas geralmente envolvem,dentre outras, Operação com Matrizes e Funções de Ativação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operações de Matrizes:\n",
    "\n",
    "**Multiplicação de Matrizes:** A operação mais comum em modelos de transformadores (como os utilizados pelo Sentence Transformers) é a multiplicação de matrizes, usada para calcular as atenções e as transformações lineares nas camadas do modelo.\n",
    "\n",
    "**Soma de Matrizes:** Soma de matrizes é utilizada para combinar os resultados de diferentes camadas ou para adicionar informações contextuais aos embeddings.\n",
    "\n",
    "**Normalização:** A normalização de vetores (por exemplo, Layer Normalization) é utilizada para estabilizar o treinamento e melhorar o desempenho do modelo.\n",
    "Outras Operações: Outras operações de matrizes, como transposição, concatenação e divisão por elemento, também podem ser utilizadas em diferentes etapas da conversão de embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de Ativação:\n",
    "\n",
    "**ReLU, GELU, etc.:** Funções de ativação não lineares são aplicadas aos resultados das operações de matrizes para introduzir não linearidade no modelo e permitir que ele aprenda padrões mais complexos nos dados.\n",
    "\n",
    "### Operações Específicas do Modelo:\n",
    "\n",
    "**Softmax:** Utilizado para calcular as probabilidades de atenção em modelos de transformadores.\n",
    "\n",
    "**Embedding Lookup:** Utilizado para converter tokens de texto em embeddings de palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerações sobre o Hardware:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU:** A maioria das operações mencionadas acima é altamente paralelizável e pode ser acelerada significativamente por GPUs, que são projetadas para realizar cálculos de matrizes de forma eficiente.\n",
    "\n",
    "**CPU:** As CPUs podem ser usadas para realizar essas operações, mas geralmente são mais lentas do que as GPUs, especialmente para grandes volumes de dados ou modelos complexos.\n",
    "\n",
    "Portanto, a conversão de embeddings é um processo computacionalmente intensivo, onde a GPU pode acelerar significativamente esse processo, especialmente para grandes modelos e volumes de dados devido principalmente a necessidade de **operações de matrizes** e **funções de ativação**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vetorização com Sentence Transformers\n",
    "\n",
    "No modelo Sentence Transformers, a conversão de embeddings envolve a passagem da entrada de texto por um modelo de transformador pré-treinado, que realiza as operações mencionadas acima em várias camadas. O resultado final é um vetor numérico (embedding) que representa o significado da frase de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquiteturas das Placas Nvidia\n",
    "\n",
    "Em geral, cada Streaming Multiprocessor (SM) de uma GPU Nvidia contém um certo número de núcleos CUDA e, em arquiteturas mais recentes, um certo número de Tensor Cores. A proporção exata entre núcleos CUDA e Tensor Cores, bem como o número de cada um por SM, varia dependendo da arquitetura da GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tesla**:        Não possuem Tensor Cores. O número de núcleos CUDA por SM varia entre as diferentes gerações da arquitetura Tesla.\n",
    "\n",
    "**Pascal**:       Não possuem Tensor Cores. Cada SM possui 128 núcleos CUDA.\n",
    "\n",
    "**Volta**:        Introduziu os Tensor Cores. Cada SM possui 64 núcleos CUDA e 8 Tensor Cores.\n",
    "\n",
    "**Turing**:       Cada SM possui 64 núcleos CUDA e 8 Tensor Cores. Introduziu os RT Cores para acelerar o ray tracing.\n",
    "\n",
    "**Ampere**:       Cada SM possui 128 núcleos CUDA e 4 Tensor Cores de terceira geração. Os Tensor Cores de terceira geração são duas vezes mais rápidos que os da geração anterior.\n",
    "\n",
    "**Ada Lovelace**: Cada SM possui 128 núcleos CUDA e 4 Tensor Cores de quarta geração. Os Tensor Cores de quarta geração são ainda mais rápidos e eficientes que os da geração anterior.\n",
    "\n",
    "**Hopper**:       Cada SM possui 128 núcleos CUDA e 8 Tensor Cores de quarta geração. Introduziu o Transformer Engine, um novo tipo de núcleo especializado em acelerar modelos de linguagem baseados em Transformer.\n",
    "\n",
    "**Blackwell**:    (arquitetura futura): Ainda não há informações oficiais sobre a arquitetura Blackwell, mas espera-se que ela continue a tendência de aumentar o número de núcleos CUDA e Tensor Cores por SM, além de introduzir novas tecnologias para acelerar ainda mais as cargas de trabalho de IA e HPC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Avaliar compatibilidade Hardware/Software</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pynvml\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from git import Repo\n",
    "from tqdm.notebook import tqdm # Importando tqdm do notebook\n",
    "from sentence_transformers import SentenceTransformer\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from competence_extraction import HardwareEvaluator, ProcessingCapacityEstimator\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "\n",
    "# !pip3 install py-cpuinfo\n",
    "hardware_evaluator = HardwareEvaluator()\n",
    "hardware_evaluator.print_hardware_info()\n",
    "\n",
    "# Estimativas de throughput\n",
    "estimator = ProcessingCapacityEstimator(hardware_evaluator)\n",
    "num_samples = 1000\n",
    "instructions_per_sample = 100\n",
    "cpu_throughput = estimator.estimate_cpu_throughput(num_samples, instructions_per_sample)\n",
    "cpu_parallel_throughput = estimator.estimate_cpu_parallel_throughput(num_samples, instructions_per_sample)\n",
    "num_operations = 1000 * 1000\n",
    "gpu_parallel_throughput = estimator.estimate_gpu_parallel_throughput(num_operations, \"FLOPS\")\n",
    "\n",
    "print(f\"\\nThroughput teórico single-threading estimado da CPU: {int(cpu_throughput)//1000000000:4} bilhões de operações/s\")\n",
    "print(f\"Throughput teórico multi-threadings estimado da CPU: {int(cpu_parallel_throughput)//1000000000:4} bilhões de operações/s\")\n",
    "print(f\"Throughput teórico multi-threadings estimado da GPU: {int(gpu_parallel_throughput)//1000000000:4} bilhões de operações/s\")\n",
    "\n",
    "# Verificação de compatibilidade PyTorch-GPU\n",
    "hardware_evaluator.check_pytorch_gpu_compatibility()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verificação da saúde da memória da GPU\n",
    "# hardware_evaluator.check_gpu_memory_health()\n",
    "\n",
    "## Estimativa de overhead CPU-GPU (requer modelo, batch_size e dados de entrada)\n",
    "# overhead = hardware_evaluator.estimate_cpu_gpu_overhead(model, batch_size, input_data)\n",
    "\n",
    "## Medição do tempo de execução real (requer modelo, batch_size e dados de entrada)\n",
    "# real_execution_time = hardware_evaluator.measure_real_execution_time(model, batch_size, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Iniciar execução</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pynvml\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def recommend_batch_size(model, X, evaluator, estimator, batch_sizes=[8, 16, 32, 64, 128]):\n",
    "    results = {}\n",
    "    gpu_memory_usage = []\n",
    "    gpu_utilization = []\n",
    "\n",
    "    # Inicializa o pynvml para monitorar a GPU\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "    for batch_size in tqdm(batch_sizes, desc=f\"Modelo: {model}\"):\n",
    "        print(f\"\\nBenchmarking e interpretação para batch size = {batch_size}\")\n",
    "\n",
    "        # Benchmark na CPU\n",
    "        cpu_time = evaluator.benchmark_model(model, X, 'cpu', batch_size=batch_size)\n",
    "\n",
    "        # Benchmark na GPU (se disponível) e transferência de dados\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            try:\n",
    "                with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "                    gpu_time = evaluator.benchmark_model(model, X, device, batch_size=batch_size)\n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA out of memory\" in str(e):\n",
    "                    print(f\"Erro de memória da GPU para o modelo {model} com batch size {batch_size}.\")\n",
    "                    gpu_time = float('inf')\n",
    "                    transfer_results = {batch_size: {'cpu_to_gpu': 0, 'gpu_to_cpu': 0}}\n",
    "                else:\n",
    "                    # Plotar o trace até o momento do erro\n",
    "                    print(f\"Erro ao processar o modelo {model} com batch size {batch_size}: {e}\")\n",
    "                    print(\"Trace até o momento do erro:\")\n",
    "                    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "                    raise e  # Repassar o erro após exibir o trace\n",
    "\n",
    "            # Extrair dados de tracing do profiler\n",
    "            trace_events = prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10)\n",
    "            print(trace_events)\n",
    "            # Gerar gráfico de tracing (exemplo)\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(trace_events.columns[0], trace_events.columns[1])\n",
    "            plt.xlabel(\"Evento\")\n",
    "            plt.ylabel(\"Tempo (ms)\")\n",
    "            plt.title(f\"Tracing da GPU para {model_name} (batch size={batch_size})\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            gpu_time = None\n",
    "            transfer_results = {batch_size: {'cpu_to_gpu': 0, 'gpu_to_cpu': 0}}\n",
    "\n",
    "        # Monitoramento da GPU\n",
    "        gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)  # Em MB\n",
    "        gpu_utilization.append(pynvml.nvmlDeviceGetUtilizationRates(handle).gpu)\n",
    "\n",
    "        # Cálculo do tempo total (incluindo transferência)\n",
    "        total_cpu_time = cpu_time\n",
    "        total_gpu_time = gpu_time + transfer_results[batch_size]['cpu_to_gpu'] + transfer_results[batch_size]['gpu_to_cpu'] if gpu_time is not None else float('inf')\n",
    "\n",
    "        best_device = \"gpu\" if total_gpu_time < total_cpu_time else \"cpu\"\n",
    "        best_time = min(total_cpu_time, total_gpu_time)\n",
    "\n",
    "        results[batch_size] = {\n",
    "            'cpu_time': cpu_time,\n",
    "            'gpu_time': gpu_time,\n",
    "            'transfer_time': transfer_results[batch_size]['cpu_to_gpu'] + transfer_results[batch_size]['gpu_to_cpu'],\n",
    "            'total_cpu_time': total_cpu_time,\n",
    "            'total_gpu_time': total_gpu_time,\n",
    "            'best_device': best_device,\n",
    "            'best_time': best_time,\n",
    "        }\n",
    "\n",
    "    # Finaliza o pynvml\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "    # Criando um DataFrame com os resultados\n",
    "    df = pd.DataFrame.from_dict(results, orient='index')\n",
    "    df['batch_size'] = df.index\n",
    "\n",
    "    # Visualização com Altair\n",
    "    chart = alt.Chart(df.melt('batch_size', var_name='metric', value_name='time')).mark_line(point=True).encode(\n",
    "        x='batch_size:Q',\n",
    "        y='time:Q',\n",
    "        color='metric:N',\n",
    "        tooltip=['batch_size', 'metric', 'time']\n",
    "    ).properties(\n",
    "        title=f'Tempos de Processamento para o Modelo {model}'\n",
    "    ).interactive()\n",
    "\n",
    "    chart.save(f'tempos_processamento_{model.replace(\"/\", \"_\")}.json')\n",
    "\n",
    "    # Encontrar o melhor batch size\n",
    "    best_batch_size = df['best_time'].idxmin()\n",
    "\n",
    "    print(f\"\\nRecomendação para o modelo {model}:\")\n",
    "    print(f\"  Melhor batch size: {best_batch_size}\")\n",
    "    print(f\"  Dispositivo recomendado: {df.loc[best_batch_size, 'best_device']}\")\n",
    "    print(f\"  Tempo de processamento estimado: {df.loc[best_batch_size, 'best_time']:.4f} segundos por amostra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "from competence_extraction import HardwareEvaluator, ProcessingCapacityEstimator\n",
    "\n",
    "# Lista de modelos a serem avaliados\n",
    "model_names = [\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "    \"sentence-transformers/LaBSE\"\n",
    "]\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curricula_file = os.path.join(root_folder, filename)\n",
    "\n",
    "# Dispositivo de processamento\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instanciando as classes de avaliação\n",
    "hardware_evaluator = HardwareEvaluator()\n",
    "estimator = ProcessingCapacityEstimator(hardware_evaluator)\n",
    "classifier = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "\n",
    "# Tamanhos de lote para testar\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "\n",
    "# Realizar benchmarks e recomendações\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        # Limpar a memória da GPU antes de carregar o modelo\n",
    "        torch.cuda.empty_cache() \n",
    "\n",
    "        # Carregue o modelo com o dispositivo especificado\n",
    "        model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "        # Preparar os dados para classificação (passe o objeto 'model')\n",
    "        X, y = classifier.prepare_data_for_classification(model)  \n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(f\"Erro de memória da GPU para o modelo {model_name}. Tente reduzir o tamanho do lote ou usar um modelo menor.\")\n",
    "        else:\n",
    "            print(f\"Erro ao processar o modelo {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar Kernel CUDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A implementação do kernel CUDA para a vetorização de competências dependerá da arquitetura do modelo escolhido. No caso do paraphrase-multilingual-mpnet-base-v2, você precisará implementar um kernel que execute as seguintes etapas:\n",
    "\n",
    "Tokenização: Dividir cada frase de competência em tokens (palavras ou subpalavras).\n",
    "\n",
    "Embedding Lookup: Converter cada token em um vetor de embedding usando o modelo pré-treinado.\n",
    "\n",
    "Codificação da Sequência: Processar a sequência de embeddings usando as camadas do modelo (transformadores, camadas de pooling, etc.).\n",
    "\n",
    "Agregação (opcional): Se necessário, agregar os embeddings da sequência em um único vetor representativo da competência (por exemplo, usando a média ou o máximo dos embeddings).\n",
    "\n",
    "Armazenamento: Armazenar os embeddings resultantes na memória da GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Lista de modelos a serem avaliados\n",
    "model_names = [\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"sentence-transformers/LaBSE\",\n",
    "    \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "]\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curricula_file = os.path.join(root_folder, filename)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA não está disponível. Verifique a instalação e configuração da CUDA.\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "# Sincronizar as operações CUDA\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Limpar o cache da GPU\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "classifier = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "\n",
    "# Benchmark e interpretação para diferentes tamanhos de lote\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name, device=device)\n",
    "        X, y = classifier.prepare_data_for_classification(model)\n",
    "\n",
    "        # Recomendar o tamanho de lote ideal\n",
    "        recommend_batch_size(model_name, X, classifier, estimator, batch_sizes)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(f\"Erro de memória da GPU para o modelo {model_name}. Tente reduzir o tamanho do lote ou usar um modelo menor.\")\n",
    "        else:\n",
    "            print(f\"Erro ao processar o modelo {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O erro mais comum ao rodar modelos na GPU tende a ser \"CUDA error: an illegal memory access was encountered\" indica que a GPU ainda está encontrando problemas para processar a quantidade de dados ou a complexidade do modelo. Como queremos rodar modelos locais, com limitação de hardware, devemos focar em estratégias para otimizar o uso da GPU e contornar esse problema.\n",
    "\n",
    "Estratégias de Otimização:\n",
    "\n",
    "Redução da Dimensão dos Embeddings:\n",
    "\n",
    "Se você estiver usando modelos com dimensões de embedding muito grandes (ex: 768), tente modelos menores (ex: 128 ou 256). Isso reduzirá significativamente a quantidade de memória utilizada pela GPU.\n",
    "\n",
    "Você pode encontrar modelos menores no Hugging Face Model Hub, como \"paraphrase-MiniLM-L6-v2\" ou \"all-MiniLM-L6-v2\".\n",
    "Limpeza de Cache e Sincronização:\n",
    "\n",
    "Certifique-se de que você está limpando o cache da GPU (torch.cuda.empty_cache()) e sincronizando as operações CUDA (torch.cuda.synchronize()) antes de cada iteração do loop for model_name in model_names:.\n",
    "Processamento em CPU:\n",
    "\n",
    "Se a redução do tamanho do modelo e do lote não for suficiente, você pode tentar processar os embeddings na CPU. Isso será mais lento, mas evitará o erro de memória da GPU.\n",
    "Divisão dos Dados em Subconjuntos:\n",
    "\n",
    "Se você tiver um grande número de pesquisadores, divida os dados em subconjuntos menores e processe cada subconjunto separadamente.\n",
    "Uso de torch.utils.checkpoint:\n",
    "\n",
    "O PyTorch oferece a função torch.utils.checkpoint que permite trocar memória por tempo de computação. Ao ativar o checkpointing, o PyTorch recalculará partes do grafo computacional durante a retropropagação, em vez de armazenar todos os tensores intermediários na memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Dados do Software\")\n",
    "print(f\"  Versão do PyTorch: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"     Versão do CUDA: {torch.version.cuda}\")\n",
    "    print(f\"    Versão do cuDNN: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"    CUDA disponível: {torch.cuda.is_available()}\")\n",
    "else:\n",
    "    print(\"CUDA não disponível, não está configurado corretamente.\")\n",
    "\n",
    "print(f\"\\nDados do Compilador CUDA\")\n",
    "!nvcc --version\n",
    "\n",
    "print(f\"\\nDetalhes do PyTorch\")\n",
    "!pip3 show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDetalhes da Memória ocupada na GPU\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_HOME\"] = \"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.3\"\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Listar competências de cada currículo</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from competence_extraction import CompetenceExtraction\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "pathfilename = os.path.join(root_folder, filename)\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not os.path.exists(pathfilename):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {pathfilename}\")\n",
    "\n",
    "try:\n",
    "    with open(pathfilename, \"r\") as f:\n",
    "        curricula_data = json.load(f)\n",
    "except json.JSONDecodeError:\n",
    "    raise ValueError(f\"Erro ao decodificar o arquivo JSON: {pathfilename}\")\n",
    "\n",
    "competence_extractor = CompetenceExtraction(curricula_file=pathfilename)\n",
    "\n",
    "# Extrair e imprimir as competências de cada pesquisador\n",
    "for researcher_index, researcher_data in enumerate(curricula_data[:6]):\n",
    "    competences = competence_extractor.extract_competences(researcher_data)\n",
    "    processed_competences = competence_extractor.preprocess_competences(competences)\n",
    "    print(f\"\\nCompetências do pesquisador {researcher_index + 1}:\")\n",
    "    for competence in processed_competences:\n",
    "        print(f\"- {competence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Visualizar áreas de pesquisa de cada currículo</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "import os\n",
    "import warnings\n",
    "from git import Repo\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "from tqdm.notebook import tqdm # Importando tqdm do notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def load_curricula():\n",
    "    with open(curricula_file, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extrair_areas(areas_dict):\n",
    "    lista_grdareas = []\n",
    "    lista_areas = []\n",
    "    lista_subareas = []\n",
    "    # Expressão regular corrigida para extrair as áreas\n",
    "    pattern = r'Grande área:\\s*(.*?)\\s*/\\s*Área:\\s*(.*?)\\s*(?:/ Subárea:\\s*(.*?)\\s*)?\\.'\n",
    "\n",
    "    for _, valor in areas_dict.items():\n",
    "        match = re.search(pattern, valor)\n",
    "        if match:\n",
    "            areas = {\n",
    "                'Grande Área': match.group(1).strip() if match.group(1) else None , \n",
    "                'Área': match.group(2).strip() if match.group(2) else None ,\n",
    "                'Subárea': match.group(3).strip() if match.group(3) else None  \n",
    "            }\n",
    "            lista_grdareas.append(areas.get('Grande Área'))\n",
    "            lista_areas.append(areas.get('Área'))\n",
    "            lista_subareas.append(areas.get('Subárea'))\n",
    "\n",
    "    return {'Grande Áreas': lista_grdareas, 'Áreas': lista_areas, 'Subáreas': lista_subareas}\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curricula_file = os.path.join(root_folder, filename)\n",
    "\n",
    "curricula_data = load_curricula()\n",
    "\n",
    "for researcher_data in curricula_data:\n",
    "    areas =  researcher_data.get('Áreas')\n",
    "    print(extrair_areas(areas).get('Áreas'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Extrair competências do currículo</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Verificar se a CUDA está disponível\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA não está disponível.\")\n",
    "\n",
    "# Carregar um modelo pré-treinado\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v2')\n",
    "\n",
    "# Dados de exemplo\n",
    "sentences = [\"Esta é uma frase de teste.\", \"Outra frase de exemplo.\"]\n",
    "\n",
    "# Codificar as frases\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "from tqdm.notebook import tqdm # Importando tqdm do notebook\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Lista de modelos a serem avaliados\n",
    "model_names = [\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"sentence-transformers/LaBSE\",\n",
    "    \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "]\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curricula_file = os.path.join(root_folder, filename)\n",
    "\n",
    "# Dados de validação (exemplo)\n",
    "validation_data = {\n",
    "    'similar': [\n",
    "        (\"machine learning\", \"deep learning\"),\n",
    "        (\"biologia molecular\", \"genética\")\n",
    "    ],\n",
    "    'dissimilar': [\n",
    "        (\"machine learning\", \"medicina\"),\n",
    "        (\"biologia molecular\", \"finanças\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA não está disponível. Verifique a instalação e configuração da CUDA.\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "# Limpar o cache da GPU\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Sincronizar as operações CUDA\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Verificar se o arquivo de currículos existe\n",
    "if not os.path.exists(curricula_file):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {curricula_file}\")\n",
    "\n",
    "# Escolha do tipo de avaliação\n",
    "use_cross_validation = True  # ou False\n",
    "classifier_name = \"LogisticRegression\"  # ou \"MultinomialNB\", \"SVC\", \"RandomForestClassifier\"\n",
    "\n",
    "# Criar o avaliador e comparar os modelos\n",
    "evaluator = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "\n",
    "# Usar validação cruzada com Regressão Logística default\n",
    "# results = evaluator.evaluate_models(validation_data, use_cross_validation=True)\n",
    "\n",
    "# Usar divisão em treinamento e teste com SVM\n",
    "results = evaluator.evaluate_models(validation_data, use_cross_validation=False, classifier_name=\"SVC\")\n",
    "\n",
    "comparator = ModelComparator(results)\n",
    "best_model, best_score = comparator.get_best_model()\n",
    "\n",
    "if best_model is not None:\n",
    "    print(f\"\\nO melhor modelo é: {best_model} com pontuação de {best_score:.4f}\")\n",
    "elif not results:\n",
    "    print(\"\\nNenhum modelo foi avaliado com sucesso.\")\n",
    "else:\n",
    "    print(\"\\nNão foi possível determinar o melhor modelo.\")\n",
    "\n",
    "# Visualizar os resultados\n",
    "visualizer = PlotlyResultVisualizer(results)\n",
    "visualizer.plot_similarity_distributions()\n",
    "if not use_cross_validation: # Plota a acurácia apenas se não for validação cruzada\n",
    "    visualizer.plot_accuracy_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import silhouette_score\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "\n",
    "def evaluate_embeddings(X, y, metric=cosine_similarity, n_splits=5):\n",
    "    scores = defaultdict(list)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "        y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "        for i, area in enumerate(y_test):\n",
    "            area_idx = [j for j, a in enumerate(y_train) if a == area]\n",
    "            competence_embeddings = [X_train[j] for j in area_idx]\n",
    "            similarities = metric([X_test[i]], competence_embeddings)\n",
    "            scores['intra_class'].append(np.mean(similarities))\n",
    "            dissimilar_idx = [j for j, a in enumerate(y_train) if a != area]\n",
    "            dissimilar_embeddings = [X_train[j] for j in dissimilar_idx]\n",
    "            similarities = metric([X_test[i]], dissimilar_embeddings)\n",
    "            scores['inter_class'].append(np.mean(similarities))\n",
    "\n",
    "        scores['silhouette'].append(silhouette_score(X_train, y_train, metric=metric))\n",
    "\n",
    "    results = {k: np.mean(v) for k, v in scores.items()}\n",
    "    results['std_dev_intra_class'] = np.std(scores['intra_class'])\n",
    "    results['std_dev_inter_class'] = np.std(scores['inter_class'])\n",
    "    results['std_dev_silhouette'] = np.std(scores['silhouette'])\n",
    "    return results\n",
    "\n",
    "model_names = [\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"sentence-transformers/LaBSE\",\n",
    "    \"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "    # Adicione outros modelos aqui\n",
    "]\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curricula_file = os.path.join(root_folder, filename)\n",
    "\n",
    "classifier = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "\n",
    "# Defina o dispositivo (GPU se disponível, caso contrário CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Especifique a GPU correta (e.g., \"cuda:0\") se tiver várias\n",
    "    print(\"Usando GPU para processamento.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA não disponível, usando CPU.\")\n",
    "\n",
    "# Loop para avaliar os modelos\n",
    "for model_name in model_names:\n",
    "    # Limpeza de cache da GPU\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Redução do tamanho do lote\n",
    "    embeddings = model.encode(processed_competences, convert_to_tensor=True, batch_size=32)  # Tente um valor menor\n",
    "\n",
    "    # Carregue o modelo com o dispositivo especificado\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    X, y = classifier.prepare_data_for_classification(model)\n",
    "    score = evaluate_embeddings(X, y)\n",
    "    print(f\"Modelo: {model_name}, Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Escolher melhor vetorização</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação individual do modelo de embeedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import spacy\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from competence_extraction import CompetenceExtraction\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "pathfilename = os.path.join(root_folder, filename)\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not os.path.exists(pathfilename):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {pathfilename}\")\n",
    "\n",
    "try:\n",
    "    with open(pathfilename, \"r\") as f:\n",
    "        curricula_data = json.load(f)\n",
    "except json.JSONDecodeError:\n",
    "    raise ValueError(f\"Erro ao decodificar o arquivo JSON: {pathfilename}\")\n",
    "\n",
    "competence_extractor = CompetenceExtraction(curricula_file=pathfilename)\n",
    "\n",
    "# Extrair e vetorizar as competências de todos os pesquisadores\n",
    "all_competences = []\n",
    "for researcher_index, researcher_data in enumerate(curricula_data):  # Itera sobre a lista\n",
    "    competences = competence_extractor.extract_competences(researcher_data)\n",
    "    processed_competences = competence_extractor.preprocess_competences(competences)\n",
    "    all_competences.extend(processed_competences)\n",
    "\n",
    "# Vetorizar as competências em lote\n",
    "competence_vectors = competence_extractor.vectorize_competences(all_competences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(competence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Suponha que você tenha uma lista de pares de competências semelhantes e não semelhantes\n",
    "similar_pairs    = [(\"machine learning\", \"deep learning\"), (\"biologia molecular\", \"genética\")]\n",
    "dissimilar_pairs = [(\"machine learning\", \"medicina\"), (\"biologia molecular\", \"finanças\")]\n",
    "\n",
    "# Calcule a similaridade de cosseno entre os embeddings dos pares\n",
    "for pair in similar_pairs + dissimilar_pairs:\n",
    "    embedding1 = competence_extractor.vectorize_competences([pair[0]])\n",
    "    embedding2 = competence_extractor.vectorize_competences([pair[1]])\n",
    "    similarity = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "    print(f\"Similaridade entre '{pair[0]}' e '{pair[1]}': {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação comparativa entre os modelos de embeedings\n",
    "\n",
    "Métricas de Avaliação: As métricas utilizadas (similaridade média e acurácia) outras métricas de acordo com necessidade.\n",
    "\n",
    "Dados de Validação: Os dados de validação fornecidos são apenas exemplos. \n",
    "\n",
    "Os dados precisam ser reais e representativos para avaliar os modelos de forma adequada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lista de modelos a serem avaliados (atualizada)\n",
    "# model_names = [\"sentence-transformers/distiluse-base-multilingual-cased-v2\", \n",
    "#                \"bert-base-multilingual-cased\", \n",
    "#                \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"]\n",
    "\n",
    "# model_names = [\n",
    "#     \"sentence-transformers/distiluse-base-multilingual-cased-v2\", \n",
    "#     \"bert-base-uncased\",  # Modelo compatível com xFormers\n",
    "#     \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "# ]\n",
    "\n",
    "# Lista de modelos a serem avaliados\n",
    "# model_names = [\"distiluse-base-multilingual-cased-v2\",\n",
    "#                 \"bert-base-multilingual-cased\", \n",
    "#                 \"paraphrase-multilingual-mpnet-base-v2\"] ## Nome antigo não mais disponível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gpustat\n",
    "import gpustat\n",
    "\n",
    "gpu_stats = gpustat.GPUStatCollection.new_query()\n",
    "print(gpu_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CUDA_LAUNCH_BLOCKING=1  # No Linux/macOS\n",
    "!set CUDA_LAUNCH_BLOCKING=1  # No Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Limpeza de memória após a avaliação de cada modelo\n",
    "# del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from git import Repo\n",
    "\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "from tqdm.notebook import tqdm # Importando tqdm do notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Lista de modelos a serem avaliados\n",
    "model_names = [\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"sentence-transformers/LaBSE\",\n",
    "    \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "]\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curricula_file = os.path.join(root_folder, filename)\n",
    "\n",
    "# Dados de validação (exemplo)\n",
    "validation_data = {\n",
    "    'similar': [\n",
    "        (\"machine learning\", \"deep learning\"),\n",
    "        (\"biologia molecular\", \"genética\")\n",
    "    ],\n",
    "    'dissimilar': [\n",
    "        (\"machine learning\", \"medicina\"),\n",
    "        (\"biologia molecular\", \"finanças\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Verificar se o arquivo de currículos existe\n",
    "if not os.path.exists(curricula_file):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {curricula_file}\")\n",
    "\n",
    "# Escolha do tipo de avaliação\n",
    "use_cross_validation = True  # ou False\n",
    "classifier_name = \"LogisticRegression\"  # ou \"MultinomialNB\", \"SVC\", \"RandomForestClassifier\"\n",
    "\n",
    "# Criar o avaliador e comparar os modelos\n",
    "evaluator = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "\n",
    "# Usar validação cruzada com Regressão Logística default\n",
    "# results = evaluator.evaluate_models(validation_data, use_cross_validation=True)\n",
    "\n",
    "# Usar divisão em treinamento e teste com SVM\n",
    "# results = evaluator.evaluate_models(validation_data, use_cross_validation=False, classifier_name=\"SVC\")\n",
    "results = evaluator.evaluate_models(validation_data, use_cross_validation=False, classifier_name=\"MultinomialNB\")\n",
    "\n",
    "comparator = ModelComparator(results)\n",
    "best_model, best_score = comparator.get_best_model()\n",
    "\n",
    "if best_model is not None:\n",
    "    print(f\"\\nO melhor modelo é: {best_model} com pontuação de {best_score:.4f}\")\n",
    "elif not results:\n",
    "    print(\"\\nNenhum modelo foi avaliado com sucesso.\")\n",
    "else:\n",
    "    print(\"\\nNão foi possível determinar o melhor modelo.\")\n",
    "\n",
    "# Visualizar os resultados\n",
    "visualizer = PlotlyResultVisualizer(results)\n",
    "visualizer.plot_similarity_distributions()\n",
    "if not use_cross_validation: # Plota a acurácia apenas se não for validação cruzada\n",
    "    visualizer.plot_accuracy_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from git import Repo\n",
    "\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "# from tqdm.notebook import tqdm # Importando tqdm do notebook\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# # Lista de modelos a serem avaliados\n",
    "# model_names = [\n",
    "#     \"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "#     \"bert-base-multilingual-cased\",\n",
    "#     \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "# ]\n",
    "\n",
    "# Lista de modelos a serem avaliados (atualizada)\n",
    "model_names = [\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"sentence-transformers/LaBSE\",\n",
    "    \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "]\n",
    "\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curricula_file = os.path.join(root_folder, filename)\n",
    "\n",
    "# Dados de validação (exemplo)\n",
    "validation_data = {\n",
    "    'similar': [\n",
    "        (\"machine learning\", \"deep learning\"),\n",
    "        (\"biologia molecular\", \"genética\")\n",
    "    ],\n",
    "    'dissimilar': [\n",
    "        (\"machine learning\", \"medicina\"),\n",
    "        (\"biologia molecular\", \"finanças\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Verificar se o arquivo de currículos existe\n",
    "if not os.path.exists(curricula_file):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {curricula_file}\")\n",
    "\n",
    "# Criar o avaliador e comparar os modelos\n",
    "evaluator = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "results = evaluator.evaluate_models(validation_data)\n",
    "\n",
    "comparator = ModelComparator(results)\n",
    "best_model, best_score = comparator.get_best_model()\n",
    "\n",
    "if best_model is not None:\n",
    "    print(f\"\\nO melhor modelo é: {best_model} com pontuação de {best_score:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNão foi possível determinar o melhor modelo devido à falta de dados para validação cruzada.\")\n",
    "\n",
    "# Visualizar os resultados\n",
    "visualizer = PlotlyResultVisualizer(results)\n",
    "visualizer.plot_similarity_distributions()\n",
    "visualizer.plot_accuracy_comparison()  # Comentar esta linha se não houver resultados de validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Resolução de entidades com Ontologias</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "class BioPortalAPI:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = '6e1ec84f-53df-45a3-9115-25d245caefb1'\n",
    "        self.base_url = \"https://data.bioontology.org/api\"\n",
    "\n",
    "    def search_entities(self, query, ontologies=None, max_results=10):\n",
    "        \"\"\"\n",
    "        Realiza uma busca por entidades no BioPortal.\n",
    "\n",
    "        Args:\n",
    "            query: Termo de busca.\n",
    "            ontologies: Lista de acrônimos de ontologias (opcional).\n",
    "            max_results: Número máximo de resultados (opcional).\n",
    "\n",
    "        Returns:\n",
    "            Lista de dicionários com informações sobre as entidades encontradas.\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/search?q={query}&apikey={self.api_key}\"\n",
    "        if ontologies:\n",
    "            url += f\"&ontologies={','.join(ontologies)}\"\n",
    "        if max_results:\n",
    "            url += f\"&pagesize={max_results}\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"collection\"]\n",
    "        else:\n",
    "            raise Exception(f\"Erro na requisição: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Obter dados de Protocolos e Diretrizes Clínicas</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluxos e fontes de dados no CONITEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluxo de elaboração/atualização de Protocolos e Diretrizes\n",
    "https://www.gov.br/conitec/pt-br/assuntos/avaliacao-de-tecnologias-em-saude/pcdt-em-elaboracao-1\n",
    "\n",
    "### Fluxo de Incorporação de Tecnologia no SUS\n",
    "https://www.gov.br/conitec/pt-br/assuntos/fluxo-de-incorporacao-de-tecnologias-no-sus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoramento Tecnológico CONITEC\n",
    "\n",
    "https://www.gov.br/conitec/pt-br/assuntos/avaliacao-de-tecnologias-em-saude/monitoramento-de-tecnologias-em-saude#MHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publicações sobre Diretrizes Clínicas do CONITEC\n",
    "\n",
    "https://www.gov.br/conitec/pt-br/assuntos/noticias/2024/abril/atualizacao-anual-de-diretrizes-clinicas-pelo-ministerio-da-saude-segue-criterios-de-priorizacao-e-considera-encaminhamento-de-areas-tecnicas\n",
    "\n",
    "https://www.gov.br/conitec/pt-br/@@search?SearchableText=Diretrizes%20Cl%C3%ADnicas\n",
    "\n",
    "https://www.gov.br/conitec/pt-br/midias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocolos clínicos vigentes no site do Ministério da Saúde\n",
    "\n",
    "Fonte de dados para extração de protocolos: https://www.gov.br/saude/pt-br/assuntos/pcdt\n",
    "\n",
    "\n",
    "## Protocolos clínicos vigentes no site do CONITEC\n",
    "Fonte de dados para extração de protocolos: https://www.gov.br/conitec/pt-br/assuntos/avaliacao-de-tecnologias-em-saude/protocolos-clinicos-e-diretrizes-terapeuticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Sempre construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(base_repo_dir, 'utils')\n",
    "folder_domain = os.path.join(base_repo_dir, 'source', 'domain')\n",
    "folder_data_input = os.path.join(base_repo_dir, '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(base_repo_dir, '_data', 'out_json')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "from environment_setup import EnvironmentSetup\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from neo4j_persister import Neo4jPersister\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DictToHDF5, attribute_to_be_non_empty\n",
    "from extract_protocol import SaudeGovDataExtractor\n",
    "# from scraper_pasteur import PasteurScraper\n",
    "# from scraper_sucupira import SucupiraScraper\n",
    "# from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "\n",
    "t00 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from extract_protocol import SaudeGovDataExtractor\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "extractor = SaudeGovDataExtractor(\"https://www.gov.br/saude/pt-br/assuntos/pcdt\")\n",
    "links = extractor.get_links()\n",
    "if links:\n",
    "    print(f'{len(links)} links extraídos')\n",
    "\n",
    "## Visualizar links para documentos de protocolos e diretrizes\n",
    "# for link in links:\n",
    "#     filename = os.path.join(root_folder,\"_data\",\"in_pdf\", link.split(\"/\")[-1])\n",
    "#     if '#' in filename:\n",
    "#         filename = filename.split('#')[0]\n",
    "#     print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixar Protocolos e Diretrizes Clínicas do Site do MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_protocol import SaudeGovDataExtractor\n",
    "\n",
    "extractor = SaudeGovDataExtractor(\"https://www.gov.br/saude/pt-br/assuntos/pcdt\")\n",
    "df_docs, sucessos, erros = extractor.download_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Obter dados sobre Inovação no Mundo</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desempenho em Inovação no mundo e no Brasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medição da inovação em nível mundial é um campo importante para entender como diferentes nações estão progredindo em termos de capacidade e sucesso em inovação. Uma das iniciativas mais conhecidas neste campo é o Global Innovation Index (GII).\n",
    "\n",
    "O GII é um ranking anual publicado pela World Intellectual Property Organization (WIPO) em parceria com a INSEAD e outras instituições, como a Cornell University. Iniciado em 2007, o índice é baseado em dados subjetivos e objetivos obtidos de várias fontes, incluindo a International Telecommunication Union, o World Bank e o World Economic Forum. O GII classifica os países com base em dois sub-índices: o Innovation Input Index e o Innovation Output Index, compostos por cinco e dois pilares, respectivamente, cada um descrevendo um atributo da inovação.\n",
    "\n",
    "Além do GII, há outras iniciativas semelhantes, como o International Innovation Index, que medem o nível de inovação de um país. Este índice é produzido em conjunto pelo Boston Consulting Group (BCG), pela National Association of Manufacturers (NAM) e pelo Manufacturing Institute (MI), o afiliado de pesquisa apartidária da NAM.\n",
    "\n",
    "Cada um desses índices oferece uma perspectiva única sobre a inovação em nível de país, ajudando governos, formuladores de políticas e acadêmicos a entender as tendências de inovação e a identificar áreas para melhoria e investimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    print(math.factorial(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(1, 100, 100)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "# Valores de entrada (tamanho do problema) - ajustados\n",
    "n = np.linspace(1, 100, 100)\n",
    "\n",
    "# Cálculo das complexidades\n",
    "O_1 = np.ones(n.shape)\n",
    "O_log_n = np.log2(n)\n",
    "O_n = n\n",
    "O_n_log_n = n * np.log2(n)\n",
    "O_n_squared = n**2\n",
    "O_2_n = 2**n-1\n",
    "max_factorial_visible = 120  # Valor máximo para o cálculo do fatorial 6!=120\n",
    "O_n_factorial = np.array([math.factorial(int(i)) if i <= max_factorial_visible else np.nan for i in n])\n",
    "\n",
    "# Exibir O_n_factorial em notação científica\n",
    "for valor in O_n_factorial[:max_factorial_visible + 1]:\n",
    "    print(f\"{valor}\")\n",
    "\n",
    "# Criar um DataFrame para os dados a serem exibidos\n",
    "df = pd.DataFrame({'n': n[:max_factorial_visible + 1], 'O(n!)': O_n_factorial[:max_factorial_visible + 1]})\n",
    "\n",
    "# Criar o gráfico de linha\n",
    "chart = alt.Chart(df).mark_line(point=True).encode(  # Adicionamos point=True para mostrar os pontos\n",
    "    x=alt.X('n:Q', axis=alt.Axis(labelAngle=-45, format='.2f')),  # Formato com 4 casas decimais\n",
    "    y=alt.Y('O(n!)', scale=alt.Scale(type='log')),\n",
    "    tooltip=['n', 'O(n!)']\n",
    ").properties(\n",
    "    title='Valores de O(n!) até n = 20',\n",
    "    height=400,\n",
    "    width=800,\n",
    ").interactive()\n",
    "\n",
    "# Exibir o gráfico\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Valores de entrada (tamanho do problema) - ajustados\n",
    "n = np.linspace(1, 100, 100)\n",
    "\n",
    "# Cálculo das complexidades\n",
    "O_1 = np.ones(n.shape)\n",
    "O_log_n = np.log2(n)\n",
    "O_n = n\n",
    "O_n_log_n = n * np.log2(n)\n",
    "O_n_squared = n**2\n",
    "O_2_n = 2**n - 1\n",
    "\n",
    "# Calcular O(n!) até um ponto visível\n",
    "max_factorial = 100  # Valor máximo de n para o qual O(n!) é visível no gráfico\n",
    "O_n_factorial = np.array([math.factorial(int(i)) if i <= max_factorial_visible else np.nan for i in n])\n",
    "\n",
    "# Paleta de cores similar à imagem de referência\n",
    "colors = ['darkgreen', 'green', 'orange', 'gold', 'red', 'darkblue', 'purple']\n",
    "\n",
    "# Criar o gráfico Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar as curvas ao gráfico, usando cores personalizadas\n",
    "fig.add_trace(go.Scatter(x=n, y=O_1, mode='lines', name='O(1)', line=dict(color=colors[0], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_log_n, mode='lines', name='O(log n)', line=dict(color=colors[1], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n, mode='lines', name='O(n)', line=dict(color=colors[2], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_log_n, mode='lines', name='O(n log n)', line=dict(color=colors[3], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_squared, mode='lines', name='O(n²)', line=dict(color=colors[4], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_2_n, mode='lines', name='O(2ⁿ)', line=dict(color=colors[5], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n[:max_factorial + 1], y=O_n_factorial[:max_factorial + 1], mode='lines', name='O(n!)', line=dict(color=colors[6], width=2)))\n",
    "\n",
    "# Configuração do layout do gráfico\n",
    "fig.update_layout(\n",
    "    title='Comparação de Complexidades Algorítmicas (Big O)',\n",
    "    xaxis_title='Tamanho dos Dados de Entrada (n)',\n",
    "    yaxis_title='Tempo de Execução em escala logarítmica (em operações)',\n",
    "    yaxis_type='log',\n",
    "    yaxis=dict(range=[-0.04, 2]),  # Escala logarítmica com limite definido para y\n",
    "    xaxis=dict(range=[1, 20]),  # Escala linear com limite definido para x\n",
    "    showlegend=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(l=50, r=50, t=80, b=50),\n",
    ")\n",
    "\n",
    "# Função para calcular as posições das anotações\n",
    "def calculate_annotations(fig):\n",
    "    annotations = []\n",
    "    x_end = fig.layout.xaxis.range[1] * 0.95  \n",
    "    for i, trace in enumerate(fig.data):\n",
    "        # Encontrar o índice do valor de x mais próximo de x_end\n",
    "        idx = np.abs(trace.x - x_end).argmin()\n",
    "\n",
    "        # Se o valor de y for NaN, usar o último valor válido antes de x_end\n",
    "        if np.isnan(trace.y[idx]):\n",
    "            valid_indices = np.where(~np.isnan(trace.y) & (trace.x <= x_end))[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                idx = valid_indices[-1]\n",
    "\n",
    "        x = trace.x[idx]\n",
    "        y = trace.y[idx]\n",
    "\n",
    "        # Calcular a posição da anotação no eixo y, considerando a escala logarítmica, exceto para O(1)\n",
    "        if trace.name == 'O(n log n)':\n",
    "            y_annotation = 1.95\n",
    "        if trace.name == \"O(n)\":\n",
    "            y_annotation = 1.35\n",
    "            x_annotation = x\n",
    "        if trace.name == \"O(log n)\":\n",
    "            y_annotation = 0.7\n",
    "            x_annotation = x-0.4\n",
    "        if trace.name == \"O(1)\":\n",
    "            y_annotation = 0.05\n",
    "            x_annotation = x\n",
    "        if trace.name == \"O(n²)\":\n",
    "            y_annotation = 2\n",
    "            x_annotation = 10\n",
    "        if trace.name == \"O(2ⁿ)\":\n",
    "            y_annotation = 2\n",
    "            x_annotation = 6.7\n",
    "        if trace.name == \"O(n!)\":\n",
    "            y_annotation = 2\n",
    "            x_annotation = 5\n",
    "\n",
    "        annotations.append(\n",
    "            dict(\n",
    "                x=x_annotation,\n",
    "                y=y_annotation,\n",
    "                text=trace.name,\n",
    "                showarrow=False,\n",
    "                xanchor='left' if trace.name != 'O(n log n)' else 'center',  # Ajustar o alinhamento para O(n log n)\n",
    "                yanchor='middle',  # Centralizar o rótulo na vertical\n",
    "                font=dict(size=12, color=colors[i], family='Arial'),  # Usar a cor correspondente da paleta\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Calcular e adicionar as anotações\n",
    "fig.update_layout(annotations=calculate_annotations(fig))\n",
    "\n",
    "# Exibir o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Valores de entrada (tamanho do problema) - ajustados\n",
    "n = np.linspace(1, 100, 1000)\n",
    "\n",
    "# Cálculo das complexidades\n",
    "O_1 = np.ones(n.shape)\n",
    "O_log_n = np.log2(n)\n",
    "O_n = n\n",
    "O_n_log_n = n * np.log2(n)\n",
    "O_n_squared = n**2\n",
    "O_2_n = 2**n - 1\n",
    "\n",
    "# Cálculo da complexidade O(n!) para inteiros até max_factorial_visible\n",
    "max_factorial_visible = 100\n",
    "n_int = np.arange(1, max_factorial_visible + 1)  # Valores inteiros de 1 a max_factorial_visible\n",
    "O_n_factorial_int = np.array([math.factorial(i) for i in n_int])\n",
    "\n",
    "# Interpolação para valores fracionários\n",
    "O_n_factorial = np.array([math.gamma(i + 1) for i in n])\n",
    "\n",
    "# Paleta de cores similar à imagem de referência\n",
    "colors = ['darkgreen', 'green', 'gold', 'orange', 'red', 'darkblue', 'black']\n",
    "\n",
    "# Criar o gráfico Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar as curvas ao gráfico, usando cores personalizadas\n",
    "fig.add_trace(go.Scatter(x=n, y=O_1, mode='lines', name='O(1)', line=dict(color=colors[0], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_log_n, mode='lines', name='O(log n)', line=dict(color=colors[1], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n, mode='lines', name='O(n)', line=dict(color=colors[2], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_log_n, mode='lines', name='O(n log n)', line=dict(color=colors[3], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_squared, mode='lines', name='O(n²)', line=dict(color=colors[4], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_2_n, mode='lines', name='O(2ⁿ)', line=dict(color=colors[5], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n[:max_factorial_visible + 1], y=O_n_factorial[:max_factorial_visible + 1], \n",
    "    mode='lines', name='O(n!)', line=dict(color=colors[6], width=4)))\n",
    "\n",
    "# Configuração do layout do gráfico\n",
    "fig.update_layout(\n",
    "    title='Comparação de Complexidades Assintótica dos Algoritmos (Big O)',\n",
    "    xaxis_title='Tamanho dos Dados de Entrada (n)',\n",
    "    yaxis_title='Tempo de Execução em escala logarítmica (em operações)',\n",
    "    yaxis_type='log',\n",
    "    xaxis=dict(\n",
    "        range=[1, 20],          # Escala linear com limite definido para x\n",
    "        gridcolor='lightgray',  # Cor mais clara para a grade\n",
    "        gridwidth=0.25,         # Largura menor para a grade\n",
    "        showgrid=True,\n",
    "        griddash='dot',\n",
    "    ),  \n",
    "    yaxis=dict(\n",
    "        range=[-0.04, 2],       # Escala logarítmica com limite definido para y\n",
    "        gridcolor='lightgray',  \n",
    "        gridwidth=0.25,\n",
    "        showgrid=True,\n",
    "        griddash='dot',\n",
    "        # tickformat=\".0e\",     # Formatar os ticks em notação científica com expoente inteiro\n",
    "        # tickformat=\",d\"         # Formatar os ticks como números inteiros com separador de milhar\n",
    "        tickvals=[1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000],  # Valores dos ticks\n",
    "        ticktext=['1', '10', '100', '1,000', '10,000', '100,000', '1,000,000', '10,000,000', '100,000,000']  # Rótulos em números inteiros\n",
    "\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(l=25, r=50, t=80, b=50),\n",
    ")\n",
    "\n",
    "# Função para calcular as posições das anotações\n",
    "def calculate_annotations(fig):\n",
    "    annotations = []\n",
    "    x_end = fig.layout.xaxis.range[1] * 0.95  \n",
    "    for i, trace in enumerate(fig.data):\n",
    "        # Encontrar o índice do valor de x mais próximo de x_end\n",
    "        idx = np.abs(trace.x - x_end).argmin()\n",
    "\n",
    "        # Se o valor de y for NaN, usar o último valor válido antes de x_end\n",
    "        if np.isnan(trace.y[idx]):\n",
    "            valid_indices = np.where(~np.isnan(trace.y) & (trace.x <= x_end))[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                idx = valid_indices[-1]\n",
    "\n",
    "        x = trace.x[idx]\n",
    "        y = trace.y[idx]\n",
    "\n",
    "        # Calcular a posição da anotação no eixo y, considerando a escala logarítmica, exceto para O(1)\n",
    "        if trace.name == 'O(n log n)':\n",
    "            y_annotation = 1.95\n",
    "        if trace.name == \"O(n)\":\n",
    "            y_annotation = 1.4\n",
    "            x_annotation = x+0.75\n",
    "        if trace.name == \"O(log n)\":\n",
    "            y_annotation = 0.7\n",
    "            x_annotation = x+0.75\n",
    "        if trace.name == \"O(1)\":\n",
    "            y_annotation = 0.075\n",
    "            x_annotation = x+0.75\n",
    "        if trace.name == \"O(n²)\":\n",
    "            y_annotation = 1.95\n",
    "            x_annotation = 11.4\n",
    "        if trace.name == \"O(2ⁿ)\":\n",
    "            y_annotation = 1.95\n",
    "            x_annotation = 8\n",
    "        if trace.name == \"O(n!)\":\n",
    "            y_annotation = 1.95\n",
    "            x_annotation = 4.75         \n",
    "\n",
    "        annotations.append(\n",
    "            dict(\n",
    "                x=x_annotation,\n",
    "                y=y_annotation,\n",
    "                text=trace.name,\n",
    "                showarrow=False,\n",
    "                xanchor='right',\n",
    "                yanchor='middle',\n",
    "                font=dict(size=20, color='black', family='Arial'),  # Usar a cor correspondente da paleta\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Calcular e adicionar as anotações\n",
    "fig.update_layout(annotations=calculate_annotations(fig),\n",
    "    shapes=[\n",
    "        # Região verde (diagonal até um pouco acima de O(log n))\n",
    "        dict(\n",
    "            type=\"path\",\n",
    "            path=f\"M 0, {10**fig.layout.yaxis.range[0]} L 20, {O_log_n[-1]} L 20, {10**fig.layout.yaxis.range[0]} Z\",\n",
    "            fillcolor=\"lightgreen\",\n",
    "            opacity=0.3,\n",
    "            line_width=0,\n",
    "            layer='below' # para que as curvas fiquem por cima\n",
    "        ),\n",
    "        # Região amarela (diagonal até um pouco acima de O(n log n))\n",
    "        dict(\n",
    "            type=\"path\",\n",
    "            path=f\"M 0, {10**fig.layout.yaxis.range[0]} L 20, {O_n_log_n[-1]-5} L 20, {O_log_n[-1]} Z\",\n",
    "            fillcolor=\"yellow\",\n",
    "            opacity=0.3,\n",
    "            line_width=0,\n",
    "            layer='below' # para que as curvas fiquem por cima\n",
    "        ),\n",
    "        # Região vermelha (diagonal até o topo do gráfico)\n",
    "        dict(\n",
    "            type=\"path\",\n",
    "            # path=f\"M 0, {10**fig.layout.yaxis.range[0]} L 20, {10**fig.layout.yaxis.range[1]} L 0, {10**fig.layout.yaxis.range[1]} Z\",\n",
    "            path=f\"M 20, {O_n_log_n[-1] - 5} L 0, {10**(int(np.log10(O_n_log_n[-1])) + 1)} L 0, {10**fig.layout.yaxis.range[0]} Z\",\n",
    "            fillcolor=\"red\",\n",
    "            opacity=0.3,\n",
    "            line_width=0,\n",
    "            layer='below'  # para que as curvas fiquem por cima\n",
    "        ),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "# Exibir o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histórico no Global Innovation Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pathfilename = os.path.join(folder_data_input, 'gii_history_data.csv')\n",
    "df_gii = pd.read_csv(pathfilename)\n",
    "\n",
    "def calculate_percentages(df_gii):\n",
    "    df_avaliacao = pd.DataFrame(columns=['Ano', 'Participantes', 'Above Brazil', 'Below Brazil', 'Above Brazil (%)', 'Below Brazil (%)'])\n",
    "    \n",
    "    for year in df_gii['Ano'].unique():\n",
    "        df_year = df_gii[df_gii['Ano'] == year]\n",
    "        total_countries = df_year['Países Participantes'].values[0]\n",
    "        brazil_position = df_year['Colocação do Brasil'].values[0]\n",
    "        above_brazil = brazil_position - 1\n",
    "        below_brazil = total_countries - brazil_position\n",
    "        \n",
    "        above_percent = (above_brazil / total_countries) * 100\n",
    "        below_percent = (below_brazil / total_countries) * 100\n",
    "        \n",
    "        df_avaliacao = pd.concat([df_avaliacao, pd.DataFrame({'Ano': [year], 'Participantes': [total_countries], 'Above Brazil': [above_brazil], 'Below Brazil': [below_brazil], 'Above Brazil (%)': [above_percent], 'Below Brazil (%)': [below_percent]})], ignore_index=True)\n",
    "    \n",
    "    return df_avaliacao\n",
    "\n",
    "# Call the function to create df_avaliacao\n",
    "df_avaliacao = calculate_percentages(df_gii)\n",
    "df_avaliacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a figure with secondary y-axis for the line plots\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add stacked bars for 'Below Brazil (%)' and 'Above Brazil (%)'\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_avaliacao['Ano'], \n",
    "           y=df_avaliacao['Below Brazil (%)'], \n",
    "           name='Below Brazil (%)'),\n",
    "           secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_avaliacao['Ano'], \n",
    "           y=df_avaliacao['Above Brazil (%)'], \n",
    "           name='Above Brazil (%)'),\n",
    "           secondary_y=False,\n",
    ")\n",
    "\n",
    "# Correct the data label positions for each segment of the stacked bars\n",
    "for index, row in df_avaliacao.iterrows():\n",
    "    # Position for label of 'Above Brazil (%)'\n",
    "    position_above = row['Below Brazil (%)'] + row['Above Brazil (%)'] / 2\n",
    "    fig.add_annotation(\n",
    "        x=row['Ano'], y=position_above,\n",
    "        text=f\"{row['Above Brazil (%)']:.1f}%\",\n",
    "        showarrow=False, font=dict(color='white')\n",
    "    )\n",
    "\n",
    "    # Position for label of 'Below Brazil (%)'\n",
    "    position_below = row['Below Brazil (%)'] / 2\n",
    "    fig.add_annotation(\n",
    "        x=row['Ano'], y=position_below,\n",
    "        text=f\"{row['Below Brazil (%)']:.1f}%\",\n",
    "        showarrow=False, font=dict(color='white')\n",
    "    )\n",
    "\n",
    "# Update layout for stacked bars\n",
    "fig.update_layout(barmode='stack')\n",
    "\n",
    "# Add line for total number of participants\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_avaliacao['Ano'], \n",
    "               y=df_avaliacao['Participantes'], \n",
    "               name='Total Participants', \n",
    "               mode='lines+markers+text', \n",
    "               text=df_avaliacao['Participantes'], \n",
    "               textposition=\"top center\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add line for Brazil's performance, but on the primary y-axis\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_avaliacao['Ano'], \n",
    "               y=df_avaliacao['Below Brazil (%)'], \n",
    "               name='Brazil Performance', \n",
    "               mode='lines+markers', \n",
    "               line=dict(color='yellow', \n",
    "                         dash='dot')),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "# Update the bar colors\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Above Brazil (%)'),\n",
    "    marker=dict(color='orange')\n",
    ")\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Below Brazil (%)'),\n",
    "    marker=dict(color='blue')\n",
    ")\n",
    "\n",
    "# Update the line trace for total number of participants to have a thickness of 4\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Total Participants'),\n",
    "    line=dict(width=2)\n",
    ")\n",
    "\n",
    "# Update the line trace for Brazil's performance to have a thickness of 4\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Brazil Performance'),\n",
    "    line=dict(width=6)\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "vr_max = max(df_avaliacao['Participantes']) * 1.1\n",
    "fig.update_layout(\n",
    "    title='Performance Comparison: Brazil vs. Other Countries',\n",
    "    height=600,\n",
    "    yaxis=dict(title='Percentage', range=[0, vr_max]),  # Extending primary y-axis range\n",
    "    yaxis2=dict(title='Total Participants', overlaying='y', side='right', range=[0, vr_max])\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickvals=df_avaliacao['Ano'])\n",
    "\n",
    "# Re-render the chart\n",
    "fig.show(renderer=\"notebook\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Modelo Grafo para Inovar por Multicamadas</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('../../templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from IPython.display import IFrame\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "        ''' \n",
    "        Busca o arquivo .git e retorna string com a pasta raiz do repositório.\n",
    "        '''\n",
    "        # Prevenir recursão infinita limitando a profundidade\n",
    "        if depth < 0:\n",
    "            return None\n",
    "        path = Path(path).absolute()\n",
    "        if (path / '.git').is_dir():\n",
    "            return path\n",
    "        # Corrigido para usar LattesScraper.find_repo_root para chamada recursiva\n",
    "        return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "# Criar o grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar nós (dominios, processos e entidades)\n",
    "dominios = [\"Pesquisar\", \"Desenvolver\", \"Inovar\"]\n",
    "processos = [\"P001\", \"P002\", \"P003\", \"P004\", \"P005\", \"P006\", \"P007\", \"P008\", \"P009\"]\n",
    "entidades = {\n",
    "    \"P001\": [\"Dores\", \"Desejos\", \"Desafios\"],\n",
    "    \"P002\": [\"Temas\", \"Tópicos\", \"Assuntos\"],\n",
    "    \"P003\": [\"Atitudes\", \"Experiências\", \"Habilidades\"],\n",
    "    \"P004\": [\"Papeis\", \"Tempo\", \"Orçamentos\"],\n",
    "    \"P005\": [\"Projetos\", \"Processos\", \"Programas\"],\n",
    "    \"P006\": [\"Ensaios\", \"Equipamentos\", \"Ambientes\"],\n",
    "    \"P007\": [\"Aplicação\", \"Solução\", \"Produto-Serviço\"],\n",
    "    \"P008\": [\"Modelos\", \"Protótipos\", \"Empreendimentos\"],\n",
    "    \"P009\": [\"Indicadores\", \"Evidências\", \"Mensuração\"]\n",
    "}\n",
    "\n",
    "# Criar visualização dos nós de acordo com a estrutura de dados\n",
    "for macroprocesso in dominios:\n",
    "    G.add_node(macroprocesso, type=\"macroprocesso\")\n",
    "\n",
    "for processo in processos:\n",
    "    G.add_node(processo, type=\"processo\")\n",
    "\n",
    "for processo, entidades_list in entidades.items():\n",
    "    for entidade in entidades_list:\n",
    "        G.add_node(entidade, type=\"entidade\")\n",
    "\n",
    "# Adicionar arestas (relacionamentos)\n",
    "for macroprocesso in dominios:\n",
    "    for i in range(1, 4):\n",
    "        G.add_edge(macroprocesso, f\"P00{i + 3*(dominios.index(macroprocesso))}\")\n",
    "\n",
    "for processo, entidades_list in entidades.items():\n",
    "    for entidade in entidades_list:\n",
    "        G.add_edge(processo, entidade)\n",
    "\n",
    "\n",
    "\n",
    "# (Opcional) Adicionar relacionamentos entre entidades, para formar Demanda, Faturamento, Lucro, Reinvestimento... etc\n",
    "# G.add_edge(\"Dores\", \"Desejos\")\n",
    "\n",
    "\n",
    "\n",
    "# Calcular distâncias, definir cores, tamanhos e tamanhos de fonte\n",
    "node_distances = {}\n",
    "for macroprocesso in dominios:\n",
    "    for node, distance in nx.shortest_path_length(G, source=macroprocesso).items():\n",
    "        node_distances[node] = distance\n",
    "\n",
    "cores_base = {\"macroprocesso\": \"#007BFF\", \"processo\": \"#28A745\", \"entidade\": \"#FFC107\"}\n",
    "node_colors = {}\n",
    "node_sizes = {}\n",
    "node_font_sizes = {}  # Dicionário para armazenar os tamanhos de fonte\n",
    "for node in G.nodes():\n",
    "    node_type = G.nodes[node][\"type\"]\n",
    "    cor_base = cores_base[node_type]\n",
    "    alpha = max(0, 255 - 25 * node_distances[node])\n",
    "    node_colors[node] = f\"{cor_base}{alpha:02X}\"\n",
    "\n",
    "    # Definir tamanhos e tamanhos de fonte com base na distância\n",
    "    tamanho_base = {\"macroprocesso\": 50, \"processo\": 30, \"entidade\": 15}\n",
    "    font_size_base = {\"macroprocesso\": 42, \"processo\": 28, \"entidade\": 18}  # Tamanhos de fonte iniciais\n",
    "    node_sizes[node] = tamanho_base[node_type] - 5 * node_distances[node]\n",
    "    node_font_sizes[node] = font_size_base[node_type] - node_distances[node]  # Reduzir 1 pixel por passo\n",
    "\n",
    "# Configurar o PyVis (notebook=False para renderizar na célula)\n",
    "net = Network(notebook=False, width=\"100%\", height=\"1200px\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
    "net.barnes_hut()\n",
    "\n",
    "# Adicionar nós e arestas ao PyVis\n",
    "for node in G.nodes():\n",
    "    net.add_node(node, label=node, color=node_colors[node], title=G.nodes[node][\"type\"], \n",
    "                 size=node_sizes[node], font={\"size\": node_font_sizes[node], \"color\": \"black\"})  # Adicionar tamanho da fonte\n",
    "\n",
    "for edge in G.edges():\n",
    "    weight = 1\n",
    "    net.add_edge(*edge, value=weight)\n",
    "\n",
    "# Configurar o ForceAtlas2\n",
    "net.options.physics.solver = \"forceAtlas2Based\"\n",
    "net.options.physics.forceAtlas2Based = {\n",
    "    \"gravitationalConstant\": -50,\n",
    "    \"centralGravity\": 0.01,\n",
    "    \"springLength\": 100,\n",
    "    \"springConstant\": 0.08,\n",
    "    \"damping\": 0.4,\n",
    "    \"avoidOverlap\": 1\n",
    "}\n",
    "\n",
    "driver_path = None\n",
    "try:\n",
    "    # Caminho para o chromedriver no sistema local\n",
    "    if platform.system() == \"Windows\":\n",
    "        driver_path=find_repo_root()/'chromedriver'/'chromedriver.exe'\n",
    "    else:\n",
    "        driver_path=find_repo_root()/'chromedriver'/'chromedriver'\n",
    "except Exception as e:\n",
    "    print(\"Não foi possível estabelecer uma conexão, verifique o chromedriver\")\n",
    "    print(e)\n",
    "\n",
    "# print(driver_path)\n",
    "# service = Service(driver_path)\n",
    "# driver = webdriver.Chrome(service=service)\n",
    "# driver.get(\"grafo_interativo.html\")\n",
    "\n",
    "# Adicionar controles interativos (opcional)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "# Adicionar estilo inline para fundo branco\n",
    "net.html = net.html.replace(\"<body>\", '<body style=\"background-color: white;\">')\n",
    "\n",
    "# Salvar o HTML na pasta templates\n",
    "template_dir = find_repo_root() / 'templates'\n",
    "net.save_graph(os.path.join(template_dir,\"grafo_interativo.html\"))\n",
    "\n",
    "print(f\"Grafo interativo salvo em: {template_dir / 'grafo_interativo.html'}\")\n",
    "# Renderizar na célula do Jupyter Notebook\n",
    "# net.show(\"'../../templates'grafo_interativo.html\")  # Definir fundo branco ao salvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Renderizar grafo no Notebook</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "net = Network(notebook=True, width=\"100%\", height=\"1200px\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
    "\n",
    "IFrame(src='http://127.0.0.1:5000/grafo_interativo.html', width='100%', height='800px')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simulação para gerar visualizar o layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyVis para simula a busca por equilíbrio entre forças físicas nas entidades que interagem no grafo para determinar o posicionamento dos nós no grafo e assim criar o layout. As principais forças envolvidas são:\n",
    "\n",
    "### 1. Força de Atração:\n",
    "\n",
    "Arestas (Links): As arestas entre os nós agem como molas, puxando os nós conectados um em direção ao outro. A intensidade dessa força é determinada pelo parâmetro springLength (comprimento ideal da mola) e springConstant (rigidez da mola).\n",
    "\n",
    "Gravidade Central: Uma força de atração em direção ao centro do grafo, controlada pelo parâmetro centralGravity. Essa força ajuda a evitar que os nós se dispersem muito e mantém o grafo mais compacto.\n",
    "\n",
    "### 2. Força de Repulsão:\n",
    "\n",
    "Repulsão entre Nós: Os nós se repelem uns aos outros, como partículas carregadas com a mesma carga. A intensidade dessa força é determinada pelo parâmetro gravitationalConstant (constante gravitacional). Um valor negativo aumenta a repulsão, enquanto um valor positivo a diminui.\n",
    "\n",
    "Evitar Sobreposição: O parâmetro avoidOverlap controla se os nós devem evitar a sobreposição. Se ativado, uma força adicional é aplicada para afastar os nós que estão muito próximos.\n",
    "\n",
    "### 3. Força de Amortecimento:\n",
    "\n",
    "Damping: O parâmetro damping controla o amortecimento do movimento dos nós. Um valor maior de amortecimento torna o movimento mais lento e suave, enquanto um valor menor permite movimentos mais rápidos e oscilatórios.\n",
    "\n",
    "### 4. Forças Adicionais (Opcionais):\n",
    "\n",
    "Outras Forças: O PyVis permite adicionar outras forças personalizadas ao layout, como forças de atração/repulsão entre grupos de nós, ou forças que direcionam os nós para posições específicas.\n",
    "\n",
    "\n",
    "## Como as Forças Interagem:\n",
    "\n",
    "O algoritmo ForceAtlas2Based iterativamente calcula as forças resultantes sobre cada nó e ajusta suas posições de acordo. O processo continua até que o layout se estabilize ou um número máximo de iterações seja atingido.\n",
    "\n",
    "O algoritmo Barnes-Hut otimiza o cálculo das forças de longo alcance, aproximando as forças entre grupos de nós distantes. Isso melhora o desempenho do algoritmo, especialmente em grafos grandes.\n",
    "\n",
    "## Equilíbrio das Forças:\n",
    "\n",
    "O objetivo do algoritmo é encontrar um equilíbrio entre as forças de atração e repulsão, de modo que os nós conectados fiquem próximos, mas sem se sobreporem, e o grafo tenha uma aparência geral agradável e informativa. O ajuste dos parâmetros do ForceAtlas2Based e do Barnes-Hut permite controlar esse equilíbrio e personalizar o layout do grafo de acordo com suas necessidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ForceAtlas2Based e Barnes-Hut:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ForceAtlas2Based é um algoritmo de layout que simula um sistema físico onde os nós se atraem e se repelem com base em certas forças. No entanto, calcular essas forças para todos os pares de nós em um grafo grande pode ser computacionalmente caro.\n",
    "\n",
    "Para otimizar o cálculo das forças, o ForceAtlas2Based utiliza o algoritmo Barnes-Hut. Esse algoritmo agrupa nós distantes em clusters e aproxima suas forças de atração/repulsão, reduzindo significativamente o número de cálculos necessários.\n",
    "\n",
    "### Theta e a Distância entre Nós:\n",
    "\n",
    "O parâmetro theta (θ) do Barnes-Hut define o limite entre as forças de curto e longo alcance. Quando a distância entre dois nós é menor que theta multiplicado pelo tamanho do cluster, as forças são calculadas individualmente (curto alcance). Caso contrário, as forças são aproximadas usando o centro de massa do cluster (longo alcance).\n",
    "\n",
    "Quando os nós estão muito próximos, a distância entre eles é menor que o limite definido por theta. Nesse caso, o algoritmo Barnes-Hut calcula as forças individualmente para cada par de nós, levando em consideração suas posições exatas. Isso permite que o ForceAtlas2Based posicione os nós próximos de forma mais precisa, evitando sobreposições e garantindo um layout visualmente agradável.\n",
    "\n",
    "Em resumo, o ForceAtlas2Based define as regras gerais para as forças de atração e repulsão entre os nós. No entanto, quando os nós estão próximos, o algoritmo Barnes-Hut assume o controle e calcula as forças de forma mais precisa, levando em consideração a distância exata entre os nós.\n",
    "\n",
    "O parâmetro theta do Barnes-Hut é fundamental para determinar o comportamento do layout em nós próximos, pois define o limite entre as forças de curto e longo alcance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é o Theta (θ)?\n",
    "\n",
    "O theta (θ) é um parâmetro interno do ForceAtlas2Based que controla o limite entre as forças de longo alcance (que afetam todos os nós) e as forças de curto alcance (que afetam apenas os nós próximos).\n",
    "\n",
    "Valores mais altos de theta: Aceleram o cálculo das forças, mas podem gerar mais erros e imprecisões no layout.\n",
    "Valores mais baixos de theta: Tornam o cálculo mais lento, mas produzem um layout mais preciso e com menos erros.\n",
    "\n",
    "### Como o Theta é Usado no ForceAtlas2Based?\n",
    "\n",
    "O ForceAtlas2Based utiliza uma técnica chamada Barnes-Hut para aproximar as forças de longo alcance, tornando o cálculo mais eficiente. O theta é usado para determinar quais nós estão \"próximos o suficiente\" para que suas forças sejam calculadas individualmente (curto alcance), e quais nós estão \"longe o suficiente\" para que suas forças sejam aproximadas usando a técnica Barnes-Hut (longo alcance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parâmetros do ForceAtlas2Based:\n",
    "\n",
    "gravitationalConstant: Define a força de atração global entre os nós. Valores negativos atraem os nós para o centro do grafo, enquanto valores positivos os repelem. Um valor mais negativo resultará em um grafo mais compacto, enquanto um valor mais positivo resultará em um grafo mais espalhado.\n",
    "\n",
    "centralGravity: Define a força de atração em direção ao centro do grafo. Valores maiores puxam os nós mais para o centro.\n",
    "\n",
    "springLength: Define o comprimento ideal das arestas (ligações entre os nós). Valores maiores resultam em arestas mais longas e um grafo mais espalhado.\n",
    "\n",
    "springConstant: Define a rigidez das arestas. Valores maiores tornam as arestas mais rígidas e o grafo menos flexível.\n",
    "\n",
    "damping: Controla a velocidade com que os nós se movem. Valores maiores amortecem o movimento, resultando em um layout mais estável, mas que pode levar mais tempo para convergir.\n",
    "\n",
    "avoidOverlap: Determina se os nós devem evitar a sobreposição. Um valor de 1 (verdadeiro) faz com que os nós se afastem uns dos outros para evitar sobreposição, enquanto um valor de 0 (falso) permite a sobreposição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renderizar na célula do Jupyter Notebook\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=\"grafo_interativo.html\", width=\"100%\", height=\"600px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Sempre construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(base_repo_dir, 'utils')\n",
    "folder_domain = os.path.join(base_repo_dir, 'source', 'domain')\n",
    "folder_data_input = os.path.join(base_repo_dir, '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(base_repo_dir, '_data', 'out_json')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "# from scraper_pasteur import PasteurScraper\n",
    "# from scraper_sucupira import SucupiraScraper\n",
    "# from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "from environment_setup import EnvironmentSetup\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from neo4j_persister import Neo4jPersister\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DictToHDF5, attribute_to_be_non_empty\n",
    "\n",
    "t00 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(folder_data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Carregar dados dos produtos prioritários, equipamentos e questões de pesquisa\n",
    "curr_pathfilename = os.path.join(folder_data_input,'docents_dict_list.json')\n",
    "with open(curr_pathfilename, 'r', encoding='utf-8') as f:\n",
    "    curriculos = json.load(f)\n",
    "\n",
    "prod_pathfilename = os.path.join(folder_data_output,'matriz_ceis.json')\n",
    "with open(prod_pathfilename, 'r', encoding='utf-8') as f:\n",
    "    matriz_produtos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(curriculos)} currículos carregados')\n",
    "# [x.get('Áreas') for x in curriculos]\n",
    "produtos = [produto.get('nome') for bloco in matriz_produtos.get('blocos', []) for produto in bloco.get('produtos', [])]\n",
    "print(f'{len(produtos)} produtos carregados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vega\n",
    "# !jupyter nbextension install --sys-prefix --py vega\n",
    "# !jupyter nbextension enable --sys-prefix --py vega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_validator import *\n",
    "\n",
    "# Exemplo de uso (substituir pelos dados reais)\n",
    "y_true = [[\"A\", \"B\"], [\"B\"], [\"A\", \"C\"]]\n",
    "y_pred = [[\"A\"], [\"B\", \"C\"], [\"A\", \"B\"]]\n",
    "classes = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "# Probabilidades aqui\n",
    "y_proba = [[0.8, 0.7, 0.3], [0.2, 0.9, 0.5], [0.6, 0.8, 0.4]]\n",
    "\n",
    "\n",
    "validar_modelo(y_true, y_pred, y_proba, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equipamentos = [...]\n",
    "questoes_pesquisa = [...]\n",
    "\n",
    "# Criar o grafo heterogêneo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar nós e arestas para pesquisadores\n",
    "for pesquisador in curriculos:\n",
    "    G.add_node(pesquisador['Identificação']['ID Lattes'], type=\"pesquisador\", **pesquisador)  # Adicionar atributos do currículo\n",
    "    \n",
    "    # Conectar pesquisador às suas áreas de atuação\n",
    "    for area in pesquisador['Áreas'].values():\n",
    "        G.add_node(area, type=\"area\")\n",
    "        G.add_edge(pesquisador['Identificação']['ID Lattes'], area)\n",
    "\n",
    "    # Conectar pesquisador às suas publicações\n",
    "    for publicacao in pesquisador['Produções']['Artigos completos publicados em periódicos']:\n",
    "        G.add_node(publicacao['titulo'], type=\"publicacao\")\n",
    "        G.add_edge(pesquisador['Identificação']['ID Lattes'], publicacao['titulo'])\n",
    "\n",
    "    # Conectar pesquisador a projetos\n",
    "    for pesquisador in curriculos:\n",
    "        for tipo_projeto in [\"ProjetosPesquisa\", \"ProjetosExtensão\", \"ProjetosDesenvolvimento\", \"ProjetosOutros\"]:\n",
    "            if tipo_projeto in pesquisador:\n",
    "                for projeto in pesquisador[tipo_projeto]:\n",
    "                    G.add_node(projeto['titulo_projeto'], type=\"projeto\")\n",
    "                    G.add_edge(pesquisador['Identificação']['ID Lattes'], projeto['titulo_projeto'])\n",
    "\n",
    "    # Conectar pesquisador a equipamentos\n",
    "    # (Assumindo que você tem uma lista de equipamentos mencionados nos currículos)\n",
    "    equipamentos_citados = []  # Preencha com os nomes dos equipamentos mencionados nos currículos\n",
    "    for pesquisador in curriculos:\n",
    "        for equipamento in equipamentos_citados:\n",
    "            if equipamento in pesquisador['Atuação Profissional'][0]['Descrição']:  # Exemplo: busca na descrição da atuação profissional\n",
    "                G.add_edge(pesquisador['Identificação']['ID Lattes'], equipamento)\n",
    "\n",
    "    # Conectar pesquisador a questões de pesquisa\n",
    "    # (Assumindo que você tem uma lista de questões de pesquisa e um método para associá-las aos pesquisadores)\n",
    "    for pesquisador in curriculos:\n",
    "        for questao in questoes_pesquisa:\n",
    "            if pesquisador_tem_interesse_na_questao(pesquisador, questao):  # Interesse por inferência\n",
    "                G.add_edge(pesquisador['Identificação']['ID Lattes'], questao['descricao'])\n",
    "\n",
    "    def extract_topicos(pesquisador, list_dict):\n",
    "        campos = ['', '', '']\n",
    "        return lista_topicos\n",
    "\n",
    "    # Função para inferir o interesse do pesquisador em uma questão\n",
    "    def pesquisador_tem_interesse_na_questao(pesquisador, questao):\n",
    "        # Analise o currículo do pesquisador (áreas de atuação, publicações, projetos, etc.)\n",
    "        # e compare com a descrição da questão de pesquisa para determinar o interesse\n",
    "        # Retorna True se houver interesse, False caso contrário\n",
    "        topicos_pesquisador = extract_topicos(pesquisador)\n",
    "        interesses_pesquisador = []\n",
    "        flag_interesse = False\n",
    "        threshold = 0.8\n",
    "        for i in topicos_pesquisador:\n",
    "            similarity = calculate_similarity(questao, i)\n",
    "            if similarity >= threshold:\n",
    "                interesses_pesquisador.append(questao)\n",
    "                flag_interesse = True\n",
    "\n",
    "        return flag_interesse\n",
    "\n",
    "\n",
    "# Adicionar nós e arestas para produtos prioritários, equipamentos e questões de pesquisa\n",
    "for produto in produtos_prioritarios:\n",
    "    G.add_node(produto['nome'], type=\"produto\", **produto)  # Adicionar atributos do produto (nome, descrição, área, etc.)\n",
    "\n",
    "    # Conectar produto às suas áreas (assumindo que o produto tem uma lista de áreas)\n",
    "    for area in produto.get('areas', []):  # Usar get() para evitar KeyError se 'areas' não existir\n",
    "        G.add_edge(produto['nome'], area)\n",
    "\n",
    "# Adicionar nós e arestas para equipamentos\n",
    "for equipamento in equipamentos:\n",
    "    G.add_node(equipamento['nome'], type=\"equipamento\", **equipamento)  # Adicionar atributos do equipamento\n",
    "\n",
    "    # Conectar equipamento às suas áreas (assumindo que o equipamento tem uma lista de áreas)\n",
    "    for area in equipamento.get('areas', []):\n",
    "        G.add_edge(equipamento['nome'], area)\n",
    "\n",
    "# Adicionar nós e arestas para questões de pesquisa\n",
    "for questao in questoes_pesquisa:\n",
    "    G.add_node(questao['descricao'], type=\"questao_pesquisa\", **questao)  # Adicionar atributos da questão\n",
    "\n",
    "    # Conectar questão de pesquisa às suas áreas (assumindo que a questão tem uma lista de áreas)\n",
    "    for area in questao.get('areas', []):\n",
    "        G.add_edge(questao['descricao'], area)\n",
    "\n",
    "# Adicionar nós e arestas para equipamentos\n",
    "for equipamento in equipamentos:\n",
    "    G.add_node(equipamento['nome'], type=\"equipamento\", **equipamento)  # Adicionar atributos do equipamento\n",
    "\n",
    "    # Conectar equipamento às suas áreas (assumindo que o equipamento tem uma lista de áreas)\n",
    "    if 'areas' in equipamento:  # Verificar se o equipamento possui áreas associadas\n",
    "        for area in equipamento['areas']:\n",
    "            if area in G.nodes:  # Verificar se a área já existe no grafo\n",
    "                G.add_edge(equipamento['nome'], area)\n",
    "            else:\n",
    "                # Se a área não existir, você pode decidir se quer adicioná-la como um novo nó\n",
    "                # G.add_node(area, type=\"area\")  \n",
    "                # G.add_edge(equipamento['nome'], area)\n",
    "                print(f\"Área '{area}' não encontrada para o equipamento '{equipamento['nome']}'.\")\n",
    "    else:\n",
    "        print(f\"Equipamento '{equipamento['nome']}' não possui áreas associadas.\")\n",
    "\n",
    "# Adicionar nós e arestas para questões de pesquisa\n",
    "for questao in questoes_pesquisa:\n",
    "    G.add_node(questao['descricao'], type=\"questao_pesquisa\", **questao)  # Adicionar atributos da questão\n",
    "\n",
    "    # Conectar questão de pesquisa às suas áreas\n",
    "    if 'areas' in questao:  # Verificar se a questão possui áreas associadas\n",
    "        for area in questao['areas']:\n",
    "            if area in G.nodes:  # Verificar se a área já existe no grafo\n",
    "                G.add_edge(questao['descricao'], area)\n",
    "            else:\n",
    "                # Se a área não existir, você pode decidir se quer adicioná-la como um novo nó\n",
    "                # G.add_node(area, type=\"area\")  \n",
    "                # G.add_edge(questao['descricao'], area)\n",
    "                print(f\"Área '{area}' não encontrada para a questão de pesquisa '{questao['descricao']}'.\")\n",
    "    else:\n",
    "        print(f\"Questão de pesquisa '{questao['descricao']}' não possui áreas associadas.\")\n",
    "\n",
    "# Análise 1: Agrupamento de Pesquisadores\n",
    "# Extrair características textuais dos currículos (ex: usando TF-IDF)\n",
    "corpus = [pesquisador['Formação']['Acadêmica'][0]['Descrição'] for pesquisador in curriculos]  # Exemplo usando a descrição da formação acadêmica\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Calcular similaridade entre pesquisadores (ex: usando cosseno)\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "\n",
    "# Aplicar algoritmo de agrupamento (ex: DBSCAN)\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=0.3, min_samples=2).fit(similarity_matrix)\n",
    "labels = clustering.labels_\n",
    "\n",
    "# Adicionar atributo 'cluster' aos nós dos pesquisadores\n",
    "for i, pesquisador in enumerate(curriculos):\n",
    "    G.nodes[pesquisador['Identificação']['ID Lattes']]['cluster'] = labels[i]\n",
    "\n",
    "\n",
    "# Análise 2: Agrupamento de Questões de Pesquisa\n",
    "# Extrair características textuais das questões de pesquisa (ex: usando TF-IDF)\n",
    "corpus_questoes = [questao['descricao'] for questao in questoes_pesquisa]\n",
    "vectorizer_questoes = TfidfVectorizer()\n",
    "X_questoes = vectorizer_questoes.fit_transform(corpus_questoes)\n",
    "\n",
    "# Calcular similaridade entre questões (ex: usando cosseno)\n",
    "similarity_matrix_questoes = cosine_similarity(X_questoes)\n",
    "\n",
    "# Aplicar algoritmo de agrupamento (ex: K-Means)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 5  # Defina o número de clusters desejado\n",
    "clustering_questoes = KMeans(n_clusters=n_clusters).fit(similarity_matrix_questoes)\n",
    "labels_questoes = clustering_questoes.labels_\n",
    "\n",
    "# Adicionar atributo 'cluster' aos nós das questões de pesquisa\n",
    "for i, questao in enumerate(questoes_pesquisa):\n",
    "    G.nodes[questao['descricao']]['cluster'] = labels_questoes[i]\n",
    "\n",
    "# Análise 3: Recomendação de Projetos (exemplo simplificado)\n",
    "def recomendar_projetos(pesquisador_id):\n",
    "    pesquisador_areas = list(G.neighbors(pesquisador_id))  # Obter áreas do pesquisador\n",
    "    projetos_recomendados = []\n",
    "    for projeto_id in G.nodes:\n",
    "        if G.nodes[projeto_id]['type'] == \"projeto\":\n",
    "            projeto_areas = list(G.neighbors(projeto_id))\n",
    "            if set(pesquisador_areas) & set(projeto_areas):  # Verificar se há áreas em comum\n",
    "                projetos_recomendados.append(projeto_id)\n",
    "    return projetos_recomendados\n",
    "\n",
    "# Análise 4: Detecção de Oportunidades\n",
    "def detectar_oportunidades(G, produtos_prioritarios, top_n=5):\n",
    "    areas_importantes = {}\n",
    "    for produto in produtos_prioritarios:\n",
    "        for area in G.neighbors(produto['nome']):\n",
    "            areas_importantes[area] = areas_importantes.get(area, 0) + 1\n",
    "\n",
    "    # Ponderar pela concentração de pesquisadores e questões de pesquisa\n",
    "    for area in areas_importantes:\n",
    "        pesquisadores_na_area = len([n for n in G.neighbors(area) if G.nodes[n]['type'] == \"pesquisador\"])\n",
    "        questoes_na_area = len([n for n in G.neighbors(area) if G.nodes[n]['type'] == \"questao_pesquisa\"])\n",
    "        areas_importantes[area] *= (pesquisadores_na_area + questoes_na_area)\n",
    "\n",
    "    # Ordenar áreas por importância\n",
    "    areas_importantes = dict(sorted(areas_importantes.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return list(areas_importantes.keys())[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "print(stopwords.words('portuguese'))\n",
    "print(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade httpx\n",
    "# !pip install --upgrade httpcore\n",
    "# !pip install --upgrade googletrans==4.0.0-rc1\n",
    "# !pip install deep-translator\n",
    "\n",
    "import nltk\n",
    "print(nltk.data.find(\"corpora/stopwords\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_path = nltk.data.find(\"corpora/stopwords\")\n",
    "stopwords_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from deep_translator import GoogleTranslator  # Import the Googletrans library\n",
    "\n",
    "def identify_researcher_topics(data, fields, translate=False, target_language='en'):\n",
    "    \"\"\"\n",
    "    Identifies the top 3 research topics of a researcher based on their Lattes CV data.\n",
    "\n",
    "    Args:\n",
    "        data: A dictionary containing the researcher's Lattes CV data.\n",
    "        fields: A list of field names to be used for topic identification.\n",
    "\n",
    "    Returns:\n",
    "        A list of the top 3 research topics.\n",
    "    \"\"\"\n",
    "\n",
    "    def translate_text(text, target_language):\n",
    "        \"\"\"\n",
    "        Translates the text to the target language using Google Translate.\n",
    "        \"\"\"\n",
    "        translated = GoogleTranslator(source='auto', target=target_language).translate(text)\n",
    "        return translated\n",
    "\n",
    "    def extract_text_from_fields(data, fields, corpus):\n",
    "        \"\"\"\n",
    "        Recursively extracts text from the specified fields in the data dictionary.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if key in fields:\n",
    "                    if isinstance(value, list):\n",
    "                        for item in value:\n",
    "                            corpus.append(item.get(\"Descricao\", \"\") + item.get(\"titulo\", \"\"))\n",
    "                    else:\n",
    "                        corpus.append(str(value))  # Convert non-string values to string\n",
    "                else:\n",
    "                    extract_text_from_fields(value, fields, corpus)\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                extract_text_from_fields(item, fields, corpus)\n",
    "\n",
    "    # def preprocess_text(text, target_language='en'):\n",
    "    #     \"\"\"\n",
    "    #     Preprocesses the text by tokenizing, removing stopwords and proper nouns, translating, and removing punctuation.\n",
    "    #     \"\"\"\n",
    "    #     # Tokenize the text\n",
    "    #     words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    #     # Remove stopwords in the original language (if available)\n",
    "    #     source_language = detect(text)\n",
    "    #     try:\n",
    "    #         words = [word for word in words if word not in stopwords.words(source_language) and not word.istitle()]\n",
    "    #     except OSError:\n",
    "    #         print(f\"Warning: Stopwords not found for language '{source_language}'. Skipping stopword removal.\")\n",
    "\n",
    "    #     # Translate the text (if it's not already in the target language)\n",
    "    #     if target_language != 'auto' and source_language != target_language:\n",
    "    #         translator = GoogleTranslator(source=source_language, target=target_language)\n",
    "    #         text = translator.translate(text)\n",
    "\n",
    "    #         # Tokenize the translated text\n",
    "    #         words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    #         # Remove stopwords in the target language\n",
    "    #         words = [word for word in words if word not in stopwords.words(target_language)]\n",
    "\n",
    "    #     # Remove punctuation and numbers\n",
    "    #     words = [re.sub(r'[^\\w\\s]', '', word) for word in words if not word.isdigit()]\n",
    "\n",
    "    #     return \" \".join(words)\n",
    "\n",
    "    def preprocess_text(text, target_language='en'):\n",
    "        \"\"\"\n",
    "        Preprocesses the text by tokenizing, removing stopwords and proper nouns, translating, and removing punctuation.\n",
    "        \"\"\"\n",
    "        # Tokenize the text\n",
    "        words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "        # Remove stopwords in the original language (if available)\n",
    "        source_language = detect(text)\n",
    "        stopwords_path = nltk.data.find(\"corpora/stopwords\")\n",
    "        # stopwords_path = os.path.join(os.getenv('APPDATA'), 'Roaming', 'nltk_data', 'corpora', 'stopwords')  # Get stopwords path\n",
    "        if os.path.exists(os.path.join(stopwords_path, source_language)):\n",
    "            with open(os.path.join(stopwords_path, source_language), 'r', encoding='utf-8') as f:\n",
    "                stop_words = set(f.read().splitlines())\n",
    "            words = [word for word in words if word not in stop_words and not word.istitle()]\n",
    "\n",
    "        # Translate the text (if it's not already in the target language)\n",
    "        if target_language != 'auto' and source_language != target_language:\n",
    "            translator = GoogleTranslator(source=source_language, target=target_language)\n",
    "            text = translator.translate(text)\n",
    "\n",
    "            # Tokenize the translated text\n",
    "            words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "            # Remove stopwords in the target language\n",
    "            with open(os.path.join(stopwords_path, target_language), 'r', encoding='utf-8') as f:\n",
    "                stop_words = set(f.read().splitlines())\n",
    "            words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Remove punctuation and numbers\n",
    "        words = [re.sub(r'[^\\w\\s]', '', word) for word in words if not word.isdigit()]\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "    # Concatenate text from specified fields\n",
    "    corpus = []\n",
    "    extract_text_from_fields(data, fields, corpus)\n",
    "\n",
    "    # Preprocess the corpus\n",
    "    corpus = [preprocess_text(text) for text in corpus]\n",
    "\n",
    "    # Vectorize the text using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Apply LDA for topic modeling\n",
    "    lda = LatentDirichletAllocation(n_components=7, random_state=0)\n",
    "    lda.fit(X)\n",
    "\n",
    "    # Get the top words for each topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words.append([feature_names[i] for i in topic.argsort()[:-4:-1]])\n",
    "\n",
    "    return top_words\n",
    "\n",
    "pathfilename = os.path.join(folder_data_input,'docents_dict_list.json')\n",
    "with open(pathfilename, 'r', encoding='utf-8') as file:\n",
    "    docents_data = json.load(file)\n",
    "    print(f'{len(docents_data)} currículos carregados')\n",
    "\n",
    "# fields_to_use = [\"Formação Acadêmica\", \"Atuação Profissional\", \"Produções\"]\n",
    "fields_to_use = [\"titulo\"]\n",
    "\n",
    "for researcher in docents_data:\n",
    "    topics = identify_researcher_topics(researcher, fields_to_use)\n",
    "    print(f\"Principais tópicos de interesse para {researcher['Identificação']['Nome']}:\")\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"  Tópico {i+1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "oportunidades = detectar_oportunidades(G, produtos_prioritarios)\n",
    "print(\"Áreas de oportunidade:\", oportunidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo\n",
    "import requests\n",
    "\n",
    "# Cabeçalhos para a requisição\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "import http.client\n",
    "import ssl\n",
    "\n",
    "# Cria um contexto SSL sem verificação de certificado\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "# conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\")\n",
    "\n",
    "conn.request(\"GET\", \"/cities/filters?language=en\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import pandas as pd\n",
    "import ssl\n",
    "\n",
    "# Cria um contexto SSL sem verificação de certificado (NÃO RECOMENDADO PARA PRODUÇÃO)\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "# Função para fazer a requisição com tratamento de erro RETORNA OBJETO BYTES\n",
    "# def fazer_requisicao(conn, endpoint, method=\"GET\", params=None, body=None):\n",
    "#     try:\n",
    "#         # Cabeçalhos para a requisição\n",
    "#         headers = {\n",
    "#             \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "#             \"Accept\": \"application/json\",\n",
    "#         }\n",
    "#         conn.request(method, endpoint, body=body, headers=headers)\n",
    "#         res = conn.getresponse()\n",
    "#         data = res.read()\n",
    "#         return json.loads(data.decode(\"utf-8\"))\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro na requisição: {e}\")\n",
    "#         return None\n",
    "\n",
    "# Função para fazer a requisição com tratamento de erro e decodificação de bytes\n",
    "def fazer_requisicao(conn, endpoint, method=\"GET\", params=None, body=None):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        }\n",
    "        conn.request(method, endpoint, body=body, headers=headers)\n",
    "        res = conn.getresponse()\n",
    "\n",
    "        # Lê os dados da resposta como bytes\n",
    "        data = res.read()  \n",
    "\n",
    "        # Decodifica os bytes para uma string UTF-8\n",
    "        data_str = data.decode(\"utf-8\")  \n",
    "\n",
    "        # Converte a string JSON para um dicionário Python\n",
    "        return json.loads(data_str)  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na requisição: {e}\")\n",
    "        return None\n",
    "\n",
    "# Conexão com a API\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "\n",
    "# Endpoint para obter os anos disponíveis\n",
    "anos_url = \"/cities/dates/years\"\n",
    "anos_response = fazer_requisicao(conn, anos_url)\n",
    "\n",
    "# Inicializa as variáveis com valores padrão\n",
    "ano_inicial = 1997  # Ano inicial padrão\n",
    "ano_final = 2024   # Ano final padrão (ou o ano atual)\n",
    "\n",
    "if anos_response:\n",
    "    if anos_response.get('success', False):  # Verifica se a requisição foi bem-sucedida\n",
    "        if 'data' in anos_response and 'min' in anos_response['data'] and 'max' in anos_response['data']:\n",
    "            ano_inicial = int(anos_response['data']['min'])\n",
    "            ano_final = int(anos_response['data']['max'])\n",
    "        else:\n",
    "            print(\"Erro: As chaves 'data', 'min' e/ou 'max' não foram encontradas na resposta da API.\")\n",
    "    else:\n",
    "        print(f\"Erro na requisição para obter os anos: {anos_response}\")\n",
    "else:\n",
    "    print(\"Erro na requisição para obter os anos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anos_response['data']['min'])\n",
    "print(anos_response['data']['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.request(\"GET\", \"/cities/filters?language=en\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint para obter os valores do filtros disponíveis na API\n",
    "endpoint = \"/cities/filters\"\n",
    "filters_params = {\"language\": \"pt\"}\n",
    "filters_response = fazer_requisicao(conn, endpoint, params=filters_params)\n",
    "filters_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtros = [x['filter'] for x in filters_response.get('data').get('list')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_options_filters(api_filter):\n",
    "    # Cria um contexto SSL sem verificação de certificado\n",
    "    context = ssl._create_unverified_context()\n",
    "    conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "    conn.request(\"GET\", f\"/general/filters/{api_filter}?language=pt\")\n",
    "\n",
    "    res = conn.getresponse()\n",
    "    # print(f'objeto       res: {type(res)}')\n",
    "    data = res.read()\n",
    "    # print(f'objeto      data: {type(data)}')\n",
    "    data_str = data.decode(\"utf-8\")\n",
    "    # print(f'objeto  data_str: {type(data_str)}')\n",
    "    data_json = json.loads(data_str)\n",
    "    # print(f'objeto data_json: {type(data_json)}')\n",
    "    try:\n",
    "        results = [x.get('text') for x in data_json.get('data')[0]]\n",
    "        print(f'{len(results):>4} resultados para filtro {api_filter}')\n",
    "    except Exception as e:\n",
    "        print(f'     Erro ao buscar dados para filtro {api_filter}: {e}')\n",
    "    return [results][0]\n",
    "\n",
    "todos_campos_filtro={}\n",
    "for n,i in enumerate(filtros):\n",
    "    try:\n",
    "        todos_campos_filtro[i] = get_options_filters(i)\n",
    "    except Exception as e:\n",
    "        print(f'     Filtro {i} não disponível na API. Erro: {e}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicos=[]\n",
    "[unicos.append(x) for x in todos_campos_filtro['economicBlock'] if x not in unicos]\n",
    "unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicos=[]\n",
    "[unicos.append(x) for x in todos_campos_filtro['state'] if x not in unicos]\n",
    "unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_interesse = [\n",
    "    'VI - Produtos das indústrias químicas ou indústrias conexas', \n",
    "    'XVIII - Instrumentos e aparelhos de ótica, fotografia ou cinematografia, medida, controle ou de precisão; Instrumentos e aparelhos médico-cirúrgicos; Relógios e aparelhos semelhantes; Instrumentos musicais; Suas partes e acessórios',\n",
    "    'XX - Mercadorias e produtos diversos',\n",
    "    'XXII - Transações especiais'\n",
    "    ]\n",
    "\n",
    "chapter_interesse = [\n",
    "    '28 - Produtos químicos inorgânicos; compostos inorgânicos ou orgânicos de metais preciosos, de elementos radioativos, de metais das terras raras ou de isótopos',\n",
    "    '29 - Produtos químicos orgânicos',\n",
    "    '30 - Produtos farmacêuticos',\n",
    "    '35 - Matérias albuminóides; produtos à base de amidos ou de féculas modificados; colas; enzimas',\n",
    "    '38 - Produtos diversos das indústrias químicas',\n",
    "    ]\n",
    "\n",
    "heading_interesse = [\n",
    "    '2801 - Flúor, cloro, bromo e iodo',\n",
    "    '2802 - Enxofre sublimado ou precipitado; enxofre coloidal',\n",
    "    '2803 - Carbono (negros-de-carbono e outras formas não compreendidas em outras posições)',\n",
    "    '2804 - Hidrogénio, gases raros e outros elementos não metálicos',\n",
    "    '2805 - Metais alcalinos ou alcalino-terrosos; metais de terras raras, escândio e ítrio, mesmo misturados ou ligados entre si; mercúrio',\n",
    "    '2806 - Cloreto de hidrogénio (ácido clorídrico); ácido clorossulfúrico',\n",
    "    '2807 - Ácido sulfúrico e ácido sulfúrico fumante (oleum)',\n",
    "    '2808 - Ácido nítrico; ácidos sulfonítricos',\n",
    "    '2809 - Pentóxido de difosfóro; ácido fosfórico; ácidos polifosfóricos, de constituição química definida ou não',\n",
    "    '2810 - Óxidos de boro; ácidos bóricos',\n",
    "    '2811 - Outros ácidos inorgânicos e outros compostos oxigenados inorgânicos dos elementos não metálicos',\n",
    "    '2812 - Halogenetos e oxialogenetos dos elementos não metálicos',\n",
    "    '2813 - Sulfuretos dos elementos não metálicos; trissulfureto de fósforo comercial',\n",
    "    '2814 - Amoníaco anidro ou em solução aquosa (amónia)',\n",
    "    '2815 - Hidróxido de sódio (soda cáustica); hidróxido de potássio (potassa cáustica); peróxidos de sódio ou de potássio',\n",
    "    '2816 - Hidróxido e peróxido de magnésio; óxidos, hidróxidos e peróxidos, de estrôncio ou de bário',\n",
    "    '2817 - Óxido de zinco; peróxido de zinco',\n",
    "    '2818 - Corindo artificial, quimicamente definido ou não; óxido de alumínio; hidróxido de alumínio',\n",
    "    '2819 - Óxidos e hidróxidos de crómio',\n",
    "    '2820 - Óxidos de manganés',\n",
    "    '2821 - Óxidos e hidróxidos de ferro; terras corantes contendo, em peso, 70\\xa0% ou mais de ferro combinado, expresso em Fe2O3',\n",
    "    '2822 - Óxidos e hidróxidos de cobalto, inclusive os comerciais',\n",
    "    '2823 - Óxidos de titânio',\n",
    "    '2824 - Óxidos de chumbo; mínio (zarcão) e mínio-laranja (mine-orange)',\n",
    "    '2825 - Hidrazina e hidroxilamina, e seus sais inorgânicos; outras bases inorgânicas; outros óxidos, hidróxidos e peróxidos, de metais',\n",
    "    '2826 - Fluoretos; fluorossilicatos, fluoroaluminatos e outros sais complexos de flúor',\n",
    "    '2827 - Cloretos, oxicloretos e hidroxicloretos; brometos e oxibrometos; iodetos e oxiiodetos',\n",
    "    '2828 - Hipocloritos; hipoclorito de cálcio comercial; cloritos; hipobromitos',\n",
    "    '2829 - Cloratos e percloratos; bromatos e perbromatos; iodatos e periodatos',\n",
    "    '2830 - Sulfuretos; polissulfuretos, de constituição química definida ou não',\n",
    "    '2831 - Ditionites e sulfoxilatos',\n",
    "    '2832 - Sulfitos; tiosulfatos',\n",
    "    '2833 - Sulfatos; alúmenes; peroxosulfatos (persulfatos)',\n",
    "    '2834 - Nitritos; nitratos',\n",
    "    '2835 - Fosfinatos (hipofosfitos), fosfonatos (fosfitos) e fosfatos; polifosfatos, de constituição química definida ou não:',\n",
    "    '2836 - Carbonatos; peroxocarbonatos (percarbonatos); carbonato de amónio comercial contendo carbamato de amónio',\n",
    "    '2837 - Cianetos, oxicianetos e cianetos complexos',\n",
    "    '2838 - Fulminatos, cianatos e tiocianatos',\n",
    "    '2839 - Silicatos; silicatos dos metais alcalinos comerciais',\n",
    "    '2840 - Boratos; peroxoboratos (perboratos)',\n",
    "    '2841 - Sais dos ácidos oxometálicos ou peroxometálicos',\n",
    "    '2842 - Outros sais dos ácidos ou peroxoácidos inorgânicos (incluindo aluminossilicatos de constituição química definida ou não), exceto azidas',\n",
    "    '2843 - Metais preciosos no estado coloidal; compostos inorgânicos ou orgânicos de metais preciosos, de constituição química definida ou não; amálgamas de metais preciosos',\n",
    "    '2844 - Elementos químicos radioactivos e isótopos radioactivos (incluídos os elementos químicos e isótopos cindíveis ou férteis), e seus compostos; misturas e resíduos contendo esses produtos',\n",
    "    '2845 - Isótopos não incluídos na posição\\xa02844; seus compostos inorgânicos ou orgânicos, de constituição química definida ou não',\n",
    "    '2846 - Compostos, inorgânicos ou orgânicos, dos metais das terras raras, de ítrio ou de escândio ou das misturas destes metais',\n",
    "    '2847 - Peróxido de hidrogênio (água oxigenada), mesmo solidificado com ureia',\n",
    "    '2848 - Fosfetos, exceto ferrofósforos, quimicamente definidos ou não',\n",
    "    '2849 - Carbonetos de constituição química definida ou não',\n",
    "    '2850 - Hidretos, nitretos, azidas, silicietos e boretos, quimicamente definidos ou não',\n",
    "    '2851 - Compostos inorgânicos nesoi: liq ar: amálgamas nesoi',\n",
    "    '2852 - Compostos, inorgânicos ou orgânicos, de mercúrio, de constituição química definida ou não, exceto as amálgamas',\n",
    "    '2853 - Outros compostos inorgânicos (incluídas as águas destiladas, de condutibilidade ou de igual grau de pureza); ar líquido (incluído o ar líquido cujos gases raros foram eliminados); ar comprimido; amálgamas, exceto de metais preciosos.',\n",
    "    '2901 - Hidrocarbonetos acíclicos',\n",
    "    '2902 - Hidrocarbonetos cíclicos',\n",
    "    '2903 - Derivados halogenados dos hidrocarbonetos',\n",
    "    '2904 - Derivados sulfonados, nitrados ou nitrosados dos hidrocarbonetos, mesmo halogenados',\n",
    "    '2905 - Álcoois acíclicos e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2906 - Álcoois cíclicos e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2907 - Fenóis; fenóis-álcoois',\n",
    "    '2908 - Derivados halogenados, sulfonados, nitrados ou nitrosados dos fenóis ou dos fenóis-álcoois',\n",
    "    '2909 - Éteres, éteres-álcoois, éteres-fenóis, éteres-álcoois-fenóis, peróxidos de álcoois, peróxidos de éteres, peróxidos de cetonas (de constituição química definida ou não), e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2910 - Epóxidos, epoxi-álcoois, epoxi-fenóis e epoxi-éteres, com três átomos no ciclo, e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2911 - Acetais, semi-acetais, mesmo contendo outras funções oxigenadas, e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2912 - Aldeídos, mesmo contendo outras funções oxigenadas; polímeros cíclicos dos aldeídos; paraformaldeído',\n",
    "    '2913 - Derivados halogenados, sulfonados, nitrados ou nitrosados dos produtos da posição 2912',\n",
    "    '2914 - Cetonas e quinonas, mesmo contendo outras funções oxigenadas, e seus derivados halogenados, sulfonados, nitratos ou nitrosados',\n",
    "    '2915 - Ácidos monocarboxílicos acíclicos saturados e seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2916 - Ácidos monocarboxílicos acíclicos não saturados e ácidos monocarboxílicos cíclicos, seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2917 - Ácidos policarboxílicos, seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2918 - Ácidos carboxílicos contendo funções oxigenadas suplementares e seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2919 - Ésteres fosfóricos e seus sais, incluindo os lactofosfatos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2920 - Ésteres de outros ácidos inorgânicos de não-metais (exceto os ésteres de halogenetos de hidrogénio) e seus sais; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2921 - Compostos de função amina',\n",
    "    '2922 - Compostos aminados de funções oxigenadas',\n",
    "    '2923 - Sais e hidróxidos de amónio quaternários; lecitinas e outros fosfoaminolípidos, de constitução química definida ou não',\n",
    "    '2924 - Compostos de função carboxiamida; compostos de função amida do ácido carbónico',\n",
    "    '2925 - Compostos de função carboxiimida (incluindo a sacarina e seus sais) ou de função imina',\n",
    "    '2926 - Compostos de função nitrilo',\n",
    "    '2927 - Compostos diazóicos, azóicos e azóxicos',\n",
    "    '2928 - Derivados orgânicos da hidrazina e hidroxilamina',\n",
    "    '2929 - Compostos de outras funções azotadas (nitrogenadas)',\n",
    "    '2930 - Tiocompostos orgânicos',\n",
    "    '2931 - Outros compostos organo-inorgânicos',\n",
    "    '2932 - Compostos heterocíclicos exclusivamente de hetero-átomo(s) de oxigénio',\n",
    "    '2933 - Compostos heterocíclicos, exclusivamente de hetero-átomo(s) de azoto (nitrogénio)',\n",
    "    '2934 - Ácidos nucleicos e seus sais, de constituição química definida ou não; outros compostos heterocíclicos',\n",
    "    '2935 - Sulfonamidas',\n",
    "    '2936 - Provitaminas e vitaminas, naturais ou sintéticas (incluídos os concentrados naturais), bem como os seus derivados utilizados principalmente como vitaminas, misturados ou não entre si, mesmo em quaisquer soluções',\n",
    "    '2937 - Hormonas, prostaglandinas, tromboxanos e leucotrienos, naturais ou reproduzidos por síntese; seus derivados e análogos estruturais, incluindo os polipéptidos de cadeia modificada, utilizados principalmente como hormonas',\n",
    "    '2938 - Heterósidos, naturais ou sintéticos, seus sais, éteres, ésteres e outros derivados',\n",
    "    '2939 - Alcalóides vegetais, naturais ou sintéticos, seus sais, éteres, ésteres e outros derivados',\n",
    "    '2940 - Açúcares quimicamente puros, exceto sacarose, lactose, maltose, glicose e frutose; seus éteres e ésteres e seus sais',\n",
    "    '2941 - Antibióticos',\n",
    "    '2942 - Outros compostos orgânicos',\n",
    "    '3001 - Glândulas e outros órgãos para usos opoterápicos, dessecados, mesmo em pó; extractos de glândulas ou de outros órgãos ou das suas secreções, para usos opoterápicos; heparina e seus sais; outras substâncias humanas ou animais preparadas para fins terapêuti',\n",
    "    '3002 - Sangue humano; sangue animal preparado para usos terapêuticos, profilácticos ou de diagnóstico; anti-soros, outras fracções do sangue, produtos imunológicos modificados, mesmo obtidos por via biotecnológica; vacinas, toxinas, culturas de microrganismos (e',\n",
    "    '3003 - Medicamentos (exceto os produtos das posições\\xa03002, 3005\\xa0ou\\xa03006) constituídos por produtos misturados entre si, preparados para fins terapêuticos ou profilácticos, mas não apresentados em doses nem acondicionados para venda a retalho',\n",
    "    '3004 - Medicamentos (exceto os produtos das posições\\xa03002, 3005\\xa0ou\\xa03006) constituídos por produtos misturados ou não misturados, preparados para fins terapêuticos ou profilácticos, apresentados em doses (incluindo os destinados a serem administrados por via sub',\n",
    "    '3005 - Pastas (ouates), gazes, ataduras e artigos análogos (por exemplo: pensos, esparadrapos, sinapismos), impregnados ou recobertos de substâncias farmacêuticas ou acondicionados para venda a retalho para usos medicinais, cirúrgicos, dentários ou veterinários',\n",
    "    '3006 - Preparações e artigos farmacêuticos indicados na Nota\\xa04\\xa0do presente capítulo',\n",
    "    '3501 - Caseínas, caseinatos e outros derivados das caseínas; colas de caseína',\n",
    "    '3502 - Albuminas (incluídos os concentrados de várias proteínas de soro de leite, contendo, em peso calculado sobre matéria seca, mais de\\xa080\\xa0% de proteínas do soro de leite), albuminatos e outros derivados das albuminas',\n",
    "    '3503 - Gelatinas e seus derivados; ictiocola e outras colas de origem animal, exceto cola de caseína',\n",
    "    '3504 - Peptonas e seus derivados; outras matérias protéicas e seus derivados; pó de peles',\n",
    "    '3505 - Dextrina e outros amidos e féculas modificados (por exemplo: amidos e féculas pré-gelatinizados ou esterificados); colas à base de amidos ou de féculas, de dextrina ou de outros amidos ou féculas modificados',\n",
    "    '3506 - Colas e outros adesivos preparados, não especificados nem compreendidos em outras posições; produtos de qualquer espécie utilizados como colas ou adesivos, acondicionados para venda a retalho como colas ou adesivos, com peso líquido não superior a\\xa01\\xa0kg',\n",
    "    '3507 - Enzimas; enzimas preparadas não especificadas nem compreendidas em outras posições',\n",
    "    '3821 - Meios de cultura preparados para o desenvolvimento e a manutenção de microrganismos (incluindo os vírus e os organismos similares) ou de células vegetais, humanas ou animais',\n",
    "    '3822 - Reagentes de diagnóstico ou de laboratório, em qualquer suporte ou preparados, exceto os das posições 3002 ou 3006; materiais de referência certificados',\n",
    "    '3823 - Ácidos gordos monocarboxílicos industriais; óleos ácidos de refinação; alcoóis gordos industriais',\n",
    "    '3824 - Aglutinantes preparados para moldes ou para núcleos de fundição; produtos químicos e preparações das indústrias químicas ou das indústrias conexas (incluídos os constituídos por misturas de produtos naturais), não especificados nem compreendidos noutras p',\n",
    "    '3825 - Produtos residuais das indústrias químicas ou das indústrias conexas, não especificados nem compreendidos em outras posições; resíduos municipais; lamas de depuração; outros resíduos mencionados na Nota\\xa06 do presente capítulo',\n",
    "    '3901 - Polímeros de etileno, em formas primárias',\n",
    "    '3902 - Polímeros de propileno ou de outras olefinas, em formas primárias',\n",
    "    '3903 - Polímeros de estireno, em formas primárias',\n",
    "    '3904 - Polímeros de cloreto de vinilo ou de outras olefinas halogenadas, em formas primárias',\n",
    "    '3905 - Polímeros de acetato de vinilo ou de outros ésteres de vinilo, em formas primárias; outros polímeros de vinilo, em formas primárias',\n",
    "    '3906 - Polímeros acrílicos, em formas primárias',\n",
    "    '3907 - Poliacetais, outros poliéteres e resinas epóxidas, em formas primárias; policarbonatos, resinas alquídicas, poliésteres alílicos e outros poliésteres, em formas primárias',\n",
    "    '3908 - Poliamidas em formas primárias',\n",
    "    '3909 - Resinas amínicas, resinas fenólicas e poliuretanos, em formas primárias',\n",
    "    '3910 - Silicones, em formas primárias',\n",
    "    '8417 - Fornos industriais ou de laboratório, incluídos os incineradores, não elétricos',\n",
    "    '8418 - Refrigeradores, congeladores (freezers) e outro material, máquinas e aparelhos para a produção de frio, com equipamento eléctrico ou outro; bombas de calor, excluídas as máquinas e aparelhos de ar condicionado da posição 8415',\n",
    "    '8419 - Aparelhos e dispositivos, mesmo aquecidos electricamente (exceto fornos e outros aparelhos da posição 8514), para tratamento de matérias por meio de operações que impliquem mudança de temperatura, tais como o aquecimento, cozimento, torrefacção, destilaç',\n",
    "    '8420 - Calandras e laminadores, exceto os destinados ao tratamento de metais ou vidro, e seus cilindros',\n",
    "    '8421 - Centrifugadores, incluídos os secadores centrífugos, aparelhos para filtrar ou depurar líquidos ou gases',\n",
    "    '8423 - Aparelhos e instrumentos de pesagem, incluídas as básculas e balanças para verificar peças fabricadas, excluídas as balanças sensíveis a pesos não superiores a 5 cg; pesos para quaisquer balanças',\n",
    "    '8471 - Máquinas automáticas para processamento de dados e suas unidades; leitores magnéticos ou ópticos, máquinas para registar dados em suporte sob forma codificada, e máquinas para processamento desses dados, não especificadas nem compreendidas em outras posiç',\n",
    "    '8472 - Outras máquinas e aparelhos de escritório [por exemplo: duplicadores hectográficos ou a stencil, máquinas para imprimir endereços, distribuidores automáticos de papel-moeda, máquinas para seleccionar, contar ou empacotar moedas, afiadores (apontadores) me',\n",
    "    '9011 - Microscópios ópticos, incluídos os microscópios para fotomicrografia, cinefotomicrografia ou microprojecção',\n",
    "    '9012 - Microscópios, exceto ópticos; difractógrafos',\n",
    "    '9013 - Dispositivos de cristais líquidos que não constituam artigos compreendidos mais especificamente em outras posições; lasers, exceto díodos laser; outros aparelhos e instrumentos de óptica, não especificados nem compreendidos em outras posições do presente',\n",
    "    '9016 - Balanças sensíveis a pesos >= 5 cg, com ou sem pesos',\n",
    "    '9021 - Artigos e aparelhos ortopédicos, incluídas as cintas e fundas médico-cirúrgicas e as muletas; talas, goteiras e outros artigos e aparelhos para fracturas; artigos e aparelhos de prótese; aparelhos para facilitar a audição dos surdos e outros aparelhos par',\n",
    "    '9022 - Aparelhos de raios X e aparelhos que utilizem as radiações alfa, beta ou gama, mesmo para usos médicos, cirúrgicos, odontológicos ou veterinários, incluídos os aparelhos de radiofotografia ou de radioterapia, os tubos de raios X e outros dispositivos gera',\n",
    "    '9023 - Instrumentos, aparelhos e modelos, concebidos para demonstração (por exemplo, no ensino e nas exposições), não suscetíveis de outros usos',\n",
    "    '9024 - Máquinas e aparelhos para ensaios de dureza, tracção, compressão, elasticidade e de outras propriedades mecânicas de materiais (por exemplo: metais, madeira, têxteis, papel, plásticos)',\n",
    "    '9025 - Densímetros, areómetros, pesa-líquidos e instrumentos flutuantes semelhantes, termómetros, pirómetros, barómetros, higrómetros e psicrómetros, registadores ou não, mesmo combinados entre si',\n",
    "    '9026 - Instrumentos e aparelhos para medida ou controlo do caudal (vazão), do nível, da pressão ou de outras características variáveis dos líquidos ou gases (por exemplo: medidores de caudal, indicadores de nível, manómetros, contadores de calor), exceto os ins',\n",
    "    '9027 - Instrumentos e aparelhos para análises físicas ou químicas (por exemplo: polarímetros, refractómetros, espectrómetros, analisadores de gases ou de fumos); instrumentos e aparelhos para ensaios de viscosidade, porosidade, dilatação, tensão superficial ou s',\n",
    "    '9028 - Contadores de gases, de líquidos ou de electricidade, incluídos os aparelhos para a sua aferição',\n",
    "    '9029 - Outros contadores (por exemplo: contadores de voltas, contadores de produção, taxímetros, totalizadores de caminho percorrido, podómetros); indicadores de velocidade e tacómetros, exceto os das posições 9014 ou 9015; estroboscópios',\n",
    "    '9030 - Osciloscópios, analisadores de espectro e outros instrumentos e aparelhos para medida ou controlo de grandezas elétricas; instrumentos e aparelhos para medida ou detecção de radiações alfa, beta, gama, X, cósmicas ou outras radiações ionizantes',\n",
    "    '9031 - Instrumentos, aparelhos e máquinas de medida ou controlo, não especificados nem compreendidos em outras posições do presente capítulo; projectores de perfis',\n",
    "    '9032 - Instrumentos e aparelhos para regulação ou controlo, automáticos',\n",
    "    '9033 - Partes e acessórios não especificados nem compreendidos noutras posições do presente Capítulo, para máquinas, aparelhos, instrumentos ou artigos do Capítulo 90',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "conn.request(\"GET\", \"/tables/ncm/02042200\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "conn.request(\"GET\", \"/tables/ncm?language=pt\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesse = []\n",
    "# f = filtros[0]\n",
    "# print(f)\n",
    "# for n,i in enumerate(todos_campos_filtro.get(f)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Endpoint para obter os valores por filtro disponíveis na API\n",
    "# # Lista os valores disponíveis para um filtro específico. Exemplos de como acessar os valores possíveis por diferentes filtros:\n",
    "# # Países: /general/filters/country?language=pt\n",
    "# # Blocos Econômicos: /general/filters/economicBlock?language=pt\n",
    "# # Seções (do Sistema Harmonizado - SH): /general/filters/section?language=pt\n",
    "# # NCM (Nomenclatura Comum do Mercosul): /general/filters/ncm?language=pt\n",
    "\n",
    "# import http.client\n",
    "\n",
    "# # Cria um contexto SSL sem verificação de certificado\n",
    "# context = ssl._create_unverified_context()\n",
    "# conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "# conn.request(\"GET\", \"/general/filters/heading?language=pt\")\n",
    "\n",
    "# res = conn.getresponse()\n",
    "# print(f'objeto       res: {type(res)}')\n",
    "# data = res.read()\n",
    "# print(f'objeto      data: {type(data)}')\n",
    "# data_str = data.decode(\"utf-8\")\n",
    "# print(f'objeto  data_str: {type(data_str)}')\n",
    "# data_json = json.loads(data_str)\n",
    "# print(f'objeto data_json: {type(data_json)}')\n",
    "\n",
    "# Lista de campos no filtro heading\n",
    "# heading_list = [y.get('text') for y in data_json.get('data')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de campos no filtro chapter\n",
    "['country', 'economicBlock', 'state', 'city', 'heading', 'chapter', 'section']\n",
    "filter='economicBlock'\n",
    "api_filters = ['']\n",
    "list_country = get_options_filters('country')\n",
    "list_economicBlock = get_options_filters('economicBlock')\n",
    "list_state = get_options_filters('state')\n",
    "list_heading = get_options_filters('heading')\n",
    "list_chapter = get_options_filters('chapter')\n",
    "list_section = get_options_filters('section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Inicializa a lista de NCMs de saúde com valores padrão\n",
    "ncms_saude = []  # Lista vazia caso a requisição falhe\n",
    "\n",
    "if ncm_response:\n",
    "    ncms = ncm_response.json()\n",
    "    # Filtrar os NCMs relacionados à saúde (exemplo)\n",
    "    ncms_saude = [ncm['id'] for ncm in ncms if ncm['desc'].startswith(\"Medicamentos\")]\n",
    "\n",
    "# Endpoint para consulta dos dados\n",
    "consulta_url = \"/cities\"\n",
    "\n",
    "# Parâmetros da consulta (exemplo)\n",
    "params = {\n",
    "    \"flow\": [\"export\", \"import\"],\n",
    "    \"monthDetail\": False,\n",
    "    \"period\": {\"from\": f\"{ano_inicial}-01\", \"to\": f\"{ano_final}-12\"},\n",
    "    \"filters\": [{\"filter\": \"ncm\", \"values\": ncms_saude}],\n",
    "    \"details\": [\"ncm\"],\n",
    "    \"metrics\": [\"metricFOB\"],\n",
    "}\n",
    "\n",
    "# Realiza a consulta\n",
    "response = fazer_requisicao(conn, consulta_url, params=params)\n",
    "\n",
    "if response:\n",
    "    data = response\n",
    "    df = pd.DataFrame(data['data'])\n",
    "\n",
    "    # Calcula o déficit por ano e NCM\n",
    "    df_pivot = df.pivot_table(\n",
    "        index=[\"year\", \"ncm\"], columns=\"flow\", values=\"metricFOB\", aggfunc=\"sum\"\n",
    "    )\n",
    "    df_pivot[\"deficit\"] = df_pivot[\"import\"] - df_pivot[\"export\"]\n",
    "\n",
    "    # Salva os resultados em um arquivo CSV\n",
    "    df_pivot.to_csv(\"deficit_balanca_comercial_saude.csv\")\n",
    "\n",
    "    print(\"Dados salvos com sucesso!\")\n",
    "\n",
    "# Fecha a conexão\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Inicializa a lista de NCMs de saúde com valores padrão\n",
    "ncms_saude = []  # Lista vazia caso a requisição falhe\n",
    "\n",
    "if ncm_response:\n",
    "    ncms = ncm_response.json()\n",
    "    # Filtrar os NCMs relacionados à saúde (exemplo)\n",
    "    ncms_saude = [ncm['id'] for ncm in ncms if ncm['desc'].startswith(\"Medicamentos\")]\n",
    "    print(ncms_saude)\n",
    "\n",
    "# Endpoint para consulta dos dados\n",
    "consulta_url = \"https://api-comexstat.mdic.gov.br/cities\"\n",
    "\n",
    "# Parâmetros da consulta (exemplo)\n",
    "params = {\n",
    "    \"flow\": [\"export\", \"import\"],\n",
    "    \"monthDetail\": False,\n",
    "    \"period\": {\"from\": f\"{ano_inicial}-01\", \"to\": f\"{ano_final}-12\"},\n",
    "    \"filters\": [{\"filter\": \"ncm\", \"values\": ncms_saude}],\n",
    "    \"details\": [\"ncm\"],\n",
    "    \"metrics\": [\"metricFOB\"],\n",
    "}\n",
    "\n",
    "# Realiza a consulta\n",
    "response = fazer_requisicao(consulta_url, json=params)\n",
    "\n",
    "if response:\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # # Calcula o déficit por ano e NCM\n",
    "    # df_pivot = df.pivot_table(\n",
    "    #     index=[\"coAno\", \"ncm\"], columns=\"flow\", values=\"metricFOB\", aggfunc=\"sum\"\n",
    "    # )\n",
    "    # df_pivot[\"deficit\"] = df_pivot[\"import\"] - df_pivot[\"export\"]\n",
    "\n",
    "    # # Salva os resultados em um arquivo CSV\n",
    "    # df_pivot.to_csv(\"deficit_balanca_comercial_saude.csv\")\n",
    "\n",
    "    # print(\"Dados salvos com sucesso!\")\n",
    "for i in df['data'].items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar classes do pacote\n",
    "from research_process_automation import QuestionFormulation, InteractiveFeedback, QuestionCriteriaEvaluator\n",
    "\n",
    "# Criar instância da classe QuestionFormulation\n",
    "question_formulator = QuestionFormulation()\n",
    "\n",
    "# Solicitar informações do usuário\n",
    "question_formulator.input_ideas()\n",
    "\n",
    "# Gerar a pergunta de pesquisa\n",
    "research_question = question_formulator.generate_question()\n",
    "print(\"PASSO 01: Questão de pesquisa\")\n",
    "print(research_question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entradas de referência:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entradas de citações para referências:\n",
    "\n",
    "@book{gil2022comoelaborarprojetosdepesquisa,\n",
    "  title={Como Elaborar Projetos de Pesquisa},\n",
    "  author={Gil, Antonio Carlos},\n",
    "  edition={7},\n",
    "  publisher={Atlas},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "@book{popper1934logic,\n",
    "  title={A Lógica da Pesquisa Científica},\n",
    "  author={Popper, Karl},\n",
    "  year={1934}\n",
    "}\n",
    "\n",
    "@misc{IEP2023Popper,\n",
    "  author = {Internet Encyclopedia of Philosophy},\n",
    "  title = {Karl Popper: Political Philosophy},\n",
    "  year = {2023},\n",
    "  howpublished = {\\url{https://iep.utm.edu/popp-pol/}},\n",
    "  note = {Acesso em: 01/01/2024}\n",
    "}\n",
    "\n",
    "@phdthesis{Broderick1984,\n",
    "  author = {David Gregory Broderick},\n",
    "  title = {Objectivity: Thomas Aquinas and Karl Popper},\n",
    "  school = {Boston College},\n",
    "  year = {1984}\n",
    "}\n",
    "\n",
    "@article{CRYFUNavara2023,\n",
    "  author = {Grupo Ciencia, Razón y Fe (CRYF)},\n",
    "  title = {The Ethical Roots of Karl Popper's Epistemology},\n",
    "  journal = {Universidad de Navarra},\n",
    "  year = {2023},\n",
    "  url = {https://www.unav.edu/web/ciencia-razon-y-fe/poppers-epistemology}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bases de Dados:\n",
    "\n",
    "@misc{arxiv,\n",
    "  title = {{arXiv}},\n",
    "  url = {https://arxiv.org}\n",
    "}\n",
    "\n",
    "@misc{core,\n",
    "  title = {{CORE}},\n",
    "  url = {https://core.ac.uk}\n",
    "}\n",
    "\n",
    "@misc{doaj,\n",
    "  title = {{Directory of Open Access Journals (DOAJ)}},\n",
    "  url = {https://doaj.org}\n",
    "}\n",
    "\n",
    "@misc{googlescholar,\n",
    "  title = {{Google Scholar}},\n",
    "  url = {https://scholar.google.com}\n",
    "}\n",
    "\n",
    "@misc{openaire,\n",
    "  title = {{OpenAIRE}},\n",
    "  url = {https://www.openaire.eu}\n",
    "}\n",
    "\n",
    "@misc{pubmedcentral,\n",
    "  title = {{PubMed Central}},\n",
    "  url = {https://www.ncbi.nlm.nih.gov/pmc/}\n",
    "}\n",
    "\n",
    "@misc{ssrn,\n",
    "  title = {{Social Science Research Network (SSRN)}},\n",
    "  url = {https://www.ssrn.com}\n",
    "}\n",
    "\n",
    "@misc{scienceopen,\n",
    "  title = {{ScienceOpen}},\n",
    "  url = {https://www.scienceopen.com}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editoras com políticas de OA:\n",
    "\n",
    "@misc{springernature,\n",
    "  title = {{Springer Nature}},\n",
    "  url = {https://www.springernature.com}\n",
    "}\n",
    "\n",
    "@misc{oup,\n",
    "  title = {{Oxford University Press (OUP)}},\n",
    "  url = {https://academic.oup.com}\n",
    "}\n",
    "\n",
    "@misc{frontiers,\n",
    "  title = {{Frontiers}},\n",
    "  url = {https://www.frontiersin.org}\n",
    "}\n",
    "\n",
    "@misc{wiley,\n",
    "  title = {{Wiley}},\n",
    "  url = {https://www.wiley.com}\n",
    "}\n",
    "\n",
    "@misc{plos,\n",
    "  title = {{Public Library of Science (PLOS)}},\n",
    "  url = {https://www.plos.org}\n",
    "}\n",
    "\n",
    "@misc{hindawi,\n",
    "  title = {{Hindawi}},\n",
    "  url = {https://www.hindawi.com}\n",
    "}\n",
    "\n",
    "@misc{mdpi,\n",
    "  title = {{MDPI AG}},\n",
    "  url = {https://www.mdpi.com}\n",
    "}\n",
    "\n",
    "@misc{informa,\n",
    "  title = {{Informa PLC}},\n",
    "  url = {https://www.informa.com}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GII\n",
    "@misc{GII-WIPO,\n",
    "      title = {Global Innovation Index - WIPO Series},\n",
    "      abstract = {The Global Innovation Index (GII) ranks the innovation performance of some 131 countries and economies around the world, based on 80+ indicators. Co-published by WIPO, Cornell University and INSEAD, the report provides an annual ranking of the innovation capabilities and performance of economies around the world.},\n",
    "      {url = https://www.wipo.int/publications/en/series/index.jsp?id=129}\n",
    "}\n",
    "\n",
    "@misc{GII-2011,\n",
    "  title = {Global Innovation Index 2011 - Accelerating Growth and Development},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=274&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2012,\n",
    "  title = {Global Innovation Index 2012 - Stronger Innovation Linkages for Global Growth},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=247&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2013,\n",
    "  title = {Global Innovation Index 2013 - The Local Dynamics of Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=368&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2014,\n",
    "  title = {Global Innovation Index 2014 - The Human Factor in Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=3254&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2015,\n",
    "  title = {Global Innovation Index 2015 - Effective Innovation Policies for Development},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=3978&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2016,\n",
    "  title = {Global Innovation Index 2016 - Winning with Global Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4064&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2017,\n",
    "  title = {Global Innovation Index 2017 - Innovation Feeding the World},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4193&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2018,\n",
    "  title = {Global Innovation Index 2018 - Energizing the World with Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4330&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2019,\n",
    "  title = {Global Innovation Index 2019 - Creating Healthy Lives — The Future of Medical Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4434&plang=EN}\n",
    "}\n",
    "\n",
    "@article{40579,\n",
    "      author = {Cornell University.},\n",
    "      url = {http://tind.wipo.int/record/40579},\n",
    "      title = {Global Innovation Index 2019 - Executive version.},\n",
    "      abstract = {The Global Innovation Index 2019 provides detailed metrics  about the innovation performance of 129 countries and  economies around the world. Its 80 indicators explore a  broad vision of innovation, including political  environment, education, infrastructure and business  sophistication. The GII 2019 analyzes the medical  innovation landscape of the next decade, looking at how  technological and non-technological medical innovation will  transform the delivery of healthcare worldwide. It also  explores the role and dynamics of medical innovation as it  shapes the future of healthcare, and the potential  influence this may have on economic growth. Chapters of the  report provide more details on this year’s theme from  academic, business, and particular country perspectives  from leading experts and decision makers.},\n",
    "      doi = {https://doi.org/10.34667/tind.40579},\n",
    "      recid = {40579},\n",
    "      pages = {214 pages ;},\n",
    "}\n",
    "\n",
    "@article{35279,\n",
    "      url = {http://tind.wipo.int/record/35279},\n",
    "      title = {Índice Global de inovação de 2019 - PRINCIPAIS  RESULTADOS.},\n",
    "      abstract = {Criar Vidas Sadias - O Futuro da Inovação Médica.},\n",
    "      doi = {https://doi.org/10.34667/tind.35279},\n",
    "      recid = {35279},\n",
    "      pages = {20 pages ;},\n",
    "}\n",
    "\n",
    "@misc{GII-2020,\n",
    "  title = {Global Innovation Index 2020 - Who Will Finance Innovation?},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4514&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2021,\n",
    "  title = {Global Innovation Index 2021 - Tracking Innovation through the COVID-19 Crisis},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4560&plang=EN}\n",
    "}\n",
    "\n",
    "@article{46620,\n",
    "      url = {http://tind.wipo.int/record/46620},\n",
    "      title = {Índice Global de Inovação 2022 : Resumo executivo.},\n",
    "      abstract = {O Índice Global da Inovação 2022 (IGI) analisa as  tendências globais no campo da inovação em um cenário  marcado pela pandemia de COVID-19 em curso, por um  crescimento desacelerado da produtividade e pelo surgimento  de novos desafios. O IGI revela as economias mais  inovadoras do mundo, classificando o desempenho em  inovação de 132 economias, destacando seus pontos fortes  e fracos em matéria de inovação e identificando lacunas  em suas métricas de inovação. Esta edição de 2022 tem  como foco o efeito da inovação sobre a produtividade e o  bem-estar da sociedade ao longo das próximas décadas.},\n",
    "      doi = {https://doi.org/10.34667/tind.46620},\n",
    "      recid = {46620},\n",
    "      pages = {28 pages :},\n",
    "}\n",
    "\n",
    "@misc{GII-2022,\n",
    "  title = {Global Innovation Index 2022 - What is the future of innovation driven growth?},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4622&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2023,\n",
    "  title = {Global Innovation Index 2023},\n",
    "  url = {https://www.globalinnovationindex.org/gii-2023}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
