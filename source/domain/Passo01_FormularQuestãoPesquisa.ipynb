{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Interesse de pesquisa</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O termo \"inovação tecnológica\", no sentido moderno, começou a ser mais amplamente discutido e compreendido no início do século XX. Joseph Schumpeter, um economista austríaco, foi uma das figuras-chave que destacou a inovação tecnológica como um motor para o crescimento econômico. Após a Primeira Guerra Mundial, pensadores como Thorstein Veblen e Herbert Hoover também enfatizaram a importância da inovação tecnológica para a segurança nacional e competitividade industrial. Esse conceito se expandiu particularmente após a Segunda Guerra Mundial, quando a inovação tecnológica começou a ser vista como crucial para a prosperidade industrial e a segurança militar, especialmente nos Estados Unidos. fonte: Encyclopedia.com sobre Inovação Tecnológica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Bases filosóficas</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grandes filósofos durante toda história tem se oposto ao totalitarismo. Pelo menos a partir da visão Aristotélica as formas degeneradas de governar já são apontadas. Dentre as chamadas formas degeneradas, a tirania pode ser entendida como a pior forma de sistema político, sendo o resultado da maior degeneração do poder centralizado em uma única pessoa (como em uma monarquia absolutista). Na tirania o tirano governa sem consultar a ninguém e, geralmente, sua tomada de poder é ilegítima, e a lei não tem papel algum. Na oligarquia, onde o poder de decisão do governo é constituído por um pequeno número de privilegiados, há uma subclassificação quanto ao número dos donos do poder, como por exemplo, na Politirania onde os oligarcas governam hereditariamente e na riqueza, mas respeitando mais a lei. Ou nas oligarquias com uma maior percentagem de oligarcas, quando passa-se da hereditariedade à nomeação dos amigos do governo, independentemente da linhagem sanguínea destes, dentre outros tipos de oligarquia. \n",
    "\n",
    "Tomas de Aquino foi um filósofo de referência no papel da moral como orientação maior para a sociedade. Mais recentemente, Karl Popper foi um severo crítico ao totalitarismo e utopias demagógicas. São perceptíveis interseções e diálogos filosóficos entre as ideias centrais dessas filosofias, especialmente em epistemologia e ética, têm sido tema de discussão acadêmica. Por exemplo, na dissertação de David Gregory Broderick, intitulada \"Objetividade: Tomás de Aquino e Karl Popper\", explora-se a relação entre as noções de objetividade nas obras de Aquino e Popper. A dissertação sugere que, embora haja diferenças terminológicas, históricas e de interesses entre Aquino e Popper, também existem paralelos significativos em suas abordagens sobre objetividade e o crescimento do conhecimento.\n",
    "\n",
    "Além disso, na discussão sobre as raízes éticas da epistemologia de Popper, explorada pelo Grupo Ciencia, Razón y Fe (CRYF) da Universidad de Navarra, destaca-se a complementaridade das posições de Aquino e Popper. Esta análise sugere que a forte defesa de Popper do realismo, da verdade objetiva e da metodologia para o crescimento do conhecimento conjectural pode ser vista como complementar, ou até mesmo fundamentada, nos princípios éticos e metafísicos delineados por Aquino.\n",
    "\n",
    "Essas análises sugerem que, embora Popper possa não ter citado explicitamente Aquino em suas principais obras, as bases filosóficas de seus pensamentos, especialmente em relação à objetividade, ética e crescimento do conhecimento, têm grandes paralelos e áreas de convergência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, tal como acontece com a sua defesa das eleições numa democracia, o argumento de Popper a favor da engenharia social fragmentada baseia-se principalmente na sua compatibilidade com o método de tentativa e erro das ciências naturais: uma teoria é proposta e testada, erros na teoria são detectados e eliminado, e uma teoria nova e melhorada emerge, reiniciando o ciclo. Através da engenharia gradual, o processo de progresso social é, portanto, paralelo ao progresso científico. Na verdade, Popper diz que a engenharia social fragmentada é a única abordagem à política pública que pode ser genuinamente científica: \"Isto - e nenhum planeamento utópico ou profecia histórica - significaria a introdução do método científico na política, uma vez que todo o segredo do método científico é uma disposição para aprender com os erros\" ( Open Society Vol 1., 163)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Metodologia da Pesquisa</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma contribuição seminal para abordagem falseável da ciência é a descrita por Karl Popper em sua obra \"A Lógica da Pesquisa Científica\", publicada originalmente em 1934. É nesta obra que Popper argumenta que a ciência deve adotar uma metodologia baseada na falseabilidade. Segundo ele, nenhuma quantidade de experimentos pode provar uma teoria, mas um único experimento ou observação reproduzível pode refutá-la. Esta abordagem destaca a importância da capacidade de uma teoria ser testada e potencialmente falsificada, em vez de apenas verificada. Popper diferencia as teorias científicas das pseudociências e da metafísica, salientando que as teorias científicas devem ser testáveis e passíveis de refutação, enquanto pseudociências e metafísicas não permitem essa possibilidade. Popper enfatiza que a ciência deve estar em constante evolução, com as hipóteses sendo submetidas a testes contínuos para acompanhar o desenvolvimento da ciência e das tecnologias (POPPER, 1934).\n",
    "\n",
    "A gênese de uma boa questão de pesquisa também é abordada por outros autores notáveis no campo da metodologia científica. Antonio Carlos Gil descreve que a elaboração de projetos de pesquisa deve ser guiada pela apresentação clara e acessível dos elementos necessários para a pesquisa, além da organização de conhecimentos dispersos. Gil enfatiza a natureza prática da pesquisa, abordando a importância de esclarecer os procedimentos para a elaboração de projetos em diversos tipos de pesquisa. (GIL, 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Passo 01: Redigir uma boa questão de pesquisa</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como chegar à uma boa questão de pesquisa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns dos critérios essenciais para avaliar a qualidade de uma questão de pesquisa, e como integrar ou considerar cada um deles no processo de formulação:\n",
    "\n",
    "<b>Clareza e Especificidade</b>: Uma boa questão de pesquisa deve ser clara e específica, evitando ambiguidades. Isto pode ser parcialmente garantido pelo processamento de linguagem natural, mas também requer revisão humana para garantir que a questão seja compreensível e precisamente focada.\n",
    "\n",
    "<b>Relevância Acadêmica ou Científica</b>: A questão deve ser relevante para o campo de estudo e contribuir de alguma forma para o conhecimento existente. Isso geralmente requer uma compreensão do contexto acadêmico e das pesquisas atuais, o que pode ser além do escopo da automação completa.\n",
    "\n",
    "<b>Viabilidade</b>: A pergunta deve ser algo que pode ser realisticamente respondido através de métodos de pesquisa disponíveis. Este aspecto pode ser parcialmente verificado por meio de regras heurísticas programadas, mas frequentemente requer avaliação humana, especialmente para julgar a disponibilidade de dados ou recursos de pesquisa.\n",
    "\n",
    "<b>Originalidade</b>: Uma boa questão de pesquisa deve oferecer novas perspectivas ou abordar lacunas existentes na literatura. A originalidade pode ser difícil de avaliar automaticamente, mas técnicas avançadas de NLP, como análise semântica e comparação com bancos de dados de literatura existente, podem ajudar.\n",
    "\n",
    "<b>Importância Prática ou Teórica</b>: A questão deve ter alguma importância prática ou contribuir para a compreensão teórica de um tópico. Isso geralmente exige conhecimento especializado na área de estudo para avaliar.\n",
    "\n",
    "<b>Estruturação Adequada</b>: A pergunta deve ser estruturada de forma a facilitar uma abordagem de pesquisa clara. Isso inclui a utilização de uma formulação que se alinhe com métodos de pesquisa qualitativos ou quantitativos, conforme apropriado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como Integrar os critérios da questão de pesquisa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerar uma boa questão de pesquisa depende de uma combinação de processamento automatizado, conhecimento especializado e revisão humana. A automação pode fornecer uma base útil, mas a supervisão e o julgamento humanos são cruciais para garantir a qualidade final da pergunta de pesquisa. As seguintes estratégias são utilizadas em nossa solução para chegar a uma boa questão de pesquisa:\n",
    "\n",
    "<b>Automatização com Revisão Humana</b>: Uma abordagem prática é usar a automação para gerar uma primeira versão da pergunta, que é então revisada e refinada por pesquisadores humanos (pesquisador principal, equipe de pesquisa e orientador). A automação garante que certos critérios básicos sejam atendidos (como clareza e estruturação), enquanto a revisão humana aborda aspectos mais sutis, como relevância, viabilidade e originalidade.\n",
    "\n",
    "<b>Feedback Interativo</b>: Incorporar um sistema de feedback no processo, onde o usuário pode refinar suas ideias iniciais ou ajustar a pergunta gerada, pode ajudar a melhorar a qualidade da questão de pesquisa final.\n",
    "\n",
    "<b>Integração de vários Bancos de Dados de Literatura</b>: Integrar um banco de dados de literatura existente pode ajudar a avaliar a originalidade e a relevância da pergunta, comparando-a com pesquisas já publicadas.\n",
    "\n",
    "<b>Educação do Usuário</b>: Fornecer orientações e exemplos de boas perguntas de pesquisa aos usuários pode ajudá-los a formular ideias iniciais mais eficazes, levando a melhores resultados na formulação automática.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Bases de dados de artigos em Open Access</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao focar a pesquisa em fontes que oferecem conteúdos completos em Open Access, há bases de dados e repositórios acadêmicos importantes que se destacam em relevância e quantidade de conteúdos:\n",
    "\n",
    "<b>Directory of Open Access Journals (DOAJ)</b>: O DOAJ é um diretório online que indexa e fornece acesso a periódicos de alta qualidade, todos de acesso livre e revisados por pares. É uma excelente fonte para pesquisar artigos em uma ampla gama de disciplinas. O DOAJ oferece uma API para acessar seu índice de periódicos e artigos. A documentação e detalhes sobre a API estão disponíveis em DOAJ API.\n",
    "\n",
    "<b>PubMed Central</b>: Operado pela Biblioteca Nacional de Medicina dos EUA, o PubMed Central é um repositório gratuito de artigos de ciências biomédicas e ciências da vida. Embora seu foco seja mais na área da saúde, ele pode ter artigos relevantes sobre inovação tecnológica no contexto da saúde. A PubMed Central oferece uma API chamada Entrez Programming Utilities (E-utilities) para interagir com a base de dados. Mais informações podem ser encontradas em Entrez Programming Utilities Help.\n",
    "\n",
    "<b>arXiv</b>: O arXiv é um repositório de preprints em campos como física, matemática, ciência da computação, biologia quantitativa, finanças quantitativas e estatística. É uma boa fonte para literatura mais técnica e teórica sobre inovação tecnológica. O arXiv fornece uma API para acesso aos seus preprints. Informações detalhadas e documentação sobre a API estão disponíveis em arXiv API.\n",
    "\n",
    "<b>OpenAIRE</b>: Uma infraestrutura que promove a descoberta e o acesso a publicações científicas europeias de acesso livre. É especialmente útil para pesquisas que envolvem colaborações europeias ou focam em políticas e práticas de inovação na Europa. OpenAIRE oferece uma API para acessar seu repositório. Você pode encontrar mais informações sobre como usar esta API em OpenAIRE API.\n",
    "\n",
    "<b>Google Scholar</b>: Apesar de não ser exclusivamente dedicado ao Open Access, o Google Scholar pode ser utilizado para localizar artigos de acesso livre. Ele indexa uma variedade de fontes acadêmicas e muitas vezes inclui links para versões de acesso livre dos artigos. O Google Scholar não oferece uma API oficial para acesso programático.\n",
    "\n",
    "<b>CORE</b>: Agregador que permite o acesso a milhões de artigos de acesso livre. Ele reúne conteúdo de repositórios e periódicos de todo o mundo, sendo uma excelente ferramenta para uma pesquisa abrangente. CORE oferece uma API que permite acessar seu vasto repositório de artigos de acesso livre. A documentação da API pode ser encontrada em CORE API.\n",
    "\n",
    "<b>ScienceOpen</b>: Plataforma de pesquisa e publicação que oferece acesso a mais de 60 milhões de artigos e registros de pesquisa em todas as áreas. Não há informações disponíveis sobre uma API pública para o ScienceOpen.\n",
    "\n",
    "<b>SSRN (Social Science Research Network)</b>: Especializado em ciências sociais, o SSRN é um repositório de preprints que abrange uma ampla gama de áreas, incluindo economia, direito e gestão corporativa, onde você pode encontrar trabalhos relacionados à gestão da inovação. O SSRN não fornece uma API pública para acesso programático aos seus conteúdos.\n",
    "\n",
    "\n",
    "O tipo de busca mais comum é por palavras-chave, usadas para descobrir e para refinar a pesquisa e localizar artigos relevantes sobre os temas de interesse, no nosso caso, a gestão da inovação tecnológica em organizações públicas e privadas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editoras científicas com políticas de Open Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Várias editoras científicas proeminentes oferecem conteúdos em Open Access, proporcionando acesso livre a uma vasta gama de pesquisas acadêmicas. Algumas das principais editoras incluem:\n",
    "\n",
    "<b>Springer Nature</b>: Com mais de 600 periódicos totalmente em acesso aberto e mais de 1000 livros em acesso aberto, a Springer Nature é uma das pioneiras no campo da pesquisa aberta. Oferece uma variedade de opções de publicação em acesso aberto, mantendo rigorosos processos de revisão por pares e editoriais.\n",
    "\n",
    "<b>Oxford University Press (OUP)</b>: A OUP publica mais de 120 periódicos totalmente em acesso aberto e mais de 250 livros em acesso aberto, abrangendo uma ampla gama de disciplinas. Muitos de seus periódicos são classificados como \"diamond OA\", o que significa que não há taxas de processamento de artigos para autores ou leitores.\n",
    "\n",
    "<b>Frontiers</b>: Reconhecida como uma editora líder em Acesso Aberto e plataforma de Ciência Aberta, a Frontiers é muito citada, com mais de um bilhão de visualizações e downloads e 1.6 milhão de citações em seus artigos acessíveis gratuitamente. Ela é ativa em áreas como neurociências, psiquiatria, fisiologia, medicina clínica, ciências naturais e engenharia.\n",
    "\n",
    "<b>Wiley</b>: A Wiley oferece mais de 150 periódicos revisados por pares em acesso aberto, abrangendo diversas disciplinas de pesquisa. Seus periódicos em acesso aberto estão disponíveis para leitura, download e compartilhamento gratuitamente através da Wiley Online Library e do PubMed Central.\n",
    "\n",
    "<b>Public Library of Science (PLOS)</b>: Fundada como uma organização sem fins lucrativos, a PLOS tem como objetivo catalisar o movimento de acesso aberto. Publica periódicos em diversas áreas, incluindo medicina e ciências da vida.\n",
    "\n",
    "<b>Hindawi</b>: Inicialmente uma editora de periódicos por assinatura, a Hindawi fez a transição para um modelo de publicação em acesso aberto entre 2004 e 2007. Ela publica periódicos em ciência e engenharia, ciências sociais e filosofia.\n",
    "\n",
    "<b>MDPI AG</b>: Uma empresa suíça com presença global, a MDPI AG publica periódicos em ciência e engenharia, ciências sociais e filosofia.\n",
    "\n",
    "<b>Informa PLC</b>: Uma das maiores editoras nas humanidades e ciências sociais, a Informa publica em acesso aberto sob quatro selos: Taylor & Francis Open, Dove Medical Press, Cogent OA e Routledge Open.\n",
    "\n",
    "Estas editoras são conhecidas não apenas pela qualidade e diversidade de suas publicações, mas também por suas contribuições significativas para o movimento de acesso aberto na comunidade acadêmica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desempenho em Inovação no mundo e no Brasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medição da inovação em nível mundial é um campo importante para entender como diferentes nações estão progredindo em termos de capacidade e sucesso em inovação. Uma das iniciativas mais conhecidas neste campo é o Global Innovation Index (GII).\n",
    "\n",
    "O GII é um ranking anual publicado pela World Intellectual Property Organization (WIPO) em parceria com a INSEAD e outras instituições, como a Cornell University. Iniciado em 2007, o índice é baseado em dados subjetivos e objetivos obtidos de várias fontes, incluindo a International Telecommunication Union, o World Bank e o World Economic Forum. O GII classifica os países com base em dois sub-índices: o Innovation Input Index e o Innovation Output Index, compostos por cinco e dois pilares, respectivamente, cada um descrevendo um atributo da inovação.\n",
    "\n",
    "Além do GII, há outras iniciativas semelhantes, como o International Innovation Index, que medem o nível de inovação de um país. Este índice é produzido em conjunto pelo Boston Consulting Group (BCG), pela National Association of Manufacturers (NAM) e pelo Manufacturing Institute (MI), o afiliado de pesquisa apartidária da NAM.\n",
    "\n",
    "Cada um desses índices oferece uma perspectiva única sobre a inovação em nível de país, ajudando governos, formuladores de políticas e acadêmicos a entender as tendências de inovação e a identificar áreas para melhoria e investimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Sempre construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(base_repo_dir, 'utils')\n",
    "folder_domain = os.path.join(base_repo_dir, 'source', 'domain')\n",
    "folder_data_input = os.path.join(base_repo_dir, '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(base_repo_dir, '_data', 'out_json')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "# from scraper_pasteur import PasteurScraper\n",
    "# from scraper_sucupira import SucupiraScraper\n",
    "# from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "from environment_setup import EnvironmentSetup\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from neo4j_persister import Neo4jPersister\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DictToHDF5, attribute_to_be_non_empty\n",
    "\n",
    "t00 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    print(math.factorial(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(1, 100, 100)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "# Valores de entrada (tamanho do problema) - ajustados\n",
    "n = np.linspace(1, 100, 100)\n",
    "\n",
    "# Cálculo das complexidades\n",
    "O_1 = np.ones(n.shape)\n",
    "O_log_n = np.log2(n)\n",
    "O_n = n\n",
    "O_n_log_n = n * np.log2(n)\n",
    "O_n_squared = n**2\n",
    "O_2_n = 2**n-1\n",
    "max_factorial_visible = 120  # Valor máximo para o cálculo do fatorial 6!=120\n",
    "O_n_factorial = np.array([math.factorial(int(i)) if i <= max_factorial_visible else np.nan for i in n])\n",
    "\n",
    "# Exibir O_n_factorial em notação científica\n",
    "for valor in O_n_factorial[:max_factorial_visible + 1]:\n",
    "    print(f\"{valor}\")\n",
    "\n",
    "# Criar um DataFrame para os dados a serem exibidos\n",
    "df = pd.DataFrame({'n': n[:max_factorial_visible + 1], 'O(n!)': O_n_factorial[:max_factorial_visible + 1]})\n",
    "\n",
    "# Criar o gráfico de linha\n",
    "chart = alt.Chart(df).mark_line(point=True).encode(  # Adicionamos point=True para mostrar os pontos\n",
    "    x=alt.X('n:Q', axis=alt.Axis(labelAngle=-45, format='.2f')),  # Formato com 4 casas decimais\n",
    "    y=alt.Y('O(n!)', scale=alt.Scale(type='log')),\n",
    "    tooltip=['n', 'O(n!)']\n",
    ").properties(\n",
    "    title='Valores de O(n!) até n = 20',\n",
    "    height=400,\n",
    "    width=800,\n",
    ").interactive()\n",
    "\n",
    "# Exibir o gráfico\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Valores de entrada (tamanho do problema) - ajustados\n",
    "n = np.linspace(1, 100, 100)\n",
    "\n",
    "# Cálculo das complexidades\n",
    "O_1 = np.ones(n.shape)\n",
    "O_log_n = np.log2(n)\n",
    "O_n = n\n",
    "O_n_log_n = n * np.log2(n)\n",
    "O_n_squared = n**2\n",
    "O_2_n = 2**n - 1\n",
    "\n",
    "# Calcular O(n!) até um ponto visível\n",
    "max_factorial = 100  # Valor máximo de n para o qual O(n!) é visível no gráfico\n",
    "O_n_factorial = np.array([math.factorial(int(i)) if i <= max_factorial_visible else np.nan for i in n])\n",
    "\n",
    "# Paleta de cores similar à imagem de referência\n",
    "colors = ['darkgreen', 'green', 'orange', 'gold', 'red', 'darkblue', 'purple']\n",
    "\n",
    "# Criar o gráfico Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar as curvas ao gráfico, usando cores personalizadas\n",
    "fig.add_trace(go.Scatter(x=n, y=O_1, mode='lines', name='O(1)', line=dict(color=colors[0], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_log_n, mode='lines', name='O(log n)', line=dict(color=colors[1], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n, mode='lines', name='O(n)', line=dict(color=colors[2], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_log_n, mode='lines', name='O(n log n)', line=dict(color=colors[3], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_squared, mode='lines', name='O(n²)', line=dict(color=colors[4], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_2_n, mode='lines', name='O(2ⁿ)', line=dict(color=colors[5], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n[:max_factorial + 1], y=O_n_factorial[:max_factorial + 1], mode='lines', name='O(n!)', line=dict(color=colors[6], width=2)))\n",
    "\n",
    "# Configuração do layout do gráfico\n",
    "fig.update_layout(\n",
    "    title='Comparação de Complexidades Algorítmicas (Big O)',\n",
    "    xaxis_title='Tamanho dos Dados de Entrada (n)',\n",
    "    yaxis_title='Tempo de Execução em escala logarítmica (em operações)',\n",
    "    yaxis_type='log',\n",
    "    yaxis=dict(range=[-0.04, 2]),  # Escala logarítmica com limite definido para y\n",
    "    xaxis=dict(range=[1, 20]),  # Escala linear com limite definido para x\n",
    "    showlegend=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(l=50, r=50, t=80, b=50),\n",
    ")\n",
    "\n",
    "# Função para calcular as posições das anotações\n",
    "def calculate_annotations(fig):\n",
    "    annotations = []\n",
    "    x_end = fig.layout.xaxis.range[1] * 0.95  \n",
    "    for i, trace in enumerate(fig.data):\n",
    "        # Encontrar o índice do valor de x mais próximo de x_end\n",
    "        idx = np.abs(trace.x - x_end).argmin()\n",
    "\n",
    "        # Se o valor de y for NaN, usar o último valor válido antes de x_end\n",
    "        if np.isnan(trace.y[idx]):\n",
    "            valid_indices = np.where(~np.isnan(trace.y) & (trace.x <= x_end))[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                idx = valid_indices[-1]\n",
    "\n",
    "        x = trace.x[idx]\n",
    "        y = trace.y[idx]\n",
    "\n",
    "        # Calcular a posição da anotação no eixo y, considerando a escala logarítmica, exceto para O(1)\n",
    "        if trace.name == 'O(n log n)':\n",
    "            y_annotation = 1.95\n",
    "        if trace.name == \"O(n)\":\n",
    "            y_annotation = 1.35\n",
    "            x_annotation = x\n",
    "        if trace.name == \"O(log n)\":\n",
    "            y_annotation = 0.7\n",
    "            x_annotation = x-0.4\n",
    "        if trace.name == \"O(1)\":\n",
    "            y_annotation = 0.05\n",
    "            x_annotation = x\n",
    "        if trace.name == \"O(n²)\":\n",
    "            y_annotation = 2\n",
    "            x_annotation = 10\n",
    "        if trace.name == \"O(2ⁿ)\":\n",
    "            y_annotation = 2\n",
    "            x_annotation = 6.7\n",
    "        if trace.name == \"O(n!)\":\n",
    "            y_annotation = 2\n",
    "            x_annotation = 5\n",
    "\n",
    "        annotations.append(\n",
    "            dict(\n",
    "                x=x_annotation,\n",
    "                y=y_annotation,\n",
    "                text=trace.name,\n",
    "                showarrow=False,\n",
    "                xanchor='left' if trace.name != 'O(n log n)' else 'center',  # Ajustar o alinhamento para O(n log n)\n",
    "                yanchor='middle',  # Centralizar o rótulo na vertical\n",
    "                font=dict(size=12, color=colors[i], family='Arial'),  # Usar a cor correspondente da paleta\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Calcular e adicionar as anotações\n",
    "fig.update_layout(annotations=calculate_annotations(fig))\n",
    "\n",
    "# Exibir o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Valores de entrada (tamanho do problema) - ajustados\n",
    "n = np.linspace(1, 100, 1000)\n",
    "\n",
    "# Cálculo das complexidades\n",
    "O_1 = np.ones(n.shape)\n",
    "O_log_n = np.log2(n)\n",
    "O_n = n\n",
    "O_n_log_n = n * np.log2(n)\n",
    "O_n_squared = n**2\n",
    "O_2_n = 2**n - 1\n",
    "\n",
    "# Cálculo da complexidade O(n!) para inteiros até max_factorial_visible\n",
    "max_factorial_visible = 100\n",
    "n_int = np.arange(1, max_factorial_visible + 1)  # Valores inteiros de 1 a max_factorial_visible\n",
    "O_n_factorial_int = np.array([math.factorial(i) for i in n_int])\n",
    "\n",
    "# Interpolação para valores fracionários\n",
    "O_n_factorial = np.array([math.gamma(i + 1) for i in n])\n",
    "\n",
    "# Paleta de cores similar à imagem de referência\n",
    "colors = ['darkgreen', 'green', 'gold', 'orange', 'red', 'darkblue', 'black']\n",
    "\n",
    "# Criar o gráfico Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar as curvas ao gráfico, usando cores personalizadas\n",
    "fig.add_trace(go.Scatter(x=n, y=O_1, mode='lines', name='O(1)', line=dict(color=colors[0], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_log_n, mode='lines', name='O(log n)', line=dict(color=colors[1], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n, mode='lines', name='O(n)', line=dict(color=colors[2], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_log_n, mode='lines', name='O(n log n)', line=dict(color=colors[3], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_squared, mode='lines', name='O(n²)', line=dict(color=colors[4], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_2_n, mode='lines', name='O(2ⁿ)', line=dict(color=colors[5], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n[:max_factorial_visible + 1], y=O_n_factorial[:max_factorial_visible + 1], \n",
    "    mode='lines', name='O(n!)', line=dict(color=colors[6], width=4)))\n",
    "\n",
    "# Configuração do layout do gráfico\n",
    "fig.update_layout(\n",
    "    title='Comparação de Complexidades Assintótica dos Algoritmos (Big O)',\n",
    "    xaxis_title='Tamanho dos Dados de Entrada (n)',\n",
    "    yaxis_title='Tempo de Execução em escala logarítmica (em operações)',\n",
    "    yaxis_type='log',\n",
    "    xaxis=dict(\n",
    "        range=[1, 20],          # Escala linear com limite definido para x\n",
    "        gridcolor='lightgray',  # Cor mais clara para a grade\n",
    "        gridwidth=0.25,         # Largura menor para a grade\n",
    "        showgrid=True,\n",
    "        griddash='dot',\n",
    "    ),  \n",
    "    yaxis=dict(\n",
    "        range=[-0.04, 2],       # Escala logarítmica com limite definido para y\n",
    "        gridcolor='lightgray',  \n",
    "        gridwidth=0.25,\n",
    "        showgrid=True,\n",
    "        griddash='dot',\n",
    "        # tickformat=\".0e\",     # Formatar os ticks em notação científica com expoente inteiro\n",
    "        # tickformat=\",d\"         # Formatar os ticks como números inteiros com separador de milhar\n",
    "        tickvals=[1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000],  # Valores dos ticks\n",
    "        ticktext=['1', '10', '100', '1,000', '10,000', '100,000', '1,000,000', '10,000,000', '100,000,000']  # Rótulos em números inteiros\n",
    "\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(l=25, r=50, t=80, b=50),\n",
    ")\n",
    "\n",
    "# Função para calcular as posições das anotações\n",
    "def calculate_annotations(fig):\n",
    "    annotations = []\n",
    "    x_end = fig.layout.xaxis.range[1] * 0.95  \n",
    "    for i, trace in enumerate(fig.data):\n",
    "        # Encontrar o índice do valor de x mais próximo de x_end\n",
    "        idx = np.abs(trace.x - x_end).argmin()\n",
    "\n",
    "        # Se o valor de y for NaN, usar o último valor válido antes de x_end\n",
    "        if np.isnan(trace.y[idx]):\n",
    "            valid_indices = np.where(~np.isnan(trace.y) & (trace.x <= x_end))[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                idx = valid_indices[-1]\n",
    "\n",
    "        x = trace.x[idx]\n",
    "        y = trace.y[idx]\n",
    "\n",
    "        # Calcular a posição da anotação no eixo y, considerando a escala logarítmica, exceto para O(1)\n",
    "        if trace.name == 'O(n log n)':\n",
    "            y_annotation = 1.95\n",
    "        if trace.name == \"O(n)\":\n",
    "            y_annotation = 1.4\n",
    "            x_annotation = x+0.75\n",
    "        if trace.name == \"O(log n)\":\n",
    "            y_annotation = 0.7\n",
    "            x_annotation = x+0.75\n",
    "        if trace.name == \"O(1)\":\n",
    "            y_annotation = 0.075\n",
    "            x_annotation = x+0.75\n",
    "        if trace.name == \"O(n²)\":\n",
    "            y_annotation = 1.95\n",
    "            x_annotation = 11.4\n",
    "        if trace.name == \"O(2ⁿ)\":\n",
    "            y_annotation = 1.95\n",
    "            x_annotation = 8\n",
    "        if trace.name == \"O(n!)\":\n",
    "            y_annotation = 1.95\n",
    "            x_annotation = 4.75         \n",
    "\n",
    "        annotations.append(\n",
    "            dict(\n",
    "                x=x_annotation,\n",
    "                y=y_annotation,\n",
    "                text=trace.name,\n",
    "                showarrow=False,\n",
    "                xanchor='right',\n",
    "                yanchor='middle',\n",
    "                font=dict(size=20, color='black', family='Arial'),  # Usar a cor correspondente da paleta\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Calcular e adicionar as anotações\n",
    "fig.update_layout(annotations=calculate_annotations(fig),\n",
    "    shapes=[\n",
    "        # Região verde (diagonal até um pouco acima de O(log n))\n",
    "        dict(\n",
    "            type=\"path\",\n",
    "            path=f\"M 0, {10**fig.layout.yaxis.range[0]} L 20, {O_log_n[-1]} L 20, {10**fig.layout.yaxis.range[0]} Z\",\n",
    "            fillcolor=\"lightgreen\",\n",
    "            opacity=0.3,\n",
    "            line_width=0,\n",
    "            layer='below' # para que as curvas fiquem por cima\n",
    "        ),\n",
    "        # Região amarela (diagonal até um pouco acima de O(n log n))\n",
    "        dict(\n",
    "            type=\"path\",\n",
    "            path=f\"M 0, {10**fig.layout.yaxis.range[0]} L 20, {O_n_log_n[-1]-5} L 20, {O_log_n[-1]} Z\",\n",
    "            fillcolor=\"yellow\",\n",
    "            opacity=0.3,\n",
    "            line_width=0,\n",
    "            layer='below' # para que as curvas fiquem por cima\n",
    "        ),\n",
    "        # Região vermelha (diagonal até o topo do gráfico)\n",
    "        dict(\n",
    "            type=\"path\",\n",
    "            # path=f\"M 0, {10**fig.layout.yaxis.range[0]} L 20, {10**fig.layout.yaxis.range[1]} L 0, {10**fig.layout.yaxis.range[1]} Z\",\n",
    "            path=f\"M 20, {O_n_log_n[-1] - 5} L 0, {10**(int(np.log10(O_n_log_n[-1])) + 1)} L 0, {10**fig.layout.yaxis.range[0]} Z\",\n",
    "            fillcolor=\"red\",\n",
    "            opacity=0.3,\n",
    "            line_width=0,\n",
    "            layer='below'  # para que as curvas fiquem por cima\n",
    "        ),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "# Exibir o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histórico no Global Innovation Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pathfilename = os.path.join(folder_data_input, 'gii_history_data.csv')\n",
    "df_gii = pd.read_csv(pathfilename)\n",
    "\n",
    "def calculate_percentages(df_gii):\n",
    "    df_avaliacao = pd.DataFrame(columns=['Ano', 'Participantes', 'Above Brazil', 'Below Brazil', 'Above Brazil (%)', 'Below Brazil (%)'])\n",
    "    \n",
    "    for year in df_gii['Ano'].unique():\n",
    "        df_year = df_gii[df_gii['Ano'] == year]\n",
    "        total_countries = df_year['Países Participantes'].values[0]\n",
    "        brazil_position = df_year['Colocação do Brasil'].values[0]\n",
    "        above_brazil = brazil_position - 1\n",
    "        below_brazil = total_countries - brazil_position\n",
    "        \n",
    "        above_percent = (above_brazil / total_countries) * 100\n",
    "        below_percent = (below_brazil / total_countries) * 100\n",
    "        \n",
    "        df_avaliacao = pd.concat([df_avaliacao, pd.DataFrame({'Ano': [year], 'Participantes': [total_countries], 'Above Brazil': [above_brazil], 'Below Brazil': [below_brazil], 'Above Brazil (%)': [above_percent], 'Below Brazil (%)': [below_percent]})], ignore_index=True)\n",
    "    \n",
    "    return df_avaliacao\n",
    "\n",
    "# Call the function to create df_avaliacao\n",
    "df_avaliacao = calculate_percentages(df_gii)\n",
    "df_avaliacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a figure with secondary y-axis for the line plots\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add stacked bars for 'Below Brazil (%)' and 'Above Brazil (%)'\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_avaliacao['Ano'], \n",
    "           y=df_avaliacao['Below Brazil (%)'], \n",
    "           name='Below Brazil (%)'),\n",
    "           secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_avaliacao['Ano'], \n",
    "           y=df_avaliacao['Above Brazil (%)'], \n",
    "           name='Above Brazil (%)'),\n",
    "           secondary_y=False,\n",
    ")\n",
    "\n",
    "# Correct the data label positions for each segment of the stacked bars\n",
    "for index, row in df_avaliacao.iterrows():\n",
    "    # Position for label of 'Above Brazil (%)'\n",
    "    position_above = row['Below Brazil (%)'] + row['Above Brazil (%)'] / 2\n",
    "    fig.add_annotation(\n",
    "        x=row['Ano'], y=position_above,\n",
    "        text=f\"{row['Above Brazil (%)']:.1f}%\",\n",
    "        showarrow=False, font=dict(color='white')\n",
    "    )\n",
    "\n",
    "    # Position for label of 'Below Brazil (%)'\n",
    "    position_below = row['Below Brazil (%)'] / 2\n",
    "    fig.add_annotation(\n",
    "        x=row['Ano'], y=position_below,\n",
    "        text=f\"{row['Below Brazil (%)']:.1f}%\",\n",
    "        showarrow=False, font=dict(color='white')\n",
    "    )\n",
    "\n",
    "# Update layout for stacked bars\n",
    "fig.update_layout(barmode='stack')\n",
    "\n",
    "# Add line for total number of participants\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_avaliacao['Ano'], \n",
    "               y=df_avaliacao['Participantes'], \n",
    "               name='Total Participants', \n",
    "               mode='lines+markers+text', \n",
    "               text=df_avaliacao['Participantes'], \n",
    "               textposition=\"top center\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add line for Brazil's performance, but on the primary y-axis\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_avaliacao['Ano'], \n",
    "               y=df_avaliacao['Below Brazil (%)'], \n",
    "               name='Brazil Performance', \n",
    "               mode='lines+markers', \n",
    "               line=dict(color='yellow', \n",
    "                         dash='dot')),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "# Update the bar colors\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Above Brazil (%)'),\n",
    "    marker=dict(color='orange')\n",
    ")\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Below Brazil (%)'),\n",
    "    marker=dict(color='blue')\n",
    ")\n",
    "\n",
    "# Update the line trace for total number of participants to have a thickness of 4\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Total Participants'),\n",
    "    line=dict(width=2)\n",
    ")\n",
    "\n",
    "# Update the line trace for Brazil's performance to have a thickness of 4\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Brazil Performance'),\n",
    "    line=dict(width=6)\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "vr_max = max(df_avaliacao['Participantes']) * 1.1\n",
    "fig.update_layout(\n",
    "    title='Performance Comparison: Brazil vs. Other Countries',\n",
    "    height=600,\n",
    "    yaxis=dict(title='Percentage', range=[0, vr_max]),  # Extending primary y-axis range\n",
    "    yaxis2=dict(title='Total Participants', overlaying='y', side='right', range=[0, vr_max])\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickvals=df_avaliacao['Ano'])\n",
    "\n",
    "# Re-render the chart\n",
    "fig.show(renderer=\"notebook\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Grafo para Inovar por Multicamadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('../../templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo interativo salvo em: c:\\Users\\marco\\ppgcs\\templates\\grafo_interativo.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from IPython.display import IFrame\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "        ''' \n",
    "        Busca o arquivo .git e retorna string com a pasta raiz do repositório.\n",
    "        '''\n",
    "        # Prevenir recursão infinita limitando a profundidade\n",
    "        if depth < 0:\n",
    "            return None\n",
    "        path = Path(path).absolute()\n",
    "        if (path / '.git').is_dir():\n",
    "            return path\n",
    "        # Corrigido para usar LattesScraper.find_repo_root para chamada recursiva\n",
    "        return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "# Criar o grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar nós (dominios, processos e entidades)\n",
    "dominios = [\"Pesquisar\", \"Desenvolver\", \"Inovar\"]\n",
    "processos = [\"P001\", \"P002\", \"P003\", \"P004\", \"P005\", \"P006\", \"P007\", \"P008\", \"P009\"]\n",
    "entidades = {\n",
    "    \"P001\": [\"Dores\", \"Desejos\", \"Desafios\"],\n",
    "    \"P002\": [\"Temas\", \"Tópicos\", \"Assuntos\"],\n",
    "    \"P003\": [\"Atitudes\", \"Experiências\", \"Habilidades\"],\n",
    "    \"P004\": [\"Papeis\", \"Tempo\", \"Orçamentos\"],\n",
    "    \"P005\": [\"Projetos\", \"Processos\", \"Programas\"],\n",
    "    \"P006\": [\"Ensaios\", \"Equipamentos\", \"Ambientes\"],\n",
    "    \"P007\": [\"Aplicação\", \"Solução\", \"Produto-Serviço\"],\n",
    "    \"P008\": [\"Modelos\", \"Protótipos\", \"Empreendimentos\"],\n",
    "    \"P009\": [\"Indicadores\", \"Evidências\", \"Mensuração\"]\n",
    "}\n",
    "\n",
    "# Criar visualização dos nós de acordo com a estrutura de dados\n",
    "for macroprocesso in dominios:\n",
    "    G.add_node(macroprocesso, type=\"macroprocesso\")\n",
    "\n",
    "for processo in processos:\n",
    "    G.add_node(processo, type=\"processo\")\n",
    "\n",
    "for processo, entidades_list in entidades.items():\n",
    "    for entidade in entidades_list:\n",
    "        G.add_node(entidade, type=\"entidade\")\n",
    "\n",
    "# Adicionar arestas (relacionamentos)\n",
    "for macroprocesso in dominios:\n",
    "    for i in range(1, 4):\n",
    "        G.add_edge(macroprocesso, f\"P00{i + 3*(dominios.index(macroprocesso))}\")\n",
    "\n",
    "for processo, entidades_list in entidades.items():\n",
    "    for entidade in entidades_list:\n",
    "        G.add_edge(processo, entidade)\n",
    "\n",
    "\n",
    "\n",
    "# (Opcional) Adicionar relacionamentos entre entidades, para formar Demanda, Faturamento, Lucro, Reinvestimento... etc\n",
    "# G.add_edge(\"Dores\", \"Desejos\")\n",
    "\n",
    "\n",
    "\n",
    "# Calcular distâncias, definir cores, tamanhos e tamanhos de fonte\n",
    "node_distances = {}\n",
    "for macroprocesso in dominios:\n",
    "    for node, distance in nx.shortest_path_length(G, source=macroprocesso).items():\n",
    "        node_distances[node] = distance\n",
    "\n",
    "cores_base = {\"macroprocesso\": \"#007BFF\", \"processo\": \"#28A745\", \"entidade\": \"#FFC107\"}\n",
    "node_colors = {}\n",
    "node_sizes = {}\n",
    "node_font_sizes = {}  # Dicionário para armazenar os tamanhos de fonte\n",
    "for node in G.nodes():\n",
    "    node_type = G.nodes[node][\"type\"]\n",
    "    cor_base = cores_base[node_type]\n",
    "    alpha = max(0, 255 - 25 * node_distances[node])\n",
    "    node_colors[node] = f\"{cor_base}{alpha:02X}\"\n",
    "\n",
    "    # Definir tamanhos e tamanhos de fonte com base na distância\n",
    "    tamanho_base = {\"macroprocesso\": 50, \"processo\": 30, \"entidade\": 15}\n",
    "    font_size_base = {\"macroprocesso\": 42, \"processo\": 28, \"entidade\": 18}  # Tamanhos de fonte iniciais\n",
    "    node_sizes[node] = tamanho_base[node_type] - 5 * node_distances[node]\n",
    "    node_font_sizes[node] = font_size_base[node_type] - node_distances[node]  # Reduzir 1 pixel por passo\n",
    "\n",
    "# Configurar o PyVis (notebook=False para renderizar na célula)\n",
    "net = Network(notebook=False, width=\"100%\", height=\"1200px\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
    "net.barnes_hut()\n",
    "\n",
    "# Adicionar nós e arestas ao PyVis\n",
    "for node in G.nodes():\n",
    "    net.add_node(node, label=node, color=node_colors[node], title=G.nodes[node][\"type\"], \n",
    "                 size=node_sizes[node], font={\"size\": node_font_sizes[node], \"color\": \"black\"})  # Adicionar tamanho da fonte\n",
    "\n",
    "for edge in G.edges():\n",
    "    weight = 1\n",
    "    net.add_edge(*edge, value=weight)\n",
    "\n",
    "# Configurar o ForceAtlas2\n",
    "net.options.physics.solver = \"forceAtlas2Based\"\n",
    "net.options.physics.forceAtlas2Based = {\n",
    "    \"gravitationalConstant\": -50,\n",
    "    \"centralGravity\": 0.01,\n",
    "    \"springLength\": 100,\n",
    "    \"springConstant\": 0.08,\n",
    "    \"damping\": 0.4,\n",
    "    \"avoidOverlap\": 1\n",
    "}\n",
    "\n",
    "driver_path = None\n",
    "try:\n",
    "    # Caminho para o chromedriver no sistema local\n",
    "    if platform.system() == \"Windows\":\n",
    "        driver_path=find_repo_root()/'chromedriver'/'chromedriver.exe'\n",
    "    else:\n",
    "        driver_path=find_repo_root()/'chromedriver'/'chromedriver'\n",
    "except Exception as e:\n",
    "    print(\"Não foi possível estabelecer uma conexão, verifique o chromedriver\")\n",
    "    print(e)\n",
    "\n",
    "# print(driver_path)\n",
    "# service = Service(driver_path)\n",
    "# driver = webdriver.Chrome(service=service)\n",
    "# driver.get(\"grafo_interativo.html\")\n",
    "\n",
    "# Adicionar controles interativos (opcional)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "# Adicionar estilo inline para fundo branco\n",
    "net.html = net.html.replace(\"<body>\", '<body style=\"background-color: white;\">')\n",
    "\n",
    "# Salvar o HTML na pasta templates\n",
    "template_dir = find_repo_root() / 'templates'\n",
    "net.save_graph(os.path.join(template_dir,\"grafo_interativo.html\"))\n",
    "\n",
    "print(f\"Grafo interativo salvo em: {template_dir / 'grafo_interativo.html'}\")\n",
    "# Renderizar na célula do Jupyter Notebook\n",
    "# net.show(\"'../../templates'grafo_interativo.html\")  # Definir fundo branco ao salvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renderizando o grafo no Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"http://127.0.0.1:5000/grafo_interativo.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x20484f3ff10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "net = Network(notebook=True, width=\"100%\", height=\"1200px\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
    "\n",
    "IFrame(src='http://127.0.0.1:5000/grafo_interativo.html', width='100%', height='800px')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simluação para gerar visualizar o layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyVis para simula a busca por equilíbrio entre forças físicas nas entidades que interagem no grafo para determinar o posicionamento dos nós no grafo e assim criar o layout. As principais forças envolvidas são:\n",
    "\n",
    "### 1. Força de Atração:\n",
    "\n",
    "Arestas (Links): As arestas entre os nós agem como molas, puxando os nós conectados um em direção ao outro. A intensidade dessa força é determinada pelo parâmetro springLength (comprimento ideal da mola) e springConstant (rigidez da mola).\n",
    "\n",
    "Gravidade Central: Uma força de atração em direção ao centro do grafo, controlada pelo parâmetro centralGravity. Essa força ajuda a evitar que os nós se dispersem muito e mantém o grafo mais compacto.\n",
    "\n",
    "### 2. Força de Repulsão:\n",
    "\n",
    "Repulsão entre Nós: Os nós se repelem uns aos outros, como partículas carregadas com a mesma carga. A intensidade dessa força é determinada pelo parâmetro gravitationalConstant (constante gravitacional). Um valor negativo aumenta a repulsão, enquanto um valor positivo a diminui.\n",
    "\n",
    "Evitar Sobreposição: O parâmetro avoidOverlap controla se os nós devem evitar a sobreposição. Se ativado, uma força adicional é aplicada para afastar os nós que estão muito próximos.\n",
    "\n",
    "### 3. Força de Amortecimento:\n",
    "\n",
    "Damping: O parâmetro damping controla o amortecimento do movimento dos nós. Um valor maior de amortecimento torna o movimento mais lento e suave, enquanto um valor menor permite movimentos mais rápidos e oscilatórios.\n",
    "\n",
    "### 4. Forças Adicionais (Opcionais):\n",
    "\n",
    "Outras Forças: O PyVis permite adicionar outras forças personalizadas ao layout, como forças de atração/repulsão entre grupos de nós, ou forças que direcionam os nós para posições específicas.\n",
    "\n",
    "\n",
    "## Como as Forças Interagem:\n",
    "\n",
    "O algoritmo ForceAtlas2Based iterativamente calcula as forças resultantes sobre cada nó e ajusta suas posições de acordo. O processo continua até que o layout se estabilize ou um número máximo de iterações seja atingido.\n",
    "\n",
    "O algoritmo Barnes-Hut otimiza o cálculo das forças de longo alcance, aproximando as forças entre grupos de nós distantes. Isso melhora o desempenho do algoritmo, especialmente em grafos grandes.\n",
    "\n",
    "## Equilíbrio das Forças:\n",
    "\n",
    "O objetivo do algoritmo é encontrar um equilíbrio entre as forças de atração e repulsão, de modo que os nós conectados fiquem próximos, mas sem se sobreporem, e o grafo tenha uma aparência geral agradável e informativa. O ajuste dos parâmetros do ForceAtlas2Based e do Barnes-Hut permite controlar esse equilíbrio e personalizar o layout do grafo de acordo com suas necessidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ForceAtlas2Based e Barnes-Hut:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ForceAtlas2Based é um algoritmo de layout que simula um sistema físico onde os nós se atraem e se repelem com base em certas forças. No entanto, calcular essas forças para todos os pares de nós em um grafo grande pode ser computacionalmente caro.\n",
    "\n",
    "Para otimizar o cálculo das forças, o ForceAtlas2Based utiliza o algoritmo Barnes-Hut. Esse algoritmo agrupa nós distantes em clusters e aproxima suas forças de atração/repulsão, reduzindo significativamente o número de cálculos necessários.\n",
    "\n",
    "### Theta e a Distância entre Nós:\n",
    "\n",
    "O parâmetro theta (θ) do Barnes-Hut define o limite entre as forças de curto e longo alcance. Quando a distância entre dois nós é menor que theta multiplicado pelo tamanho do cluster, as forças são calculadas individualmente (curto alcance). Caso contrário, as forças são aproximadas usando o centro de massa do cluster (longo alcance).\n",
    "\n",
    "Quando os nós estão muito próximos, a distância entre eles é menor que o limite definido por theta. Nesse caso, o algoritmo Barnes-Hut calcula as forças individualmente para cada par de nós, levando em consideração suas posições exatas. Isso permite que o ForceAtlas2Based posicione os nós próximos de forma mais precisa, evitando sobreposições e garantindo um layout visualmente agradável.\n",
    "\n",
    "Em resumo, o ForceAtlas2Based define as regras gerais para as forças de atração e repulsão entre os nós. No entanto, quando os nós estão próximos, o algoritmo Barnes-Hut assume o controle e calcula as forças de forma mais precisa, levando em consideração a distância exata entre os nós.\n",
    "\n",
    "O parâmetro theta do Barnes-Hut é fundamental para determinar o comportamento do layout em nós próximos, pois define o limite entre as forças de curto e longo alcance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é o Theta (θ)?\n",
    "\n",
    "O theta (θ) é um parâmetro interno do ForceAtlas2Based que controla o limite entre as forças de longo alcance (que afetam todos os nós) e as forças de curto alcance (que afetam apenas os nós próximos).\n",
    "\n",
    "Valores mais altos de theta: Aceleram o cálculo das forças, mas podem gerar mais erros e imprecisões no layout.\n",
    "Valores mais baixos de theta: Tornam o cálculo mais lento, mas produzem um layout mais preciso e com menos erros.\n",
    "\n",
    "### Como o Theta é Usado no ForceAtlas2Based?\n",
    "\n",
    "O ForceAtlas2Based utiliza uma técnica chamada Barnes-Hut para aproximar as forças de longo alcance, tornando o cálculo mais eficiente. O theta é usado para determinar quais nós estão \"próximos o suficiente\" para que suas forças sejam calculadas individualmente (curto alcance), e quais nós estão \"longe o suficiente\" para que suas forças sejam aproximadas usando a técnica Barnes-Hut (longo alcance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parâmetros do ForceAtlas2Based:\n",
    "\n",
    "gravitationalConstant: Define a força de atração global entre os nós. Valores negativos atraem os nós para o centro do grafo, enquanto valores positivos os repelem. Um valor mais negativo resultará em um grafo mais compacto, enquanto um valor mais positivo resultará em um grafo mais espalhado.\n",
    "\n",
    "centralGravity: Define a força de atração em direção ao centro do grafo. Valores maiores puxam os nós mais para o centro.\n",
    "\n",
    "springLength: Define o comprimento ideal das arestas (ligações entre os nós). Valores maiores resultam em arestas mais longas e um grafo mais espalhado.\n",
    "\n",
    "springConstant: Define a rigidez das arestas. Valores maiores tornam as arestas mais rígidas e o grafo menos flexível.\n",
    "\n",
    "damping: Controla a velocidade com que os nós se movem. Valores maiores amortecem o movimento, resultando em um layout mais estável, mas que pode levar mais tempo para convergir.\n",
    "\n",
    "avoidOverlap: Determina se os nós devem evitar a sobreposição. Um valor de 1 (verdadeiro) faz com que os nós se afastem uns dos outros para evitar sobreposição, enquanto um valor de 0 (falso) permite a sobreposição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"grafo_interativo.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2048449ce50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renderizar na célula do Jupyter Notebook\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=\"grafo_interativo.html\", width=\"100%\", height=\"600px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Sempre construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(base_repo_dir, 'utils')\n",
    "folder_domain = os.path.join(base_repo_dir, 'source', 'domain')\n",
    "folder_data_input = os.path.join(base_repo_dir, '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(base_repo_dir, '_data', 'out_json')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "# from scraper_pasteur import PasteurScraper\n",
    "# from scraper_sucupira import SucupiraScraper\n",
    "# from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "from environment_setup import EnvironmentSetup\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from neo4j_persister import Neo4jPersister\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DictToHDF5, attribute_to_be_non_empty\n",
    "\n",
    "t00 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(folder_data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Carregar dados dos produtos prioritários, equipamentos e questões de pesquisa\n",
    "curr_pathfilename = os.path.join(folder_data_input,'docents_dict_list.json')\n",
    "with open(curr_pathfilename, 'r', encoding='utf-8') as f:\n",
    "    curriculos = json.load(f)\n",
    "\n",
    "prod_pathfilename = os.path.join(folder_data_output,'matriz_ceis.json')\n",
    "with open(prod_pathfilename, 'r', encoding='utf-8') as f:\n",
    "    matriz_produtos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(curriculos)} currículos carregados')\n",
    "# [x.get('Áreas') for x in curriculos]\n",
    "produtos = [produto.get('nome') for bloco in matriz_produtos.get('blocos', []) for produto in bloco.get('produtos', [])]\n",
    "print(f'{len(produtos)} produtos carregados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vega\n",
    "# !jupyter nbextension install --sys-prefix --py vega\n",
    "# !jupyter nbextension enable --sys-prefix --py vega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_validator import *\n",
    "\n",
    "# Exemplo de uso (substituir pelos dados reais)\n",
    "y_true = [[\"A\", \"B\"], [\"B\"], [\"A\", \"C\"]]\n",
    "y_pred = [[\"A\"], [\"B\", \"C\"], [\"A\", \"B\"]]\n",
    "classes = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "# Probabilidades aqui\n",
    "y_proba = [[0.8, 0.7, 0.3], [0.2, 0.9, 0.5], [0.6, 0.8, 0.4]]\n",
    "\n",
    "\n",
    "validar_modelo(y_true, y_pred, y_proba, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equipamentos = [...]\n",
    "questoes_pesquisa = [...]\n",
    "\n",
    "# Criar o grafo heterogêneo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar nós e arestas para pesquisadores\n",
    "for pesquisador in curriculos:\n",
    "    G.add_node(pesquisador['Identificação']['ID Lattes'], type=\"pesquisador\", **pesquisador)  # Adicionar atributos do currículo\n",
    "    \n",
    "    # Conectar pesquisador às suas áreas de atuação\n",
    "    for area in pesquisador['Áreas'].values():\n",
    "        G.add_node(area, type=\"area\")\n",
    "        G.add_edge(pesquisador['Identificação']['ID Lattes'], area)\n",
    "\n",
    "    # Conectar pesquisador às suas publicações\n",
    "    for publicacao in pesquisador['Produções']['Artigos completos publicados em periódicos']:\n",
    "        G.add_node(publicacao['titulo'], type=\"publicacao\")\n",
    "        G.add_edge(pesquisador['Identificação']['ID Lattes'], publicacao['titulo'])\n",
    "\n",
    "    # Conectar pesquisador a projetos\n",
    "    for pesquisador in curriculos:\n",
    "        for tipo_projeto in [\"ProjetosPesquisa\", \"ProjetosExtensão\", \"ProjetosDesenvolvimento\", \"ProjetosOutros\"]:\n",
    "            if tipo_projeto in pesquisador:\n",
    "                for projeto in pesquisador[tipo_projeto]:\n",
    "                    G.add_node(projeto['titulo_projeto'], type=\"projeto\")\n",
    "                    G.add_edge(pesquisador['Identificação']['ID Lattes'], projeto['titulo_projeto'])\n",
    "\n",
    "    # Conectar pesquisador a equipamentos\n",
    "    # (Assumindo que você tem uma lista de equipamentos mencionados nos currículos)\n",
    "    equipamentos_citados = []  # Preencha com os nomes dos equipamentos mencionados nos currículos\n",
    "    for pesquisador in curriculos:\n",
    "        for equipamento in equipamentos_citados:\n",
    "            if equipamento in pesquisador['Atuação Profissional'][0]['Descrição']:  # Exemplo: busca na descrição da atuação profissional\n",
    "                G.add_edge(pesquisador['Identificação']['ID Lattes'], equipamento)\n",
    "\n",
    "    # Conectar pesquisador a questões de pesquisa\n",
    "    # (Assumindo que você tem uma lista de questões de pesquisa e um método para associá-las aos pesquisadores)\n",
    "    for pesquisador in curriculos:\n",
    "        for questao in questoes_pesquisa:\n",
    "            if pesquisador_tem_interesse_na_questao(pesquisador, questao):  # Interesse por inferência\n",
    "                G.add_edge(pesquisador['Identificação']['ID Lattes'], questao['descricao'])\n",
    "\n",
    "    def extract_topicos(pesquisador, list_dict):\n",
    "        campos = ['', '', '']\n",
    "        return lista_topicos\n",
    "\n",
    "    # Função para inferir o interesse do pesquisador em uma questão\n",
    "    def pesquisador_tem_interesse_na_questao(pesquisador, questao):\n",
    "        # Analise o currículo do pesquisador (áreas de atuação, publicações, projetos, etc.)\n",
    "        # e compare com a descrição da questão de pesquisa para determinar o interesse\n",
    "        # Retorna True se houver interesse, False caso contrário\n",
    "        topicos_pesquisador = extract_topicos(pesquisador)\n",
    "        interesses_pesquisador = []\n",
    "        flag_interesse = False\n",
    "        threshold = 0.8\n",
    "        for i in topicos_pesquisador:\n",
    "            similarity = calculate_similarity(questao, i)\n",
    "            if similarity >= threshold:\n",
    "                interesses_pesquisador.append(questao)\n",
    "                flag_interesse = True\n",
    "\n",
    "        return flag_interesse\n",
    "\n",
    "\n",
    "# Adicionar nós e arestas para produtos prioritários, equipamentos e questões de pesquisa\n",
    "for produto in produtos_prioritarios:\n",
    "    G.add_node(produto['nome'], type=\"produto\", **produto)  # Adicionar atributos do produto (nome, descrição, área, etc.)\n",
    "\n",
    "    # Conectar produto às suas áreas (assumindo que o produto tem uma lista de áreas)\n",
    "    for area in produto.get('areas', []):  # Usar get() para evitar KeyError se 'areas' não existir\n",
    "        G.add_edge(produto['nome'], area)\n",
    "\n",
    "# Adicionar nós e arestas para equipamentos\n",
    "for equipamento in equipamentos:\n",
    "    G.add_node(equipamento['nome'], type=\"equipamento\", **equipamento)  # Adicionar atributos do equipamento\n",
    "\n",
    "    # Conectar equipamento às suas áreas (assumindo que o equipamento tem uma lista de áreas)\n",
    "    for area in equipamento.get('areas', []):\n",
    "        G.add_edge(equipamento['nome'], area)\n",
    "\n",
    "# Adicionar nós e arestas para questões de pesquisa\n",
    "for questao in questoes_pesquisa:\n",
    "    G.add_node(questao['descricao'], type=\"questao_pesquisa\", **questao)  # Adicionar atributos da questão\n",
    "\n",
    "    # Conectar questão de pesquisa às suas áreas (assumindo que a questão tem uma lista de áreas)\n",
    "    for area in questao.get('areas', []):\n",
    "        G.add_edge(questao['descricao'], area)\n",
    "\n",
    "# Adicionar nós e arestas para equipamentos\n",
    "for equipamento in equipamentos:\n",
    "    G.add_node(equipamento['nome'], type=\"equipamento\", **equipamento)  # Adicionar atributos do equipamento\n",
    "\n",
    "    # Conectar equipamento às suas áreas (assumindo que o equipamento tem uma lista de áreas)\n",
    "    if 'areas' in equipamento:  # Verificar se o equipamento possui áreas associadas\n",
    "        for area in equipamento['areas']:\n",
    "            if area in G.nodes:  # Verificar se a área já existe no grafo\n",
    "                G.add_edge(equipamento['nome'], area)\n",
    "            else:\n",
    "                # Se a área não existir, você pode decidir se quer adicioná-la como um novo nó\n",
    "                # G.add_node(area, type=\"area\")  \n",
    "                # G.add_edge(equipamento['nome'], area)\n",
    "                print(f\"Área '{area}' não encontrada para o equipamento '{equipamento['nome']}'.\")\n",
    "    else:\n",
    "        print(f\"Equipamento '{equipamento['nome']}' não possui áreas associadas.\")\n",
    "\n",
    "# Adicionar nós e arestas para questões de pesquisa\n",
    "for questao in questoes_pesquisa:\n",
    "    G.add_node(questao['descricao'], type=\"questao_pesquisa\", **questao)  # Adicionar atributos da questão\n",
    "\n",
    "    # Conectar questão de pesquisa às suas áreas\n",
    "    if 'areas' in questao:  # Verificar se a questão possui áreas associadas\n",
    "        for area in questao['areas']:\n",
    "            if area in G.nodes:  # Verificar se a área já existe no grafo\n",
    "                G.add_edge(questao['descricao'], area)\n",
    "            else:\n",
    "                # Se a área não existir, você pode decidir se quer adicioná-la como um novo nó\n",
    "                # G.add_node(area, type=\"area\")  \n",
    "                # G.add_edge(questao['descricao'], area)\n",
    "                print(f\"Área '{area}' não encontrada para a questão de pesquisa '{questao['descricao']}'.\")\n",
    "    else:\n",
    "        print(f\"Questão de pesquisa '{questao['descricao']}' não possui áreas associadas.\")\n",
    "\n",
    "# Análise 1: Agrupamento de Pesquisadores\n",
    "# Extrair características textuais dos currículos (ex: usando TF-IDF)\n",
    "corpus = [pesquisador['Formação']['Acadêmica'][0]['Descrição'] for pesquisador in curriculos]  # Exemplo usando a descrição da formação acadêmica\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Calcular similaridade entre pesquisadores (ex: usando cosseno)\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "\n",
    "# Aplicar algoritmo de agrupamento (ex: DBSCAN)\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=0.3, min_samples=2).fit(similarity_matrix)\n",
    "labels = clustering.labels_\n",
    "\n",
    "# Adicionar atributo 'cluster' aos nós dos pesquisadores\n",
    "for i, pesquisador in enumerate(curriculos):\n",
    "    G.nodes[pesquisador['Identificação']['ID Lattes']]['cluster'] = labels[i]\n",
    "\n",
    "\n",
    "# Análise 2: Agrupamento de Questões de Pesquisa\n",
    "# Extrair características textuais das questões de pesquisa (ex: usando TF-IDF)\n",
    "corpus_questoes = [questao['descricao'] for questao in questoes_pesquisa]\n",
    "vectorizer_questoes = TfidfVectorizer()\n",
    "X_questoes = vectorizer_questoes.fit_transform(corpus_questoes)\n",
    "\n",
    "# Calcular similaridade entre questões (ex: usando cosseno)\n",
    "similarity_matrix_questoes = cosine_similarity(X_questoes)\n",
    "\n",
    "# Aplicar algoritmo de agrupamento (ex: K-Means)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 5  # Defina o número de clusters desejado\n",
    "clustering_questoes = KMeans(n_clusters=n_clusters).fit(similarity_matrix_questoes)\n",
    "labels_questoes = clustering_questoes.labels_\n",
    "\n",
    "# Adicionar atributo 'cluster' aos nós das questões de pesquisa\n",
    "for i, questao in enumerate(questoes_pesquisa):\n",
    "    G.nodes[questao['descricao']]['cluster'] = labels_questoes[i]\n",
    "\n",
    "# Análise 3: Recomendação de Projetos (exemplo simplificado)\n",
    "def recomendar_projetos(pesquisador_id):\n",
    "    pesquisador_areas = list(G.neighbors(pesquisador_id))  # Obter áreas do pesquisador\n",
    "    projetos_recomendados = []\n",
    "    for projeto_id in G.nodes:\n",
    "        if G.nodes[projeto_id]['type'] == \"projeto\":\n",
    "            projeto_areas = list(G.neighbors(projeto_id))\n",
    "            if set(pesquisador_areas) & set(projeto_areas):  # Verificar se há áreas em comum\n",
    "                projetos_recomendados.append(projeto_id)\n",
    "    return projetos_recomendados\n",
    "\n",
    "# Análise 4: Detecção de Oportunidades\n",
    "def detectar_oportunidades(G, produtos_prioritarios, top_n=5):\n",
    "    areas_importantes = {}\n",
    "    for produto in produtos_prioritarios:\n",
    "        for area in G.neighbors(produto['nome']):\n",
    "            areas_importantes[area] = areas_importantes.get(area, 0) + 1\n",
    "\n",
    "    # Ponderar pela concentração de pesquisadores e questões de pesquisa\n",
    "    for area in areas_importantes:\n",
    "        pesquisadores_na_area = len([n for n in G.neighbors(area) if G.nodes[n]['type'] == \"pesquisador\"])\n",
    "        questoes_na_area = len([n for n in G.neighbors(area) if G.nodes[n]['type'] == \"questao_pesquisa\"])\n",
    "        areas_importantes[area] *= (pesquisadores_na_area + questoes_na_area)\n",
    "\n",
    "    # Ordenar áreas por importância\n",
    "    areas_importantes = dict(sorted(areas_importantes.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return list(areas_importantes.keys())[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "print(stopwords.words('portuguese'))\n",
    "print(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade httpx\n",
    "# !pip install --upgrade httpcore\n",
    "# !pip install --upgrade googletrans==4.0.0-rc1\n",
    "# !pip install deep-translator\n",
    "\n",
    "import nltk\n",
    "print(nltk.data.find(\"corpora/stopwords\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_path = nltk.data.find(\"corpora/stopwords\")\n",
    "stopwords_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from deep_translator import GoogleTranslator  # Import the Googletrans library\n",
    "\n",
    "def identify_researcher_topics(data, fields, translate=False, target_language='en'):\n",
    "    \"\"\"\n",
    "    Identifies the top 3 research topics of a researcher based on their Lattes CV data.\n",
    "\n",
    "    Args:\n",
    "        data: A dictionary containing the researcher's Lattes CV data.\n",
    "        fields: A list of field names to be used for topic identification.\n",
    "\n",
    "    Returns:\n",
    "        A list of the top 3 research topics.\n",
    "    \"\"\"\n",
    "\n",
    "    def translate_text(text, target_language):\n",
    "        \"\"\"\n",
    "        Translates the text to the target language using Google Translate.\n",
    "        \"\"\"\n",
    "        translated = GoogleTranslator(source='auto', target=target_language).translate(text)\n",
    "        return translated\n",
    "\n",
    "    def extract_text_from_fields(data, fields, corpus):\n",
    "        \"\"\"\n",
    "        Recursively extracts text from the specified fields in the data dictionary.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if key in fields:\n",
    "                    if isinstance(value, list):\n",
    "                        for item in value:\n",
    "                            corpus.append(item.get(\"Descricao\", \"\") + item.get(\"titulo\", \"\"))\n",
    "                    else:\n",
    "                        corpus.append(str(value))  # Convert non-string values to string\n",
    "                else:\n",
    "                    extract_text_from_fields(value, fields, corpus)\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                extract_text_from_fields(item, fields, corpus)\n",
    "\n",
    "    # def preprocess_text(text, target_language='en'):\n",
    "    #     \"\"\"\n",
    "    #     Preprocesses the text by tokenizing, removing stopwords and proper nouns, translating, and removing punctuation.\n",
    "    #     \"\"\"\n",
    "    #     # Tokenize the text\n",
    "    #     words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    #     # Remove stopwords in the original language (if available)\n",
    "    #     source_language = detect(text)\n",
    "    #     try:\n",
    "    #         words = [word for word in words if word not in stopwords.words(source_language) and not word.istitle()]\n",
    "    #     except OSError:\n",
    "    #         print(f\"Warning: Stopwords not found for language '{source_language}'. Skipping stopword removal.\")\n",
    "\n",
    "    #     # Translate the text (if it's not already in the target language)\n",
    "    #     if target_language != 'auto' and source_language != target_language:\n",
    "    #         translator = GoogleTranslator(source=source_language, target=target_language)\n",
    "    #         text = translator.translate(text)\n",
    "\n",
    "    #         # Tokenize the translated text\n",
    "    #         words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    #         # Remove stopwords in the target language\n",
    "    #         words = [word for word in words if word not in stopwords.words(target_language)]\n",
    "\n",
    "    #     # Remove punctuation and numbers\n",
    "    #     words = [re.sub(r'[^\\w\\s]', '', word) for word in words if not word.isdigit()]\n",
    "\n",
    "    #     return \" \".join(words)\n",
    "\n",
    "    def preprocess_text(text, target_language='en'):\n",
    "        \"\"\"\n",
    "        Preprocesses the text by tokenizing, removing stopwords and proper nouns, translating, and removing punctuation.\n",
    "        \"\"\"\n",
    "        # Tokenize the text\n",
    "        words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "        # Remove stopwords in the original language (if available)\n",
    "        source_language = detect(text)\n",
    "        stopwords_path = nltk.data.find(\"corpora/stopwords\")\n",
    "        # stopwords_path = os.path.join(os.getenv('APPDATA'), 'Roaming', 'nltk_data', 'corpora', 'stopwords')  # Get stopwords path\n",
    "        if os.path.exists(os.path.join(stopwords_path, source_language)):\n",
    "            with open(os.path.join(stopwords_path, source_language), 'r', encoding='utf-8') as f:\n",
    "                stop_words = set(f.read().splitlines())\n",
    "            words = [word for word in words if word not in stop_words and not word.istitle()]\n",
    "\n",
    "        # Translate the text (if it's not already in the target language)\n",
    "        if target_language != 'auto' and source_language != target_language:\n",
    "            translator = GoogleTranslator(source=source_language, target=target_language)\n",
    "            text = translator.translate(text)\n",
    "\n",
    "            # Tokenize the translated text\n",
    "            words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "            # Remove stopwords in the target language\n",
    "            with open(os.path.join(stopwords_path, target_language), 'r', encoding='utf-8') as f:\n",
    "                stop_words = set(f.read().splitlines())\n",
    "            words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Remove punctuation and numbers\n",
    "        words = [re.sub(r'[^\\w\\s]', '', word) for word in words if not word.isdigit()]\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "    # Concatenate text from specified fields\n",
    "    corpus = []\n",
    "    extract_text_from_fields(data, fields, corpus)\n",
    "\n",
    "    # Preprocess the corpus\n",
    "    corpus = [preprocess_text(text) for text in corpus]\n",
    "\n",
    "    # Vectorize the text using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Apply LDA for topic modeling\n",
    "    lda = LatentDirichletAllocation(n_components=7, random_state=0)\n",
    "    lda.fit(X)\n",
    "\n",
    "    # Get the top words for each topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words.append([feature_names[i] for i in topic.argsort()[:-4:-1]])\n",
    "\n",
    "    return top_words\n",
    "\n",
    "pathfilename = os.path.join(folder_data_input,'docents_dict_list.json')\n",
    "with open(pathfilename, 'r', encoding='utf-8') as file:\n",
    "    docents_data = json.load(file)\n",
    "    print(f'{len(docents_data)} currículos carregados')\n",
    "\n",
    "# fields_to_use = [\"Formação Acadêmica\", \"Atuação Profissional\", \"Produções\"]\n",
    "fields_to_use = [\"titulo\"]\n",
    "\n",
    "for researcher in docents_data:\n",
    "    topics = identify_researcher_topics(researcher, fields_to_use)\n",
    "    print(f\"Principais tópicos de interesse para {researcher['Identificação']['Nome']}:\")\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"  Tópico {i+1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "oportunidades = detectar_oportunidades(G, produtos_prioritarios)\n",
    "print(\"Áreas de oportunidade:\", oportunidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo\n",
    "import requests\n",
    "\n",
    "# Cabeçalhos para a requisição\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "import http.client\n",
    "import ssl\n",
    "\n",
    "# Cria um contexto SSL sem verificação de certificado\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "# conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\")\n",
    "\n",
    "conn.request(\"GET\", \"/cities/filters?language=en\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import pandas as pd\n",
    "import ssl\n",
    "\n",
    "# Cria um contexto SSL sem verificação de certificado (NÃO RECOMENDADO PARA PRODUÇÃO)\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "# Função para fazer a requisição com tratamento de erro RETORNA OBJETO BYTES\n",
    "# def fazer_requisicao(conn, endpoint, method=\"GET\", params=None, body=None):\n",
    "#     try:\n",
    "#         # Cabeçalhos para a requisição\n",
    "#         headers = {\n",
    "#             \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "#             \"Accept\": \"application/json\",\n",
    "#         }\n",
    "#         conn.request(method, endpoint, body=body, headers=headers)\n",
    "#         res = conn.getresponse()\n",
    "#         data = res.read()\n",
    "#         return json.loads(data.decode(\"utf-8\"))\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro na requisição: {e}\")\n",
    "#         return None\n",
    "\n",
    "# Função para fazer a requisição com tratamento de erro e decodificação de bytes\n",
    "def fazer_requisicao(conn, endpoint, method=\"GET\", params=None, body=None):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        }\n",
    "        conn.request(method, endpoint, body=body, headers=headers)\n",
    "        res = conn.getresponse()\n",
    "\n",
    "        # Lê os dados da resposta como bytes\n",
    "        data = res.read()  \n",
    "\n",
    "        # Decodifica os bytes para uma string UTF-8\n",
    "        data_str = data.decode(\"utf-8\")  \n",
    "\n",
    "        # Converte a string JSON para um dicionário Python\n",
    "        return json.loads(data_str)  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na requisição: {e}\")\n",
    "        return None\n",
    "\n",
    "# Conexão com a API\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "\n",
    "# Endpoint para obter os anos disponíveis\n",
    "anos_url = \"/cities/dates/years\"\n",
    "anos_response = fazer_requisicao(conn, anos_url)\n",
    "\n",
    "# Inicializa as variáveis com valores padrão\n",
    "ano_inicial = 1997  # Ano inicial padrão\n",
    "ano_final = 2024   # Ano final padrão (ou o ano atual)\n",
    "\n",
    "if anos_response:\n",
    "    if anos_response.get('success', False):  # Verifica se a requisição foi bem-sucedida\n",
    "        if 'data' in anos_response and 'min' in anos_response['data'] and 'max' in anos_response['data']:\n",
    "            ano_inicial = int(anos_response['data']['min'])\n",
    "            ano_final = int(anos_response['data']['max'])\n",
    "        else:\n",
    "            print(\"Erro: As chaves 'data', 'min' e/ou 'max' não foram encontradas na resposta da API.\")\n",
    "    else:\n",
    "        print(f\"Erro na requisição para obter os anos: {anos_response}\")\n",
    "else:\n",
    "    print(\"Erro na requisição para obter os anos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anos_response['data']['min'])\n",
    "print(anos_response['data']['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.request(\"GET\", \"/cities/filters?language=en\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint para obter os valores do filtros disponíveis na API\n",
    "endpoint = \"/cities/filters\"\n",
    "filters_params = {\"language\": \"pt\"}\n",
    "filters_response = fazer_requisicao(conn, endpoint, params=filters_params)\n",
    "filters_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtros = [x['filter'] for x in filters_response.get('data').get('list')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_options_filters(api_filter):\n",
    "    # Cria um contexto SSL sem verificação de certificado\n",
    "    context = ssl._create_unverified_context()\n",
    "    conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "    conn.request(\"GET\", f\"/general/filters/{api_filter}?language=pt\")\n",
    "\n",
    "    res = conn.getresponse()\n",
    "    # print(f'objeto       res: {type(res)}')\n",
    "    data = res.read()\n",
    "    # print(f'objeto      data: {type(data)}')\n",
    "    data_str = data.decode(\"utf-8\")\n",
    "    # print(f'objeto  data_str: {type(data_str)}')\n",
    "    data_json = json.loads(data_str)\n",
    "    # print(f'objeto data_json: {type(data_json)}')\n",
    "    try:\n",
    "        results = [x.get('text') for x in data_json.get('data')[0]]\n",
    "        print(f'{len(results):>4} resultados para filtro {api_filter}')\n",
    "    except Exception as e:\n",
    "        print(f'     Erro ao buscar dados para filtro {api_filter}: {e}')\n",
    "    return [results][0]\n",
    "\n",
    "todos_campos_filtro={}\n",
    "for n,i in enumerate(filtros):\n",
    "    try:\n",
    "        todos_campos_filtro[i] = get_options_filters(i)\n",
    "    except Exception as e:\n",
    "        print(f'     Filtro {i} não disponível na API. Erro: {e}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicos=[]\n",
    "[unicos.append(x) for x in todos_campos_filtro['economicBlock'] if x not in unicos]\n",
    "unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicos=[]\n",
    "[unicos.append(x) for x in todos_campos_filtro['state'] if x not in unicos]\n",
    "unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_interesse = [\n",
    "    'VI - Produtos das indústrias químicas ou indústrias conexas', \n",
    "    'XVIII - Instrumentos e aparelhos de ótica, fotografia ou cinematografia, medida, controle ou de precisão; Instrumentos e aparelhos médico-cirúrgicos; Relógios e aparelhos semelhantes; Instrumentos musicais; Suas partes e acessórios',\n",
    "    'XX - Mercadorias e produtos diversos',\n",
    "    'XXII - Transações especiais'\n",
    "    ]\n",
    "\n",
    "chapter_interesse = [\n",
    "    '28 - Produtos químicos inorgânicos; compostos inorgânicos ou orgânicos de metais preciosos, de elementos radioativos, de metais das terras raras ou de isótopos',\n",
    "    '29 - Produtos químicos orgânicos',\n",
    "    '30 - Produtos farmacêuticos',\n",
    "    '35 - Matérias albuminóides; produtos à base de amidos ou de féculas modificados; colas; enzimas',\n",
    "    '38 - Produtos diversos das indústrias químicas',\n",
    "    ]\n",
    "\n",
    "heading_interesse = [\n",
    "    '2801 - Flúor, cloro, bromo e iodo',\n",
    "    '2802 - Enxofre sublimado ou precipitado; enxofre coloidal',\n",
    "    '2803 - Carbono (negros-de-carbono e outras formas não compreendidas em outras posições)',\n",
    "    '2804 - Hidrogénio, gases raros e outros elementos não metálicos',\n",
    "    '2805 - Metais alcalinos ou alcalino-terrosos; metais de terras raras, escândio e ítrio, mesmo misturados ou ligados entre si; mercúrio',\n",
    "    '2806 - Cloreto de hidrogénio (ácido clorídrico); ácido clorossulfúrico',\n",
    "    '2807 - Ácido sulfúrico e ácido sulfúrico fumante (oleum)',\n",
    "    '2808 - Ácido nítrico; ácidos sulfonítricos',\n",
    "    '2809 - Pentóxido de difosfóro; ácido fosfórico; ácidos polifosfóricos, de constituição química definida ou não',\n",
    "    '2810 - Óxidos de boro; ácidos bóricos',\n",
    "    '2811 - Outros ácidos inorgânicos e outros compostos oxigenados inorgânicos dos elementos não metálicos',\n",
    "    '2812 - Halogenetos e oxialogenetos dos elementos não metálicos',\n",
    "    '2813 - Sulfuretos dos elementos não metálicos; trissulfureto de fósforo comercial',\n",
    "    '2814 - Amoníaco anidro ou em solução aquosa (amónia)',\n",
    "    '2815 - Hidróxido de sódio (soda cáustica); hidróxido de potássio (potassa cáustica); peróxidos de sódio ou de potássio',\n",
    "    '2816 - Hidróxido e peróxido de magnésio; óxidos, hidróxidos e peróxidos, de estrôncio ou de bário',\n",
    "    '2817 - Óxido de zinco; peróxido de zinco',\n",
    "    '2818 - Corindo artificial, quimicamente definido ou não; óxido de alumínio; hidróxido de alumínio',\n",
    "    '2819 - Óxidos e hidróxidos de crómio',\n",
    "    '2820 - Óxidos de manganés',\n",
    "    '2821 - Óxidos e hidróxidos de ferro; terras corantes contendo, em peso, 70\\xa0% ou mais de ferro combinado, expresso em Fe2O3',\n",
    "    '2822 - Óxidos e hidróxidos de cobalto, inclusive os comerciais',\n",
    "    '2823 - Óxidos de titânio',\n",
    "    '2824 - Óxidos de chumbo; mínio (zarcão) e mínio-laranja (mine-orange)',\n",
    "    '2825 - Hidrazina e hidroxilamina, e seus sais inorgânicos; outras bases inorgânicas; outros óxidos, hidróxidos e peróxidos, de metais',\n",
    "    '2826 - Fluoretos; fluorossilicatos, fluoroaluminatos e outros sais complexos de flúor',\n",
    "    '2827 - Cloretos, oxicloretos e hidroxicloretos; brometos e oxibrometos; iodetos e oxiiodetos',\n",
    "    '2828 - Hipocloritos; hipoclorito de cálcio comercial; cloritos; hipobromitos',\n",
    "    '2829 - Cloratos e percloratos; bromatos e perbromatos; iodatos e periodatos',\n",
    "    '2830 - Sulfuretos; polissulfuretos, de constituição química definida ou não',\n",
    "    '2831 - Ditionites e sulfoxilatos',\n",
    "    '2832 - Sulfitos; tiosulfatos',\n",
    "    '2833 - Sulfatos; alúmenes; peroxosulfatos (persulfatos)',\n",
    "    '2834 - Nitritos; nitratos',\n",
    "    '2835 - Fosfinatos (hipofosfitos), fosfonatos (fosfitos) e fosfatos; polifosfatos, de constituição química definida ou não:',\n",
    "    '2836 - Carbonatos; peroxocarbonatos (percarbonatos); carbonato de amónio comercial contendo carbamato de amónio',\n",
    "    '2837 - Cianetos, oxicianetos e cianetos complexos',\n",
    "    '2838 - Fulminatos, cianatos e tiocianatos',\n",
    "    '2839 - Silicatos; silicatos dos metais alcalinos comerciais',\n",
    "    '2840 - Boratos; peroxoboratos (perboratos)',\n",
    "    '2841 - Sais dos ácidos oxometálicos ou peroxometálicos',\n",
    "    '2842 - Outros sais dos ácidos ou peroxoácidos inorgânicos (incluindo aluminossilicatos de constituição química definida ou não), exceto azidas',\n",
    "    '2843 - Metais preciosos no estado coloidal; compostos inorgânicos ou orgânicos de metais preciosos, de constituição química definida ou não; amálgamas de metais preciosos',\n",
    "    '2844 - Elementos químicos radioactivos e isótopos radioactivos (incluídos os elementos químicos e isótopos cindíveis ou férteis), e seus compostos; misturas e resíduos contendo esses produtos',\n",
    "    '2845 - Isótopos não incluídos na posição\\xa02844; seus compostos inorgânicos ou orgânicos, de constituição química definida ou não',\n",
    "    '2846 - Compostos, inorgânicos ou orgânicos, dos metais das terras raras, de ítrio ou de escândio ou das misturas destes metais',\n",
    "    '2847 - Peróxido de hidrogênio (água oxigenada), mesmo solidificado com ureia',\n",
    "    '2848 - Fosfetos, exceto ferrofósforos, quimicamente definidos ou não',\n",
    "    '2849 - Carbonetos de constituição química definida ou não',\n",
    "    '2850 - Hidretos, nitretos, azidas, silicietos e boretos, quimicamente definidos ou não',\n",
    "    '2851 - Compostos inorgânicos nesoi: liq ar: amálgamas nesoi',\n",
    "    '2852 - Compostos, inorgânicos ou orgânicos, de mercúrio, de constituição química definida ou não, exceto as amálgamas',\n",
    "    '2853 - Outros compostos inorgânicos (incluídas as águas destiladas, de condutibilidade ou de igual grau de pureza); ar líquido (incluído o ar líquido cujos gases raros foram eliminados); ar comprimido; amálgamas, exceto de metais preciosos.',\n",
    "    '2901 - Hidrocarbonetos acíclicos',\n",
    "    '2902 - Hidrocarbonetos cíclicos',\n",
    "    '2903 - Derivados halogenados dos hidrocarbonetos',\n",
    "    '2904 - Derivados sulfonados, nitrados ou nitrosados dos hidrocarbonetos, mesmo halogenados',\n",
    "    '2905 - Álcoois acíclicos e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2906 - Álcoois cíclicos e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2907 - Fenóis; fenóis-álcoois',\n",
    "    '2908 - Derivados halogenados, sulfonados, nitrados ou nitrosados dos fenóis ou dos fenóis-álcoois',\n",
    "    '2909 - Éteres, éteres-álcoois, éteres-fenóis, éteres-álcoois-fenóis, peróxidos de álcoois, peróxidos de éteres, peróxidos de cetonas (de constituição química definida ou não), e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2910 - Epóxidos, epoxi-álcoois, epoxi-fenóis e epoxi-éteres, com três átomos no ciclo, e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2911 - Acetais, semi-acetais, mesmo contendo outras funções oxigenadas, e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2912 - Aldeídos, mesmo contendo outras funções oxigenadas; polímeros cíclicos dos aldeídos; paraformaldeído',\n",
    "    '2913 - Derivados halogenados, sulfonados, nitrados ou nitrosados dos produtos da posição 2912',\n",
    "    '2914 - Cetonas e quinonas, mesmo contendo outras funções oxigenadas, e seus derivados halogenados, sulfonados, nitratos ou nitrosados',\n",
    "    '2915 - Ácidos monocarboxílicos acíclicos saturados e seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2916 - Ácidos monocarboxílicos acíclicos não saturados e ácidos monocarboxílicos cíclicos, seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2917 - Ácidos policarboxílicos, seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2918 - Ácidos carboxílicos contendo funções oxigenadas suplementares e seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2919 - Ésteres fosfóricos e seus sais, incluindo os lactofosfatos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2920 - Ésteres de outros ácidos inorgânicos de não-metais (exceto os ésteres de halogenetos de hidrogénio) e seus sais; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2921 - Compostos de função amina',\n",
    "    '2922 - Compostos aminados de funções oxigenadas',\n",
    "    '2923 - Sais e hidróxidos de amónio quaternários; lecitinas e outros fosfoaminolípidos, de constitução química definida ou não',\n",
    "    '2924 - Compostos de função carboxiamida; compostos de função amida do ácido carbónico',\n",
    "    '2925 - Compostos de função carboxiimida (incluindo a sacarina e seus sais) ou de função imina',\n",
    "    '2926 - Compostos de função nitrilo',\n",
    "    '2927 - Compostos diazóicos, azóicos e azóxicos',\n",
    "    '2928 - Derivados orgânicos da hidrazina e hidroxilamina',\n",
    "    '2929 - Compostos de outras funções azotadas (nitrogenadas)',\n",
    "    '2930 - Tiocompostos orgânicos',\n",
    "    '2931 - Outros compostos organo-inorgânicos',\n",
    "    '2932 - Compostos heterocíclicos exclusivamente de hetero-átomo(s) de oxigénio',\n",
    "    '2933 - Compostos heterocíclicos, exclusivamente de hetero-átomo(s) de azoto (nitrogénio)',\n",
    "    '2934 - Ácidos nucleicos e seus sais, de constituição química definida ou não; outros compostos heterocíclicos',\n",
    "    '2935 - Sulfonamidas',\n",
    "    '2936 - Provitaminas e vitaminas, naturais ou sintéticas (incluídos os concentrados naturais), bem como os seus derivados utilizados principalmente como vitaminas, misturados ou não entre si, mesmo em quaisquer soluções',\n",
    "    '2937 - Hormonas, prostaglandinas, tromboxanos e leucotrienos, naturais ou reproduzidos por síntese; seus derivados e análogos estruturais, incluindo os polipéptidos de cadeia modificada, utilizados principalmente como hormonas',\n",
    "    '2938 - Heterósidos, naturais ou sintéticos, seus sais, éteres, ésteres e outros derivados',\n",
    "    '2939 - Alcalóides vegetais, naturais ou sintéticos, seus sais, éteres, ésteres e outros derivados',\n",
    "    '2940 - Açúcares quimicamente puros, exceto sacarose, lactose, maltose, glicose e frutose; seus éteres e ésteres e seus sais',\n",
    "    '2941 - Antibióticos',\n",
    "    '2942 - Outros compostos orgânicos',\n",
    "    '3001 - Glândulas e outros órgãos para usos opoterápicos, dessecados, mesmo em pó; extractos de glândulas ou de outros órgãos ou das suas secreções, para usos opoterápicos; heparina e seus sais; outras substâncias humanas ou animais preparadas para fins terapêuti',\n",
    "    '3002 - Sangue humano; sangue animal preparado para usos terapêuticos, profilácticos ou de diagnóstico; anti-soros, outras fracções do sangue, produtos imunológicos modificados, mesmo obtidos por via biotecnológica; vacinas, toxinas, culturas de microrganismos (e',\n",
    "    '3003 - Medicamentos (exceto os produtos das posições\\xa03002, 3005\\xa0ou\\xa03006) constituídos por produtos misturados entre si, preparados para fins terapêuticos ou profilácticos, mas não apresentados em doses nem acondicionados para venda a retalho',\n",
    "    '3004 - Medicamentos (exceto os produtos das posições\\xa03002, 3005\\xa0ou\\xa03006) constituídos por produtos misturados ou não misturados, preparados para fins terapêuticos ou profilácticos, apresentados em doses (incluindo os destinados a serem administrados por via sub',\n",
    "    '3005 - Pastas (ouates), gazes, ataduras e artigos análogos (por exemplo: pensos, esparadrapos, sinapismos), impregnados ou recobertos de substâncias farmacêuticas ou acondicionados para venda a retalho para usos medicinais, cirúrgicos, dentários ou veterinários',\n",
    "    '3006 - Preparações e artigos farmacêuticos indicados na Nota\\xa04\\xa0do presente capítulo',\n",
    "    '3501 - Caseínas, caseinatos e outros derivados das caseínas; colas de caseína',\n",
    "    '3502 - Albuminas (incluídos os concentrados de várias proteínas de soro de leite, contendo, em peso calculado sobre matéria seca, mais de\\xa080\\xa0% de proteínas do soro de leite), albuminatos e outros derivados das albuminas',\n",
    "    '3503 - Gelatinas e seus derivados; ictiocola e outras colas de origem animal, exceto cola de caseína',\n",
    "    '3504 - Peptonas e seus derivados; outras matérias protéicas e seus derivados; pó de peles',\n",
    "    '3505 - Dextrina e outros amidos e féculas modificados (por exemplo: amidos e féculas pré-gelatinizados ou esterificados); colas à base de amidos ou de féculas, de dextrina ou de outros amidos ou féculas modificados',\n",
    "    '3506 - Colas e outros adesivos preparados, não especificados nem compreendidos em outras posições; produtos de qualquer espécie utilizados como colas ou adesivos, acondicionados para venda a retalho como colas ou adesivos, com peso líquido não superior a\\xa01\\xa0kg',\n",
    "    '3507 - Enzimas; enzimas preparadas não especificadas nem compreendidas em outras posições',\n",
    "    '3821 - Meios de cultura preparados para o desenvolvimento e a manutenção de microrganismos (incluindo os vírus e os organismos similares) ou de células vegetais, humanas ou animais',\n",
    "    '3822 - Reagentes de diagnóstico ou de laboratório, em qualquer suporte ou preparados, exceto os das posições 3002 ou 3006; materiais de referência certificados',\n",
    "    '3823 - Ácidos gordos monocarboxílicos industriais; óleos ácidos de refinação; alcoóis gordos industriais',\n",
    "    '3824 - Aglutinantes preparados para moldes ou para núcleos de fundição; produtos químicos e preparações das indústrias químicas ou das indústrias conexas (incluídos os constituídos por misturas de produtos naturais), não especificados nem compreendidos noutras p',\n",
    "    '3825 - Produtos residuais das indústrias químicas ou das indústrias conexas, não especificados nem compreendidos em outras posições; resíduos municipais; lamas de depuração; outros resíduos mencionados na Nota\\xa06 do presente capítulo',\n",
    "    '3901 - Polímeros de etileno, em formas primárias',\n",
    "    '3902 - Polímeros de propileno ou de outras olefinas, em formas primárias',\n",
    "    '3903 - Polímeros de estireno, em formas primárias',\n",
    "    '3904 - Polímeros de cloreto de vinilo ou de outras olefinas halogenadas, em formas primárias',\n",
    "    '3905 - Polímeros de acetato de vinilo ou de outros ésteres de vinilo, em formas primárias; outros polímeros de vinilo, em formas primárias',\n",
    "    '3906 - Polímeros acrílicos, em formas primárias',\n",
    "    '3907 - Poliacetais, outros poliéteres e resinas epóxidas, em formas primárias; policarbonatos, resinas alquídicas, poliésteres alílicos e outros poliésteres, em formas primárias',\n",
    "    '3908 - Poliamidas em formas primárias',\n",
    "    '3909 - Resinas amínicas, resinas fenólicas e poliuretanos, em formas primárias',\n",
    "    '3910 - Silicones, em formas primárias',\n",
    "    '8417 - Fornos industriais ou de laboratório, incluídos os incineradores, não elétricos',\n",
    "    '8418 - Refrigeradores, congeladores (freezers) e outro material, máquinas e aparelhos para a produção de frio, com equipamento eléctrico ou outro; bombas de calor, excluídas as máquinas e aparelhos de ar condicionado da posição 8415',\n",
    "    '8419 - Aparelhos e dispositivos, mesmo aquecidos electricamente (exceto fornos e outros aparelhos da posição 8514), para tratamento de matérias por meio de operações que impliquem mudança de temperatura, tais como o aquecimento, cozimento, torrefacção, destilaç',\n",
    "    '8420 - Calandras e laminadores, exceto os destinados ao tratamento de metais ou vidro, e seus cilindros',\n",
    "    '8421 - Centrifugadores, incluídos os secadores centrífugos, aparelhos para filtrar ou depurar líquidos ou gases',\n",
    "    '8423 - Aparelhos e instrumentos de pesagem, incluídas as básculas e balanças para verificar peças fabricadas, excluídas as balanças sensíveis a pesos não superiores a 5 cg; pesos para quaisquer balanças',\n",
    "    '8471 - Máquinas automáticas para processamento de dados e suas unidades; leitores magnéticos ou ópticos, máquinas para registar dados em suporte sob forma codificada, e máquinas para processamento desses dados, não especificadas nem compreendidas em outras posiç',\n",
    "    '8472 - Outras máquinas e aparelhos de escritório [por exemplo: duplicadores hectográficos ou a stencil, máquinas para imprimir endereços, distribuidores automáticos de papel-moeda, máquinas para seleccionar, contar ou empacotar moedas, afiadores (apontadores) me',\n",
    "    '9011 - Microscópios ópticos, incluídos os microscópios para fotomicrografia, cinefotomicrografia ou microprojecção',\n",
    "    '9012 - Microscópios, exceto ópticos; difractógrafos',\n",
    "    '9013 - Dispositivos de cristais líquidos que não constituam artigos compreendidos mais especificamente em outras posições; lasers, exceto díodos laser; outros aparelhos e instrumentos de óptica, não especificados nem compreendidos em outras posições do presente',\n",
    "    '9016 - Balanças sensíveis a pesos >= 5 cg, com ou sem pesos',\n",
    "    '9021 - Artigos e aparelhos ortopédicos, incluídas as cintas e fundas médico-cirúrgicas e as muletas; talas, goteiras e outros artigos e aparelhos para fracturas; artigos e aparelhos de prótese; aparelhos para facilitar a audição dos surdos e outros aparelhos par',\n",
    "    '9022 - Aparelhos de raios X e aparelhos que utilizem as radiações alfa, beta ou gama, mesmo para usos médicos, cirúrgicos, odontológicos ou veterinários, incluídos os aparelhos de radiofotografia ou de radioterapia, os tubos de raios X e outros dispositivos gera',\n",
    "    '9023 - Instrumentos, aparelhos e modelos, concebidos para demonstração (por exemplo, no ensino e nas exposições), não suscetíveis de outros usos',\n",
    "    '9024 - Máquinas e aparelhos para ensaios de dureza, tracção, compressão, elasticidade e de outras propriedades mecânicas de materiais (por exemplo: metais, madeira, têxteis, papel, plásticos)',\n",
    "    '9025 - Densímetros, areómetros, pesa-líquidos e instrumentos flutuantes semelhantes, termómetros, pirómetros, barómetros, higrómetros e psicrómetros, registadores ou não, mesmo combinados entre si',\n",
    "    '9026 - Instrumentos e aparelhos para medida ou controlo do caudal (vazão), do nível, da pressão ou de outras características variáveis dos líquidos ou gases (por exemplo: medidores de caudal, indicadores de nível, manómetros, contadores de calor), exceto os ins',\n",
    "    '9027 - Instrumentos e aparelhos para análises físicas ou químicas (por exemplo: polarímetros, refractómetros, espectrómetros, analisadores de gases ou de fumos); instrumentos e aparelhos para ensaios de viscosidade, porosidade, dilatação, tensão superficial ou s',\n",
    "    '9028 - Contadores de gases, de líquidos ou de electricidade, incluídos os aparelhos para a sua aferição',\n",
    "    '9029 - Outros contadores (por exemplo: contadores de voltas, contadores de produção, taxímetros, totalizadores de caminho percorrido, podómetros); indicadores de velocidade e tacómetros, exceto os das posições 9014 ou 9015; estroboscópios',\n",
    "    '9030 - Osciloscópios, analisadores de espectro e outros instrumentos e aparelhos para medida ou controlo de grandezas elétricas; instrumentos e aparelhos para medida ou detecção de radiações alfa, beta, gama, X, cósmicas ou outras radiações ionizantes',\n",
    "    '9031 - Instrumentos, aparelhos e máquinas de medida ou controlo, não especificados nem compreendidos em outras posições do presente capítulo; projectores de perfis',\n",
    "    '9032 - Instrumentos e aparelhos para regulação ou controlo, automáticos',\n",
    "    '9033 - Partes e acessórios não especificados nem compreendidos noutras posições do presente Capítulo, para máquinas, aparelhos, instrumentos ou artigos do Capítulo 90',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "conn.request(\"GET\", \"/tables/ncm/02042200\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "conn.request(\"GET\", \"/tables/ncm?language=pt\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesse = []\n",
    "# f = filtros[0]\n",
    "# print(f)\n",
    "# for n,i in enumerate(todos_campos_filtro.get(f)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Endpoint para obter os valores por filtro disponíveis na API\n",
    "# # Lista os valores disponíveis para um filtro específico. Exemplos de como acessar os valores possíveis por diferentes filtros:\n",
    "# # Países: /general/filters/country?language=pt\n",
    "# # Blocos Econômicos: /general/filters/economicBlock?language=pt\n",
    "# # Seções (do Sistema Harmonizado - SH): /general/filters/section?language=pt\n",
    "# # NCM (Nomenclatura Comum do Mercosul): /general/filters/ncm?language=pt\n",
    "\n",
    "# import http.client\n",
    "\n",
    "# # Cria um contexto SSL sem verificação de certificado\n",
    "# context = ssl._create_unverified_context()\n",
    "# conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "# conn.request(\"GET\", \"/general/filters/heading?language=pt\")\n",
    "\n",
    "# res = conn.getresponse()\n",
    "# print(f'objeto       res: {type(res)}')\n",
    "# data = res.read()\n",
    "# print(f'objeto      data: {type(data)}')\n",
    "# data_str = data.decode(\"utf-8\")\n",
    "# print(f'objeto  data_str: {type(data_str)}')\n",
    "# data_json = json.loads(data_str)\n",
    "# print(f'objeto data_json: {type(data_json)}')\n",
    "\n",
    "# Lista de campos no filtro heading\n",
    "# heading_list = [y.get('text') for y in data_json.get('data')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de campos no filtro chapter\n",
    "['country', 'economicBlock', 'state', 'city', 'heading', 'chapter', 'section']\n",
    "filter='economicBlock'\n",
    "api_filters = ['']\n",
    "list_country = get_options_filters('country')\n",
    "list_economicBlock = get_options_filters('economicBlock')\n",
    "list_state = get_options_filters('state')\n",
    "list_heading = get_options_filters('heading')\n",
    "list_chapter = get_options_filters('chapter')\n",
    "list_section = get_options_filters('section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Inicializa a lista de NCMs de saúde com valores padrão\n",
    "ncms_saude = []  # Lista vazia caso a requisição falhe\n",
    "\n",
    "if ncm_response:\n",
    "    ncms = ncm_response.json()\n",
    "    # Filtrar os NCMs relacionados à saúde (exemplo)\n",
    "    ncms_saude = [ncm['id'] for ncm in ncms if ncm['desc'].startswith(\"Medicamentos\")]\n",
    "\n",
    "# Endpoint para consulta dos dados\n",
    "consulta_url = \"/cities\"\n",
    "\n",
    "# Parâmetros da consulta (exemplo)\n",
    "params = {\n",
    "    \"flow\": [\"export\", \"import\"],\n",
    "    \"monthDetail\": False,\n",
    "    \"period\": {\"from\": f\"{ano_inicial}-01\", \"to\": f\"{ano_final}-12\"},\n",
    "    \"filters\": [{\"filter\": \"ncm\", \"values\": ncms_saude}],\n",
    "    \"details\": [\"ncm\"],\n",
    "    \"metrics\": [\"metricFOB\"],\n",
    "}\n",
    "\n",
    "# Realiza a consulta\n",
    "response = fazer_requisicao(conn, consulta_url, params=params)\n",
    "\n",
    "if response:\n",
    "    data = response\n",
    "    df = pd.DataFrame(data['data'])\n",
    "\n",
    "    # Calcula o déficit por ano e NCM\n",
    "    df_pivot = df.pivot_table(\n",
    "        index=[\"year\", \"ncm\"], columns=\"flow\", values=\"metricFOB\", aggfunc=\"sum\"\n",
    "    )\n",
    "    df_pivot[\"deficit\"] = df_pivot[\"import\"] - df_pivot[\"export\"]\n",
    "\n",
    "    # Salva os resultados em um arquivo CSV\n",
    "    df_pivot.to_csv(\"deficit_balanca_comercial_saude.csv\")\n",
    "\n",
    "    print(\"Dados salvos com sucesso!\")\n",
    "\n",
    "# Fecha a conexão\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Inicializa a lista de NCMs de saúde com valores padrão\n",
    "ncms_saude = []  # Lista vazia caso a requisição falhe\n",
    "\n",
    "if ncm_response:\n",
    "    ncms = ncm_response.json()\n",
    "    # Filtrar os NCMs relacionados à saúde (exemplo)\n",
    "    ncms_saude = [ncm['id'] for ncm in ncms if ncm['desc'].startswith(\"Medicamentos\")]\n",
    "    print(ncms_saude)\n",
    "\n",
    "# Endpoint para consulta dos dados\n",
    "consulta_url = \"https://api-comexstat.mdic.gov.br/cities\"\n",
    "\n",
    "# Parâmetros da consulta (exemplo)\n",
    "params = {\n",
    "    \"flow\": [\"export\", \"import\"],\n",
    "    \"monthDetail\": False,\n",
    "    \"period\": {\"from\": f\"{ano_inicial}-01\", \"to\": f\"{ano_final}-12\"},\n",
    "    \"filters\": [{\"filter\": \"ncm\", \"values\": ncms_saude}],\n",
    "    \"details\": [\"ncm\"],\n",
    "    \"metrics\": [\"metricFOB\"],\n",
    "}\n",
    "\n",
    "# Realiza a consulta\n",
    "response = fazer_requisicao(consulta_url, json=params)\n",
    "\n",
    "if response:\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # # Calcula o déficit por ano e NCM\n",
    "    # df_pivot = df.pivot_table(\n",
    "    #     index=[\"coAno\", \"ncm\"], columns=\"flow\", values=\"metricFOB\", aggfunc=\"sum\"\n",
    "    # )\n",
    "    # df_pivot[\"deficit\"] = df_pivot[\"import\"] - df_pivot[\"export\"]\n",
    "\n",
    "    # # Salva os resultados em um arquivo CSV\n",
    "    # df_pivot.to_csv(\"deficit_balanca_comercial_saude.csv\")\n",
    "\n",
    "    # print(\"Dados salvos com sucesso!\")\n",
    "for i in df['data'].items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar classes do pacote\n",
    "from research_process_automation import QuestionFormulation, InteractiveFeedback, QuestionCriteriaEvaluator\n",
    "\n",
    "# Criar instância da classe QuestionFormulation\n",
    "question_formulator = QuestionFormulation()\n",
    "\n",
    "# Solicitar informações do usuário\n",
    "question_formulator.input_ideas()\n",
    "\n",
    "# Gerar a pergunta de pesquisa\n",
    "research_question = question_formulator.generate_question()\n",
    "print(\"PASSO 01: Questão de pesquisa\")\n",
    "print(research_question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entradas de referência:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entradas de citações para referências:\n",
    "\n",
    "@book{gil2022comoelaborarprojetosdepesquisa,\n",
    "  title={Como Elaborar Projetos de Pesquisa},\n",
    "  author={Gil, Antonio Carlos},\n",
    "  edition={7},\n",
    "  publisher={Atlas},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "@book{popper1934logic,\n",
    "  title={A Lógica da Pesquisa Científica},\n",
    "  author={Popper, Karl},\n",
    "  year={1934}\n",
    "}\n",
    "\n",
    "@misc{IEP2023Popper,\n",
    "  author = {Internet Encyclopedia of Philosophy},\n",
    "  title = {Karl Popper: Political Philosophy},\n",
    "  year = {2023},\n",
    "  howpublished = {\\url{https://iep.utm.edu/popp-pol/}},\n",
    "  note = {Acesso em: 01/01/2024}\n",
    "}\n",
    "\n",
    "@phdthesis{Broderick1984,\n",
    "  author = {David Gregory Broderick},\n",
    "  title = {Objectivity: Thomas Aquinas and Karl Popper},\n",
    "  school = {Boston College},\n",
    "  year = {1984}\n",
    "}\n",
    "\n",
    "@article{CRYFUNavara2023,\n",
    "  author = {Grupo Ciencia, Razón y Fe (CRYF)},\n",
    "  title = {The Ethical Roots of Karl Popper's Epistemology},\n",
    "  journal = {Universidad de Navarra},\n",
    "  year = {2023},\n",
    "  url = {https://www.unav.edu/web/ciencia-razon-y-fe/poppers-epistemology}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bases de Dados:\n",
    "\n",
    "@misc{arxiv,\n",
    "  title = {{arXiv}},\n",
    "  url = {https://arxiv.org}\n",
    "}\n",
    "\n",
    "@misc{core,\n",
    "  title = {{CORE}},\n",
    "  url = {https://core.ac.uk}\n",
    "}\n",
    "\n",
    "@misc{doaj,\n",
    "  title = {{Directory of Open Access Journals (DOAJ)}},\n",
    "  url = {https://doaj.org}\n",
    "}\n",
    "\n",
    "@misc{googlescholar,\n",
    "  title = {{Google Scholar}},\n",
    "  url = {https://scholar.google.com}\n",
    "}\n",
    "\n",
    "@misc{openaire,\n",
    "  title = {{OpenAIRE}},\n",
    "  url = {https://www.openaire.eu}\n",
    "}\n",
    "\n",
    "@misc{pubmedcentral,\n",
    "  title = {{PubMed Central}},\n",
    "  url = {https://www.ncbi.nlm.nih.gov/pmc/}\n",
    "}\n",
    "\n",
    "@misc{ssrn,\n",
    "  title = {{Social Science Research Network (SSRN)}},\n",
    "  url = {https://www.ssrn.com}\n",
    "}\n",
    "\n",
    "@misc{scienceopen,\n",
    "  title = {{ScienceOpen}},\n",
    "  url = {https://www.scienceopen.com}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editoras com políticas de OA:\n",
    "\n",
    "@misc{springernature,\n",
    "  title = {{Springer Nature}},\n",
    "  url = {https://www.springernature.com}\n",
    "}\n",
    "\n",
    "@misc{oup,\n",
    "  title = {{Oxford University Press (OUP)}},\n",
    "  url = {https://academic.oup.com}\n",
    "}\n",
    "\n",
    "@misc{frontiers,\n",
    "  title = {{Frontiers}},\n",
    "  url = {https://www.frontiersin.org}\n",
    "}\n",
    "\n",
    "@misc{wiley,\n",
    "  title = {{Wiley}},\n",
    "  url = {https://www.wiley.com}\n",
    "}\n",
    "\n",
    "@misc{plos,\n",
    "  title = {{Public Library of Science (PLOS)}},\n",
    "  url = {https://www.plos.org}\n",
    "}\n",
    "\n",
    "@misc{hindawi,\n",
    "  title = {{Hindawi}},\n",
    "  url = {https://www.hindawi.com}\n",
    "}\n",
    "\n",
    "@misc{mdpi,\n",
    "  title = {{MDPI AG}},\n",
    "  url = {https://www.mdpi.com}\n",
    "}\n",
    "\n",
    "@misc{informa,\n",
    "  title = {{Informa PLC}},\n",
    "  url = {https://www.informa.com}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GII\n",
    "@misc{GII-WIPO,\n",
    "      title = {Global Innovation Index - WIPO Series},\n",
    "      abstract = {The Global Innovation Index (GII) ranks the innovation performance of some 131 countries and economies around the world, based on 80+ indicators. Co-published by WIPO, Cornell University and INSEAD, the report provides an annual ranking of the innovation capabilities and performance of economies around the world.},\n",
    "      {url = https://www.wipo.int/publications/en/series/index.jsp?id=129}\n",
    "}\n",
    "\n",
    "@misc{GII-2011,\n",
    "  title = {Global Innovation Index 2011 - Accelerating Growth and Development},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=274&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2012,\n",
    "  title = {Global Innovation Index 2012 - Stronger Innovation Linkages for Global Growth},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=247&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2013,\n",
    "  title = {Global Innovation Index 2013 - The Local Dynamics of Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=368&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2014,\n",
    "  title = {Global Innovation Index 2014 - The Human Factor in Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=3254&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2015,\n",
    "  title = {Global Innovation Index 2015 - Effective Innovation Policies for Development},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=3978&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2016,\n",
    "  title = {Global Innovation Index 2016 - Winning with Global Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4064&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2017,\n",
    "  title = {Global Innovation Index 2017 - Innovation Feeding the World},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4193&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2018,\n",
    "  title = {Global Innovation Index 2018 - Energizing the World with Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4330&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2019,\n",
    "  title = {Global Innovation Index 2019 - Creating Healthy Lives — The Future of Medical Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4434&plang=EN}\n",
    "}\n",
    "\n",
    "@article{40579,\n",
    "      author = {Cornell University.},\n",
    "      url = {http://tind.wipo.int/record/40579},\n",
    "      title = {Global Innovation Index 2019 - Executive version.},\n",
    "      abstract = {The Global Innovation Index 2019 provides detailed metrics  about the innovation performance of 129 countries and  economies around the world. Its 80 indicators explore a  broad vision of innovation, including political  environment, education, infrastructure and business  sophistication. The GII 2019 analyzes the medical  innovation landscape of the next decade, looking at how  technological and non-technological medical innovation will  transform the delivery of healthcare worldwide. It also  explores the role and dynamics of medical innovation as it  shapes the future of healthcare, and the potential  influence this may have on economic growth. Chapters of the  report provide more details on this year’s theme from  academic, business, and particular country perspectives  from leading experts and decision makers.},\n",
    "      doi = {https://doi.org/10.34667/tind.40579},\n",
    "      recid = {40579},\n",
    "      pages = {214 pages ;},\n",
    "}\n",
    "\n",
    "@article{35279,\n",
    "      url = {http://tind.wipo.int/record/35279},\n",
    "      title = {Índice Global de inovação de 2019 - PRINCIPAIS  RESULTADOS.},\n",
    "      abstract = {Criar Vidas Sadias - O Futuro da Inovação Médica.},\n",
    "      doi = {https://doi.org/10.34667/tind.35279},\n",
    "      recid = {35279},\n",
    "      pages = {20 pages ;},\n",
    "}\n",
    "\n",
    "@misc{GII-2020,\n",
    "  title = {Global Innovation Index 2020 - Who Will Finance Innovation?},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4514&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2021,\n",
    "  title = {Global Innovation Index 2021 - Tracking Innovation through the COVID-19 Crisis},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4560&plang=EN}\n",
    "}\n",
    "\n",
    "@article{46620,\n",
    "      url = {http://tind.wipo.int/record/46620},\n",
    "      title = {Índice Global de Inovação 2022 : Resumo executivo.},\n",
    "      abstract = {O Índice Global da Inovação 2022 (IGI) analisa as  tendências globais no campo da inovação em um cenário  marcado pela pandemia de COVID-19 em curso, por um  crescimento desacelerado da produtividade e pelo surgimento  de novos desafios. O IGI revela as economias mais  inovadoras do mundo, classificando o desempenho em  inovação de 132 economias, destacando seus pontos fortes  e fracos em matéria de inovação e identificando lacunas  em suas métricas de inovação. Esta edição de 2022 tem  como foco o efeito da inovação sobre a produtividade e o  bem-estar da sociedade ao longo das próximas décadas.},\n",
    "      doi = {https://doi.org/10.34667/tind.46620},\n",
    "      recid = {46620},\n",
    "      pages = {28 pages :},\n",
    "}\n",
    "\n",
    "@misc{GII-2022,\n",
    "  title = {Global Innovation Index 2022 - What is the future of innovation driven growth?},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4622&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2023,\n",
    "  title = {Global Innovation Index 2023},\n",
    "  url = {https://www.globalinnovationindex.org/gii-2023}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
