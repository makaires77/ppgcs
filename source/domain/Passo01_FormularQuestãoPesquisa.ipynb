{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Interesse de pesquisa</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O termo \"inovação tecnológica\", no sentido moderno, começou a ser mais amplamente discutido e compreendido no início do século XX. Joseph Schumpeter, um economista austríaco, foi uma das figuras-chave que destacou a inovação tecnológica como um motor para o crescimento econômico. Após a Primeira Guerra Mundial, pensadores como Thorstein Veblen e Herbert Hoover também enfatizaram a importância da inovação tecnológica para a segurança nacional e competitividade industrial. Esse conceito se expandiu particularmente após a Segunda Guerra Mundial, quando a inovação tecnológica começou a ser vista como crucial para a prosperidade industrial e a segurança militar, especialmente nos Estados Unidos. fonte: Encyclopedia.com sobre Inovação Tecnológica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Bases filosóficas</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karl Popper foi um severo crítico ao totalitarismo e utopias demagógicas. Embora em suas obras filosóficas, até onde nossos melhores esforços de busca alcançam, não tenha feito referências diretas significativas ao filósofo Tomás de Aquino, as interseções e diálogos filosóficos entre suas ideias, especialmente em epistemologia e ética, têm sido tema de discussão acadêmica. Por exemplo, na dissertação de David Gregory Broderick, intitulada \"Objetividade: Tomás de Aquino e Karl Popper\", explora-se a relação entre as noções de objetividade nas obras de Aquino e Popper. A dissertação sugere que, embora haja diferenças terminológicas, históricas e de interesses entre Aquino e Popper, também existem paralelos significativos em suas abordagens sobre objetividade e o crescimento do conhecimento.\n",
    "\n",
    "Além disso, na discussão sobre as raízes éticas da epistemologia de Popper, explorada pelo Grupo Ciencia, Razón y Fe (CRYF) da Universidad de Navarra, destaca-se a complementaridade das posições de Aquino e Popper. Esta análise sugere que a forte defesa de Popper do realismo, da verdade objetiva e da metodologia para o crescimento do conhecimento conjectural pode ser vista como complementar, ou até mesmo fundamentada, nos princípios éticos e metafísicos delineados por Aquino.\n",
    "\n",
    "Essas análises sugerem que, embora Popper possa não ter citado explicitamente Aquino em suas principais obras, as bases filosóficas de seus pensamentos, especialmente em relação à objetividade, ética e crescimento do conhecimento, têm grandes paralelos e áreas de convergência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, tal como acontece com a sua defesa das eleições numa democracia, o argumento de Popper a favor da engenharia social fragmentada baseia-se principalmente na sua compatibilidade com o método de tentativa e erro das ciências naturais: uma teoria é proposta e testada, erros na teoria são detectados e eliminado, e uma teoria nova e melhorada emerge, reiniciando o ciclo. Através da engenharia gradual, o processo de progresso social é, portanto, paralelo ao progresso científico. Na verdade, Popper diz que a engenharia social fragmentada é a única abordagem à política pública que pode ser genuinamente científica: \"Isto - e nenhum planeamento utópico ou profecia histórica - significaria a introdução do método científico na política, uma vez que todo o segredo do método científico é uma disposição para aprender com os erros\" ( Open Society Vol 1., 163)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Metodologia da Pesquisa</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma contribuição seminal para abordagem falseável da ciência é a descrita por Karl Popper em sua obra \"A Lógica da Pesquisa Científica\", publicada originalmente em 1934. É nesta obra que Popper argumenta que a ciência deve adotar uma metodologia baseada na falseabilidade. Segundo ele, nenhuma quantidade de experimentos pode provar uma teoria, mas um único experimento ou observação reproduzível pode refutá-la. Esta abordagem destaca a importância da capacidade de uma teoria ser testada e potencialmente falsificada, em vez de apenas verificada. Popper diferencia as teorias científicas das pseudociências e da metafísica, salientando que as teorias científicas devem ser testáveis e passíveis de refutação, enquanto pseudociências e metafísicas não permitem essa possibilidade. Popper enfatiza que a ciência deve estar em constante evolução, com as hipóteses sendo submetidas a testes contínuos para acompanhar o desenvolvimento da ciência e das tecnologias (POPPER, 1934).\n",
    "\n",
    "A gênese de uma boa questão de pesquisa também é abordada por outros autores notáveis no campo da metodologia científica. Antonio Carlos Gil descreve que a elaboração de projetos de pesquisa deve ser guiada pela apresentação clara e acessível dos elementos necessários para a pesquisa, além da organização de conhecimentos dispersos. Gil enfatiza a natureza prática da pesquisa, abordando a importância de esclarecer os procedimentos para a elaboração de projetos em diversos tipos de pesquisa. (GIL, 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Passo 01: Redigir uma boa questão de pesquisa</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como chegar à uma boa questão de pesquisa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns dos critérios essenciais para avaliar a qualidade de uma questão de pesquisa, e como integrar ou considerar cada um deles no processo de formulação:\n",
    "\n",
    "<b>Clareza e Especificidade</b>: Uma boa questão de pesquisa deve ser clara e específica, evitando ambiguidades. Isto pode ser parcialmente garantido pelo processamento de linguagem natural, mas também requer revisão humana para garantir que a questão seja compreensível e precisamente focada.\n",
    "\n",
    "<b>Relevância Acadêmica ou Científica</b>: A questão deve ser relevante para o campo de estudo e contribuir de alguma forma para o conhecimento existente. Isso geralmente requer uma compreensão do contexto acadêmico e das pesquisas atuais, o que pode ser além do escopo da automação completa.\n",
    "\n",
    "<b>Viabilidade</b>: A pergunta deve ser algo que pode ser realisticamente respondido através de métodos de pesquisa disponíveis. Este aspecto pode ser parcialmente verificado por meio de regras heurísticas programadas, mas frequentemente requer avaliação humana, especialmente para julgar a disponibilidade de dados ou recursos de pesquisa.\n",
    "\n",
    "<b>Originalidade</b>: Uma boa questão de pesquisa deve oferecer novas perspectivas ou abordar lacunas existentes na literatura. A originalidade pode ser difícil de avaliar automaticamente, mas técnicas avançadas de NLP, como análise semântica e comparação com bancos de dados de literatura existente, podem ajudar.\n",
    "\n",
    "<b>Importância Prática ou Teórica</b>: A questão deve ter alguma importância prática ou contribuir para a compreensão teórica de um tópico. Isso geralmente exige conhecimento especializado na área de estudo para avaliar.\n",
    "\n",
    "<b>Estruturação Adequada</b>: A pergunta deve ser estruturada de forma a facilitar uma abordagem de pesquisa clara. Isso inclui a utilização de uma formulação que se alinhe com métodos de pesquisa qualitativos ou quantitativos, conforme apropriado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como Integrar os critérios da questão de pesquisa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerar uma boa questão de pesquisa depende de uma combinação de processamento automatizado, conhecimento especializado e revisão humana. A automação pode fornecer uma base útil, mas a supervisão e o julgamento humanos são cruciais para garantir a qualidade final da pergunta de pesquisa. As seguintes estratégias são utilizadas em nossa solução para chegar a uma boa questão de pesquisa:\n",
    "\n",
    "<b>Automatização com Revisão Humana</b>: Uma abordagem prática é usar a automação para gerar uma primeira versão da pergunta, que é então revisada e refinada por pesquisadores humanos (pesquisador principal, equipe de pesquisa e orientador). A automação garante que certos critérios básicos sejam atendidos (como clareza e estruturação), enquanto a revisão humana aborda aspectos mais sutis, como relevância, viabilidade e originalidade.\n",
    "\n",
    "<b>Feedback Interativo</b>: Incorporar um sistema de feedback no processo, onde o usuário pode refinar suas ideias iniciais ou ajustar a pergunta gerada, pode ajudar a melhorar a qualidade da questão de pesquisa final.\n",
    "\n",
    "<b>Integração de vários Bancos de Dados de Literatura</b>: Integrar um banco de dados de literatura existente pode ajudar a avaliar a originalidade e a relevância da pergunta, comparando-a com pesquisas já publicadas.\n",
    "\n",
    "<b>Educação do Usuário</b>: Fornecer orientações e exemplos de boas perguntas de pesquisa aos usuários pode ajudá-los a formular ideias iniciais mais eficazes, levando a melhores resultados na formulação automática.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Bases de dados de artigos em Open Access</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao focar a pesquisa em fontes que oferecem conteúdos completos em Open Access, há bases de dados e repositórios acadêmicos importantes que se destacam em relevância e quantidade de conteúdos:\n",
    "\n",
    "<b>Directory of Open Access Journals (DOAJ)</b>: O DOAJ é um diretório online que indexa e fornece acesso a periódicos de alta qualidade, todos de acesso livre e revisados por pares. É uma excelente fonte para pesquisar artigos em uma ampla gama de disciplinas. O DOAJ oferece uma API para acessar seu índice de periódicos e artigos. A documentação e detalhes sobre a API estão disponíveis em DOAJ API.\n",
    "\n",
    "<b>PubMed Central</b>: Operado pela Biblioteca Nacional de Medicina dos EUA, o PubMed Central é um repositório gratuito de artigos de ciências biomédicas e ciências da vida. Embora seu foco seja mais na área da saúde, ele pode ter artigos relevantes sobre inovação tecnológica no contexto da saúde. A PubMed Central oferece uma API chamada Entrez Programming Utilities (E-utilities) para interagir com a base de dados. Mais informações podem ser encontradas em Entrez Programming Utilities Help.\n",
    "\n",
    "<b>arXiv</b>: O arXiv é um repositório de preprints em campos como física, matemática, ciência da computação, biologia quantitativa, finanças quantitativas e estatística. É uma boa fonte para literatura mais técnica e teórica sobre inovação tecnológica. O arXiv fornece uma API para acesso aos seus preprints. Informações detalhadas e documentação sobre a API estão disponíveis em arXiv API.\n",
    "\n",
    "<b>OpenAIRE</b>: Uma infraestrutura que promove a descoberta e o acesso a publicações científicas europeias de acesso livre. É especialmente útil para pesquisas que envolvem colaborações europeias ou focam em políticas e práticas de inovação na Europa. OpenAIRE oferece uma API para acessar seu repositório. Você pode encontrar mais informações sobre como usar esta API em OpenAIRE API.\n",
    "\n",
    "<b>Google Scholar</b>: Apesar de não ser exclusivamente dedicado ao Open Access, o Google Scholar pode ser utilizado para localizar artigos de acesso livre. Ele indexa uma variedade de fontes acadêmicas e muitas vezes inclui links para versões de acesso livre dos artigos. O Google Scholar não oferece uma API oficial para acesso programático.\n",
    "\n",
    "<b>CORE</b>: Agregador que permite o acesso a milhões de artigos de acesso livre. Ele reúne conteúdo de repositórios e periódicos de todo o mundo, sendo uma excelente ferramenta para uma pesquisa abrangente. CORE oferece uma API que permite acessar seu vasto repositório de artigos de acesso livre. A documentação da API pode ser encontrada em CORE API.\n",
    "\n",
    "<b>ScienceOpen</b>: Plataforma de pesquisa e publicação que oferece acesso a mais de 60 milhões de artigos e registros de pesquisa em todas as áreas. Não há informações disponíveis sobre uma API pública para o ScienceOpen.\n",
    "\n",
    "<b>SSRN (Social Science Research Network)</b>: Especializado em ciências sociais, o SSRN é um repositório de preprints que abrange uma ampla gama de áreas, incluindo economia, direito e gestão corporativa, onde você pode encontrar trabalhos relacionados à gestão da inovação. O SSRN não fornece uma API pública para acesso programático aos seus conteúdos.\n",
    "\n",
    "\n",
    "O tipo de busca mais comum é por palavras-chave, usadas para descobrir e para refinar a pesquisa e localizar artigos relevantes sobre os temas de interesse, no nosso caso, a gestão da inovação tecnológica em organizações públicas e privadas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editoras científicas com políticas de Open Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Várias editoras científicas proeminentes oferecem conteúdos em Open Access, proporcionando acesso livre a uma vasta gama de pesquisas acadêmicas. Algumas das principais editoras incluem:\n",
    "\n",
    "<b>Springer Nature</b>: Com mais de 600 periódicos totalmente em acesso aberto e mais de 1000 livros em acesso aberto, a Springer Nature é uma das pioneiras no campo da pesquisa aberta. Oferece uma variedade de opções de publicação em acesso aberto, mantendo rigorosos processos de revisão por pares e editoriais.\n",
    "\n",
    "<b>Oxford University Press (OUP)</b>: A OUP publica mais de 120 periódicos totalmente em acesso aberto e mais de 250 livros em acesso aberto, abrangendo uma ampla gama de disciplinas. Muitos de seus periódicos são classificados como \"diamond OA\", o que significa que não há taxas de processamento de artigos para autores ou leitores.\n",
    "\n",
    "<b>Frontiers</b>: Reconhecida como uma editora líder em Acesso Aberto e plataforma de Ciência Aberta, a Frontiers é muito citada, com mais de um bilhão de visualizações e downloads e 1.6 milhão de citações em seus artigos acessíveis gratuitamente. Ela é ativa em áreas como neurociências, psiquiatria, fisiologia, medicina clínica, ciências naturais e engenharia.\n",
    "\n",
    "<b>Wiley</b>: A Wiley oferece mais de 150 periódicos revisados por pares em acesso aberto, abrangendo diversas disciplinas de pesquisa. Seus periódicos em acesso aberto estão disponíveis para leitura, download e compartilhamento gratuitamente através da Wiley Online Library e do PubMed Central.\n",
    "\n",
    "<b>Public Library of Science (PLOS)</b>: Fundada como uma organização sem fins lucrativos, a PLOS tem como objetivo catalisar o movimento de acesso aberto. Publica periódicos em diversas áreas, incluindo medicina e ciências da vida.\n",
    "\n",
    "<b>Hindawi</b>: Inicialmente uma editora de periódicos por assinatura, a Hindawi fez a transição para um modelo de publicação em acesso aberto entre 2004 e 2007. Ela publica periódicos em ciência e engenharia, ciências sociais e filosofia.\n",
    "\n",
    "<b>MDPI AG</b>: Uma empresa suíça com presença global, a MDPI AG publica periódicos em ciência e engenharia, ciências sociais e filosofia.\n",
    "\n",
    "<b>Informa PLC</b>: Uma das maiores editoras nas humanidades e ciências sociais, a Informa publica em acesso aberto sob quatro selos: Taylor & Francis Open, Dove Medical Press, Cogent OA e Routledge Open.\n",
    "\n",
    "Estas editoras são conhecidas não apenas pela qualidade e diversidade de suas publicações, mas também por suas contribuições significativas para o movimento de acesso aberto na comunidade acadêmica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desempenho em Inovação no mundo e no Brasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medição da inovação em nível mundial é um campo importante para entender como diferentes nações estão progredindo em termos de capacidade e sucesso em inovação. Uma das iniciativas mais conhecidas neste campo é o Global Innovation Index (GII).\n",
    "\n",
    "O GII é um ranking anual publicado pela World Intellectual Property Organization (WIPO) em parceria com a INSEAD e outras instituições, como a Cornell University. Iniciado em 2007, o índice é baseado em dados subjetivos e objetivos obtidos de várias fontes, incluindo a International Telecommunication Union, o World Bank e o World Economic Forum. O GII classifica os países com base em dois sub-índices: o Innovation Input Index e o Innovation Output Index, compostos por cinco e dois pilares, respectivamente, cada um descrevendo um atributo da inovação.\n",
    "\n",
    "Além do GII, há outras iniciativas semelhantes, como o International Innovation Index, que medem o nível de inovação de um país. Este índice é produzido em conjunto pelo Boston Consulting Group (BCG), pela National Association of Manufacturers (NAM) e pelo Manufacturing Institute (MI), o afiliado de pesquisa apartidária da NAM.\n",
    "\n",
    "Cada um desses índices oferece uma perspectiva única sobre a inovação em nível de país, ajudando governos, formuladores de políticas e acadêmicos a entender as tendências de inovação e a identificar áreas para melhoria e investimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Sempre construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(base_repo_dir, 'utils')\n",
    "folder_domain = os.path.join(base_repo_dir, 'source', 'domain')\n",
    "folder_data_input = os.path.join(base_repo_dir, '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(base_repo_dir, '_data', 'out_json')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "# from scraper_pasteur import PasteurScraper\n",
    "# from scraper_sucupira import SucupiraScraper\n",
    "# from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "from environment_setup import EnvironmentSetup\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from neo4j_persister import Neo4jPersister\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DictToHDF5, attribute_to_be_non_empty\n",
    "\n",
    "t00 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pathfilename = os.path.join(folder_data_input, 'gii_history_data.csv')\n",
    "df_gii = pd.read_csv(pathfilename)\n",
    "\n",
    "def calculate_percentages(df_gii):\n",
    "    df_avaliacao = pd.DataFrame(columns=['Ano', 'Participantes', 'Above Brazil', 'Below Brazil', 'Above Brazil (%)', 'Below Brazil (%)'])\n",
    "    \n",
    "    for year in df_gii['Ano'].unique():\n",
    "        df_year = df_gii[df_gii['Ano'] == year]\n",
    "        total_countries = df_year['Países Participantes'].values[0]\n",
    "        brazil_position = df_year['Colocação do Brasil'].values[0]\n",
    "        above_brazil = brazil_position - 1\n",
    "        below_brazil = total_countries - brazil_position\n",
    "        \n",
    "        above_percent = (above_brazil / total_countries) * 100\n",
    "        below_percent = (below_brazil / total_countries) * 100\n",
    "        \n",
    "        df_avaliacao = pd.concat([df_avaliacao, pd.DataFrame({'Ano': [year], 'Participantes': [total_countries], 'Above Brazil': [above_brazil], 'Below Brazil': [below_brazil], 'Above Brazil (%)': [above_percent], 'Below Brazil (%)': [below_percent]})], ignore_index=True)\n",
    "    \n",
    "    return df_avaliacao\n",
    "\n",
    "# Call the function to create df_avaliacao\n",
    "df_avaliacao = calculate_percentages(df_gii)\n",
    "df_avaliacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a figure with secondary y-axis for the line plots\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add stacked bars for 'Below Brazil (%)' and 'Above Brazil (%)'\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_avaliacao['Ano'], \n",
    "           y=df_avaliacao['Below Brazil (%)'], \n",
    "           name='Below Brazil (%)'),\n",
    "           secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_avaliacao['Ano'], \n",
    "           y=df_avaliacao['Above Brazil (%)'], \n",
    "           name='Above Brazil (%)'),\n",
    "           secondary_y=False,\n",
    ")\n",
    "\n",
    "# Correct the data label positions for each segment of the stacked bars\n",
    "for index, row in df_avaliacao.iterrows():\n",
    "    # Position for label of 'Above Brazil (%)'\n",
    "    position_above = row['Below Brazil (%)'] + row['Above Brazil (%)'] / 2\n",
    "    fig.add_annotation(\n",
    "        x=row['Ano'], y=position_above,\n",
    "        text=f\"{row['Above Brazil (%)']:.1f}%\",\n",
    "        showarrow=False, font=dict(color='white')\n",
    "    )\n",
    "\n",
    "    # Position for label of 'Below Brazil (%)'\n",
    "    position_below = row['Below Brazil (%)'] / 2\n",
    "    fig.add_annotation(\n",
    "        x=row['Ano'], y=position_below,\n",
    "        text=f\"{row['Below Brazil (%)']:.1f}%\",\n",
    "        showarrow=False, font=dict(color='white')\n",
    "    )\n",
    "\n",
    "# Update layout for stacked bars\n",
    "fig.update_layout(barmode='stack')\n",
    "\n",
    "# Add line for total number of participants\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_avaliacao['Ano'], \n",
    "               y=df_avaliacao['Participantes'], \n",
    "               name='Total Participants', \n",
    "               mode='lines+markers+text', \n",
    "               text=df_avaliacao['Participantes'], \n",
    "               textposition=\"top center\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add line for Brazil's performance, but on the primary y-axis\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_avaliacao['Ano'], \n",
    "               y=df_avaliacao['Below Brazil (%)'], \n",
    "               name='Brazil Performance', \n",
    "               mode='lines+markers', \n",
    "               line=dict(color='yellow', \n",
    "                         dash='dot')),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "# Update the bar colors\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Above Brazil (%)'),\n",
    "    marker=dict(color='orange')\n",
    ")\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Below Brazil (%)'),\n",
    "    marker=dict(color='blue')\n",
    ")\n",
    "\n",
    "# Update the line trace for total number of participants to have a thickness of 4\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Total Participants'),\n",
    "    line=dict(width=2)\n",
    ")\n",
    "\n",
    "# Update the line trace for Brazil's performance to have a thickness of 4\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Brazil Performance'),\n",
    "    line=dict(width=6)\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "vr_max = max(df_avaliacao['Participantes']) * 1.1\n",
    "fig.update_layout(\n",
    "    title='Performance Comparison: Brazil vs. Other Countries',\n",
    "    height=600,\n",
    "    yaxis=dict(title='Percentage', range=[0, vr_max]),  # Extending primary y-axis range\n",
    "    yaxis2=dict(title='Total Participants', overlaying='y', side='right', range=[0, vr_max])\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickvals=df_avaliacao['Ano'])\n",
    "\n",
    "# Re-render the chart\n",
    "fig.show(renderer=\"notebook\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Grafo para Inovar por Multicamadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from IPython.display import IFrame\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "        ''' \n",
    "        Busca o arquivo .git e retorna string com a pasta raiz do repositório.\n",
    "        '''\n",
    "        # Prevenir recursão infinita limitando a profundidade\n",
    "        if depth < 0:\n",
    "            return None\n",
    "        path = Path(path).absolute()\n",
    "        if (path / '.git').is_dir():\n",
    "            return path\n",
    "        # Corrigido para usar LattesScraper.find_repo_root para chamada recursiva\n",
    "        return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "# Criar o grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar nós (dominios, processos e entidades)\n",
    "dominios = [\"Pesquisar\", \"Desenvolver\", \"Inovar\"]\n",
    "processos = [\"P001\", \"P002\", \"P003\", \"P004\", \"P005\", \"P006\", \"P007\", \"P008\", \"P009\"]\n",
    "entidades = {\n",
    "    \"P001\": [\"Dores\", \"Desejos\", \"Desafios\"],\n",
    "    \"P002\": [\"Temas\", \"Tópicos\", \"Assuntos\"],\n",
    "    \"P003\": [\"Atitudes\", \"Experiências\", \"Habilidades\"],\n",
    "    \"P004\": [\"Papeis\", \"Tempo\", \"Orçamentos\"],\n",
    "    \"P005\": [\"Projetos\", \"Processos\", \"Programas\"],\n",
    "    \"P006\": [\"Ensaios\", \"Equipamentos\", \"Ambientes\"],\n",
    "    \"P007\": [\"Aplicação\", \"Solução\", \"Produto-Serviço\"],\n",
    "    \"P008\": [\"Modelos\", \"Protótipos\", \"Empreendimentos\"],\n",
    "    \"P009\": [\"Indicadores\", \"Evidências\", \"Mensuração\"]\n",
    "}\n",
    "\n",
    "# Criar visualização dos nós de acordo com a estrutura de dados\n",
    "for macroprocesso in dominios:\n",
    "    G.add_node(macroprocesso, type=\"macroprocesso\")\n",
    "\n",
    "for processo in processos:\n",
    "    G.add_node(processo, type=\"processo\")\n",
    "\n",
    "for processo, entidades_list in entidades.items():\n",
    "    for entidade in entidades_list:\n",
    "        G.add_node(entidade, type=\"entidade\")\n",
    "\n",
    "# Adicionar arestas (relacionamentos)\n",
    "for macroprocesso in dominios:\n",
    "    for i in range(1, 4):\n",
    "        G.add_edge(macroprocesso, f\"P00{i + 3*(dominios.index(macroprocesso))}\")\n",
    "\n",
    "for processo, entidades_list in entidades.items():\n",
    "    for entidade in entidades_list:\n",
    "        G.add_edge(processo, entidade)\n",
    "\n",
    "\n",
    "\n",
    "# (Opcional) Adicionar relacionamentos entre entidades, para formar Demanda, Faturamento, Lucro, Reinvestimento... etc\n",
    "# G.add_edge(\"Dores\", \"Desejos\")\n",
    "\n",
    "\n",
    "\n",
    "# Calcular distâncias, definir cores, tamanhos e tamanhos de fonte\n",
    "node_distances = {}\n",
    "for macroprocesso in dominios:\n",
    "    for node, distance in nx.shortest_path_length(G, source=macroprocesso).items():\n",
    "        node_distances[node] = distance\n",
    "\n",
    "cores_base = {\"macroprocesso\": \"#007BFF\", \"processo\": \"#28A745\", \"entidade\": \"#FFC107\"}\n",
    "node_colors = {}\n",
    "node_sizes = {}\n",
    "node_font_sizes = {}  # Dicionário para armazenar os tamanhos de fonte\n",
    "for node in G.nodes():\n",
    "    node_type = G.nodes[node][\"type\"]\n",
    "    cor_base = cores_base[node_type]\n",
    "    alpha = max(0, 255 - 25 * node_distances[node])\n",
    "    node_colors[node] = f\"{cor_base}{alpha:02X}\"\n",
    "\n",
    "    # Definir tamanhos e tamanhos de fonte com base na distância\n",
    "    tamanho_base = {\"macroprocesso\": 50, \"processo\": 30, \"entidade\": 15}\n",
    "    font_size_base = {\"macroprocesso\": 42, \"processo\": 28, \"entidade\": 18}  # Tamanhos de fonte iniciais\n",
    "    node_sizes[node] = tamanho_base[node_type] - 5 * node_distances[node]\n",
    "    node_font_sizes[node] = font_size_base[node_type] - node_distances[node]  # Reduzir 1 pixel por passo\n",
    "\n",
    "# Configurar o PyVis (notebook=False para renderizar na célula)\n",
    "net = Network(notebook=False, width=\"100%\", height=\"1200px\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
    "net.barnes_hut()\n",
    "\n",
    "# Adicionar nós e arestas ao PyVis\n",
    "for node in G.nodes():\n",
    "    net.add_node(node, label=node, color=node_colors[node], title=G.nodes[node][\"type\"], \n",
    "                 size=node_sizes[node], font={\"size\": node_font_sizes[node], \"color\": \"black\"})  # Adicionar tamanho da fonte\n",
    "\n",
    "for edge in G.edges():\n",
    "    weight = 1\n",
    "    net.add_edge(*edge, value=weight)\n",
    "\n",
    "# Configurar o ForceAtlas2\n",
    "net.options.physics.solver = \"forceAtlas2Based\"\n",
    "net.options.physics.forceAtlas2Based = {\n",
    "    \"gravitationalConstant\": -50,\n",
    "    \"centralGravity\": 0.01,\n",
    "    \"springLength\": 100,\n",
    "    \"springConstant\": 0.08,\n",
    "    \"damping\": 0.4,\n",
    "    \"avoidOverlap\": 1\n",
    "}\n",
    "\n",
    "driver_path = None\n",
    "try:\n",
    "    # Caminho para o chromedriver no sistema local\n",
    "    if platform.system() == \"Windows\":\n",
    "        driver_path=find_repo_root()/'chromedriver'/'chromedriver.exe'\n",
    "    else:\n",
    "        driver_path=find_repo_root()/'chromedriver'/'chromedriver'\n",
    "except Exception as e:\n",
    "    print(\"Não foi possível estabelecer uma conexão, verifique o chromedriver\")\n",
    "    print(e)\n",
    "\n",
    "# print(driver_path)\n",
    "# service = Service(driver_path)\n",
    "# driver = webdriver.Chrome(service=service)\n",
    "# driver.get(\"grafo_interativo.html\")\n",
    "\n",
    "# Adicionar controles interativos (opcional)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "# Adicionar estilo inline para fundo branco\n",
    "net.html = net.html.replace(\"<body>\", '<body style=\"background-color: white;\">')\n",
    "\n",
    "# Renderizar na célula do Jupyter Notebook\n",
    "net.show(\"grafo_interativo.html\")  # Definir fundo branco ao salvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renderizando no Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renderizar na célula do Jupyter Notebook\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=\"grafo_interativo.html\", width=\"100%\", height=\"600px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Sempre construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(base_repo_dir, 'utils')\n",
    "folder_domain = os.path.join(base_repo_dir, 'source', 'domain')\n",
    "folder_data_input = os.path.join(base_repo_dir, '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(base_repo_dir, '_data', 'out_json')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "# from scraper_pasteur import PasteurScraper\n",
    "# from scraper_sucupira import SucupiraScraper\n",
    "# from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "from environment_setup import EnvironmentSetup\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from neo4j_persister import Neo4jPersister\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DictToHDF5, attribute_to_be_non_empty\n",
    "\n",
    "t00 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(folder_data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Carregar dados dos produtos prioritários, equipamentos e questões de pesquisa\n",
    "curr_pathfilename = os.path.join(folder_data_input,'docents_dict_list.json')\n",
    "with open(curr_pathfilename, 'r', encoding='utf-8') as f:\n",
    "    curriculos = json.load(f)\n",
    "\n",
    "prod_pathfilename = os.path.join(folder_data_output,'matriz_ceis.json')\n",
    "with open(prod_pathfilename, 'r', encoding='utf-8') as f:\n",
    "    matriz_produtos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(curriculos)} currículos carregados')\n",
    "# [x.get('Áreas') for x in curriculos]\n",
    "produtos = [produto.get('nome') for bloco in matriz_produtos.get('blocos', []) for produto in bloco.get('produtos', [])]\n",
    "print(f'{len(produtos)} produtos carregados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equipamentos = [...]\n",
    "questoes_pesquisa = [...]\n",
    "\n",
    "# Criar o grafo heterogêneo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar nós e arestas para pesquisadores\n",
    "for pesquisador in curriculos:\n",
    "    G.add_node(pesquisador['Identificação']['ID Lattes'], type=\"pesquisador\", **pesquisador)  # Adicionar atributos do currículo\n",
    "    \n",
    "    # Conectar pesquisador às suas áreas de atuação\n",
    "    for area in pesquisador['Áreas'].values():\n",
    "        G.add_node(area, type=\"area\")\n",
    "        G.add_edge(pesquisador['Identificação']['ID Lattes'], area)\n",
    "\n",
    "    # Conectar pesquisador às suas publicações\n",
    "    for publicacao in pesquisador['Produções']['Artigos completos publicados em periódicos']:\n",
    "        G.add_node(publicacao['titulo'], type=\"publicacao\")\n",
    "        G.add_edge(pesquisador['Identificação']['ID Lattes'], publicacao['titulo'])\n",
    "\n",
    "    # Conectar pesquisador a projetos\n",
    "    for pesquisador in curriculos:\n",
    "        for tipo_projeto in [\"ProjetosPesquisa\", \"ProjetosExtensão\", \"ProjetosDesenvolvimento\", \"ProjetosOutros\"]:\n",
    "            if tipo_projeto in pesquisador:\n",
    "                for projeto in pesquisador[tipo_projeto]:\n",
    "                    G.add_node(projeto['titulo_projeto'], type=\"projeto\")\n",
    "                    G.add_edge(pesquisador['Identificação']['ID Lattes'], projeto['titulo_projeto'])\n",
    "\n",
    "    # Conectar pesquisador a equipamentos\n",
    "    # (Assumindo que você tem uma lista de equipamentos mencionados nos currículos)\n",
    "    equipamentos_citados = []  # Preencha com os nomes dos equipamentos mencionados nos currículos\n",
    "    for pesquisador in curriculos:\n",
    "        for equipamento in equipamentos_citados:\n",
    "            if equipamento in pesquisador['Atuação Profissional'][0]['Descrição']:  # Exemplo: busca na descrição da atuação profissional\n",
    "                G.add_edge(pesquisador['Identificação']['ID Lattes'], equipamento)\n",
    "\n",
    "    # Conectar pesquisador a questões de pesquisa\n",
    "    # (Assumindo que você tem uma lista de questões de pesquisa e um método para associá-las aos pesquisadores)\n",
    "    for pesquisador in curriculos:\n",
    "        for questao in questoes_pesquisa:\n",
    "            if pesquisador_tem_interesse_na_questao(pesquisador, questao):  # Interesse por inferência\n",
    "                G.add_edge(pesquisador['Identificação']['ID Lattes'], questao['descricao'])\n",
    "\n",
    "\n",
    "    # Função hipotética para verificar o interesse do pesquisador em uma questão (você precisa implementar essa lógica)\n",
    "    def pesquisador_tem_interesse_na_questao(pesquisador, questao):\n",
    "        # Analise o currículo do pesquisador (áreas de atuação, publicações, projetos, etc.)\n",
    "        # e compare com a descrição da questão de pesquisa para determinar o interesse\n",
    "        # Retorne True se houver interesse, False caso contrário\n",
    "        pass  # Substitua por sua lógica de análise\n",
    "\n",
    "\n",
    "# Adicionar nós e arestas para produtos prioritários, equipamentos e questões de pesquisa\n",
    "for produto in produtos_prioritarios:\n",
    "    G.add_node(produto['nome'], type=\"produto\", **produto)  # Adicionar atributos do produto (nome, descrição, área, etc.)\n",
    "\n",
    "    # Conectar produto às suas áreas (assumindo que o produto tem uma lista de áreas)\n",
    "    for area in produto.get('areas', []):  # Usar get() para evitar KeyError se 'areas' não existir\n",
    "        G.add_edge(produto['nome'], area)\n",
    "\n",
    "# Adicionar nós e arestas para equipamentos\n",
    "for equipamento in equipamentos:\n",
    "    G.add_node(equipamento['nome'], type=\"equipamento\", **equipamento)  # Adicionar atributos do equipamento\n",
    "\n",
    "    # Conectar equipamento às suas áreas (assumindo que o equipamento tem uma lista de áreas)\n",
    "    for area in equipamento.get('areas', []):\n",
    "        G.add_edge(equipamento['nome'], area)\n",
    "\n",
    "# Adicionar nós e arestas para questões de pesquisa\n",
    "for questao in questoes_pesquisa:\n",
    "    G.add_node(questao['descricao'], type=\"questao_pesquisa\", **questao)  # Adicionar atributos da questão\n",
    "\n",
    "    # Conectar questão de pesquisa às suas áreas (assumindo que a questão tem uma lista de áreas)\n",
    "    for area in questao.get('areas', []):\n",
    "        G.add_edge(questao['descricao'], area)\n",
    "\n",
    "# Adicionar nós e arestas para equipamentos\n",
    "for equipamento in equipamentos:\n",
    "    G.add_node(equipamento['nome'], type=\"equipamento\", **equipamento)  # Adicionar atributos do equipamento\n",
    "\n",
    "    # Conectar equipamento às suas áreas (assumindo que o equipamento tem uma lista de áreas)\n",
    "    if 'areas' in equipamento:  # Verificar se o equipamento possui áreas associadas\n",
    "        for area in equipamento['areas']:\n",
    "            if area in G.nodes:  # Verificar se a área já existe no grafo\n",
    "                G.add_edge(equipamento['nome'], area)\n",
    "            else:\n",
    "                # Se a área não existir, você pode decidir se quer adicioná-la como um novo nó\n",
    "                # G.add_node(area, type=\"area\")  \n",
    "                # G.add_edge(equipamento['nome'], area)\n",
    "                print(f\"Área '{area}' não encontrada para o equipamento '{equipamento['nome']}'.\")\n",
    "    else:\n",
    "        print(f\"Equipamento '{equipamento['nome']}' não possui áreas associadas.\")\n",
    "\n",
    "# Adicionar nós e arestas para questões de pesquisa\n",
    "for questao in questoes_pesquisa:\n",
    "    G.add_node(questao['descricao'], type=\"questao_pesquisa\", **questao)  # Adicionar atributos da questão\n",
    "\n",
    "    # Conectar questão de pesquisa às suas áreas\n",
    "    if 'areas' in questao:  # Verificar se a questão possui áreas associadas\n",
    "        for area in questao['areas']:\n",
    "            if area in G.nodes:  # Verificar se a área já existe no grafo\n",
    "                G.add_edge(questao['descricao'], area)\n",
    "            else:\n",
    "                # Se a área não existir, você pode decidir se quer adicioná-la como um novo nó\n",
    "                # G.add_node(area, type=\"area\")  \n",
    "                # G.add_edge(questao['descricao'], area)\n",
    "                print(f\"Área '{area}' não encontrada para a questão de pesquisa '{questao['descricao']}'.\")\n",
    "    else:\n",
    "        print(f\"Questão de pesquisa '{questao['descricao']}' não possui áreas associadas.\")\n",
    "\n",
    "# Análise 1: Agrupamento de Pesquisadores\n",
    "# Extrair características textuais dos currículos (ex: usando TF-IDF)\n",
    "corpus = [pesquisador['Formação']['Acadêmica'][0]['Descrição'] for pesquisador in curriculos]  # Exemplo usando a descrição da formação acadêmica\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Calcular similaridade entre pesquisadores (ex: usando cosseno)\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "\n",
    "# Aplicar algoritmo de agrupamento (ex: DBSCAN)\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=0.3, min_samples=2).fit(similarity_matrix)\n",
    "labels = clustering.labels_\n",
    "\n",
    "# Adicionar atributo 'cluster' aos nós dos pesquisadores\n",
    "for i, pesquisador in enumerate(curriculos):\n",
    "    G.nodes[pesquisador['Identificação']['ID Lattes']]['cluster'] = labels[i]\n",
    "\n",
    "\n",
    "# Análise 2: Agrupamento de Questões de Pesquisa\n",
    "# Extrair características textuais das questões de pesquisa (ex: usando TF-IDF)\n",
    "corpus_questoes = [questao['descricao'] for questao in questoes_pesquisa]\n",
    "vectorizer_questoes = TfidfVectorizer()\n",
    "X_questoes = vectorizer_questoes.fit_transform(corpus_questoes)\n",
    "\n",
    "# Calcular similaridade entre questões (ex: usando cosseno)\n",
    "similarity_matrix_questoes = cosine_similarity(X_questoes)\n",
    "\n",
    "# Aplicar algoritmo de agrupamento (ex: K-Means)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 5  # Defina o número de clusters desejado\n",
    "clustering_questoes = KMeans(n_clusters=n_clusters).fit(similarity_matrix_questoes)\n",
    "labels_questoes = clustering_questoes.labels_\n",
    "\n",
    "# Adicionar atributo 'cluster' aos nós das questões de pesquisa\n",
    "for i, questao in enumerate(questoes_pesquisa):\n",
    "    G.nodes[questao['descricao']]['cluster'] = labels_questoes[i]\n",
    "\n",
    "# Análise 3: Recomendação de Projetos (exemplo simplificado)\n",
    "def recomendar_projetos(pesquisador_id):\n",
    "    pesquisador_areas = list(G.neighbors(pesquisador_id))  # Obter áreas do pesquisador\n",
    "    projetos_recomendados = []\n",
    "    for projeto_id in G.nodes:\n",
    "        if G.nodes[projeto_id]['type'] == \"projeto\":\n",
    "            projeto_areas = list(G.neighbors(projeto_id))\n",
    "            if set(pesquisador_areas) & set(projeto_areas):  # Verificar se há áreas em comum\n",
    "                projetos_recomendados.append(projeto_id)\n",
    "    return projetos_recomendados\n",
    "\n",
    "# Análise 4: Detecção de Oportunidades\n",
    "def detectar_oportunidades(G, produtos_prioritarios, top_n=5):\n",
    "    areas_importantes = {}\n",
    "    for produto in produtos_prioritarios:\n",
    "        for area in G.neighbors(produto['nome']):\n",
    "            areas_importantes[area] = areas_importantes.get(area, 0) + 1\n",
    "\n",
    "    # Ponderar pela concentração de pesquisadores e questões de pesquisa\n",
    "    for area in areas_importantes:\n",
    "        pesquisadores_na_area = len([n for n in G.neighbors(area) if G.nodes[n]['type'] == \"pesquisador\"])\n",
    "        questoes_na_area = len([n for n in G.neighbors(area) if G.nodes[n]['type'] == \"questao_pesquisa\"])\n",
    "        areas_importantes[area] *= (pesquisadores_na_area + questoes_na_area)\n",
    "\n",
    "    # Ordenar áreas por importância\n",
    "    areas_importantes = dict(sorted(areas_importantes.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return list(areas_importantes.keys())[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "print(stopwords.words('portuguese'))\n",
    "print(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade httpx\n",
    "# !pip install --upgrade httpcore\n",
    "# !pip install --upgrade googletrans==4.0.0-rc1\n",
    "# !pip install deep-translator\n",
    "\n",
    "import nltk\n",
    "print(nltk.data.find(\"corpora/stopwords\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_path = nltk.data.find(\"corpora/stopwords\")\n",
    "stopwords_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from deep_translator import GoogleTranslator  # Import the Googletrans library\n",
    "\n",
    "def identify_researcher_topics(data, fields, translate=False, target_language='en'):\n",
    "    \"\"\"\n",
    "    Identifies the top 3 research topics of a researcher based on their Lattes CV data.\n",
    "\n",
    "    Args:\n",
    "        data: A dictionary containing the researcher's Lattes CV data.\n",
    "        fields: A list of field names to be used for topic identification.\n",
    "\n",
    "    Returns:\n",
    "        A list of the top 3 research topics.\n",
    "    \"\"\"\n",
    "\n",
    "    def translate_text(text, target_language):\n",
    "        \"\"\"\n",
    "        Translates the text to the target language using Google Translate.\n",
    "        \"\"\"\n",
    "        translated = GoogleTranslator(source='auto', target=target_language).translate(text)\n",
    "        return translated\n",
    "\n",
    "    def extract_text_from_fields(data, fields, corpus):\n",
    "        \"\"\"\n",
    "        Recursively extracts text from the specified fields in the data dictionary.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if key in fields:\n",
    "                    if isinstance(value, list):\n",
    "                        for item in value:\n",
    "                            corpus.append(item.get(\"Descricao\", \"\") + item.get(\"titulo\", \"\"))\n",
    "                    else:\n",
    "                        corpus.append(str(value))  # Convert non-string values to string\n",
    "                else:\n",
    "                    extract_text_from_fields(value, fields, corpus)\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                extract_text_from_fields(item, fields, corpus)\n",
    "\n",
    "    # def preprocess_text(text, target_language='en'):\n",
    "    #     \"\"\"\n",
    "    #     Preprocesses the text by tokenizing, removing stopwords and proper nouns, translating, and removing punctuation.\n",
    "    #     \"\"\"\n",
    "    #     # Tokenize the text\n",
    "    #     words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    #     # Remove stopwords in the original language (if available)\n",
    "    #     source_language = detect(text)\n",
    "    #     try:\n",
    "    #         words = [word for word in words if word not in stopwords.words(source_language) and not word.istitle()]\n",
    "    #     except OSError:\n",
    "    #         print(f\"Warning: Stopwords not found for language '{source_language}'. Skipping stopword removal.\")\n",
    "\n",
    "    #     # Translate the text (if it's not already in the target language)\n",
    "    #     if target_language != 'auto' and source_language != target_language:\n",
    "    #         translator = GoogleTranslator(source=source_language, target=target_language)\n",
    "    #         text = translator.translate(text)\n",
    "\n",
    "    #         # Tokenize the translated text\n",
    "    #         words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    #         # Remove stopwords in the target language\n",
    "    #         words = [word for word in words if word not in stopwords.words(target_language)]\n",
    "\n",
    "    #     # Remove punctuation and numbers\n",
    "    #     words = [re.sub(r'[^\\w\\s]', '', word) for word in words if not word.isdigit()]\n",
    "\n",
    "    #     return \" \".join(words)\n",
    "\n",
    "    def preprocess_text(text, target_language='en'):\n",
    "        \"\"\"\n",
    "        Preprocesses the text by tokenizing, removing stopwords and proper nouns, translating, and removing punctuation.\n",
    "        \"\"\"\n",
    "        # Tokenize the text\n",
    "        words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "        # Remove stopwords in the original language (if available)\n",
    "        source_language = detect(text)\n",
    "        stopwords_path = nltk.data.find(\"corpora/stopwords\")\n",
    "        # stopwords_path = os.path.join(os.getenv('APPDATA'), 'Roaming', 'nltk_data', 'corpora', 'stopwords')  # Get stopwords path\n",
    "        if os.path.exists(os.path.join(stopwords_path, source_language)):\n",
    "            with open(os.path.join(stopwords_path, source_language), 'r', encoding='utf-8') as f:\n",
    "                stop_words = set(f.read().splitlines())\n",
    "            words = [word for word in words if word not in stop_words and not word.istitle()]\n",
    "\n",
    "        # Translate the text (if it's not already in the target language)\n",
    "        if target_language != 'auto' and source_language != target_language:\n",
    "            translator = GoogleTranslator(source=source_language, target=target_language)\n",
    "            text = translator.translate(text)\n",
    "\n",
    "            # Tokenize the translated text\n",
    "            words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "            # Remove stopwords in the target language\n",
    "            with open(os.path.join(stopwords_path, target_language), 'r', encoding='utf-8') as f:\n",
    "                stop_words = set(f.read().splitlines())\n",
    "            words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Remove punctuation and numbers\n",
    "        words = [re.sub(r'[^\\w\\s]', '', word) for word in words if not word.isdigit()]\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "    # Concatenate text from specified fields\n",
    "    corpus = []\n",
    "    extract_text_from_fields(data, fields, corpus)\n",
    "\n",
    "    # Preprocess the corpus\n",
    "    corpus = [preprocess_text(text) for text in corpus]\n",
    "\n",
    "    # Vectorize the text using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Apply LDA for topic modeling\n",
    "    lda = LatentDirichletAllocation(n_components=7, random_state=0)\n",
    "    lda.fit(X)\n",
    "\n",
    "    # Get the top words for each topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words.append([feature_names[i] for i in topic.argsort()[:-4:-1]])\n",
    "\n",
    "    return top_words\n",
    "\n",
    "pathfilename = os.path.join(folder_data_input,'docents_dict_list.json')\n",
    "with open(pathfilename, 'r', encoding='utf-8') as file:\n",
    "    docents_data = json.load(file)\n",
    "    print(f'{len(docents_data)} currículos carregados')\n",
    "\n",
    "# fields_to_use = [\"Formação Acadêmica\", \"Atuação Profissional\", \"Produções\"]\n",
    "fields_to_use = [\"titulo\"]\n",
    "\n",
    "for researcher in docents_data:\n",
    "    topics = identify_researcher_topics(researcher, fields_to_use)\n",
    "    print(f\"Principais tópicos de interesse para {researcher['Identificação']['Nome']}:\")\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"  Tópico {i+1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "oportunidades = detectar_oportunidades(G, produtos_prioritarios)\n",
    "print(\"Áreas de oportunidade:\", oportunidades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar classes do pacote\n",
    "from research_process_automation import QuestionFormulation, InteractiveFeedback, QuestionCriteriaEvaluator\n",
    "\n",
    "# Criar instância da classe QuestionFormulation\n",
    "question_formulator = QuestionFormulation()\n",
    "\n",
    "# Solicitar informações do usuário\n",
    "question_formulator.input_ideas()\n",
    "\n",
    "# Gerar a pergunta de pesquisa\n",
    "research_question = question_formulator.generate_question()\n",
    "print(\"PASSO 01: Questão de pesquisa\")\n",
    "print(research_question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entradas de referência:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entradas de citações para referências:\n",
    "\n",
    "@book{gil2022comoelaborarprojetosdepesquisa,\n",
    "  title={Como Elaborar Projetos de Pesquisa},\n",
    "  author={Gil, Antonio Carlos},\n",
    "  edition={7},\n",
    "  publisher={Atlas},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "@book{popper1934logic,\n",
    "  title={A Lógica da Pesquisa Científica},\n",
    "  author={Popper, Karl},\n",
    "  year={1934}\n",
    "}\n",
    "\n",
    "@misc{IEP2023Popper,\n",
    "  author = {Internet Encyclopedia of Philosophy},\n",
    "  title = {Karl Popper: Political Philosophy},\n",
    "  year = {2023},\n",
    "  howpublished = {\\url{https://iep.utm.edu/popp-pol/}},\n",
    "  note = {Acesso em: 01/01/2024}\n",
    "}\n",
    "\n",
    "@phdthesis{Broderick1984,\n",
    "  author = {David Gregory Broderick},\n",
    "  title = {Objectivity: Thomas Aquinas and Karl Popper},\n",
    "  school = {Boston College},\n",
    "  year = {1984}\n",
    "}\n",
    "\n",
    "@article{CRYFUNavara2023,\n",
    "  author = {Grupo Ciencia, Razón y Fe (CRYF)},\n",
    "  title = {The Ethical Roots of Karl Popper's Epistemology},\n",
    "  journal = {Universidad de Navarra},\n",
    "  year = {2023},\n",
    "  url = {https://www.unav.edu/web/ciencia-razon-y-fe/poppers-epistemology}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bases de Dados:\n",
    "\n",
    "@misc{arxiv,\n",
    "  title = {{arXiv}},\n",
    "  url = {https://arxiv.org}\n",
    "}\n",
    "\n",
    "@misc{core,\n",
    "  title = {{CORE}},\n",
    "  url = {https://core.ac.uk}\n",
    "}\n",
    "\n",
    "@misc{doaj,\n",
    "  title = {{Directory of Open Access Journals (DOAJ)}},\n",
    "  url = {https://doaj.org}\n",
    "}\n",
    "\n",
    "@misc{googlescholar,\n",
    "  title = {{Google Scholar}},\n",
    "  url = {https://scholar.google.com}\n",
    "}\n",
    "\n",
    "@misc{openaire,\n",
    "  title = {{OpenAIRE}},\n",
    "  url = {https://www.openaire.eu}\n",
    "}\n",
    "\n",
    "@misc{pubmedcentral,\n",
    "  title = {{PubMed Central}},\n",
    "  url = {https://www.ncbi.nlm.nih.gov/pmc/}\n",
    "}\n",
    "\n",
    "@misc{ssrn,\n",
    "  title = {{Social Science Research Network (SSRN)}},\n",
    "  url = {https://www.ssrn.com}\n",
    "}\n",
    "\n",
    "@misc{scienceopen,\n",
    "  title = {{ScienceOpen}},\n",
    "  url = {https://www.scienceopen.com}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editoras com políticas de OA:\n",
    "\n",
    "@misc{springernature,\n",
    "  title = {{Springer Nature}},\n",
    "  url = {https://www.springernature.com}\n",
    "}\n",
    "\n",
    "@misc{oup,\n",
    "  title = {{Oxford University Press (OUP)}},\n",
    "  url = {https://academic.oup.com}\n",
    "}\n",
    "\n",
    "@misc{frontiers,\n",
    "  title = {{Frontiers}},\n",
    "  url = {https://www.frontiersin.org}\n",
    "}\n",
    "\n",
    "@misc{wiley,\n",
    "  title = {{Wiley}},\n",
    "  url = {https://www.wiley.com}\n",
    "}\n",
    "\n",
    "@misc{plos,\n",
    "  title = {{Public Library of Science (PLOS)}},\n",
    "  url = {https://www.plos.org}\n",
    "}\n",
    "\n",
    "@misc{hindawi,\n",
    "  title = {{Hindawi}},\n",
    "  url = {https://www.hindawi.com}\n",
    "}\n",
    "\n",
    "@misc{mdpi,\n",
    "  title = {{MDPI AG}},\n",
    "  url = {https://www.mdpi.com}\n",
    "}\n",
    "\n",
    "@misc{informa,\n",
    "  title = {{Informa PLC}},\n",
    "  url = {https://www.informa.com}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GII\n",
    "@misc{GII-WIPO,\n",
    "      title = {Global Innovation Index - WIPO Series},\n",
    "      abstract = {The Global Innovation Index (GII) ranks the innovation performance of some 131 countries and economies around the world, based on 80+ indicators. Co-published by WIPO, Cornell University and INSEAD, the report provides an annual ranking of the innovation capabilities and performance of economies around the world.},\n",
    "      {url = https://www.wipo.int/publications/en/series/index.jsp?id=129}\n",
    "}\n",
    "\n",
    "@misc{GII-2011,\n",
    "  title = {Global Innovation Index 2011 - Accelerating Growth and Development},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=274&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2012,\n",
    "  title = {Global Innovation Index 2012 - Stronger Innovation Linkages for Global Growth},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=247&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2013,\n",
    "  title = {Global Innovation Index 2013 - The Local Dynamics of Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=368&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2014,\n",
    "  title = {Global Innovation Index 2014 - The Human Factor in Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=3254&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2015,\n",
    "  title = {Global Innovation Index 2015 - Effective Innovation Policies for Development},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=3978&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2016,\n",
    "  title = {Global Innovation Index 2016 - Winning with Global Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4064&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2017,\n",
    "  title = {Global Innovation Index 2017 - Innovation Feeding the World},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4193&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2018,\n",
    "  title = {Global Innovation Index 2018 - Energizing the World with Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4330&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2019,\n",
    "  title = {Global Innovation Index 2019 - Creating Healthy Lives — The Future of Medical Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4434&plang=EN}\n",
    "}\n",
    "\n",
    "@article{40579,\n",
    "      author = {Cornell University.},\n",
    "      url = {http://tind.wipo.int/record/40579},\n",
    "      title = {Global Innovation Index 2019 - Executive version.},\n",
    "      abstract = {The Global Innovation Index 2019 provides detailed metrics  about the innovation performance of 129 countries and  economies around the world. Its 80 indicators explore a  broad vision of innovation, including political  environment, education, infrastructure and business  sophistication. The GII 2019 analyzes the medical  innovation landscape of the next decade, looking at how  technological and non-technological medical innovation will  transform the delivery of healthcare worldwide. It also  explores the role and dynamics of medical innovation as it  shapes the future of healthcare, and the potential  influence this may have on economic growth. Chapters of the  report provide more details on this year’s theme from  academic, business, and particular country perspectives  from leading experts and decision makers.},\n",
    "      doi = {https://doi.org/10.34667/tind.40579},\n",
    "      recid = {40579},\n",
    "      pages = {214 pages ;},\n",
    "}\n",
    "\n",
    "@article{35279,\n",
    "      url = {http://tind.wipo.int/record/35279},\n",
    "      title = {Índice Global de inovação de 2019 - PRINCIPAIS  RESULTADOS.},\n",
    "      abstract = {Criar Vidas Sadias - O Futuro da Inovação Médica.},\n",
    "      doi = {https://doi.org/10.34667/tind.35279},\n",
    "      recid = {35279},\n",
    "      pages = {20 pages ;},\n",
    "}\n",
    "\n",
    "@misc{GII-2020,\n",
    "  title = {Global Innovation Index 2020 - Who Will Finance Innovation?},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4514&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2021,\n",
    "  title = {Global Innovation Index 2021 - Tracking Innovation through the COVID-19 Crisis},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4560&plang=EN}\n",
    "}\n",
    "\n",
    "@article{46620,\n",
    "      url = {http://tind.wipo.int/record/46620},\n",
    "      title = {Índice Global de Inovação 2022 : Resumo executivo.},\n",
    "      abstract = {O Índice Global da Inovação 2022 (IGI) analisa as  tendências globais no campo da inovação em um cenário  marcado pela pandemia de COVID-19 em curso, por um  crescimento desacelerado da produtividade e pelo surgimento  de novos desafios. O IGI revela as economias mais  inovadoras do mundo, classificando o desempenho em  inovação de 132 economias, destacando seus pontos fortes  e fracos em matéria de inovação e identificando lacunas  em suas métricas de inovação. Esta edição de 2022 tem  como foco o efeito da inovação sobre a produtividade e o  bem-estar da sociedade ao longo das próximas décadas.},\n",
    "      doi = {https://doi.org/10.34667/tind.46620},\n",
    "      recid = {46620},\n",
    "      pages = {28 pages :},\n",
    "}\n",
    "\n",
    "@misc{GII-2022,\n",
    "  title = {Global Innovation Index 2022 - What is the future of innovation driven growth?},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4622&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2023,\n",
    "  title = {Global Innovation Index 2023},\n",
    "  url = {https://www.globalinnovationindex.org/gii-2023}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
