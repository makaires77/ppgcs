{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "video_url = \"https://developer.download.nvidia.com/video/HPC/HPC-SDK_KV_1145x220_v005.mp4\"\n",
    "# Criar a tag de vídeo HTML com o atributo loop\n",
    "video_tag = f'<video width=\"100%\" controls loop> <source src=\"{video_url}\" type=\"video/mp4\"> </video>'\n",
    "HTML(video_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Contexto e definição do fluxo da pesquisa</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filosofia e ciência da economia baseada em inovação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grandes filósofos durante toda história tem se oposto ao totalitarismo. Pelo menos a partir da visão Aristotélica as formas degeneradas de governar já são apontadas. Dentre as chamadas formas degeneradas, a tirania pode ser entendida como a pior forma de sistema político, sendo o resultado da maior degeneração do poder centralizado em uma única pessoa (como em uma monarquia absolutista). Na tirania o tirano governa sem consultar a ninguém e, geralmente, sua tomada de poder é ilegítima, e a lei não tem papel algum. Na oligarquia, onde o poder de decisão do governo é constituído por um pequeno número de privilegiados, há uma subclassificação quanto ao número dos donos do poder, como por exemplo, na Politirania onde os oligarcas governam hereditariamente e na riqueza, mas respeitando mais a lei. Ou nas oligarquias com uma maior percentagem de oligarcas, quando passa-se da hereditariedade à nomeação dos amigos do governo, independentemente da linhagem sanguínea destes, dentre outros tipos de oligarquia. \n",
    "\n",
    "Tomas de Aquino foi um filósofo de referência no papel da moral como orientação maior para a sociedade. Mais recentemente, Karl Popper foi um severo crítico ao totalitarismo e utopias demagógicas. São perceptíveis interseções e diálogos filosóficos entre as ideias centrais dessas filosofias, especialmente em epistemologia e ética, têm sido tema de discussão acadêmica. Por exemplo, na dissertação de David Gregory Broderick, intitulada \"Objetividade: Tomás de Aquino e Karl Popper\", explora-se a relação entre as noções de objetividade nas obras de Aquino e Popper. A dissertação sugere que, embora haja diferenças terminológicas, históricas e de interesses entre Aquino e Popper, também existem paralelos significativos em suas abordagens sobre objetividade e o crescimento do conhecimento. Além disso, na discussão sobre as raízes éticas da epistemologia de Popper, explorada pelo Grupo Ciencia, Razón y Fe (CRYF) da Universidad de Navarra, destaca-se a complementaridade das posições de Aquino e Popper. Esta análise sugere que a forte defesa de Popper do realismo, da verdade objetiva e da metodologia para o crescimento do conhecimento conjectural pode ser vista como complementar, ou até mesmo fundamentada, nos princípios éticos e metafísicos delineados por Aquino.\n",
    "\n",
    "Entendemos portanto, que embora Popper possa não ter citado explicitamente Aquino em suas principais obras, as bases filosóficas de seus pensamentos, especialmente em relação à objetividade, ética e crescimento do conhecimento, têm grandes paralelos e áreas de convergência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, tal como acontece com a sua defesa das eleições numa democracia, o argumento de Popper a favor da engenharia social fragmentada baseia-se principalmente na sua compatibilidade com o método de tentativa e erro das ciências naturais: uma teoria é proposta e testada, erros na teoria são detectados e eliminado, e uma teoria nova e melhorada emerge, reiniciando o ciclo. Através da engenharia gradual, o processo de progresso social é, portanto, paralelo ao progresso científico. Na verdade, Popper diz que a engenharia social fragmentada é a única abordagem à política pública que pode ser genuinamente científica: \"Isto - e nenhum planeamento utópico ou profecia histórica - significaria a introdução do método científico na política, uma vez que todo o segredo do método científico é uma disposição para aprender com os erros\" ( Open Society Vol 1., 163)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papel da Inovação Tecnológica nas economias modernas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O termo \"inovação tecnológica\", no sentido moderno, começou a ser mais amplamente discutido e compreendido no início do século XX. Joseph Schumpeter, um economista austríaco, foi uma das figuras-chave que destacou a inovação tecnológica como um motor para o crescimento econômico. Após a Primeira Guerra Mundial, pensadores como Thorstein Veblen e Herbert Hoover também enfatizaram a importância da inovação tecnológica para a segurança nacional e competitividade industrial. Esse conceito se expandiu particularmente após a Segunda Guerra Mundial, quando a inovação tecnológica começou a ser vista como crucial para a prosperidade industrial e a segurança militar, especialmente nos Estados Unidos. \n",
    "\n",
    "fonte: Encyclopedia.com sobre Inovação Tecnológica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abordagem falseável em Metodologia da Pesquisa Científica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma contribuição seminal para abordagem falseável da ciência é a descrita por Karl Popper em sua obra \"A Lógica da Pesquisa Científica\", publicada originalmente em 1934. É nesta obra que Popper argumenta que a ciência deve adotar uma metodologia baseada na falseabilidade. Segundo ele, nenhuma quantidade de experimentos pode provar uma teoria, mas um único experimento ou observação reproduzível pode refutá-la. Esta abordagem destaca a importância da capacidade de uma teoria ser testada e potencialmente falsificada, em vez de apenas verificada. Popper diferencia as teorias científicas das pseudociências e da metafísica, salientando que as teorias científicas devem ser testáveis e passíveis de refutação, enquanto pseudociências e metafísicas não permitem essa possibilidade. Popper enfatiza que a ciência deve estar em constante evolução, com as hipóteses sendo submetidas a testes contínuos para acompanhar o desenvolvimento da ciência e das tecnologias (POPPER, 1934)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estruturar etapas da pesquisa para a tese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 01: Redigir uma boa questão de pesquisa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como chegar à uma boa questão de pesquisa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gênese de uma boa questão de pesquisa também é abordada por outros autores notáveis no campo da metodologia científica. Antonio Carlos Gil descreve que a elaboração de projetos de pesquisa deve ser guiada pela apresentação clara e acessível dos elementos necessários para a pesquisa, além da organização de conhecimentos dispersos. Gil enfatiza a natureza prática da pesquisa, abordando a importância de esclarecer os procedimentos para a elaboração de projetos em diversos tipos de pesquisa. (GIL, 2022).\n",
    "\n",
    "Alguns dos critérios essenciais para avaliar a qualidade de uma questão de pesquisa, e como integrar ou considerar cada um deles no processo de formulação:\n",
    "\n",
    "<b>Clareza e Especificidade</b>: Uma boa questão de pesquisa deve ser clara e específica, evitando ambiguidades. Isto pode ser parcialmente garantido pelo processamento de linguagem natural, mas também requer revisão humana para garantir que a questão seja compreensível e precisamente focada.\n",
    "\n",
    "<b>Relevância Acadêmica ou Científica</b>: A questão deve ser relevante para o campo de estudo e contribuir de alguma forma para o conhecimento existente. Isso geralmente requer uma compreensão do contexto acadêmico e das pesquisas atuais, o que pode ser além do escopo da automação completa.\n",
    "\n",
    "<b>Viabilidade</b>: A pergunta deve ser algo que pode ser realisticamente respondido através de métodos de pesquisa disponíveis. Este aspecto pode ser parcialmente verificado por meio de regras heurísticas programadas, mas frequentemente requer avaliação humana, especialmente para julgar a disponibilidade de dados ou recursos de pesquisa.\n",
    "\n",
    "<b>Originalidade</b>: Uma boa questão de pesquisa deve oferecer novas perspectivas ou abordar lacunas existentes na literatura. A originalidade pode ser difícil de avaliar automaticamente, mas técnicas avançadas de NLP, como análise semântica e comparação com bancos de dados de literatura existente, podem ajudar.\n",
    "\n",
    "<b>Importância Prática ou Teórica</b>: A questão deve ter alguma importância prática ou contribuir para a compreensão teórica de um tópico. Isso geralmente exige conhecimento especializado na área de estudo para avaliar.\n",
    "\n",
    "<b>Estruturação Adequada</b>: A pergunta deve ser estruturada de forma a facilitar uma abordagem de pesquisa clara. Isso inclui a utilização de uma formulação que se alinhe com métodos de pesquisa qualitativos ou quantitativos, conforme apropriado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como Integrar critérios de validação para responder a questão de pesquisa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerar uma boa questão de pesquisa depende de uma combinação de processamento automatizado, conhecimento especializado e revisão humana. A automação pode fornecer uma base útil, mas a supervisão e o julgamento humanos são cruciais para garantir a qualidade final da pergunta de pesquisa. As seguintes estratégias são utilizadas em nossa solução para chegar a uma boa questão de pesquisa:\n",
    "\n",
    "\n",
    "<b>Integração de várias Fontes de Dados de Literatura</b>: Integrar um banco de dados de literatura existente pode ajudar a avaliar a originalidade e a relevância da pergunta, comparando-a com pesquisas já publicadas.\n",
    "\n",
    "<b>Automatização com Revisão Humana</b>: Uma abordagem prática é usar a automação para gerar uma primeira versão da pergunta, que é então revisada e refinada por pesquisadores humanos (pesquisador principal, equipe de pesquisa e orientador). A automação garante que certos critérios básicos sejam atendidos (como clareza e estruturação), enquanto a revisão humana aborda aspectos mais sutis, como relevância, viabilidade e originalidade.\n",
    "\n",
    "<b>Instrução ao Usuário</b>: Fornecer orientações e exemplos de boas perguntas de pesquisa aos usuários pode ajudá-los a formular ideias iniciais mais eficazes, levando a melhores resultados na formulação automática.\n",
    "\n",
    "<b>Feedback Interativo</b>: Incorporar um sistema de feedback no processo, onde o usuário pode refinar suas ideias iniciais ou ajustar a pergunta gerada, pode ajudar a melhorar a qualidade da questão de pesquisa final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 02: Encontrar fontes de dados adequadas para realizar a pesquisa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editoras científicas com políticas de Open Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Várias editoras científicas proeminentes oferecem conteúdos em Open Access, proporcionando acesso livre a uma vasta gama de pesquisas acadêmicas. Algumas das principais editoras incluem:\n",
    "\n",
    "<b>Public Library of Science (PLOS)</b>: Fundada como uma organização sem fins lucrativos, a PLOS tem como objetivo catalisar o movimento de acesso aberto. Publica periódicos em diversas áreas, incluindo medicina e ciências da vida.\n",
    "\n",
    "<b>Springer Nature</b>: Com mais de 600 periódicos totalmente em acesso aberto e mais de 1000 livros em acesso aberto, a Springer Nature é uma das pioneiras no campo da pesquisa aberta. Oferece uma variedade de opções de publicação em acesso aberto, mantendo rigorosos processos de revisão por pares e editoriais.\n",
    "\n",
    "<b>Wiley</b>: A Wiley oferece mais de 150 periódicos revisados por pares em acesso aberto, abrangendo diversas disciplinas de pesquisa. Seus periódicos em acesso aberto estão disponíveis para leitura, download e compartilhamento gratuitamente através da Wiley Online Library e do PubMed Central.\n",
    "\n",
    "<b>Oxford University Press (OUP)</b>: A OUP publica mais de 120 periódicos totalmente em acesso aberto e mais de 250 livros em acesso aberto, abrangendo uma ampla gama de disciplinas. Muitos de seus periódicos são classificados como \"diamond OA\", o que significa que não há taxas de processamento de artigos para autores ou leitores.\n",
    "\n",
    "<b>Frontiers</b>: Reconhecida como uma editora líder em Acesso Aberto e plataforma de Ciência Aberta, a Frontiers é muito citada, com mais de um bilhão de visualizações e downloads e 1.6 milhão de citações em seus artigos acessíveis gratuitamente. Ela é ativa em áreas como neurociências, psiquiatria, fisiologia, medicina clínica, ciências naturais e engenharia.\n",
    "\n",
    "<b>MDPI AG</b>: Uma empresa suíça com presença global, a MDPI AG publica periódicos em ciência e engenharia, ciências sociais e filosofia.\n",
    "\n",
    "<b>Informa PLC</b>: Uma das maiores editoras nas humanidades e ciências sociais, a Informa publica em acesso aberto sob quatro selos: Taylor & Francis Open, Dove Medical Press, Cogent OA e Routledge Open.\n",
    "\n",
    "<b>Hindawi</b>: Inicialmente uma editora de periódicos por assinatura, a Hindawi fez a transição para um modelo de publicação em acesso aberto entre 2004 e 2007. Ela publica periódicos em ciência e engenharia, ciências sociais e filosofia.\n",
    "\n",
    "Estas editoras são conhecidas não apenas pela qualidade e diversidade de suas publicações, mas também por suas contribuições significativas para o movimento de acesso aberto na comunidade acadêmica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bases de dados digitais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao focar a pesquisa em fontes que oferecem conteúdos completos em Open Access, há bases de dados e repositórios acadêmicos importantes que se destacam em relevância e quantidade de conteúdos:\n",
    "\n",
    "<b>Directory of Open Access Journals (DOAJ)</b>: O DOAJ é um diretório online que indexa e fornece acesso a periódicos de alta qualidade, todos de acesso livre e revisados por pares. É uma excelente fonte para pesquisar artigos em uma ampla gama de disciplinas. O DOAJ oferece uma API para acessar seu índice de periódicos e artigos. A documentação e detalhes sobre a API estão disponíveis em DOAJ API.\n",
    "\n",
    "<b>PubMed Central</b>: Operado pela Biblioteca Nacional de Medicina dos EUA, o PubMed Central é um repositório gratuito de artigos de ciências biomédicas e ciências da vida. Embora seu foco seja mais na área da saúde, ele pode ter artigos relevantes sobre inovação tecnológica no contexto da saúde. A PubMed Central oferece uma API chamada Entrez Programming Utilities (E-utilities) para interagir com a base de dados. Mais informações podem ser encontradas em Entrez Programming Utilities Help.\n",
    "\n",
    "<b>arXiv</b>: O arXiv é um repositório de preprints em campos como física, matemática, ciência da computação, biologia quantitativa, finanças quantitativas e estatística. É uma boa fonte para literatura mais técnica e teórica sobre inovação tecnológica. O arXiv fornece uma API para acesso aos seus preprints. Informações detalhadas e documentação sobre a API estão disponíveis em arXiv API.\n",
    "\n",
    "<b>OpenAIRE</b>: Uma infraestrutura que promove a descoberta e o acesso a publicações científicas europeias de acesso livre. É especialmente útil para pesquisas que envolvem colaborações europeias ou focam em políticas e práticas de inovação na Europa. OpenAIRE oferece uma API para acessar seu repositório. Você pode encontrar mais informações sobre como usar esta API em OpenAIRE API.\n",
    "\n",
    "<b>Google Scholar</b>: Apesar de não ser exclusivamente dedicado ao Open Access, o Google Scholar pode ser utilizado para localizar artigos de acesso livre. Ele indexa uma variedade de fontes acadêmicas e muitas vezes inclui links para versões de acesso livre dos artigos. O Google Scholar não oferece uma API oficial para acesso programático.\n",
    "\n",
    "<b>CORE</b>: Agregador que permite o acesso a milhões de artigos de acesso livre. Ele reúne conteúdo de repositórios e periódicos de todo o mundo, sendo uma excelente ferramenta para uma pesquisa abrangente. CORE oferece uma API que permite acessar seu vasto repositório de artigos de acesso livre. A documentação da API pode ser encontrada em CORE API.\n",
    "\n",
    "<b>ScienceOpen</b>: Plataforma de pesquisa e publicação que oferece acesso a mais de 60 milhões de artigos e registros de pesquisa em todas as áreas. Não há informações disponíveis sobre uma API pública para o ScienceOpen.\n",
    "\n",
    "<b>SSRN (Social Science Research Network)</b>: Especializado em ciências sociais, o SSRN é um repositório de preprints que abrange uma ampla gama de áreas, incluindo economia, direito e gestão corporativa, onde você pode encontrar trabalhos relacionados à gestão da inovação. O SSRN não fornece uma API pública para acesso programático aos seus conteúdos.\n",
    "\n",
    "\n",
    "O tipo de busca mais comum é por palavras-chave, usadas para descobrir e para refinar a pesquisa e localizar artigos relevantes sobre os temas de interesse, no nosso caso, a gestão da inovação tecnológica em organizações públicas e privadas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 03: Modelar o problema de pesquisa em grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagrama estratégias para modelar o mundo real em grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "  mindmap\n",
    "  root((Modelagem do Mundo Real em Grafos para Aplicações em Saúde))\n",
    "    Grafos Simples\n",
    "      Aplicações\n",
    "        Interações Medicamento-Medicamento (Efeitos Adversos)\n",
    "        Co-ocorrência de Doenças (Comorbidades)\n",
    "      Limitações\n",
    "        Não modela a direção ou força das relações\n",
    "    Grafos Direcionados\n",
    "      Aplicações\n",
    "        Propagação de Doenças Infecciosas (Transmissão)\n",
    "        Vias Metabólicas (Reações Bioquímicas)\n",
    "      Limitações\n",
    "        Não modela múltiplas relações entre os mesmos elementos\n",
    "    Grafos Ponderados\n",
    "      Aplicações\n",
    "        Redes de Interação Proteína-Proteína (Afinidade de Ligação)\n",
    "        Redes de Coexpressão Gênica (Correlação)\n",
    "      Limitações\n",
    "        Não modela diferentes tipos de relações simultaneamente\n",
    "    Grafos Multicamadas (Multiplex)\n",
    "      Aplicações\n",
    "        Redes de Interação Fármaco-Alvo-Doença (Múltiplos Mecanismos)\n",
    "        Redes de Microbioma Humano (Diferentes Nichos Tecnológicos)\n",
    "      Limitações\n",
    "        Aumenta a complexidade da análise\n",
    "    Hipergrafos\n",
    "      Aplicações\n",
    "        Análise de Dados de Saúde Multimodais (Prontuários, Imagens, Genética)\n",
    "        Modelagem de Fatores de Risco Complexos para Doenças Multifatoriais\n",
    "      Limitações\n",
    "        Dificuldade de visualização e análise\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagrama desafio de promover inovação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "mindmap\n",
    "  root((Inovação, Competitividade e Novo Padrão Tecnológico da Indústria Brasileira))\n",
    "    (Conclusões)\n",
    "      Baixa intensidade de PDI como desafio para a competitividade\n",
    "      Necessidade de investimentos em PDI para aumentar a produtividade\n",
    "      Importância de políticas públicas de incentivo à inovação\n",
    "\n",
    "      (Implicações)\n",
    "          Foco em setores estratégicos com potencial de crescimento e inovação\n",
    "          Aumento do investimento em PDI\n",
    "          Políticas públicas que incentivem a inovação\n",
    "\n",
    "          (Recomendações)\n",
    "              Aumento do investimento público e privado em PDI\n",
    "              Incentivos fiscais para empresas que investem em inovação\n",
    "              Fortalecimento da cooperação entre universidades e empresas\n",
    "              Investimento em educação e formação de mão de obra qualificada\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "[![](https://mermaid.ink/img/pako:eNp9VM2O0zAQfpVRDqiVygvkxrJC6oFVBRxzmdqT7ojYk_VPBVrtuyA4IA6cEJe95sUYN_2L2iWKFHsy-f7s-LEyYqmqK8feOuwbDxBE0my29LLF4efwQxbwVlxPiRNv2aIlILiTrcAKbdD38ImMl274s2EjYBGW3g7PMQVGuAkYuSMOOJ8XbFAsb7och78UxwJAXeuHftYitPh6LfJ5fngDcIP8BYF9Ih9Hcr1Xr25VAx8VghGnzBSxZYEeAwKW2rnoE-QdGYonMPZbiokd-SQRyF2ij4C5dGBQ5D6IzZe4S9dLSMN3b9S4Aveaye_EBiP0w_O62412hEaRWAMcvp3RNH4CVdq1fhbTeL0TDVlVRkoSSAVr0JiGXyX8WEwrr4alGrpCZrTHjOYmpqagb_LYYWUSx7U0ph-urnl8yCeTioAvmCzXB1LF5O1Vq_9TtmcrpvrAW7QHsZcQy0PeEVqOBjmOK0qu13COegu4Quj9ktkx_pCwo0Oktuwz6Snst4oWA0H2ajyMW0xX6Mh0Tds0bbLZHKCgleD2E11Jt3_KWrU_ZOy41bQtNr5aVI60la3-yI-FpKnSPTlqqlqHllrMXWqqxj9pK-YkH796U9UpZFpUQfLmvqpb7KLOcm8x0S3jJqA7Vsmy7rb341GxOzGe_gFTjYIM?type=png)](https://mermaid.live/edit#pako:eNp9VM2O0zAQfpVRDqiVygvkxrJC6oFVBRxzmdqT7ojYk_VPBVrtuyA4IA6cEJe95sUYN_2L2iWKFHsy-f7s-LEyYqmqK8feOuwbDxBE0my29LLF4efwQxbwVlxPiRNv2aIlILiTrcAKbdD38ImMl274s2EjYBGW3g7PMQVGuAkYuSMOOJ8XbFAsb7och78UxwJAXeuHftYitPh6LfJ5fngDcIP8BYF9Ih9Hcr1Xr25VAx8VghGnzBSxZYEeAwKW2rnoE-QdGYonMPZbiokd-SQRyF2ij4C5dGBQ5D6IzZe4S9dLSMN3b9S4Aveaye_EBiP0w_O62412hEaRWAMcvp3RNH4CVdq1fhbTeL0TDVlVRkoSSAVr0JiGXyX8WEwrr4alGrpCZrTHjOYmpqagb_LYYWUSx7U0ph-urnl8yCeTioAvmCzXB1LF5O1Vq_9TtmcrpvrAW7QHsZcQy0PeEVqOBjmOK0qu13COegu4Quj9ktkx_pCwo0Oktuwz6Snst4oWA0H2ajyMW0xX6Mh0Tds0bbLZHKCgleD2E11Jt3_KWrU_ZOy41bQtNr5aVI60la3-yI-FpKnSPTlqqlqHllrMXWqqxj9pK-YkH796U9UpZFpUQfLmvqpb7KLOcm8x0S3jJqA7Vsmy7rb341GxOzGe_gFTjYIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagrama de blocos das fases da implementação do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "graph LR\n",
    "    subgraph Entrar e Pré-processar Dados\n",
    "        A[Currículos Pesquisadores\\nda ICT]\n",
    "        B[Entidades do\\nCEIS]\n",
    "        C[Produtos Estratégicos\\nCEIS]\n",
    "        D[Normalizar,\\nLimpar e\\nUnificar Termos]\n",
    "    end\n",
    "\n",
    "    subgraph Gerar Embeddings em GPU\n",
    "        H[Embedding Multilingue\\n Sentence Transformers]\n",
    "        I[Codificar\\nCompetências]\n",
    "        J[Codificar\\nÁreas de Pesquisa]\n",
    "    end\n",
    "\n",
    "    subgraph Construir e Analisar Grafos em GPU/CPU\n",
    "        K[Adicionar\\nNós e Arestas]\n",
    "        L[Grafo Multiplex\\nMacroprocessos PDI]\n",
    "    end\n",
    "\n",
    "    subgraph Modelo de Aprendizagem\\nNão-Supervisionada em Grafos    \n",
    "        M[Detectar\\nComunidades]\n",
    "        N[Analisar\\nSimilaridade]\n",
    "        O[Identificar Lacunas\\nao maximizar \\nModularidade]\n",
    "    end\n",
    "\n",
    "    subgraph Recomendar e Visualizar\n",
    "        P[Recomendar\\nAlinhamento\\nCompetências/Produtos]\n",
    "        Q[Visualizar\\nResultados]\n",
    "    end\n",
    "\n",
    "    A --> D\n",
    "    B --> D\n",
    "    C --> D\n",
    "    D --> H\n",
    "    H --> I\n",
    "    H --> J\n",
    "    I --> K\n",
    "    J --> K\n",
    "    N <--> M\n",
    "    L --> M\n",
    "    L --> N    \n",
    "    M --> O\n",
    "    N --> O\n",
    "    K --> L\n",
    "    O --> P\n",
    "    P --> Q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagrama de componentes do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "flowchart LR\n",
    "    id1((Circle))\n",
    "    id2([Stadium])\n",
    "    id3[(Database)]\n",
    "    id4(Box with round corner)\n",
    "    id5{{Hex}}\n",
    "    id6[\\Parallelogram\\]\n",
    "    id7[\\Trapezoid/]\n",
    "\n",
    "    id1-- 1st line ---id2\n",
    "    id1--> |2nd line| id3\n",
    "    id1--- |3rd line| id4\n",
    "    id2-.-|4th line| id5\n",
    "    id3 == 5th line ==> id6\n",
    "    id4 <--> id7 --> id6\n",
    "\n",
    "    style id1 fill:green,stroke:black\n",
    "    style id2 fill:white,stroke:#f66,stroke-dasharray: 5, 5,color:black\n",
    "    style id3 fill:#66f,stroke:#f6f,stroke-width:4px\n",
    "    style id4 fill:red,stroke:yellow\n",
    "    style id5 fill:orange,stroke:white,color:black\n",
    "    style id6 fill:yellow,stroke:blue,color:black\n",
    "    style id7 fill:brown,stroke:blue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quadrant diagram with dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "quadrantChart\n",
    "    x-axis x1 --> x2\n",
    "    y-axis y1 --> y2\n",
    "    quadrant-1 Comenta\n",
    "    quadrant-2 Gosta\n",
    "    quadrant-3 Inscreve\n",
    "    quadrant-4 Compartilha\n",
    "    p1: [0.3, 0.7]\n",
    "    p2: [0.6, 0.2]\n",
    "    p3: [0.8, 0.9]\n",
    "    p4: [0.2, 0.4]\n",
    "    p5: [0.6, 0.7]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIMPAR CONDA PARA LIBERAR ESPAÇO EM DISCO, APAGAR AMBIENTES INATIVOS, USAR COM CUIDADO!\n",
    "# import subprocess\n",
    "\n",
    "# # Executa o comando e captura a saída\n",
    "# process = subprocess.Popen(['conda', 'clean', '--all'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# # Lê a saída e verifica se há uma linha indicando arquivos a serem removidos\n",
    "# for line in process.stdout:\n",
    "#     print(line, end='')  # Imprime a saída na célula do notebook\n",
    "#     if 'tarball(s)' in line:  # Procura pela linha com a informação dos arquivos\n",
    "#         # Solicita a confirmação do usuário\n",
    "#         confirmacao = input(\"Deseja prosseguir com a limpeza? (s/n): \")\n",
    "#         if confirmacao.lower() == 's':\n",
    "#             process.stdin.write('y\\n')  # Envia 'y' para confirmar\n",
    "#         else:\n",
    "#             process.stdin.write('n\\n')  # Envia 'n' para cancelar\n",
    "#         process.stdin.flush()\n",
    "\n",
    "# # Aguarda o término do processo e imprime o resultado\n",
    "# stdout, stderr = process.communicate()\n",
    "# print(stdout)\n",
    "# print(stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principais operações utilizadas no modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operações de Matrizes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiplicação de Matrizes:** A operação mais comum em modelos de transformadores (como os utilizados pelo Sentence Transformers) é a multiplicação de matrizes, usada para calcular as atenções e as transformações lineares nas camadas do modelo.\n",
    "\n",
    "**Soma de Matrizes:** Soma de matrizes é utilizada para combinar os resultados de diferentes camadas ou para adicionar informações contextuais aos embeddings.\n",
    "\n",
    "**Normalização:** A normalização de vetores (por exemplo, Layer Normalization) é utilizada para estabilizar o treinamento e melhorar o desempenho do modelo.\n",
    "Outras Operações: Outras operações de matrizes, como transposição, concatenação e divisão por elemento, também podem ser utilizadas em diferentes etapas da conversão de embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de Ativação:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU, GELU, etc.:** Funções de ativação não lineares são aplicadas aos resultados das operações de matrizes para introduzir não linearidade no modelo e permitir que ele aprenda padrões mais complexos nos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operações Específicas do Modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax:** Utilizado para calcular as probabilidades de atenção em modelos de transformadores.\n",
    "\n",
    "**Embedding Lookup:** Utilizado para converter tokens de texto em embeddings de palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Princípios para otimizar cálculo em GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerações sobre o Hardware CPU x GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU:** A maioria das operações mencionadas acima é altamente paralelizável e pode ser acelerada significativamente por GPUs, que são projetadas para realizar cálculos de matrizes de forma eficiente.\n",
    "\n",
    "**CPU:** As CPUs podem ser usadas para realizar essas operações, mas geralmente são mais lentas do que as GPUs, especialmente para grandes volumes de dados ou modelos complexos.\n",
    "\n",
    "Portanto, a conversão de embeddings é um processo computacionalmente intensivo, onde a GPU pode acelerar significativamente esse processo, especialmente para grandes modelos e volumes de dados devido principalmente a necessidade de **operações de matrizes** e **funções de ativação**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arquiteturas das Placas Nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tesla**:        Não possuem Tensor Cores. O número de núcleos CUDA por SM varia entre as diferentes gerações da arquitetura Tesla.\n",
    "\n",
    "**Pascal**:       Não possuem Tensor Cores. Cada SM possui 128 núcleos CUDA.\n",
    "\n",
    "**Volta**:        Introduziu os Tensor Cores. Cada SM possui 64 núcleos CUDA e 8 Tensor Cores.\n",
    "\n",
    "**Turing**:       Cada SM possui 64 núcleos CUDA e 8 Tensor Cores. Introduziu os RT Cores para acelerar o ray tracing.\n",
    "\n",
    "**Ampere**:       Cada SM possui 128 núcleos CUDA e 4 Tensor Cores de terceira geração. Os Tensor Cores de terceira geração são duas vezes mais rápidos que os da geração anterior.\n",
    "\n",
    "**Ada Lovelace**: Cada SM possui 128 núcleos CUDA e 4 Tensor Cores de quarta geração. Os Tensor Cores de quarta geração são ainda mais rápidos e eficientes que os da geração anterior.\n",
    "\n",
    "**Hopper**:       Cada SM possui 128 núcleos CUDA e 8 Tensor Cores de quarta geração. Introduziu o Transformer Engine, um novo tipo de núcleo especializado em acelerar modelos de linguagem baseados em Transformer.\n",
    "\n",
    "**Blackwell**:    (arquitetura futura): Ainda não há informações oficiais sobre a arquitetura Blackwell, mas espera-se que ela continue a tendência de aumentar o número de núcleos CUDA e Tensor Cores por SM, além de introduzir novas tecnologias para acelerar ainda mais as cargas de trabalho de IA e HPC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sobre a utilização de Programação paralela em GPUs NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compreender como cada componente da GPU contribui para o desempenho em GML permite que você escolha a GPU certa, otimize seu código e obtenha o máximo desempenho em suas aplicações de aprendizado de máquina em grafos. Temos três tipos básicos de componentes processadores (CUDA cores e SMs) e memória, sendo eles:\n",
    "\n",
    "- Núcleos CUDA: Unidades básicas de processamento individuais na GPU.\n",
    "- SMs (Streaming Multiprocessor): Blocos maiores de processamento que agrupam núcleos CUDA e outros recursos.\n",
    "- Memória (VRAM): Onde os dados são armazenados para serem acessados pelos núcleos CUDA.\n",
    "\n",
    "Cada tipo de dispositivo na GPU tem sua respectivas frequências de clock, fornecendo informações sobre o desempenho atual da GPU em diferentes áreas de processamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados de clock e memória a cada instante são visualizáveis pelo nvidia-smi com os seguintes nomes:\n",
    "\n",
    "- Graphics (CUDA Cores): São as unidades de processamento mais básicas da GPU. Cada núcleo CUDA é capaz de executar uma única instrução de shader por vez. Eles são responsáveis pela execução em paralelo das operações matemáticas e lógicas necessárias para renderizar gráficos e realizar cálculos gerais. Os shaders são programas que processam gráficos e cálculos em paralelo. A frequência de clock \"Graphics\" indica a velocidade com que esses núcleos estão operando no momento. São os núcleos CUDA que acessam dados da memória da GPU (VRAM) para realizar seus cálculos. Quanto mais núcleos CUDA uma GPU tiver, maior será sua capacidade de processamento paralelo, porém, a velocidade com que eles podem acessar esses dados é influenciada pela largura de banda da memória.\n",
    "\n",
    "- SM: Significa \"Streaming Multiprocessor\". Cada SM é um conjunto de núcleos de processamento, caches e outros recursos dentro da GPU. A frequência de clock \"SM\" indica a velocidade com que os SMs estão operando.\n",
    "\n",
    "- Memory: Refere-se à memória de vídeo (VRAM) da GPU, onde são armazenados dados como texturas, modelos 3D e outros elementos gráficos, além de dados intermediários durante os cálculos. A frequência de clock \"Memory\" indica a velocidade com que a memória está operando.\n",
    "\n",
    "- Video: Refere-se ao mecanismo de codificação e decodificação de vídeo da GPU. É um componente de hardware dedicado ao processamento de vídeo que funciona como um motor de vídeo que pode acessar a Memory (VRAM) da GPU para ler e gravar dados de vídeo durante a codificação ou decodificação. Bem como pode usar os núcleos CUDA (Graphics) para realizar algumas etapas do processamento de vídeo, especialmente em codecs modernos que utilizam aceleração por hardware. A frequência de clock \"Video\" indica a velocidade com que esse mecanismo está operando. Os SMs gerenciam a execução dos threads nos núcleos CUDA, incluindo aqueles usados pelo motor de vídeo.\n",
    "\n",
    "Em síntese, os núcleos CUDA (Graphics) executam os shaders (programas que processam gráficos e cálculos em paralelo), que acessam dados na memória VRAM (Memory) e realizam cálculos. Os Streaming Multiprocessors (SMs) agrupam vários núcleos Graphics e outros recursos, permitindo que a GPU execute muitos shaders em paralelo. O motor de codificação e decodificação de vídeo (Video) é responsável por processar vídeos, codificando-os ou decodificando-os, e também pode acessar dados na Memory.\n",
    "\n",
    "Nem todas as GPUs possuem todos esses componentes. Algumas GPUs mais antigas podem não ter um mecanismo de vídeo dedicado, por exemplo. As frequências de clock podem variar dependendo da carga de trabalho da GPU e das configurações de energia. A seção \"Max Clocks\" no nvidia-smi mostra as frequências máximas que cada componente pode atingir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indo além do nvidi-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O nvidia-smi não fornece informações diretas sobre a utilização dos Tensor Cores, uma das evoluções mais recentes na fabricação de GPUs. Essa é uma limitação da ferramenta, que se concentra principalmente em métricas gerais de utilização da GPU, como a porcentagem de tempo em que os núcleos CUDA estão ativos e o uso da memória. Isso acontece devido ao nível de abstração e complexidade mais superficial do nvidia-smi.\n",
    "\n",
    "Abstração de Hardware: O nvidia-smi é projetado para fornecer uma visão geral do estado da GPU, sem se aprofundar em detalhes específicos da arquitetura, como a utilização de unidades de hardware especializadas como os Tensor Cores.\n",
    "\n",
    "Complexidade de Monitoramento: O monitoramento preciso da utilização dos Tensor Cores exigiria um nível mais profundo de acesso ao hardware e à execução dos kernels CUDA, o que poderia impactar o desempenho geral da GPU.\n",
    "Alternativas para Monitorar a Utilização dos Tensor Cores\n",
    "\n",
    "Para monitorar a utilização dos Tensor Cores, é necessário usar ferramentas de profiling mais avançadas, como o NVidia Profiler (Nsight Compute) ou recursos de profiling fornecidos por bibliotecas de Deep Learning.\n",
    "\n",
    "A escolha da ferramenta depende do seu nível de conhecimento em CUDA e das necessidades específicas da sua aplicação.\n",
    "Mesmo sem monitorar diretamente a utilização dos Tensor Cores, você ainda pode otimizar seu código para aproveitá-los ao máximo, utilizando operações e formatos de dados que sejam compatíveis com eles, como operações de multiplicação de matrizes em precisão mista (FP16 ou BF16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os Tensor Cores são unidades de hardware especializadas localizadas dentro dos SMs da GPU. Eles aceleram operações de multiplicação de matrizes em baixa precisão, melhorando o desempenho e a eficiência energética em tarefas como Deep Learning. Embora o nvidia-smi não mostre a utilização dos Tensor Cores diretamente, um alto uso da GPU e um menor consumo de energia podem indicar que eles estão sendo utilizados.\n",
    "\n",
    "Os Tensor Cores, embora não sejam explicitamente mostrados pelo nvidia-smi, estão intimamente relacionados aos Streaming Multiprocessors (SMs). Cada SM em uma GPU NVIDIA compatível com Tensor Cores contém um certo número desses núcleos especializados. Os Tensor Cores são projetados para acelerar operações específicas de multiplicação de matrizes em formatos de baixa precisão, como FP16 (meia precisão) e INT8 (inteiros de 8 bits).\n",
    "\n",
    "Os Tensor Cores funcionam em conjunto com os SMs em execução paralela aumentando o desempenho e eficiência energética:\n",
    "\n",
    "Execução Paralela: Os Tensor Cores operam em paralelo com os núcleos CUDA dentro de cada SM. Enquanto os núcleos CUDA executam instruções gerais de shader, os Tensor Cores se concentram em acelerar as operações de multiplicação de matrizes específicas para as quais foram projetados.\n",
    "\n",
    "Aumento de Desempenho: Ao descarregar essas operações de multiplicação de matrizes para os Tensor Cores, a GPU pode alcançar um desempenho significativamente maior em tarefas que se beneficiam desses cálculos, como o treinamento e a inferência de redes neurais profundas.\n",
    "\n",
    "Eficiência Energética: Os Tensor Cores são projetados para serem mais eficientes em termos de energia do que os núcleos CUDA para realizar essas operações específicas, o que pode levar a um menor consumo de energia da GPU.\n",
    "\n",
    "Monitoramento indireto de uso de Tensor cores pelo impacto nos dados do nvidia-smi:\n",
    "\n",
    "Utilização da GPU: Embora o nvidia-smi não mostre a utilização dos Tensor Cores diretamente, um alto uso da GPU (indicado pela métrica \"Gpu\" na seção \"Utilization\") pode sugerir que os Tensor Cores estão sendo utilizados, especialmente se a carga de trabalho envolve operações de multiplicação de matrizes em baixa precisão.\n",
    "\n",
    "Consumo de Energia: Se sua aplicação estiver utilizando os Tensor Cores de forma eficiente, você poderá observar um menor consumo de energia da GPU em comparação com uma aplicação semelhante que não utiliza os Tensor Cores.\n",
    "\n",
    "Desempenho: Em geral, aplicações que aproveitam os Tensor Cores tendem a ter um desempenho significativamente melhor em GPUs que os possuem, especialmente em tarefas de Deep Learning e outras que envolvem cálculos intensivos de matrizes.\n",
    "\n",
    "Para aproveitar ao máximo os Tensor Cores, é importante usar corretamente as bibliotecas e frameworks de Deep Learning que suportem operações em precisão mista e que sejam otimizados para GPUs NVIDIA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados principais disponíveis no nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seção: <b>PCIe Generation</b>\n",
    "\n",
    "    Max: A geração máxima do PCIe que a sua GPU suporta (neste caso, PCIe 4.0).\n",
    "    Current: A geração do PCIe que a sua GPU está atualmente usando (também PCIe 4.0).\n",
    "    Device Current/Max: A geração do PCIe que o dispositivo (GPU) está usando/suporta.\n",
    "    Host Max: A geração máxima do PCIe que a placa-mãe (host) suporta.\n",
    "    Seção: Link Width\n",
    "    Max: A largura de banda máxima do link PCIe que a sua GPU suporta (16x neste caso).\n",
    "    Current: A largura de banda do link PCIe que está sendo usada atualmente (também 16x).\n",
    "\n",
    "Seção: <b>Bridge Chip</b>\n",
    "\n",
    "    Replays Since Reset: O número de vezes que o barramento PCIe teve que retransmitir dados desde a última reinicialização. Idealmente, esse número deve ser baixo, indicando uma conexão estável.\n",
    "    Replay Number Rollovers: O número de vezes que o contador de replays foi reiniciado (estourou) desde a última reinicialização. Também deve ser baixo.\n",
    "    Tx Throughput: A taxa de transferência de dados transmitidos pela GPU através do barramento PCIe.\n",
    "    Rx Throughput: A taxa de transferência de dados recebidos pela GPU através do barramento PCIe.\n",
    "    Fan Speed: A velocidade atual da ventoinha da GPU em porcentagem da velocidade máxima.\n",
    "    Performance State: O estado de desempenho atual da GPU. P8 é o estado de desempenho mais alto, indicando que a GPU está operando em sua frequência máxima.\n",
    "\n",
    "Seção: <b>Clocks Event Reasons</b>\n",
    "\n",
    "    Idle: Indica se a GPU está ociosa (\"Active\") ou se está sendo usada por algum processo.\n",
    "\n",
    "Seção: <b>FB Memory Usage</b>\n",
    "\n",
    "    Total: A quantidade total de memória de vídeo (framebuffer) disponível na GPU.\n",
    "    Reserved: A quantidade de memória reservada pelo sistema operacional ou drivers.\n",
    "    Used: A quantidade de memória de vídeo atualmente em uso.\n",
    "    Free: A quantidade de memória de vídeo disponível para uso.\n",
    "\n",
    "Seção: <b>BAR1 Memory Usage (Base Address Register 1)</b>\n",
    "\n",
    "    Total: Tamanho total da BAR1 (Base Address Register 1), região de memória acessível pela CPU para comunicar com a GPU.\n",
    "    Used: A quantidade de memória BAR1 atualmente em uso.\n",
    "    Free: A quantidade de memória BAR1 disponível para uso.\n",
    "    \n",
    "Seção: <b>Conf Compute Protected Memory Usage</b>\n",
    "\n",
    "    Compute Mode: O modo de computação atual da GPU. \"Default\" significa que a GPU está operando no modo padrão, sem restrições especiais de acesso à memória.\n",
    "\n",
    "Seção: <b>Utilization</b>\n",
    "\n",
    "    Gpu: A porcentagem de utilização dos núcleos da GPU.\n",
    "    Memory: A porcentagem de utilização da memória de vídeo.\n",
    "    Encoder/Decoder/JPEG/OFA: Porcentagens de utilização de diferentes unidades de processamento da GPU, se aplicável.\n",
    "\n",
    "Seção: <b>Encoder Stats & FBC Stats</b>\n",
    "\n",
    "    Active Sessions/Average FPS/Average Latency: Informações sobre sessões de codificação e decodificação de vídeo ativas, se houver.\n",
    "\n",
    "Seção: <b>Temperature</b>\n",
    "\n",
    "    GPU Current Temp: A temperatura atual da GPU.\n",
    "    GPU Shutdown Temp: A temperatura na qual a GPU será desligada para evitar danos.\n",
    "    GPU Slowdown Temp: A temperatura na qual a GPU reduzirá sua frequência para evitar o superaquecimento.\n",
    "    GPU Max Operating Temp: A temperatura máxima de operação segura da GPU.\n",
    "    GPU Target Temperature: A temperatura alvo que a GPU tentará manter através do controle da ventoinha.\n",
    "\n",
    "Seção: <b>GPU Power Readings</b>\n",
    "\n",
    "    Power Draw: A potência atual sendo consumida pela GPU.\n",
    "    Current/Requested/Default/Min/Max Power Limit: Limites de potência configurados para a GPU.\n",
    "\n",
    "Seção: <b>Clocks</b>\n",
    "\n",
    "    Graphics/SM/Memory/Video: As frequências de clock atuais para diferentes componentes da GPU.\n",
    "\n",
    "Seção: <b>Max Clocks</b>\n",
    "\n",
    "    Graphics/SM/Memory/Video: As frequências de clock máximas que cada componente da GPU pode atingir.\n",
    "\n",
    "Seção: <b>Voltage</b>\n",
    "\n",
    "    Graphics: A tensão atual aplicada aos núcleos da GPU.\n",
    "\n",
    "Seção: <b>Health</b>\n",
    "\n",
    "    Processes: Lista os processos que estão usando a GPU atualmente. \"None\" significa que nenhum processo está usando a GPU no momento.\n",
    "\n",
    "Seção: <b>Capabilities</b>\n",
    "\n",
    "    EGM: Indica se o recurso de Gerenciamento de Memória de Erro (Error Memory Management) está habilitado ou desabilitado na GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profilling completo também a nível de Tensor Cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NVidia Profiler (Nsight Compute)*:\n",
    "Esta é a ferramenta oficial da NVIDIA para profiling e análise de desempenho de aplicações CUDA. Ela permite coletar métricas detalhadas sobre a execução de kernels CUDA, incluindo a utilização dos Tensor Cores. No entanto, o Nsight Compute requer conhecimento de CUDA e pode ter uma curva de aprendizado mais acentuada.\n",
    "\n",
    "*Bibliotecas de Deep Learning*:\n",
    "Algumas bibliotecas de Deep Learning, como o PyTorch e o TensorFlow, oferecem ferramentas de profiling que podem fornecer informações sobre a utilização dos Tensor Cores durante o treinamento e a inferência de modelos. É necessário\n",
    "consultar a documentação atualizada da biblioteca de Deep Learning em utilização para ver quando ela oferece recursos de profiling e como usá-los para monitorar os Tensor Cores.\n",
    "\n",
    "*Outras Ferramentas de Profiling*:\n",
    "Existem outras ferramentas de profiling de terceiros que podem fornecer informações sobre a utilização dos Tensor Cores, como o NSight Systems e o TAU (Tuning and Analysis Utilities).\n",
    "\n",
    "Apesar de muito completas e eficientes, essas ferramentas podem ser mais complexas de configurar e usar, e nem sempre oferecem suporte completo para todas as GPUs e versões do CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal uso em Graph Machine Learning dos compontentes das GPUs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Streaming Multiprocessor (SMs): Organizar e gerenciar a execução paralela.\n",
    "- Graphics (núcleos CUDA): Processar paralelamente as operações do GML.\n",
    "- Memory (VRAM): Armazenar temporáriamente os dados do grafo e do modelo.\n",
    "- Video: Motor de codificação e decodificação de vídeo tem o papel de visualizar os grafos e aplicações multimodais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguagem CUDA para criar kernels otimizados para GML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O controle e a interação dos componentes da GPU (CUDA Cores, SMs, Memória) com outros elementos de hardware e software em CUDA para otimizar kernels de Graph Machine Learning (GML) envolvem vários aspectos:\n",
    "\n",
    "1. Gerenciamento de Threads e Blocos:\n",
    "\n",
    "- Threads e Blocos: Em CUDA, o código é executado por threads organizados em blocos. Cada thread dentro de um bloco tem um ID único, permitindo que você acesse e processe diferentes partes do grafo em paralelo.\n",
    "Hierarquia de Memória: CUDA oferece diferentes níveis de memória com velocidades e capacidades variadas:\n",
    "- Registradores: Memória mais rápida, privada para cada thread.\n",
    "- Memória Compartilhada: Memória mais rápida, compartilhada entre threads de um mesmo bloco. Ideal para armazenar dados acessados frequentemente dentro de um bloco.\n",
    "- Memória Global: Memória principal da GPU, acessível por todos os threads. Usada para armazenar a maior parte dos dados do grafo e do modelo.\n",
    "- Sincronização: Para garantir a correção dos resultados, é crucial sincronizar os threads dentro de um bloco (__syncthreads()) ou entre blocos (cudaDeviceSynchronize()), especialmente quando há dependências de dados entre eles.\n",
    "\n",
    "2. Otimização de Acesso à Memória:\n",
    "\n",
    "- Coalescência de Memória: Acessar dados da memória global de forma sequencial e alinhada maximiza a largura de banda da memória. Evite acessos aleatórios e desalinhados, que podem causar grandes penalidades de desempenho.\n",
    "- Caches: Utilize a memória compartilhada para armazenar dados acessados com frequência dentro de um bloco, reduzindo o número de acessos à memória global mais lenta.\n",
    "- Transferência de Dados: Minimize a transferência de dados entre a CPU e a GPU, pois isso pode ser um gargalo de desempenho. Utilize cudaMemcpyAsync para transferências assíncronas e pinned memory para otimizar a cópia de dados.\n",
    "\n",
    "3. Otimização para GML:\n",
    "\n",
    "- Particionamento de Grafos: Divida o grafo em partições menores que caibam na memória da GPU, processando cada partição em paralelo.\n",
    "- Amostragem de Vizinhança: Em grafos grandes, utilize técnicas de amostragem para selecionar apenas um subconjunto dos vizinhos de cada nó durante o treinamento, reduzindo a quantidade de dados a serem processados.\n",
    "- Compressão de Grafos: Utilize formatos de representação de grafos mais compactos, como CSR (Compressed Sparse Row) ou COO (Coordinate Format), para reduzir o consumo de memória e melhorar a eficiência do acesso aos dados.\n",
    "- Modelos Eficientes: Utilize modelos de GML projetados para serem eficientes em GPUs, como GNNs com camadas convolucionais esparsas ou modelos que exploram a estrutura do grafo para reduzir o número de operações."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Exemplo de Kernel CUDA para Agregação de Mensagens em GNNs:\n",
    "\n",
    "C++\n",
    "__global__ void aggregate_messages(float* features, int* edge_index, int num_nodes, int num_edges) {\n",
    "    int node_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (node_id < num_nodes) {\n",
    "        float aggregated_message = 0.0;\n",
    "        for (int i = 0; i < num_edges; i++) {\n",
    "            if (edge_index[2 * i + 1] == node_id) {  // Se o nó é o destino da aresta\n",
    "                int source_node = edge_index[2 * i];\n",
    "                aggregated_message += features[source_node];  // Agrega a mensagem do nó de origem\n",
    "            }\n",
    "        }\n",
    "        features[node_id] = aggregated_message;  // Atualiza as features do nó\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mesma função para Agregar Mensagens em GNNs pode ser criada em python por meio do PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def aggregate_messages(features, edge_index):\n",
    "    \"\"\"\n",
    "    Agrega mensagens em um grafo usando PyTorch.\n",
    "\n",
    "    Args:\n",
    "        features: Tensor com as features dos nós (shape: [num_nodes, num_features]).\n",
    "        edge_index: Tensor com os índices das arestas (shape: [2, num_edges]).\n",
    "\n",
    "    Returns:\n",
    "        Tensor com as features agregadas dos nós (shape: [num_nodes, num_features]).\n",
    "    \"\"\"\n",
    "\n",
    "    # Criar um tensor esparso para representar o grafo\n",
    "    sparse_adj = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.shape[1]), \n",
    "                                         size=(features.shape[0], features.shape[0]))\n",
    "\n",
    "    # Realizar a agregação de mensagens usando multiplicação de matrizes esparsas\n",
    "    aggregated_messages = torch.sparse.mm(sparse_adj, features)\n",
    "\n",
    "    return aggregated_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frameworks e APIs de alto nível para otimizar execução em GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Geometric e DGL facilitam a criação de kernels otimizados para Graph Machine Learning (GML) de várias maneiras, abstraindo a complexidade da programação CUDA de baixo nível e fornecendo implementações eficientes de operações comuns em grafos.\n",
    "\n",
    "1. Abstração de CUDA:\n",
    "\n",
    "Foco no modelo, não na implementação: Ambas as bibliotecas permitem que você se concentre na definição do seu modelo de GML (GNNs, etc.) em Python, sem precisar escrever código CUDA diretamente. Elas cuidam da conversão do seu modelo em kernels CUDA otimizados nos bastidores.\n",
    "Flexibilidade: Você pode usar construções de alto nível do PyTorch (ou TensorFlow, no caso do DGL) para definir seu modelo, aproveitando a flexibilidade e expressividade dessas bibliotecas.\n",
    "\n",
    "Portabilidade: O código do seu modelo se torna mais portátil, pois não está diretamente vinculado a detalhes específicos da arquitetura da GPU ou da CUDA.\n",
    "\n",
    "2. Implementações Otimizadas:\n",
    "\n",
    "Operações comuns em grafos: Ambas as bibliotecas fornecem implementações otimizadas em CUDA para operações comuns em GML, como:\n",
    "\n",
    "- Convolução de grafos: Diferentes tipos de convolução, como GCNConv, GATConv, SAGEConv, etc.\n",
    "- Pooling de grafos: Para reduzir o tamanho do grafo, como TopKPooling, SAGPooling, etc.\n",
    "- Normalização de grafos: Para normalizar as features dos nós, como BatchNorm, LayerNorm, etc.\n",
    "- Outras operações: Funções de ativação, camadas lineares, etc.\n",
    "\n",
    "Aproveitamento de recursos da GPU: Essas implementações são projetadas para aproveitar ao máximo os recursos da GPU, como paralelismo, memória compartilhada e caches, para obter o melhor desempenho possível.\n",
    "\n",
    "Atualizações e otimizações: As bibliotecas são mantidas ativamente e recebem atualizações frequentes com novas otimizações e melhorias de desempenho.\n",
    "\n",
    "3. Abstração de detalhes de baixo nível:\n",
    "\n",
    "Gerenciamento de memória: As bibliotecas cuidam do gerenciamento de memória na GPU, alocando e liberando memória conforme necessário, para que você não precise se preocupar com esses detalhes.\n",
    "\n",
    "Sincronização de threads: A sincronização de threads é tratada automaticamente pelas bibliotecas, garantindo a correção dos resultados sem a necessidade de chamadas explícitas a __syncthreads().\n",
    "\n",
    "Transferência de dados: As bibliotecas otimizam a transferência de dados entre a CPU e a GPU, usando técnicas como cudaMemcpyAsync e pinned memory.\n",
    "\n",
    "4. Facilidade de uso:\n",
    "\n",
    "API intuitiva: Ambas as bibliotecas oferecem uma API Python de alto nível que é fácil de aprender e usar, mesmo para quem não tem experiência em CUDA.\n",
    "\n",
    "Documentação e exemplos: As bibliotecas possuem documentação abrangente e muitos exemplos de código que demonstram como usar as diferentes funcionalidades e construir modelos de GML.\n",
    "\n",
    "Comunidade ativa: Ambas as bibliotecas têm comunidades ativas e fóruns de suporte onde você pode encontrar ajuda e trocar ideias com outros usuários.\n",
    "\n",
    "Em resumo, PyTorch Geometric e DGL facilitam a criação de kernels otimizados para GML, permitindo que você se concentre na definição do seu modelo e abstraindo os detalhes de baixo nível da programação CUDA. Elas fornecem implementações eficientes de operações comuns em grafos, otimizam o uso dos recursos da GPU e oferecem uma API intuitiva e fácil de usar. Com essas bibliotecas, você pode desenvolver e implementar modelos de GML de forma mais rápida e eficiente, sem precisar ser um especialista em CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizando uso da NVIDIA GeForce RTX 4080 SUPER para GML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na utilização da NVIDIA GeForce RTX 4080 SUPER em aplicações de Graph Machine Learning, no contexto de pesquisas com restrições orçamentárias, Ao otimizar o uso de cada componente da GPU e aplicar as melhores práticas de GML, um desempenho razoável nas tarefas de aprendizado de máquina em grafos pode ser alcançado. Esta GPU oferece um grau de paralelismo, uma capacidade de memória e recursos avançados de vídeo com boa relação custo/benefício tanto em aquisição como em consumo de energia elétrica dentre os modelos não-profissionais. \n",
    "\n",
    "Cada componente dessa GPU pode ser utilizada de forma otimizada:\n",
    "\n",
    "1. Núcleos CUDA (Graphics)\n",
    "\n",
    "- Paralelismo Massivo: A RTX 4080 SUPER possui 76 núcleos CUDA, permitindo um alto grau de paralelismo. Esses núcleos podem ser bem utilizados para processar um grande número de nós e arestas simultaneamente, acelerando operações como agregação de mensagens, propagação de features e cálculos de atenção em GNNs.\n",
    "\n",
    "- Uso em Grafos Densos: A arquitetura Ada Lovelace da RTX 4080 SUPER é particularmente eficiente em operações com matrizes densas, que podem ser usadas para representar grafos densos em GML.\n",
    "\n",
    "2. Streaming Multiprocessors (SMs)\n",
    "\n",
    "- Gerenciamento Eficiente de Threads: Os 76 SMs da RTX 4080 SUPER gerenciam a execução dos threads nos núcleos CUDA, garantindo uma utilização eficiente dos recursos da GPU.\n",
    "\n",
    "- Modelos Complexos: Utilize os SMs para lidar com modelos de GML complexos com um grande número de parâmetros e operações, permitindo um treinamento e inferência mais rápidos.\n",
    "\n",
    "- Técnicas Avançadas: Implemente técnicas avançadas de GML, como Graph Attention Networks (GATs) ou Graph Transformers, que exigem um alto grau de paralelismo e gerenciamento eficiente de threads.\n",
    "\n",
    "3. Memória de Vídeo (VRAM)\n",
    "\n",
    "- Capacidade de 16 GB: A RTX 4080 SUPER possui 16 GB de VRAM, o que permite armazenar grafos de tamanho moderado diretamente na memória da GPU.\n",
    "\n",
    "- Grafos de Tamanho Médio: Utilize a VRAM para armazenar grafos com até alguns milhões de nós e arestas, evitando a necessidade de transferir dados entre a CPU e a GPU durante o processamento, o que pode ser um gargalo de desempenho.\n",
    "\n",
    "- Técnicas de Otimização de Memória: Para grafos maiores que não cabem inteiramente na VRAM, utilize técnicas de otimização de memória, como particionamento de grafos, amostragem de vizinhos ou carregamento de dados em lotes.\n",
    "\n",
    "4. Codificador/Decodificador de Vídeo\n",
    "\n",
    "- Visualização Interativa: A RTX 4080 SUPER possui um poderoso mecanismo de vídeo que pode ser usado para visualizar grafos e seus embeddings em tempo real, permitindo uma análise interativa e exploratória dos seus dados e resultados.\n",
    "\n",
    "- Aplicações Multimodais: Se sua aplicação de GML envolve dados multimodais, como imagens ou vídeos associados aos nós do grafo, o codificador/decodificador de vídeo pode ser usado para processar e incorporar esses dados, enriquecendo a representação do grafo e melhorando o desempenho do modelo.\n",
    "\n",
    "Recomendações Gerais:\n",
    "\n",
    "Bibliotecas: Utilize bibliotecas de GML otimizadas para GPUs NVIDIA, como o PyTorch Geometric e o DGL, que aproveitam ao máximo os recursos da RTX 4080 SUPER.\n",
    "\n",
    "Precisão Mista (Mixed Precision): Utilize treinamento em precisão mista (FP16 ou BF16) para acelerar o treinamento e reduzir o consumo de memória, especialmente em modelos grandes e complexos.\n",
    "\n",
    "Paralelismo de Dados: Se possível, paralelize o treinamento do seu modelo em várias GPUs para acelerar ainda mais o processo.\n",
    "\n",
    "Monitoramento: Utilize ferramentas como o nvidia-smi para monitorar o uso da GPU durante o treinamento e a inferência, identificando gargalos de desempenho e oportunidades de otimização.\n",
    "\n",
    "Conclusão: A GeForce RTX 4080 SUPER é uma GPU apresente o melhor custo/benefício em relação processamento/preço, dispõe de recursos que podem ser bem aproveitados para algoritmos de Graph Machine Learning (GML) apresentando desempenho razoável, dentre os melhores na categoria de placas GPU de uso não-profissional. \n",
    "\n",
    "Com relação ao modelo mais potente da arquitetura Ada Lovelace a RTX 4090 (24 GB GDDR6X, 450 Watt), a RTX 4080 Super (16 GB GDDR6X, 320 Watt) atinge desempenho de 88% do desempenho da RTX 4090 por apenas 60% do custo de aquisição nos Estados Unidos. Para aquisição no Brasil a distância pode ser ainda maior, pode-se encontrar 4080 Super po aproximadamente 31% do preço de aquisição da 4090 (Preços na Amazon: RTX 4090 R$34.970,00 versus RTX 4080 Super R$11.001 no Brasil em 2024). \n",
    "\n",
    "A maior limitação realmente é o tamanho da memória em 16Gb, contra os 24Gb da 4090 RTX.\n",
    "\n",
    "A 4080 Super ainda apresenta eficiência energética bem superior (45,62%) com relação à 4090 (28,81%). \n",
    "\n",
    "https://technical.city/pt/video/GeForce-RTX-4090-vs-GeForce-RTX-4080-SUPER\n",
    "\n",
    "https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Um Modelo Grafo Multicamada para PDI</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade Semântica em Grafos com Classificação Dinâmica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para alcançar o objetivo de criar um motor de análise de similaridade semântica em grafos com classificação dinâmica de nós, a combinação de Graph Machine Learning (GML) com técnicas de otimização de modularidade demanda implementar estratégias para representar as entidades do domínio, na forma de nós no modelo grafo, bem como representar as interações no mundo real entre essas entidades, através da representação dos relacionamentos em arestas do modelo grafo. Para tanto seguem as principais atividades a serem realizadas para implementar o modelo de análise em grafos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Representar entidades de mundo real como Nós e relacionamentos como Arestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incorporação de Nós (Node Embeddings)**: Modelos de GML como Graph Neural Networks (GNNs) foram utilizados para aprender representações vetoriais (embeddings) dos nós [1,2], capturando assim, a similaridade semântica com base na estrutura do grafo e nas propriedades dos nós.\n",
    "\n",
    "**Incorporação de Arestas (Edge Embeddings)**: De acordo com o contexto real do domínio em análise, se além dos nós em si, as relações entre os nós também carregam informações semânticas importantes, é necessário aprender representações vetoriais também para as arestas.\n",
    "\n",
    "Referências:\n",
    "\n",
    "    [1] Graph Convolutional Networks for Text Classification (https://arxiv.org/abs/1810.08403)\n",
    "    [2] Inductive Representation Learning on Large Graphs (https://arxiv.org/abs/1706.02216)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cálcular Similaridade Semântica no modelo grafo (entidades e relacionamentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Métricas de Distância**: Utilizadas métricas de distância como cosseno, ou distância euclidiana, dependendo da questão específica tratada, para calcular a similaridade entre os embeddings dos nós."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Realizar a classificação dinâmica de nós e arestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algoritmos de Clustering**: Para permitir uma classificação dinâmica de nós e relacionamentos, aqui utilizamos algoritmos de clustering, que permitiram classificar os nós com base na similaridade semântica de suas propriedades, passadas como parâmetros de classificação, e na maximização da modularidade do grafo gerado pela interação entre essas entidades. \n",
    "\n",
    "Foram experimentados para essa atividade os algoritmos:\n",
    "\n",
    "    Louvain: Algoritmo eficiente e amplamente utilizado para otimização de modularidade.\n",
    "\n",
    "    Leiden: Variação do Louvain com maior precisão na detecção de comunidades.\n",
    "\n",
    "    Infomap: Algoritmo baseado em fluxo de informação para identificar comunidades.\n",
    "\n",
    "**Algoritmos de Clustering Hierárquico**: Permitem visualizar a estrutura de comunidades em diferentes níveis de granularidade.\n",
    "\n",
    "    Maximização da Modularidade: A modularidade mede a qualidade da divisão do grafo em comunidades. Utilize a modularidade como função objetivo durante o processo de clustering para garantir que os nós sejam agrupados em comunidades coesas e significativas. \n",
    "\n",
    "Referências:\n",
    "\n",
    "    [3] Finding community structure in very large networks (https://arxiv.org/abs/cond-mat/0408187)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Modelagem do Schema do Grafo e Questões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema do Grafo: Defina o schema do grafo de forma a representar adequadamente os dados textuais e as relações entre eles. Utilize propriedades dos nós para armazenar informações relevantes para a análise de similaridade semântica.\n",
    "Questões: Formule as questões que deseja responder com a análise de similaridade semântica. Isso guiará a escolha das propriedades dos nós, o cálculo de similaridade e a interpretação dos resultados do clustering.\n",
    "Considerações Adicionais\n",
    "\n",
    "Pré-processamento de Texto: Aplique técnicas de pré-processamento de texto (tokenização, remoção de stop words, stemming/lemmatization) para melhorar a qualidade das representações vetoriais e a análise de similaridade semântica.\n",
    "Escolha do Modelo de GML: A escolha do modelo de GML (tipo de GNN, número de camadas, etc.) depende da complexidade do grafo e das características dos dados. Experimente diferentes modelos e avalie seu desempenho.\n",
    "Avaliação dos Resultados: Utilize métricas de avaliação de clustering para medir a qualidade das comunidades encontradas e a efetividade da classificação dinâmica dos nós.\n",
    "Escalabilidade: Para grafos muito grandes, considere técnicas de amostragem ou algoritmos de clustering distribuídos para lidar com a escalabilidade.\n",
    "\n",
    "Outas literaturas relevantes incluem os artigos:\n",
    "\n",
    "    [4] A Comprehensive Survey on Graph Neural Networks (https://arxiv.org/abs/1901.00596)\n",
    "    [5] Community Detection in Graphs\n",
    "\n",
    "Bibliotecas Python\n",
    "\n",
    "NetworkX: Para criação, manipulação e visualização de grafos.\n",
    "\n",
    "PyTorch Geometric: Para implementação de modelos de GML.\n",
    "\n",
    "Scikit-learn: Para algoritmos de clustering e métricas de avaliação.\n",
    "\n",
    "As características, tanto dos dados sendo processados, como das questões a responder são fundamentais para melhor abordagem de treinamento para combinar as técnicas e criar o motor de análise de similaridade semântica eficaz e dinâmico em grafos. Os testes de benchmarking para essa escolha são mostrados a seguir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simular interação de forças para visualizar análises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de forças no sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyVis para simula a busca por equilíbrio entre forças físicas nas entidades que interagem no grafo para determinar o posicionamento dos nós no grafo e assim criar o layout. As principais forças envolvidas são:\n",
    "\n",
    "#### 1. Força de Atração:\n",
    "\n",
    "Arestas (Links): As arestas entre os nós agem como molas, puxando os nós conectados um em direção ao outro. A intensidade dessa força é determinada pelo parâmetro springLength (comprimento ideal da mola) e springConstant (rigidez da mola).\n",
    "\n",
    "Gravidade Central: Uma força de atração em direção ao centro do grafo, controlada pelo parâmetro centralGravity. Essa força ajuda a evitar que os nós se dispersem muito e mantém o grafo mais compacto.\n",
    "\n",
    "#### 2. Força de Repulsão:\n",
    "\n",
    "Repulsão entre Nós: Os nós se repelem uns aos outros, como partículas carregadas com a mesma carga. A intensidade dessa força é determinada pelo parâmetro gravitationalConstant (constante gravitacional). Um valor negativo aumenta a repulsão, enquanto um valor positivo a diminui.\n",
    "\n",
    "Evitar Sobreposição: O parâmetro avoidOverlap controla se os nós devem evitar a sobreposição. Se ativado, uma força adicional é aplicada para afastar os nós que estão muito próximos.\n",
    "\n",
    "#### 3. Força de Amortecimento:\n",
    "\n",
    "Damping: O parâmetro damping controla o amortecimento do movimento dos nós. Um valor maior de amortecimento torna o movimento mais lento e suave, enquanto um valor menor permite movimentos mais rápidos e oscilatórios.\n",
    "\n",
    "#### 4. Forças Adicionais (Opcionais):\n",
    "\n",
    "Outras Forças: O PyVis permite adicionar outras forças personalizadas ao layout, como forças de atração/repulsão entre grupos de nós, ou forças que direcionam os nós para posições específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como as Forças Interagem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo ForceAtlas2Based iterativamente calcula as forças resultantes sobre cada nó e ajusta suas posições de acordo. O processo continua até que o layout se estabilize ou um número máximo de iterações seja atingido.\n",
    "\n",
    "O algoritmo Barnes-Hut otimiza o cálculo das forças de longo alcance, aproximando as forças entre grupos de nós distantes. Isso melhora o desempenho do algoritmo, especialmente em grafos grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equilíbrio das Forças:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "O objetivo do algoritmo é encontrar um equilíbrio entre as forças de atração e repulsão, de modo que os nós conectados fiquem próximos, mas sem se sobreporem, e o grafo tenha uma aparência geral agradável e informativa. O ajuste dos parâmetros do ForceAtlas2Based e do Barnes-Hut permite controlar esse equilíbrio e personalizar o layout do grafo de acordo com suas necessidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos ForceAtlas2Based e Barnes-Hut:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ForceAtlas2Based é um algoritmo de layout que simula um sistema físico onde os nós se atraem e se repelem com base em certas forças. No entanto, calcular essas forças para todos os pares de nós em um grafo grande pode ser computacionalmente caro.\n",
    "\n",
    "Para otimizar o cálculo das forças, o ForceAtlas2Based utiliza o algoritmo Barnes-Hut. Esse algoritmo agrupa nós distantes em clusters e aproxima suas forças de atração/repulsão, reduzindo significativamente o número de cálculos necessários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theta e a Distância entre Nós:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O parâmetro theta (θ) do Barnes-Hut define o limite entre as forças de curto e longo alcance. Quando a distância entre dois nós é menor que theta multiplicado pelo tamanho do cluster, as forças são calculadas individualmente (curto alcance). Caso contrário, as forças são aproximadas usando o centro de massa do cluster (longo alcance).\n",
    "\n",
    "Quando os nós estão muito próximos, a distância entre eles é menor que o limite definido por theta. Nesse caso, o algoritmo Barnes-Hut calcula as forças individualmente para cada par de nós, levando em consideração suas posições exatas. Isso permite que o ForceAtlas2Based posicione os nós próximos de forma mais precisa, evitando sobreposições e garantindo um layout visualmente agradável.\n",
    "\n",
    "Em resumo, o ForceAtlas2Based define as regras gerais para as forças de atração e repulsão entre os nós. No entanto, quando os nós estão próximos, o algoritmo Barnes-Hut assume o controle e calcula as forças de forma mais precisa, levando em consideração a distância exata entre os nós.\n",
    "\n",
    "O parâmetro theta do Barnes-Hut é fundamental para determinar o comportamento do layout em nós próximos, pois define o limite entre as forças de curto e longo alcance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O que é o Theta (θ)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O theta (θ) é um parâmetro interno do ForceAtlas2Based que controla o limite entre as forças de longo alcance (que afetam todos os nós) e as forças de curto alcance (que afetam apenas os nós próximos).\n",
    "\n",
    "Valores mais altos de theta: Aceleram o cálculo das forças, mas podem gerar mais erros e imprecisões no layout.\n",
    "Valores mais baixos de theta: Tornam o cálculo mais lento, mas produzem um layout mais preciso e com menos erros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como o Theta é Usado no ForceAtlas2Based?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ForceAtlas2Based utiliza uma técnica chamada Barnes-Hut para aproximar as forças de longo alcance, tornando o cálculo mais eficiente. O theta é usado para determinar quais nós estão \"próximos o suficiente\" para que suas forças sejam calculadas individualmente (curto alcance), e quais nós estão \"longe o suficiente\" para que suas forças sejam aproximadas usando a técnica Barnes-Hut (longo alcance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parâmetros do ForceAtlas2Based:\n",
    "\n",
    "gravitationalConstant: Define a força de atração global entre os nós. Valores negativos atraem os nós para o centro do grafo, enquanto valores positivos os repelem. Um valor mais negativo resultará em um grafo mais compacto, enquanto um valor mais positivo resultará em um grafo mais espalhado.\n",
    "\n",
    "centralGravity: Define a força de atração em direção ao centro do grafo. Valores maiores puxam os nós mais para o centro.\n",
    "\n",
    "springLength: Define o comprimento ideal das arestas (ligações entre os nós). Valores maiores resultam em arestas mais longas e um grafo mais espalhado.\n",
    "\n",
    "springConstant: Define a rigidez das arestas. Valores maiores tornam as arestas mais rígidas e o grafo menos flexível.\n",
    "\n",
    "damping: Controla a velocidade com que os nós se movem. Valores maiores amortecem o movimento, resultando em um layout mais estável, mas que pode levar mais tempo para convergir.\n",
    "\n",
    "avoidOverlap: Determina se os nós devem evitar a sobreposição. Um valor de 1 (verdadeiro) faz com que os nós se afastem uns dos outros para evitar sobreposição, enquanto um valor de 0 (falso) permite a sobreposição."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Benchmarking de partes da implementação</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delinear formas de melhorar a eficiência dos processos de Gestão em PDI, com base na escolha bem fundamentada em dados sobre evidências do mundo real, é uma oportunidade para melhorar tanto a efetividade como a eficiência dos processos de gestão em Pesquisa Desenvolvimento e Inovação (PDI). No âmbito das Instituições de Ciência e Tecnologia brasileiras (ICT), para propor novas tecnologias e melhorar as existentes, a gestão em PDI precisa implementar processsos computacionais de extração, tratamento, análise, visualização e apresentação de resultados para tratar a massa de dados disponível e permitir uma melhor tomada de decisão aos níveis gerenciais e executivos das ICTs. \n",
    "\n",
    "Tais necessidades e oportunidades não deve ser negligenciadas, principalmente nas economias em desenvolvimento, por representar uma forma viável para reduzir custos, e com suporte ao desenvolvimento de tecnologias nacionais, reduzir os déficits da balança comercial do País. Para aproveitar essa oportunidade, aplicar uma abordagem de benchmarking não supervisionado para modelos de aprendizagem de máquina, destinados à suportar os processos em PDI, é uma contribuição valiosa para suportar à criação de processos e aplicações mais inteligentes na tomada de decisão em PDI. \n",
    "\n",
    "Neste artigo demonstramos como comparar e selecionar modelos pré-treinados mais eficientes para gerar vetores de incorporação de textos (embeedings), de forma objetiva e sistemática, mesmo na ausência de rótulos de dados. Foram exploradas diferentes métricas de avaliação, algoritmos de clustering e técnicas de visualização para aprimorar ainda mais o processo de benchmarking e seleção de modelos, baseado em dados da realidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking de Leituras de dataframes Paralelizável"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura de dataframes com Pandas ou cuDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which python\n",
    "# !echo $PATH\n",
    "# !conda list git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install GitPython\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import cudf\n",
    "import cugraph\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from git import Repo\n",
    "\n",
    "def convert_to_dict(text):\n",
    "    try:\n",
    "        return ast.literal_eval(text)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "folder_utils = os.path.join(str(root_folder), 'utils') # type: ignore\n",
    "folder_domain = os.path.join(str(root_folder), 'source', 'domain') # type: ignore\n",
    "folder_data_input = os.path.join(str(root_folder), '_data', 'in_csv') # type: ignore\n",
    "folder_data_output = os.path.join(str(root_folder), '_data', 'out_json') # type: ignore\n",
    "filename = 'revistas_capes.csv'\n",
    "pathfilename = os.path.join(folder_data_input, filename)\n",
    "\n",
    "# read data into a Pandas DataFrame using read_csv\n",
    "start_time_cpu = time.time()\n",
    "pdf = pd.read_csv(pathfilename, header=0, delimiter=';')\n",
    "end_time_cpu = time.time()\n",
    "time_cpu = end_time_cpu - start_time_cpu\n",
    "print(f\"Tempos para gerar dataframes:\") \n",
    "print(f\"Na CPU com  Pandas: {time_cpu:>.2f} segundos\")\n",
    "\n",
    "# Apply the function to the 'detalhes' column\n",
    "# pdf['detalhes'] = pdf['detalhes'].apply(convert_to_dict)\n",
    "\n",
    "# Print the first 5 rows to verify\n",
    "# print(pdf.head()) \n",
    "\n",
    "# read data into a cuDF DataFrame using read_csv\n",
    "start_time_gpu = time.time()\n",
    "gdf = cudf.read_csv(pathfilename, header=0, delimiter=';')  # type: ignore\n",
    "qlin = len(gdf.index)\n",
    "end_time_gpu = time.time()\n",
    "time_gpu = end_time_gpu - start_time_gpu\n",
    "print(f\"Na GPU com cuGraph: {time_gpu:>.2f} segundos\")\n",
    "diff = time_cpu-time_gpu\n",
    "percent = np.round(diff/time_cpu*100,2)\n",
    "if time_gpu < time_cpu:\n",
    "    print(f\"cuGraph reduziu {percent:.2f}% do tempo do cálculo em CPU\")\n",
    "    print(f\"A leitura foi {time_cpu/time_gpu:.1f} vezes mais rápida com cuGraph\")\n",
    "else:\n",
    "    percent = np.round(-diff/time_cpu*100,2)\n",
    "    print(f\"cuGraph aumentou em {-diff:.2f} segundos o tempo de leitura do dataframe com {qlin} linhas\")\n",
    "    print(f\"A leitura foi {time_gpu/time_cpu:.1f} vezes mais lenta com cuGraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas lê arquivos CSV mais rápido que o cuDF em algumas situações, para arquivos menores ou em casos onde a sobrecarga de transferência de dados para a GPU supera o ganho de desempenho do processamento paralelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking do Cálculo de Centralidade Paralelizável"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medir tempo de cálculos de centralidade com e sem GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import cugraph as cnx\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"mensagem de aviso específica\")\n",
    "# help(cugraph.experimental)\n",
    "# help(cugraph.experimental.compat)\n",
    "# print(dir(cugraph.experimental))\n",
    "\n",
    "# Criar um grafo aleatório com 10mil nós e probabilidade de 1% de haver arestas entre cada par de nós\n",
    "print(f\"Criando grafo com 10000 nós e 10% de probabilidade de aresta entre nós...\")\n",
    "G = nx.gnp_random_graph(10000, 0.01) \n",
    "\n",
    "# Medir tempo para calcular centralidade de intermediação somente com uso de CPU\n",
    "print(f\"Iniciando cálculo de centralidade de intermediação com CPU...\")\n",
    "start_time = time.time()\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "end_time = time.time()\n",
    "time_cpu = end_time - start_time\n",
    "print(f\"  Tempos para calcular betweenness_centrality:\") \n",
    "print(f\"  Na CPU sem cuGraph: {time_cpu:>.2f} segundos\")\n",
    "\n",
    "# Medir tempo para calcular centralidade de intermediação com aceleração na GPU\n",
    "print(f\"Iniciando cálculo de centralidade de intermediação com GPU...\")\n",
    "start_time_gpu = time.time()\n",
    "betweenness_centrality_cugraph = cnx.betweenness_centrality(G)\n",
    "end_time_gpu = time.time()\n",
    "time_gpu = end_time_gpu - start_time_gpu\n",
    "\n",
    "# Mostrar os resultados comparativos\n",
    "print(f\"  Na GPU com cuGraph: {time_gpu:>.2f} segundos\")\n",
    "diff = time_cpu-time_gpu\n",
    "percent = np.round(diff/time_cpu*100,2)\n",
    "if time_gpu < time_cpu:\n",
    "    print(f\"\\ncuGraph reduziu {percent:.2f}% do tempo do cálculo em CPU\")\n",
    "    print(f\"O cálculo foi {time_cpu/time_gpu:.1f} vezes mais rápido com cuGraph\")\n",
    "else:\n",
    "    percent = np.round(-diff/time_cpu*100,2)\n",
    "    print(f\"cuGraph aumentou em {-diff:.2f} segundos o tempo de cálculo da centralidade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking da Geração de Embeedings Paralelizável"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar métricas adequadas para avaliar os resultados de clustering é crucial para determinar qual algoritmo e configuração funcionam melhor para os dados no contexto em análise. As seguintes métricas foram utilizadas para avaliar a qualidade dos clusters com os embeedings de cada modelo:\n",
    "\n",
    "Silhouette Score: avalia a qualidade dos clusters, combinando medidas de coesão (quão próximos os pontos dentro de um cluster estão uns dos outros) e separação (quão distantes os clusters estão entre si). Os score varia de -1 a 1, onde valores próximos a 1 indicam que os pontos estão bem agrupados em seus clusters e bem separados dos outros clusters, e valores próximos a 0 indicam que os pontos estão perto da fronteira entre clusters, ou que o número de clusters pode não ser ideal. Já valores próximos a -1 sugerem que os pontos podem estar atribuídos aos clusters errados.\n",
    "\n",
    "Calinski-Harabasz Index (ou Variance Ratio Criterion): mede a razão entre a dispersão entre clusters e a dispersão dentro dos clusters. Quanto maior o valor, melhor a qualidade do clustering. Ou seja, um valor alto indica que os clusters estão bem separados e densos.\n",
    "\n",
    "Davies-Bouldin Index: mede a similaridade média entre cada cluster e seu cluster mais similar. Quanto menor o valor, melhor a qualidade do clustering. Ou seja, um valor baixo indica que os clusters estão mais separados e menos dispersos.\n",
    "\n",
    "O teste de benchmarking destaca:\n",
    "\n",
    "    o maior Silhouette Score dentre os resultados para indicar uma melhor qualidade de clustering, com clusters mais coesos e separados.\n",
    "\n",
    "    os maiores valores do Calinski-Harabasz Index para cada algoritmo e configuração, pois isso sugere uma melhor separação entre os clusters.\n",
    "\n",
    "    o algoritmo e configuração com o menor Davies-Bouldin Index, pois isso indica clusters mais distintos e compactos."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cuml.metrics.cluster.silhouette_score.cython_silhouette_score(X, labels, metric='euclidean', chunksize=None, convert_dtype=True, handle=None)\n",
    "\n",
    "[fonte]\n",
    "Calcula o coeficiente de silhueta médio para os dados fornecidos.\n",
    "\n",
    "Dado um conjunto de rótulos de cluster para cada amostra nos dados fornecidos, calcula a distância média intracluster (a) e a distância média do cluster mais próximo (b) para cada amostra. O coeficiente de silhueta para uma amostra é então (b - a) / max(a, b).\n",
    "\n",
    "Parâmetros\n",
    "X\n",
    "array-like, shape = (n_samples, n_features)\n",
    "Os vetores de recursos para todas as amostras.\n",
    "\n",
    "labels\n",
    "array-like, shape = (n_samples,)\n",
    "Os rótulos de cluster atribuídos para cada amostra.\n",
    "\n",
    "metric\n",
    "string\n",
    "Uma representação de string da métrica de distância a ser usada para avaliar a pontuação da silhueta. As opções disponíveis são \"cityblock\", \"cosine\", \"euclidean\", \"l1\", \"l2\", \"manhattan\" e \"sqeuclidean\".\n",
    "\n",
    "chunksize\n",
    "inteiro (padrão = Nenhum)\n",
    "Um inteiro, 1 <= chunksize <= n_samples para agrupar os cálculos da matriz de distância em pares, de modo a reduzir o uso de memória quadrática de ter toda a matriz de distância em pares na memória da GPU. Se Nenhum, o chunksize será automaticamente definido como 40000, o que por meio de experimentos provou ser um número seguro para o cálculo ser executado em uma GPU com 16 GB de VRAM.\n",
    "\n",
    "handle\n",
    "cuml.Handle\n",
    "Especifica o cuml.handle que contém o estado CUDA interno para cálculos neste modelo. Mais importante, isso especifica o fluxo CUDA que será usado para os cálculos do modelo, para que os usuários possam executar diferentes modelos simultaneamente em diferentes fluxos, criando identificadores em vários fluxos. Se for Nenhum, um novo é criado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install -c rapidsai -c conda-forge -c nvidia nx-cugraph\n",
    "\n",
    "# %pip install sentence_transformers\n",
    "# %pip install seaborn\n",
    "# %pip install nltk\n",
    "# %pip install contextualSpellCheck\n",
    "# %pip install langdetect\n",
    "# %pip install ipywidgets\n",
    "# %pip install huggingface_hub\n",
    "# %pip install --upgrade httpcore\n",
    "# %pip install --upgrade httpx\n",
    "\n",
    "## Importações não mais usadas na classe de análise de embeedings\n",
    "# from transformers.tokenization_utils_base import TruncationStrategy\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# from transformers import pipeline, TranslationPipeline\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# from langdetect import detect\n",
    "# import contextualSpellCheck\n",
    "\n",
    "## GoogleTranslate apresentou muitos conflitos e não funcionou, optei por langdetect\n",
    "## Versão httpx==0.13.3 Para compatibilizar com GoogleTranslate, porém atrapalha openai 1.44.0 e jupyterlab 4.2.4\n",
    "# ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "# openai 1.44.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
    "# jupyterlab 4.2.4 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Escolha multicritério para modelo de Embeddings</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No contexto de aprendizado de máquina e processamento de linguagem natural, \"embeddings\" se refere à representação matemática de um objeto (como uma palavra, frase ou imagem) em um espaço vetorial.\n",
    "\n",
    "Para analisar a qualidade dos clusters criados a partir do embeeding com modelos pré-treinados foi criada a classe EmbeddingsMulticriteriaAnalysis para oferecer uma análise completa e cientificamente válida para a escolha do melhor modelo de embedding. Foram utilizadas práticas de múltiplas rodadas e validação cruzada na classe nos métodos evaluate_clustering e calcular_pontuacao_multicriterio.\n",
    "\n",
    "O método evaluate_clustering executa o processo de clustering múltiplas vezes e calcula a média e o desvio padrão das métricas. A validação cruzada é incorporada dividindo os embeddings em conjuntos de treinamento e teste.\n",
    "\n",
    "O método calcular_pontuacao_multicriterio calcula a pontuação multicritério para cada algoritmo usando as médias das métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n",
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torchvision; print(torchvision.__version__)\"\n",
    "\n",
    "# !pip uninstall torchvision\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instanciar a classe para manipular modelos da biblioteca SenteceTransformer\n",
    "from gml_embeddings_analyser import EmbeddingsMulticriteriaAnalysis\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Definir os nomes de modelo do SentenceTransformer a serem comparados\n",
    "model_names = [\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'all-MiniLM-L6-v2',\n",
    "    'all-mpnet-base-v2'\n",
    "]\n",
    "\n",
    "# Criar uma instância da classe EmbeddingsMulticriteriaAnalysis\n",
    "analise = EmbeddingsMulticriteriaAnalysis(\n",
    "    model_names=model_names,\n",
    "    models= [SentenceTransformer(model_name) for model_name in model_names]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gerar embeedings com os vários modelos e salvar\n",
    "embeddings_dict = analise.generate_embeddings_batch()\n",
    "analise.save_embeddings_dict(\"embeddings_dict_gpu.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carregar os embeddings previamente gerados\n",
    "embeddings_dict = analise.load_embeddings_dict(\"embeddings_dict_gpu.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar o conteúdo do dicionário de embeddings\n",
    "print(\"Quantidade de Modelos:\", len(embeddings_dict))\n",
    "print(\"Conteúdo de self.embeddings:\", analise.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instanciar a classe para manipular modelos da biblioteca SenteceTransformer\n",
    "from gml_embeddings_analyser import EmbeddingsMulticriteriaAnalysis\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Definir os nomes de modelo do SentenceTransformer a serem comparados\n",
    "model_names = [\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'all-MiniLM-L6-v2',\n",
    "    'all-mpnet-base-v2'\n",
    "]\n",
    "\n",
    "# Criar uma instância da classe EmbeddingsMulticriteriaAnalysis\n",
    "analise = EmbeddingsMulticriteriaAnalysis(\n",
    "    model_names=model_names,\n",
    "    models= [SentenceTransformer(model_name) for model_name in model_names]\n",
    ")\n",
    "\n",
    "## Carregar os embeddings previamente gerados\n",
    "embeddings_dict = analise.load_embeddings_dict(\"embeddings_dict_gpu.pt\")\n",
    "\n",
    "# Acessar o tensor de embeddings para o modelo 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "best_embeddings_tensor = embeddings_dict.get('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Exibir as dimensões do tensor\n",
    "if best_embeddings_tensor is not None:\n",
    "    print(\"Dimensões do tensor:\", best_embeddings_tensor.shape)\n",
    "else:\n",
    "    print(\"Modelo não encontrado no dicionário de embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Realizar a análise de clusterização\n",
    "resultados = analise.evaluate_clustering(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_clustering_results_bars(resultados):\n",
    "    \"\"\"\n",
    "    Plota os resultados do clustering usando Plotly, exibindo gráficos de barras\n",
    "    para comparar o desempenho de cada modelo em cada métrica para cada algoritmo.\n",
    "    \"\"\"\n",
    "    metrics = ['silhouette', 'calinski_harabasz', 'davies_bouldin']\n",
    "    algorithms = ['KMeans', 'DBSCAN', 'HDBSCAN']\n",
    "\n",
    "    # Ajustar o número de subplots de acordo com o número de métricas\n",
    "    fig = make_subplots(rows=1, cols=len(metrics), subplot_titles=metrics)\n",
    "\n",
    "    # Loop pelas métricas\n",
    "    for i, metric_name in enumerate(metrics):\n",
    "        # Loop pelos algoritmos\n",
    "        for j, algorithm_name in enumerate(algorithms):\n",
    "            # Obter os valores da métrica para cada modelo\n",
    "            model_values = {}\n",
    "            for model_name, model_results in resultados.items():\n",
    "                # Obter os valores da métrica para cada split\n",
    "                metric_values = [result[metric_name] for result in model_results[algorithm_name]['resultados']]\n",
    "                model_values[model_name] = np.mean(metric_values)  # Calcular a média dos splits\n",
    "\n",
    "            # Adicionar um gráfico de barras para a métrica e algoritmo, especificando a posição do subplot\n",
    "            for k, model_name in enumerate(model_values.keys()):\n",
    "                fig.add_trace(go.Bar(\n",
    "                    x=[algorithm_name],\n",
    "                    y=[model_values[model_name]],\n",
    "                    name=model_name,  # Usar o nome do modelo na legenda\n",
    "                    showlegend=i == 0 and j == 0,  # Exibe a legenda apenas no primeiro subplot\n",
    "                    legendgroup=model_name,  # Agrupa as barras por modelo\n",
    "                    offsetgroup=k,  # Define o offset para agrupar as barras por modelo\n",
    "                    marker_color=['blue', 'green'][k]  # Cores para os modelos\n",
    "                ), row=1, col=i+1)  # Especificar a linha e coluna do subplot\n",
    "\n",
    "    # Configura o layout do gráfico\n",
    "    fig.update_layout(\n",
    "        title=\"Comparação do Desempenho dos Modelos\",\n",
    "        height=600,\n",
    "        width=1200\n",
    "    )\n",
    "\n",
    "    # Ajustar os títulos dos eixos\n",
    "    for i, metric_name in enumerate(metrics):\n",
    "        fig.update_xaxes(title_text=\"Algoritmo\", row=1, col=i+1)\n",
    "        fig.update_yaxes(title_text=\"\", row=1, col=i+1)\n",
    "\n",
    "    # Adicionar anotações para indicar as melhores regiões\n",
    "    fig.add_annotation(\n",
    "        text=\"Maior Melhor\",\n",
    "        x=0.23, y=1,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        showarrow=True, arrowhead=4,\n",
    "        ax=0, ay=30\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        text=\"Maior Melhor\",\n",
    "        x=0.59, y=1,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        showarrow=True, arrowhead=4,\n",
    "        ax=0, ay=30\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        text=\"Menor Melhor\",\n",
    "        x=0.77, y=0.9,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        showarrow=True, arrowhead=4,\n",
    "        ax=0, ay=-30\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretação das Métricas\n",
    "\n",
    "    Silhouette score [-1 a 1], quanto maior melhor a qualidade de clustering, clusters coesos e separados\n",
    "    Calinski_Harabasz quanto maior melhor, sugere uma melhor separação\n",
    "    Davies-Bouldin quanto menor melhor,  indica clusters mais distintos e compactos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_results_bars(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for i,j in resultados.items():\n",
    "    print(i)\n",
    "    for k,l in j.items():\n",
    "        print(k)\n",
    "        for m,n in l.items():\n",
    "            print(f\"  {m}: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolher o melhor modelo\n",
    "melhor_modelo = analise.escolher_melhor_modelo(resultados)\n",
    "print(f\"O melhor modelo é: {melhor_modelo}\")\n",
    "\n",
    "# Gerar o relatório de benchmarking (opcional)\n",
    "analise.generate_report(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "template_folder = \"source/template/\"\n",
    "os.listdir(os.path.join(str(root_folder),template_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarking_reports import BenchmarkReportGenerator\n",
    "try:\n",
    "    # Criar uma instância do BenchmarkReportGenerator\n",
    "    report_generator = BenchmarkReportGenerator(\"benchmark_report.html\") \n",
    "\n",
    "    # Gerar o relatório HTML\n",
    "    report_generator.generate_beckmarking_clustering_report(resultados, melhor_modelo)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar relatório: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(template_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montagem Grafo de Conhecimento Currículo - Fomento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from git import Repo\n",
    "\n",
    "# Acessar o tensor de embeddings para o modelo 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "editais_embeddings = embeddings_dict.get('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Carregar os currículos\n",
    "filename = \"input_curriculos.json\"\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "pathfilename = os.path.join(str(root_folder), \"_data\", \"out_json\",filename)\n",
    "with open(pathfilename, \"r\", encoding='utf-8') as f:\n",
    "    curriculos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,curriculo in enumerate(curriculos):\n",
    "    print([x.get('Descrição') for x in curriculos[n].get('Linhas de Pesquisa')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,curriculo in enumerate(curriculos):\n",
    "    print([x.get('titulo') for x in curriculos[n].get('Produções').get('Artigos completos publicados em periódicos')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculos[1].get('Áreas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculos[1].get('Áreas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar embeedings para currículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Instanciar o modelo de linguagem (use o mesmo modelo usado para os editais)\n",
    "modelo = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # Substitua pelo seu modelo\n",
    "\n",
    "# Extrair o texto dos currículos\n",
    "curriculos_texto = []\n",
    "for curriculo in curriculos:\n",
    "    texto = \"\"\n",
    "    for key, value in curriculo.items():\n",
    "        if isinstance(value, str):\n",
    "            texto += value + \" \"\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, str):\n",
    "                    texto += item + \" \"\n",
    "                elif isinstance(item, dict):\n",
    "                    for subkey, subvalue in item.items():\n",
    "                        texto += str(subvalue) + \" \"\n",
    "    curriculos_texto.append(texto)\n",
    "\n",
    "# Gerar os embeddings dos currículos\n",
    "curriculos_embeddings = modelo.encode(curriculos_texto, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a similaridade do cosseno entre os embeddings dos editais e dos currículos\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similaridade = cosine_similarity(editais_embeddings.cpu().numpy(), curriculos_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o grafo de conhecimento\n",
    "import networkx as nx\n",
    "\n",
    "# Criar um grafo direcionado\n",
    "grafo = nx.DiGraph()\n",
    "\n",
    "# Adicionar nós para os editais\n",
    "for i, edital_embedding in enumerate(editais_embeddings):\n",
    "    grafo.add_node(f\"edital_{i}\")\n",
    "\n",
    "# Adicionar nós para os currículos\n",
    "for i, curriculo_embedding in enumerate(curriculos_embeddings):\n",
    "    grafo.add_node(f\"curriculo_{i}\")\n",
    "\n",
    "# Adicionar arestas com base na similaridade (limiar de 0.25)\n",
    "limiar_similaridade = 0.25\n",
    "for i, edital_embedding in enumerate(editais_embeddings):\n",
    "    for j, curriculo_embedding in enumerate(curriculos_embeddings):\n",
    "        if similaridade[i, j] > limiar_similaridade:\n",
    "            grafo.add_edge(f\"edital_{i}\", f\"curriculo_{j}\", weight=similaridade[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar as comunidades (utilizando o algoritmo Louvain)\n",
    "\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "# Encontrar as comunidades\n",
    "comunidades = list(greedy_modularity_communities(grafo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comunidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exibir o grafo com Pyvis\n",
    "# from pyvis.network import Network\n",
    "\n",
    "# # Criar uma rede PyVis\n",
    "# net = Network(notebook=True, directed=True)\n",
    "\n",
    "# # Adicionar nós e arestas ao grafo PyVis\n",
    "# net.from_nx(grafo)\n",
    "\n",
    "# # Configurar a cor dos nós por comunidade\n",
    "# for i, comunidade in enumerate(comunidades):\n",
    "#     cor = f\"hsl({i * 360 / len(comunidades)}, 100%, 50%)\"\n",
    "#     for no in comunidade:\n",
    "#         net.get_node(no)[\"color\"] = cor\n",
    "\n",
    "# # Exibir o grafo\n",
    "# net.show(\"grafo_conhecimento.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomendar editais\n",
    "def recomendar_editais(curriculo_id, grafo, similaridade, top_n=5):\n",
    "    \"\"\"\n",
    "    Recomenda os editais mais propícios para um currículo com base no grafo de conhecimento.\n",
    "    \"\"\"\n",
    "    editais_similares = []\n",
    "    for no in grafo.neighbors(curriculo_id):\n",
    "        editais_similares.append((no, grafo.get_edge_data(curriculo_id, no)['weight']))\n",
    "    editais_similares.sort(key=lambda x: x[1], reverse=True)\n",
    "    return editais_similares[:top_n]\n",
    "\n",
    "# Exemplo de recomendação para o currículo 0\n",
    "curriculo_id = \"curriculo_0\"\n",
    "editais_recomendados = recomendar_editais(curriculo_id, grafo, similaridade)\n",
    "print(f\"Editais recomendados para o currículo {curriculo_id}: {editais_recomendados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montagem Grafo de Conhecimento Artigos - Fomento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerar embeeding de Artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 05:36:41,546 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-09 05:36:41,546 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2024-11-09 05:36:44,907 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-09 05:36:44,907 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2024-11-09 05:36:46,750 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-09 05:36:46,751 - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "    Modelo de Base: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Tarefas aplicáveis: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Número de features: 384\n",
      "Detalhes da classe: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "    Modelo de Base: sentence-transformers/all-MiniLM-L6-v2\n",
      "Tarefas aplicáveis: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Número de features: 384\n",
      "Detalhes da classe: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "    Modelo de Base: sentence-transformers/all-mpnet-base-v2\n",
      "Tarefas aplicáveis: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Número de features: 768\n",
      "Detalhes da classe: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Arquivo de embeddings carregado: embeddings_dict_gpu.pt\n",
      "Número de modelos carregados: 2\n",
      "\n",
      "Dimensões do tensor: torch.Size([346, 384])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from googletrans import Translator\n",
    "from json import JSONDecodeError\n",
    "from git import Repo\n",
    "\n",
    "## Instanciar a classe para manipular modelos da biblioteca SenteceTransformer\n",
    "from gml_embeddings_analyser import EmbeddingsMulticriteriaAnalysis\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Definir os nomes de modelo do SentenceTransformer a serem comparados\n",
    "model_names = [\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'all-MiniLM-L6-v2',\n",
    "    'all-mpnet-base-v2'\n",
    "]\n",
    "\n",
    "# Criar uma instância da classe EmbeddingsMulticriteriaAnalysis\n",
    "analise = EmbeddingsMulticriteriaAnalysis(\n",
    "    model_names=model_names,\n",
    "    models= [SentenceTransformer(model_name) for model_name in model_names]\n",
    ")\n",
    "\n",
    "## Carregar os embeddings previamente gerados\n",
    "embeddings_dict = analise.load_embeddings_dict(\"embeddings_dict_gpu.pt\")\n",
    "\n",
    "best_model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "print(f\"Modelo escolhido para gerar embeedings: {best_model_name}\")\n",
    "\n",
    "# Acessar o tensor de embeddings gerados com modelo 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "editais_embeddings = embeddings_dict.get(best_model_name)\n",
    "\n",
    "# Exibir as dimensões do tensor\n",
    "if editais_embeddings is not None:\n",
    "    print(\"Dimensões do tensor:\", editais_embeddings.shape)\n",
    "else:\n",
    "    print(\"Tensor com embeddings de editais não encontrado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kgnn import KGNN\n",
    "\n",
    "# Criar uma instância do KGNN\n",
    "kgnn = KGNN(\n",
    "    embedding_model_name=\"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    neo4j_uri=\"bolt://localhost:7687\",\n",
    "    neo4j_user=\"neo4j\",\n",
    "    neo4j_password=\"password\"\n",
    ")\n",
    "\n",
    "# Carregar os currículos\n",
    "filename = \"input_curriculos.json\"\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "pathfilename = os.path.join(str(root_folder), \"_data\", \"out_json\",filename)\n",
    "with open(pathfilename, \"r\", encoding='utf-8') as f:\n",
    "    curriculos = json.load(f)\n",
    "\n",
    "# Criar subgrafos para cada um dos currículos\n",
    "for curriculo in curriculos:\n",
    "    try:\n",
    "        subgrafo = kgnn.criar_subgrafo_curriculo(curriculo)\n",
    "        kgnn.ingerir_subgrafo(subgrafo)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Identificação': {'Nome': 'Ana Cláudia de Araújo Teixeira',\n",
       "  'ID Lattes': '4719434295908748',\n",
       "  'Última atualização': '05/07/2024'},\n",
       " 'Idiomas': [{'Idioma': 'Inglês',\n",
       "   'Proficiência': 'Compreende Razoavelmente, Fala Pouco, Lê Bem, Escreve Pouco.'},\n",
       "  {'Idioma': 'Espanhol',\n",
       "   'Proficiência': 'Compreende Bem, Fala Pouco, Lê Bem, Escreve Pouco.'}],\n",
       " 'Formação': {'Acadêmica': [{'Ano': '2004 - 2008',\n",
       "    'Descrição': 'Doutorado em Educação Brasileira. Universidade Federal do Ceará, UFC, Brasil. Título: O trabalho no mangue nas tramas do (des)envolvimento e da des(ilusão) com esse furacão chamado, Ano de obtenção: 2008. Orientador: Ângela Maria Bessa Linhares. Coorientador: Raquel Maria Rigotto. Bolsista do(a): Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES, Brasil. Palavras-chave: Trabalno no mangue; carcinicultura; conflito socioambiental. Grande área: Ciências Humanas Grande Área: Ciências Humanas / Área: Educação / Subárea: Tópicos Específicos de Educação.'},\n",
       "   {'Ano': '1995 - 1998',\n",
       "    'Descrição': 'Mestrado em Saúde Pública. Universidade Federal do Ceará, UFC, Brasil. Título: Adesão ao Tratamento Farmacológico da Hipertensão Arterial e seus Determinantes em Pacientes de Ambulatórios, Ano de Obtenção: 1998. Orientador: Helena Lutéscia Luna Coelho. Bolsista do(a): Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES, Brasil. Palavras-chave: estudo de adesão; hipertensão; utilização de medicamentos. Grande área: Ciências da Saúde'},\n",
       "   {'Ano': '2010 - 2011',\n",
       "    'Descrição': 'Especialização em Formação Docente na Área de Vigilância da Saúde.  (Carga Horária: 360h). Escola Nacional de Saúde Pública / Fundação Oswaldo Cruz, ENSP/FIOCRUZ, Brasil. Título: A formação em vigilância da saúde de base territorial local: aportes conceituais e metodológicos. Orientador: Fernando Ferreira Carneiro.'},\n",
       "   {'Ano': '1992 - 1992',\n",
       "    'Descrição': 'Aperfeiçoamento em Política e Administração de Medicamentos.  (Carga Horária: 200h). Secretaria de Saúde do Estado e Núcleo de Estudos Em Saúde Coletiva, SESA - NESC, Brasil.\\n\\t\\t\\n\\tAno de finalização: 1992.'},\n",
       "   {'Ano': '1987 - 1991',\n",
       "    'Descrição': 'Graduação em Farmácia. Universidade Federal do Ceará, UFC, Brasil.'}],\n",
       "  'Pos-Doc': [{'Ano': '2010 - 2013',\n",
       "    'Descrição': 'Pós-Doutorado. Universidade Federal do Ceará, UFC, Brasil. Bolsista do(a): Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES, Brasil. Grande área: Ciências da Saúde Grande Área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Produção, Ambiente e Saúde.'}],\n",
       "  'Complementar': [{'Ano': '2021 - 2021',\n",
       "    'Descrição': 'Evento Educacional - Primavera Paulo Freire.  (Carga horária: 24h). Fundação Oswaldo Cruz, FIOCRUZ, Brasil.'},\n",
       "   {'Ano': '2009 - 2009',\n",
       "    'Descrição': 'Dezenho de Currículo Baseado em Competência.  (Carga horária: 80h). Escola de Saúde Pública do Ceará, ESP/CE, Brasil.'},\n",
       "   {'Ano': '2008 - 2008',\n",
       "    'Descrição': 'Curso Sistema de Acompanhamento de Programas e Pro.  (Carga horária: 20h). Escola de Saúde Pública do Ceará, ESP/CE, Brasil.'},\n",
       "   {'Ano': '2006 - 2006',\n",
       "    'Descrição': 'Formação de Formadores em Saúde do Trabalhador-ABS.  (Carga horária: 60h). Centro de Referência em Saúde do Trabalhador, CEREST, Brasil.'},\n",
       "   {'Ano': '2005 - 2005',\n",
       "    'Descrição': 'Capacitação em Direitos e Saúde do Trabalhador.  (Carga horária: 40h). Universidade Federal do Ceará, UFC, Brasil.'},\n",
       "   {'Ano': '2002 - 2002',\n",
       "    'Descrição': 'Curso Básico de Sistema de Informação Geográfica.  (Carga horária: 80h). Escola de Saúde Pública do Ceará, ESP/CE, Brasil.'},\n",
       "   {'Ano': '2000 - 2000',\n",
       "    'Descrição': 'Atualiz das Ações de Vig Epid p/Erradic.  (Carga horária: 32h). Secretaria de Saúde do Estado do Ceará, SESA, Brasil.'},\n",
       "   {'Ano': '1999 - 1999',\n",
       "    'Descrição': 'Curso de Atualização Farmacêutica.  (Carga horária: 20h). Secretaria de Saúde do Estado do Ceará, SESA, Brasil.'},\n",
       "   {'Ano': '1999 - 1999',\n",
       "    'Descrição': 'Introd à Aprendizagem Baseada em Problemas-PBL.  (Carga horária: 16h). Escola de Saúde Pública do Ceará, ESP/CE, Brasil.'},\n",
       "   {'Ano': '1998 - 1998',\n",
       "    'Descrição': 'Metodologia Qualitativa. Universidade Federal do Ceará Departamento de Enfermagem, UFC - DE, Brasil.'},\n",
       "   {'Ano': '1996 - 1996',\n",
       "    'Descrição': 'Curso de Comunicação Radiofônica.  (Carga horária: 12h). Rádio Extra Comunicação, REC, Brasil.'},\n",
       "   {'Ano': '1996 - 1996',\n",
       "    'Descrição': 'Ensaio Clínico.  (Carga horária: 8h). Universidade Federal do Ceará e Grupo de Prevenção Ao Uso Indevido de Medic, UFC/GPUIM, Brasil.'},\n",
       "   {'Ano': '1996 - 1996',\n",
       "    'Descrição': 'Farmacovigilância.  (Carga horária: 8h). Universidade Federal do Ceará e Grupo de Prevenção Ao Uso Indevido de Medic, UFC/GPUIM, Brasil.'},\n",
       "   {'Ano': '1996 - 1996',\n",
       "    'Descrição': 'Curso Básico de Epidemiologia Clínica.  (Carga horária: 40h). Escola de Saúde Pública do Ceará, ESP/CE, Brasil.'},\n",
       "   {'Ano': '1993 - 1993',\n",
       "    'Descrição': 'Curso de Dispensação Farmacêutica.  (Carga horária: 12h). Conselho Regional de Farmácia, CRF, Brasil.'},\n",
       "   {'Ano': '1992 - 1992',\n",
       "    'Descrição': 'Inglês Instrumental.  (Carga horária: 100h). Casa de Cultura Britânica, CCB, Brasil.'}]},\n",
       " 'Atuação Profissional': [{'Instituição': 'Fundação Oswaldo Cruz - Fiocruz Ceará, FIOCRUZ CEARÁ, Brasil.',\n",
       "   'Ano': '2015 - Atual',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Servidor Público, Enquadramento Funcional: Pesquisadora em Saúde Pública, Carga horária: 40'},\n",
       "  {'Instituição': 'Fundação Oswaldo Cruz, FIOCRUZ, Brasil.',\n",
       "   'Ano': '2015 - Atual',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Servidor Público, Enquadramento Funcional: Pesquisadora em Saúde Pública, Carga horária: 40 Pesquisadora em Saúde Pública - Área de Saúde e Ambiente\\n\\nEixos estruturantes da área de Saúde e Ambiente:\\n\\na) Saúde, Saneamento, Água e Direitos Humanos; Impacto de grandes empreendimentos na saúde; b) Atenção Primária em Saúde, Ambiente e Trabalho; c) Novas metodologias de pesquisa ? crítico/dialógicas, epidemiologia crítica, ecologia de saberes; d) Formas sustentáveis de usufruto do território e sua relação com a saúde: Agroecologia, pesca artesanal e extrativismo\\n\\n\\u200bNa Fiocruz focaliza a estruturação da A Rede Saúde, Saneamento, Água e Direitos Humanos (RESSADH) no Semiárido foi criada em março de 2017 [1] a partir de uma articulação da Fiocruz Ceará em parceria com a Secretaria de Saúde do Estado (SESA-CE), o Instituto Federal do Ceará (IFCE), a Caritas Brasileira Regional Ceará, o Movimento dos Trabalhadores Rurais Sem Terra (MST) e o Centro de Estudos do Trabalho e de Assessoria ao Trabalhador (Cetra). \\u200bA RESSADH se propõe a atuar em formação, pesquisa e cooperação de modo articulado.\\n\\u200b\\u200b[1] https://portal.fiocruz.br/pt-br/content/rede-sobre-saude-agua-e-direitos-humanos-e-criada-no-ceara'},\n",
       "  {'Instituição': 'Fundação Oswaldo Cruz, FIOCRUZ, Brasil.',\n",
       "   'Ano': '08/2021 - 08/2021',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Mestrado Profissional em Saúde Pública - Fiocruz Pernambuco, Nível: Pós-Graduação Disciplinas ministradas resultado do Projeto Emergência em Saúde Pública por desastres no Sistema Único de Saúde: inundações graduais na Região Amazônica e seca e estiagem no  Semiárido,'},\n",
       "  {'Instituição': 'Fundação Oswaldo Cruz, FIOCRUZ, Brasil.',\n",
       "   'Ano': '05/2021 - 05/2021',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Programa de Pós Graduação em Políticas Públicas em Saúde, Nível: Pós-Graduação Disciplinas ministradas Disciplina Vigilância em Saúde - Colaboradora'},\n",
       "  {'Instituição': 'Fundação Oswaldo Cruz, FIOCRUZ, Brasil.',\n",
       "   'Ano': '02/2021 - 03/2021',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Mestrado em Saúde da Família - RENASF, Nível: Pós-Graduação Disciplinas ministradas Módulo de Promoção da Saúde Módulo de Educação em Saúde'},\n",
       "  {'Instituição': 'Fundação Oswaldo Cruz, FIOCRUZ, Brasil.',\n",
       "   'Ano': '01/2017 - 12/2017',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Mestrado em Saúde da Família - RENASF, Nível: Pós-Graduação Disciplinas ministradas Módulo Educação em Saúde Módulo Promoção da Saúde'},\n",
       "  {'Instituição': 'Universidade de Brasília, UnB, Brasil.',\n",
       "   'Ano': '2013 - 2015',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Professor Visitante, Enquadramento Funcional: Professora Visitante, Regime: Dedicação exclusiva. Professora Visitante da área Epidemiologia, Ambiente e Trabalho/Departamento de Saúde Coletiva/Faculdade de Ciências da Saúde/Universidade de Brasília, desde setembro de 2013.\\nAtua como docente nas Disciplinas Ambiente, Saúde e Trabalho, Fundamentos Biológicos I, Seminários Integradores em Saúde Coletiva I, Práticas Integradas em Saúde Coletiva I e Estágio Supervisionado do Curso de Graduação em Gestão em Saúde Coletiva.\\n\\nAtuou como coordenadora e docente da disciplina Saúde, Ambiente e Trabalho (Carga Horária de 30 horas) do Curso de Especialização em Políticas Públicas e Gestão Participativa, oferecido pelo Departamento de Saúde Coletiva em parceria com o Ministério da Saúde, no período de 3 a 7 de novembro de 2014.'},\n",
       "  {'Instituição': 'Universidade de Brasília, UnB, Brasil.',\n",
       "   'Ano': '2013 - 2015',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Colaborador, Enquadramento Funcional: Pesquisadora, Carga horária: 20 Pesquisadora do grupo de pesquisa Observatório da Política de Saúde Integral das Populações do Campo, da Floresta e das Águas - Teia de Ecologia de Saberes e Práticas (OBTEIA), cadastrado no diretório do CNPq, coordenado por Fernando Ferreira Carneiro e Vanira Matos Pessoa.\\nO OBTEIA criou uma estrutura diferente para a maioria dos Observatórios Acadêmicos. Em todos os seus níveis de organização, do comitê gestor, ao grupo executivo e a Teia de Saberes e Práticas conta-se com a participação de acadêmicos, representantes de movimentos sociais e do SUS. Em todas as instâncias são realizados debates teóricos e políticos com implicações práticas. Essa forma de existir se reflete nas oficinas, e por meio de todos esses espaços as decisões são tomadas e implementadas. O próximo desafio para o OBTEIA é desenvolver pesquisas de campo que reflitam essa filosofia. O grupo de pesquisa tem alcançado uma ampla repercussão de seu trabalho em função de seu portal (www.saudecampofloresta.unb.br), publicação de artigos científicos, livros, cartilhas e boletins informativos semanais e mensais distribuídos pela internet. \\n\\nLinhas de Pesquisa:\\n? Ecologia de Saberes\\n? Impactos dos Agrotóxicos\\n? Saúde das populações do campo, floresta e das águas\\n\\nIntegrantes do grupo de pesquisa:\\n\\nAlan Freihof Tygel, Ana Cláudia de Araújo Teixeira, Ana Cássia Ferreira Firmo, Antonio da Silva Matos, Barbara Lyrio Ursine, Bernardo Amaral Vaz, Carlos André Moura Arruda, Cheila Nataly Galindo Bedor, Cleber Adriano Rodrigues Folgado, Dagmar Olmo Talga, Fábio André Diniz Merladet, Fernando Ferreira Carneiro (coordenador) Izabela Almeida de Souza, Juliana Acosta Santorum, Marciano Toledo da Silva, Maria dos Anjos Nunes da Silva, Mariane Emanuelle da Silva Lucena, Murilo Mendonça Oliveira de Souza, Rackynelly Alves Sarmento Soares, Rosana Kirsch, Vanira Matos Pessoa (coordenadora) Vicente Eduardo Soares de Almeida'},\n",
       "  {'Instituição': 'Universidade de Brasília, UnB, Brasil.',\n",
       "   'Ano': '2013 - 2015',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Servidor Público, Enquadramento Funcional: Pesquisadora, Regime: Dedicação exclusiva.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2011 - 2013',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Colaborador, Enquadramento Funcional: Professora Colaboradora Membro do colegiado da Pós-Graduação em Saúde Coletiva - Mestrado Acadêmico em Saúde Pública - com atividades de docência, pesquisa e extensão universitária. \\n\\nOrientações realizadas no âmbito da linha de pesquisa Territorialização em saúde e as relações com o trabalho e o ambiente na atenção primária:\\n\\nPeríodo de março de 2011 a agosto de 2013 - mestrando Pablo Araújo Alves, com a dissertação intitulada ?Vigilância Popular da Saúde: cartografia dos riscos e vulnerabilidades socioambientais no contexto de implantação da mineração de urânio e fosfato no Ceará?, apresentada em 26 de agosto de 2013. \\n\\nPeríodo de março de 2012 a agosto de 2013 - mestranda Manoela Cavalcanti Frota com projeto de dissertação de mestrado sobre a temática ?A construção social do indivíduo e sua reinterpretação do ?ser saúde? em contexto de conflito ambiental relacionado à implantação da mineração de urânio e fosfato no Ceará?\\n\\nPeríodo de março a agosto de 2013 - mestranda Danielli da Silva Costa com projeto de dissertação de mestrado sobre a temática ?A constituição de processos de resistências em territórios que vivenciam conflitos ambientais relacionados à implantação de projetos de desenvolvimentos no Ceará?'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2010 - 2013',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista recém-doutor, Enquadramento Funcional: Bolsista PRODOC/CAPES, Regime: Dedicação exclusiva. Desenvolve o projeto Territorialização em Saúde: estudo das relações produção, ambiente, saúde e cultura na atenção primária à saúde no âmbito da Linha de Pesquisa Produção, Ambiente, Saúde e Cultura do Programa de Pós-graduação em Saúde Coletiva da Universidade Federal do Ceará. Edital MEC/PRODOC/CAPES 029/2010\\n\\nProfessora Colaboradora da Gradução em Medicina - Disciplina Saúde, Trabalho, Ambiente e Cultura\\n\\nProfessora Colaboradora da Pós-graduação em Saúde Pública - Disciplinas: Produção, Ambiente e Saúde e Tópicos Avançados em Saúde do Trabalhador e Saúde Ambiental \\nPesquisadora do Núcleo TRAMAS - Trabalho, Meio Ambiente e Saúde'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2010 - 2013',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Colaborador, Enquadramento Funcional: Professora Colaboradora, Carga horária: 20 Professora colaboradora da Disciplina Saúde, Trabalho, Ambiente e Cultura ministrada no 3º. Semestre da graduação em Medicina, Departamento de Saúde Comunitária, Faculdade de Medicina, Universidade Federal do Ceará.\\nAtividades desenvolvidas: - planejamento da disciplina (elaboração do programa, seleção de bibliografia, organização), participação das atividades didáticas ? aulas teóricas e em campo, - supervisão dos alunos em aulas em campo, - orientação dos alunos na preparação dos temas de seminários, - elaboração de avaliações parciais.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2004 - 2013',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Pesquisadora, Enquadramento Funcional: Pesquisadora do Núcleo TRAMAS, Carga horária: 20 Núcleo TRAMAS - Trabalho, Meio Ambiente e Saúde/Departamento de Saúde Comunitária/Faculdade de Medicina\\n\\nAs principais contribuições do grupo de pesquisa se inserem no campo da Saúde Coletiva, enfocando o campo das relações Produção-Trabalho, Ambiente e Saúde, as quais são abordadas numa perspectiva crítica no contexto da civilização do capital, especialmente em suas formas de expressão no Nordeste do Brasil, no Ceará. No intuito de contribuir para a construção de uma ciência emancipatória, o grupo tem buscado desenvolver, na dinâmica relação com os territórios e seus sujeitos e no diálogo com outros pesquisadores, perspectivas epistemológicas e abordagens metodológicas que inovem na superação dos limites da ciência moderna visando formular novas bases para o saber. Desse modo, articulando formação, pesquisa e cooperação social, e com o intuito de dialogar com a complexidade dos problemas derivados dos processos e projetos de desenvolvimento, o grupo tem optado em atender demandas de produção de conhecimento advindas de comunidades em conflito ambiental, que disputam seus territórios com empreendimentos produtivos ou obras de infraestrutura, para contribuir na defesa da equidade ambiental. A partir desta perspectiva coloca-se também a contribuição com a construção de políticas públicas que venham a garantir o direito de todos à saúde, ao trabalho digno e ao ambiente. Por fim, o grupo tem contribuído com a formação de pessoas (estudantes de graduação e pós-graduação, profissionais de saúde, grupos sociais) na área de Saúde Coletiva, com ênfase nas relações Produção-Trabalho, Ambiente e Saúde, bem como promovido espaços públicos de debate sobre as questões pertinentes a esse campo.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2012 - 2012',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Colaborador, Enquadramento Funcional: Professora Colaboradora, Carga horária: 20 Professora colaboradora da disciplina Tópicos Avançados em Saúde do Trabalhador e Saúde Ambiental sobre o tema Ecologia de saberes para a promoção da equidade ambiental e da saúde, coordenada pela Profa. Raquel Maria Rigotto, ministrada no Programa de Pós-Graduação em Saúde Coletiva ? Mestrado Acadêmico em Saúde Pública ? , Departamento de Saúde Comunitária, Faculdade de Medicina da Universidade Federal do Ceará.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2012 - 2012',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Colaborador, Enquadramento Funcional: Professora Colaboradora, Carga horária: 20 Professora colaboradora da disciplina Produção, Ambiente e Saúde, coordenada pela Profa. Raquel Maria Rigotto, ministrada no Programa de Pós-Graduação em Saúde Coletiva ? Mestrado Acadêmico em Saúde Pública ? , Departamento de Saúde Comunitária, Faculdade de Medicina da Universidade Federal do Ceará.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2010 - 2010',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Colaborador, Enquadramento Funcional: Professora Colaboradora, Carga horária: 20 Professora colaboradora da disciplina Produção, Ambiente e Saúde, coordenada pela Profa. Raquel Maria Rigotto, ministrada no Programa de Pós-Graduação em Saúde Coletiva ? Mestrado Acadêmico em Saúde Pública ?, Departamento de Saúde Comunitária, Faculdade de Medicina da Universidade Federal do Ceará.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2010 - 2010',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Colaborador, Enquadramento Funcional: Professora Colaboradora, Carga horária: 20 Tópicos Avançados em Saúde do Trabalhador e Saúde Ambiental sobre o tema Mineração de urânio no Ceará: equidade ambiental, políticas de saúde e direito de saber, coordenada pela Profa. Raquel Maria Rigotto, ministrada no Programa de Pós-Graduação em Saúde Coletiva ? Mestrado Acadêmico em Saúde Pública ?, Departamento de Saúde Comunitária, Faculdade de Medicina da Universidade Federal do Ceará.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2009 - 2010',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: , Enquadramento Funcional: Professora Substituta, Carga horária: 20 Professora substituta da Disciplina Saúde, Trabalho, Ambiente e Cultura ministrada no 3º. Semestre da graduação em Medicina, Departamento de Saúde Comunitária, Faculdade de Medicina, Universidade Federal do Ceará.\\nAtividades desenvolvidas: - planejamento da disciplina (elaboração do programa, seleção de bibliografia, organização), participação das atividades didáticas ? aulas teóricas e em campo, - supervisão dos alunos em aulas em campo, - orientação dos alunos na preparação dos temas de seminários, - elaboração de avaliações parciais, - elaboração de projetos: Monitoria e Programa de Educação pelo Trabalho para a Saúde (PET Saúde).'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2009 - 2009',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Colaborador, Enquadramento Funcional: Professora Colaboradora, Carga horária: 20 Tópicos Avançados em Saúde do Trabalhador e Saúde Ambiental sobre o tema Conflitos Socioambientais e a Saúde-Doença no Ceará: processos de produção de conhecimento na relação Universidade-Movimentos Sociais, coordenada pela Profa. Raquel Maria Rigotto, ministrada no Programa de Pós-Graduação em Saúde Coletiva ? Mestrado Acadêmico em Saúde Pública ?, Departamento de Saúde Comunitária, Faculdade de Medicina da Universidade Federal do Ceará.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2004 - 2008',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista, Enquadramento Funcional: Bolsista de Doutorado - CAPES, Regime: Dedicação exclusiva. Tese de doutorado: O trabalho no mangue nas tramas do (des)envolvimento e da des(ilusão) com esse furacão chamado \"carcinicultura\": conflito socioambiental no Cumbe-Aracati-CE\\n\\nBolsa: Coordenação de Aperfeiçoamento de Pessoal de Nível Superior'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '1996 - 1998',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista, Enquadramento Funcional: Bolsista de Mestrado - CAPES, Regime: Dedicação exclusiva. Dissertação de Mestrado: Adesão ao Tratamento Farmacológico da Hipertensão Arterial e seus Determinantes em Pacientes de Ambulatórios\\n\\nBolsa: Coordenação de Aperfeiçoamento de Pessoal de Nível Superior'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '1995 - 1995',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Servidor Público, Enquadramento Funcional: Professora Substituta, Carga horária: 20'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '1995 - 1995',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Sipervisor, Enquadramento Funcional: Supervisor de Campo Supervisor de Campo do Estágio Curricular Supervisionado em Farmácia.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '1994 - 1994',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Supervisor, Enquadramento Funcional: Supervisor de Campo Supervisor de Campo do Estágio Curricular Supervisionado em Farmácia'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '1993 - 1994',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista, Enquadramento Funcional: Bolsista de Iniciação Científica, Carga horária: 20 Projetos de pesquisa \\nPrevalência do uso de medicamentos por mulheres em idade fértil'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '1992 - 1993',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista, Enquadramento Funcional: Bolsista de Iniciação Científica, Carga horária: 20 Projeto de pesquisa Misoprostol como Abortivo: a experiência das mulheres de Fortaleza'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '1991 - 1992',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Monitoria, Enquadramento Funcional: Monitora Monitora das disciplinas de farmacodinamica , fisiologia e farmacologia'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '1990 - 1990',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista de Inic Científica, Enquadramento Funcional: Bolsista de Iniciação Científica, Carga horária: 20'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '1989 - 1989',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista de Inic Científica, Enquadramento Funcional: Bolsista de Iniciação  Científica, Carga horária: 20'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '1989 - 1989',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista, Enquadramento Funcional: Bolsista de Iniciação Científica, Carga horária: 20 Bolsista de Iniciação Científica do Grupo de Prevenção ao Uso Indevido de Medicamentos - núcleo de ensino, pesquisa e extensão da Universidade Federal do Ceará.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '12/2012 - Atual',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Pesquisa e desenvolvimento, Faculdade de Medicina  / Departamento de Saúde Comunitária / Núcleo TRAMAS. Linhas de pesquisa Desenvolvimento, Injustiça Ambiental e Saúde: o caso da Mineração de Urânio e Fosfato no Ceará'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2004 - 10/2013',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Outras atividades técnico-científicas , Núcleo TRAMAS - Trabalho, Meio Ambiene e Saúde/DSC/FAMED/UFC, Núcleo TRAMAS - Trabalho, Meio Ambiene e Saúde/DSC/FAMED/UFC. Atividade realizada Desenho do curriculo do Curso de Especialização em Saúde do Trabalhador e Saúde Ambiental.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '11/2010 - 06/2013',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Medicina, Nível: Graduação Disciplinas ministradas Saúde, Trabalho, Ambiente e Cultura'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2004 - 06/2013',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Extensão universitária , Núcleo TRAMAS - Trabalho Meio Ambiente e Saúde para a Sustentabilidade. Atividade de extensão realizada Participou representando o TRAMAS em atividades de extensão junto aos movimentos socioambientais qua fazem parte do Fórum em Defesa da Zona Costeira do Ceará (GT - Aquicultura); GT - Combate ao Racismo Ambiental da Rede Brasileira de Justiça Ambiental.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '10/2012 - 12/2012',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Mestrado em Saúde Pública, Nível: Pós-Graduação Disciplinas ministradas Tópicos Avançados em Saúde do Trabalhador e Saúde Ambiental - Ecologia de saberes para a promoção da equidade ambiental e da saúde'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '05/2012 - 06/2012',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Mestrado em Saúde Pública, Nível: Pós-Graduação Disciplinas ministradas Produção, Ambiente e Saúde'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '06/2011 - 12/2011',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Pós-Graduação em Saúde Pública, Nível: Pós-Graduação Disciplinas ministradas Políticas de Saúde'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '06/2010 - 12/2010',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Pós-Graduação em Saúde Pública, Nível: Pós-Graduação Disciplinas ministradas Tópicos Avançados em Saúde Ambiental e Saúde do Trabalhador - Mineração de urânio no Ceará: equidade ambiental, políticas de saúde e direito de saber'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '01/2010 - 06/2010',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Pós-Graduação em Saúde Pública, Nível: Pós-Graduação Disciplinas ministradas Produção, Ambiente e Saúde'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '06/2009 - 12/2009',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Pós-Graduação em Saúde Pública, Nível: Pós-Graduação Disciplinas ministradas Tópicos Avançados em Saúde Ambiental e Saúde do Trabalhador - Conflitos Sócio-Ambientais e a Saúde-Doença no Ceará: processos de produção de conhecimento na relação Universidade-Movimentos Sociais'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '05/2007 - 05/2007',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Ciências Farmacêuticas, Nível: Pós-Graduação Disciplinas ministradas Metodologia da Investigação Científica'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '2004 - 12/2005',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Medicina, Nível: Graduação Disciplinas ministradas Desenvolvimento de atividade didático-pedagógica na Disciplina de Saúde, Trabalho, Ambiente e Cultura, do Departamento de Saúde Comunitária da Faculdade de Medicina'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '1990 - 2004',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Serviços técnicos especializados , Universidade Federal do Ceará. Serviço realizado Membro do Grupo de Prevenção ao Uso Indevido de Medicamentos - Núcleo de Ensino, Pesquisa e Extensão da UFC.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '02/1996 - 08/1998',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Extensão universitária , Pró-Reitoria de Pesquisa e Pós-Graduação. Atividade de extensão realizada Bolsista do Programa de Demanda Social/CAPES/UFC.'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '6/1995 - 12/1995',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Farmácia, Nível: Graduação Disciplinas ministradas Saúde Pública'},\n",
       "  {'Instituição': 'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "   'Ano': '6/1995 - 12/1995',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Enfermagem, Nível: Graduação Disciplinas ministradas Epidemiologia Especial'},\n",
       "  {'Instituição': 'Escola de Saúde Pública do Ceará, ESP/CE, Brasil.',\n",
       "   'Ano': '2008 - 2010',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista de Ext. Tecnológica, Enquadramento Funcional: Bolsista de Extensão Tecnológica Atuou na estruturação e capacitação em Vigilância em Saúde, nos componentes Vigilância Sanitária, Vigilância Epidemiológica e Vigilância em Saúde Ambiental e Saúde do Trabalhador, bem como na área de Assistência Farmacêutica.\\n\\n1. Atuou na coordenação das seguintes disciplinas/módulos:\\n- Vigilância em Saúde Ambiental e Saúde do Trabalhador ? Módulo VIII ? Curso de Especialização em Gestão em Vigilância Sanitária - período de 20 a 24 de outubro de 2008.\\n\\n- As Inter-relações Saúde e Ambiente no Território ? Módulo III ? Curso de Especialização em Vigilância em Saúde Ambiental ? período de 23 a 25 de novembro de 2009.\\n\\n- Produção, Ambiente e Saúde/Unidade I ? Módulo I ? Curso de Especialização em Saúde do Trabalhador? período 14 a 16 de julho de 2010.\\n\\n- Produção, Ambiente e Saúde/Unidade II ? Módulo I ? Curso de Especialização em Saúde do Trabalhador ? período 11 a 13 de agosto de 2010.\\n\\n- Módulo Transversal Metodologia da Investigação Científica do Curso de Especialização em Saúde do Trabalhador ? período 1 de julho a 30 de novembro de 2010.\\n\\n2. Atuou na coordenação dos seguintes cursos:\\n- Especialização em Assistência Farmacêutica ? período de agosto a 2000 de novembro de 2001.\\n\\n- Especialização em Vigilância Epidemiológica ? período de dezembro de 2001 a outubro de 2003.'},\n",
       "  {'Instituição': 'Escola de Saúde Pública do Ceará, ESP/CE, Brasil.',\n",
       "   'Ano': '2000 - 2003',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista de Ext. Tecnológica, Enquadramento Funcional: Bolsista de Extensão Tecnológica, Carga horária: 20'},\n",
       "  {'Instituição': 'Escola de Saúde Pública do Ceará, ESP/CE, Brasil.',\n",
       "   'Ano': '06/2010 - 10/2011',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Especialização em Saúde do Trabalhador, Nível: Especialização Disciplinas ministradas Encontro de Análise do Problema (Unidade III) Encontro de Resolução do Problema (Unidade II) Estudo de Caso: Perda auditiva induzida por ruído: Doênças da pele e do tecido subcutâneo relacionadas ao trabalho Estudo Dirigido: Perda auditiva induzida por ruído; doênças da pele tecido subcutâneo relacionadas ao trabalho Oficina de Trabalho em Grupo e Apresentação em Plenária: Construindo um Protocolo de Acolhimento para o o cuidado em saúde do trabalhador. Encontro de Resolução do Problema (Unidade III)'},\n",
       "  {'Instituição': 'Escola de Saúde Pública do Ceará, ESP/CE, Brasil.',\n",
       "   'Ano': '09/2011 - 09/2011',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Treinamentos ministrados , Escola de Saúde Pública do Ceará. Treinamentos ministrados Treinamento de Habilidades: Aplicação do Roteiro de ACO em Serviços de Sáude e Sistematização da Atividade.'},\n",
       "  {'Instituição': 'Escola de Saúde Pública do Ceará, ESP/CE, Brasil.',\n",
       "   'Ano': '04/2011 - 04/2011',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Especialização em Saúde do Trabalhador, Nível: Especialização Disciplinas ministradas Desenvolvendo as atividades de visita de campo e conduzindo a oficina Riscos ocupacionais gerados em atividades produtivas e seus efeitos sobre a saúde do trabalhador. na qualidade de Instrutora no Módulo III/Unidade II - Estudos dos Ambientes e Processos de Trabalho.'},\n",
       "  {'Instituição': 'Escola de Saúde Pública do Ceará, ESP/CE, Brasil.',\n",
       "   'Ano': '1/2008 - 11/2010',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Outras atividades técnico-científicas , Área de Vigilância em Saúde, Área de Vigilância em Saúde. Atividade realizada Responsável pela organização do módulo transversal metodologia da investigação científica; elaboração de relatórios dos cursos realizados; elaboração e desenvolvimento de projetos de pesquisa..'},\n",
       "  {'Instituição': 'Escola de Saúde Pública do Ceará, ESP/CE, Brasil.',\n",
       "   'Ano': '1/2008 - 11/2010',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Outras atividades técnico-científicas , Área de Vigilância em Saúde, Área de Vigilância em Saúde. Atividade realizada Desenho do currículo do curso de especialização (Vigilância em Saúde Ambiental turmas 2009 e 2010).'},\n",
       "  {'Instituição': 'Escola de Saúde Pública do Ceará, ESP/CE, Brasil.',\n",
       "   'Ano': '12/2001 - 11/2003',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Direção e administração, Escola de Saúde Pública do Ceará. Cargo ou função Coordenadora do Curso de Especialização em Vigilância Epidemiológica - Turmas 2002 e 2003.'},\n",
       "  {'Instituição': 'Escola de Saúde Pública do Ceará, ESP/CE, Brasil.',\n",
       "   'Ano': '12/2001 - 11/2003',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Serviços técnicos especializados , Escola de Saúde Pública do Ceará. Serviço realizado Docente do Curso de Especialização em Vigilância Epidemiológica, Turmas 2001, 2002 e 2003.'},\n",
       "  {'Instituição': 'Escola de Saúde Pública do Ceará, ESP/CE, Brasil.',\n",
       "   'Ano': '8/2000 - 11/2001',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Direção e administração, Escola de Saúde Pública do Ceará. Cargo ou função Coordenadora do Curso de Especialização em Assistência Farmacêutica. Serviços técnicos especializados , Escola de Saúde Pública do Ceará. Serviço realizado Docente do Curso de Especialização em Assistência Farmacêutica.'},\n",
       "  {'Instituição': 'Secretaria de Saúde do Estado do Ceará, SESA, Brasil.',\n",
       "   'Ano': '2003 - 2004',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Serviço prestado, Enquadramento Funcional: Assessor Técnico-Científico, Carga horária: 40 Assessora técnica para implementação do Sistema de Informação da Qualidade da Água para Consumo Humano; emissão de relatórios do acompanhamento das ações referentes a Programação Pactuada Integrada da Vigilância em Saúde dentre outras atividades.'},\n",
       "  {'Instituição': 'Secretaria de Saúde do Estado do Ceará, SESA, Brasil.',\n",
       "   'Ano': '11/2003 - 10/2004',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Serviços técnicos especializados , Núcleo de Vigilância Sanitária, Vigilância à Saúde do Trabalhador e do Meio Ambiente. Serviço realizado Contrata pela ASTAC para realizar ativ de assessoria para implementação do Sistema de Informação da Qual da Água para Consumo Humano; emissão de relatórios do acompanhamento das ações referentes a Prog Pac. Integrada da Vig em Saúde dentre outras ati.'},\n",
       "  {'Instituição': 'Sociedade de Ensino Superior Faculdade Integrada do Ceará, FIC*, Brasil.',\n",
       "   'Ano': '2002 - 2004',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Celetista, Enquadramento Funcional: Professora Assistente I, Carga horária: 0 Ensino, Gestão Hospitalar, Nível: Outro Disciplinas ministradas Organização da Saúde Coletiva, curso de Nível Superior'},\n",
       "  {'Instituição': 'Universidade de Fortaleza, UNIFOR, Brasil.',\n",
       "   'Ano': '2000 - 2000',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Celetista, Enquadramento Funcional: , Carga horária: 0 Ensino, Fisioterapia e Terapia Ocupacional, Nível: Graduação Disciplinas ministradas Epidemiologia Ensino, Enfermagem, Nível: Graduação Disciplinas ministradas Epidemiologia'},\n",
       "  {'Instituição': 'Universidade Estadual do Ceará e Universidade Centro de Estudos Integrados, UECE/UNICEI, Brasil.',\n",
       "   'Ano': '2000 - 2003',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Serviço prestado, Enquadramento Funcional: Professora, Carga horária: 0'},\n",
       "  {'Instituição': 'Universidade Estadual do Ceará e Universidade Centro de Estudos Integrados, UECE/UNICEI, Brasil.',\n",
       "   'Ano': '2000 - 2003',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Adm Hosp e Gestão da Qualidade em Sist de Saúde, Nível: Especialização Disciplinas ministradas Metodologia da Investigação Científica'},\n",
       "  {'Instituição': 'Centro de Assessoria Pedagógico, CAP, Brasil.',\n",
       "   'Ano': '2002 - 2003',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Colaborador, Enquadramento Funcional: Membro'},\n",
       "  {'Instituição': 'Centro de Assessoria Pedagógico, CAP, Brasil.',\n",
       "   'Ano': '2002 - 2003',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Outras atividades técnico-científicas , GEPAP, GEPAP. Atividade realizada Participante do Grupo de Estudo, Pesquisa e Ação Pedagógica da OfinArtes - GEPAP.'},\n",
       "  {'Instituição': 'Centro de Saúde Manoel Carlos de Gouveia, CSMCG, Brasil.',\n",
       "   'Ano': '1994 - 1996',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista, Enquadramento Funcional: Farmacêutica (Bolsista), Carga horária: 0 Conselhos, Comissões e Consultoria, Secretaria Municipal de Desenvolvimento Social. Cargo ou função Membro integrante da Comissão de Farmácia e Terapêutica do Município de Fortaleza. Direção e administração, Centro de Saúde Manoel Carlos de Gouveia. Cargo ou função Supervisora de campo em Farmácia Viva de estagiários do Curso de Farmácia do Estágio Curricular Supervisionado em Farmácia. Serviços técnicos especializados , Centro de Saúde Manoel Carlos de Gouveia. Serviço realizado Farmacêutica responsável pelo Programa Farmácia Viva no Centro de Saúde Manoel Carlos de Gouveia realizando atividades de manipulação de fitoterápicos além de palestras educativas.'},\n",
       "  {'Instituição': 'Centro de Saúde Manoel Carlos de Gouveia, CSMCG, Brasil.',\n",
       "   'Ano': '10/1994 - 12/1994',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Direção e administração, Centro de Saúde Manoel Carlos de Gouveia. Cargo ou função Supervisora de campo em Farmácia Viva de estagiários do Curso de Farmácia do Estágio Curricular Supervisionado em Farmácia.'},\n",
       "  {'Instituição': 'Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, Brasil.',\n",
       "   'Ano': '1992 - 1994',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista de Inic Científica, Enquadramento Funcional: , Carga horária: 0'},\n",
       "  {'Instituição': 'Conselho Regional de Farmacia do Estado do Ceará, CRF-CE, Brasil.',\n",
       "   'Ano': '1992 - 1993',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista, Enquadramento Funcional: Bolsista, Carga horária: 4 Projeto de Pesquisa Misoprostol como Abortivo: a experiência das mulheres de Fortaleza Estágios , Conselho Regional de farmácia do Estado do Ceará. Estágio realizado Bolsita do Projeto de Pesquisa Misoprostol como Abortivo: a experiência das mulheres de Fortaleza.'},\n",
       "  {'Instituição': 'Conselho Regional de Farmácia do Estado do Ceará, CRF/CE, Brasil.',\n",
       "   'Ano': '1993 - 1993',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Bolsista de Inic. Científica, Enquadramento Funcional: Bolsista de Iniciação  Científica, Carga horária: 4'},\n",
       "  {'Instituição': 'Hospital de Messejana, HM, Brasil.',\n",
       "   'Ano': '1996 - 2000',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Serviço prestado, Enquadramento Funcional: Farmacêutica, Carga horária: 0 Serviços técnicos especializados , Hospital de Messejana. Serviço realizado Desenvolveu atividades de Orientação Farmacêutica ao paciente como Membro da equipe Multidisciplinar da Liga de Hipertensão da UNPEX-HM.'},\n",
       "  {'Instituição': 'Prefeitura Municipal de Fortaleza, PMF, Brasil.',\n",
       "   'Ano': '1994 - 1996',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Integrante, Enquadramento Funcional: Membro da Comissão de farmácia e terapêutica'},\n",
       "  {'Instituição': 'Universidade Federal da Fronteira Sul, UFFS, Brasil.',\n",
       "   'Ano': '2021 - 2021',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: Colaborador, Enquadramento Funcional: Professor Convidado'},\n",
       "  {'Instituição': 'Universidade Federal da Fronteira Sul, UFFS, Brasil.',\n",
       "   'Ano': '09/2021 - 09/2021',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Ensino, Residência Multiprofissional em Saúde, Nível: Pós-Graduação Disciplinas ministradas Seminário Integrado sobre Cartografia Social'},\n",
       "  {'Instituição': 'Escola Nacional de Saúde Pública / Fundação Oswaldo Cruz, ENSP/FIOCRUZ, Brasil.',\n",
       "   'Ano': '2021 - Atual',\n",
       "   'Descrição': '',\n",
       "   'Outras informações': 'Vínculo: , Enquadramento Funcional:'}],\n",
       " 'Linhas de Pesquisa': [{'Descrição': 'Desenvolvimento, Injustiça Ambiental e Saúde: o caso da Mineração de Urânio e Fosfato no Ceará',\n",
       "   'Detalhes': ''},\n",
       "  {'Descrição': 'Objetivo: A partir da produção compartilhada de conhecimentos, busca-se: desvelar o contexto de risco e as implicações da implantação da mineração de urânio e fosfato prevista para Santa Quitéria-CE no que diz respeito ao trabalho, o ambiente, a saúde e os modos de vida das comunidades atingidas; Contribuir com o desenvolvimento de metodologias e instrumentos que incorporem os princípios da avaliação de equidade ambiental visando à investigação das relações produção, ambiente e saúde nos territórios de influência do projeto de mineração de urânio e fosfato; Contribuir com a formulação de bases conceituais e metodológicas visando à construção das concepções de Vigilância de Base Territorial Local e de Vigilância Popular em Saúde; Subsidiar o SUS no âmbito da atenção primária à saúde para a efetivação das ações e estratégias que tenham como finalidade a reorientação do modelo de atenção à saúde na perspectiva da vigilância e da promoção da saúde; Subsidiar a formulação de políticas públicas intersetoriais que agreguem os princípios da avaliação de equidade ambiental com vistas à promoção da saúde e da justiça ambiental.\\nProjetos de pesquisa desenvolvidos e em desenvolvimento:\\nTerritorialização em Saúde: estudo das relações produção, ambiente, saúde e cultura na atenção primária ä saúde;\\nVigilância popular da saúde: cartografia dos riscos e vulnerabilidades socioambientais no contexto de implantação da Mineração de Urânio e Fosfato no Ceará.Grande área: Ciências da SaúdeGrande Área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Produção, Ambiente e Saúde.Palavras-chave: Mineração de Urânio e Fosfato; Justiça Ambiental; Saúde Ambiental; Saúde do Trabalhador; Territorialização em Saúde; Vigilância Popular em Saúde.',\n",
       "   'Detalhes': ''}],\n",
       " 'Áreas': {'1.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva.',\n",
       "  '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Produção, Ambiente e Saúde.'},\n",
       " 'Produções': {'Artigos completos publicados em periódicos': [{'ano': '2023',\n",
       "    'fator_impacto_jcr': '1.1',\n",
       "    'ISSN': '14138123',\n",
       "    'titulo': 'Vigilância Popular da Saúde, Ambiente e Trabalho (VPSAT): uma revisão integrativa da literatura',\n",
       "    'revista': 'Ciencia ',\n",
       "    'autores': 'SILVA, LUIZ RONS CAÚLA DA2023SILVA, LUIZ RONS CAÚLA DA ; DIÓGENES, SAULO DA SILVA ; MENESES, MICHELE NEVES ; ARJONA, FELIPE BAGATOLI SILVEIRA ; ARRUDA, CARLOS ANDRÉ MOURA ;TEIXEIRA, ANA CLÁUDIA DE ARAÚJO; PESSOA, VANIRA MATOS ; CARNEIRO, FERNANDO FERREIRA . ',\n",
       "    'data_issn': '14138123',\n",
       "    'DOI': 'http://dx.doi.org/10.1590/1413-81232023289.13142022',\n",
       "    'Qualis': 'A1'},\n",
       "   {'ano': '2022',\n",
       "    'fator_impacto_jcr': '1.1',\n",
       "    'ISSN': '19817746',\n",
       "    'titulo': 'NASF-AB no campo das águas: o cuidado em torno do trabalho, ambiente e saúde de famílias agricultoras e pescadoras',\n",
       "    'revista': 'TRABALHO, EDUCAÇÃO E SAÚDE (ONLINE)',\n",
       "    'autores': 'Andrezza Graziella Veríssimo Pontes2022Andrezza Graziella Veríssimo Pontes ; SILVA, J. V. ; ARRUDA, C. A. M. ;CARNEIRO, F. F.; MOREIRA, F. J. F. ; BEZERRA, C. P. ;TEIXEIRA, A. C. A.;PESSOA, V. M.. ',\n",
       "    'data_issn': '19817746',\n",
       "    'DOI': 'http://dx.doi.org/10.1590/1981-7746-ojs275',\n",
       "    'Qualis': 'B1'},\n",
       "   {'ano': '2016',\n",
       "    'fator_impacto_jcr': '0.2',\n",
       "    'ISSN': '2317269X',\n",
       "    'titulo': 'Experiência bem-sucedida no controle do Aedes aegypti sem uso de venenos no sertão cearense',\n",
       "    'revista': 'Vigilância Sanitária em Debate: Sociedade, Ciência ',\n",
       "    'autores': 'CARNEIRO, FERNANDO FERREIRA2016CARNEIRO, FERNANDO FERREIRA ; PESSOA, VANIRA MATOS ;TEIXEIRA, ANA CLÁUDIA DE ARAÚJO; BARBOSA, MARIA IDALICE SILVA ; HOLANDA LAVOR, ANTONIO CARLILE ; SILVA, JURANDI FRUTUOSO . ',\n",
       "    'data_issn': '2317269X',\n",
       "    'DOI': 'http://dx.doi.org/10.3395/2317-269x.00775',\n",
       "    'Qualis': 'B2'},\n",
       "   {'ano': '2013',\n",
       "    'fator_impacto_jcr': '1.1',\n",
       "    'ISSN': '14138123',\n",
       "    'titulo': 'Sentidos e métodos de territorialização na atenção primária à saúde',\n",
       "    'revista': 'Ciência e Saúde Coletiva (Impresso)',\n",
       "    'autores': 'PESSOA, VANIRA MATOS2013PESSOA, VANIRA MATOS ;RIGOTTO, Raquel Maria; CARNEIRO, FERNANDO FERREIRA ;TEIXEIRA, ANA CLÁUDIA DE ARAÚJO. ',\n",
       "    'data_issn': '14138123',\n",
       "    'DOI': 'http://dx.doi.org/10.1590/S1413-81232013000800009',\n",
       "    'Qualis': 'A1'},\n",
       "   {'ano': '2013',\n",
       "    'fator_impacto_jcr': '0.7',\n",
       "    'ISSN': '18075762',\n",
       "    'titulo': 'Pesquisa-ação: proposição metodológica para o planejamento das ações nos serviços de atenção primária no contexto da saúde ambiental e da saúde do trabalhador',\n",
       "    'revista': 'Interface (Botucatu. Online)',\n",
       "    'autores': 'PESSOA, V. M.2013PESSOA, V. M.;TEIXEIRA, A. C. A.; ARRUDA, C. A. M. ; MACHADO, M. F. A. S. ; MACHADO, M. M. T. ; BEZERRA, M. G. V. ',\n",
       "    'data_issn': '18075762',\n",
       "    'DOI': 'http://dx.doi.org/10.1590/s1414-32832013005000004',\n",
       "    'Qualis': 'A3'},\n",
       "   {'ano': '2012',\n",
       "    'fator_impacto_jcr': '1.1',\n",
       "    'ISSN': '14138123',\n",
       "    'titulo': 'O verde da economia no campo: desafios à pesquisa e às políticas públicas para a promoção da saúde no avanço da modernização agrícola',\n",
       "    'revista': 'Ciência e Saúde Coletiva (Impresso)',\n",
       "    'autores': 'RIGOTTO, Raquel Maria2012TEIXEIRA, A. C. A.;RIGOTTO, Raquel Maria;CARNEIRO, F. F.; MARINHO, A. M. C. P. ; ROCHA, Mayara Melo ; FERREIRA, M. J. M. ;PESSOA, V. M.; SILVA, Maria de Lourdes Vicente da ; BRAGA, L. Q. V. ; TEIXEIRA, M. M. ',\n",
       "    'data_issn': '14138123',\n",
       "    'DOI': 'http://dx.doi.org/10.1590/s1413-81232012000600017',\n",
       "    'Qualis': 'A1'},\n",
       "   {'ano': '2009',\n",
       "    'fator_impacto_jcr': '',\n",
       "    'ISSN': '03037657',\n",
       "    'titulo': 'O agronegócio do camarão: processo de trabalho e riscos à saúde dos trabalhadores no município de Aracati/Ceará',\n",
       "    'revista': 'Revista Brasileira de Saúde Ocupacional',\n",
       "    'autores': 'NOGUEIRA, F. N. A.2009TEIXEIRA, A. C. A.; NOGUEIRA, F. N. A. ;RIGOTTO, Raquel Maria. ',\n",
       "    'data_issn': '03037657',\n",
       "    'DOI': 'http://dx.doi.org/10.1590/s0303-76572009000100005',\n",
       "    'Qualis': 'B1'},\n",
       "   {'ano': '1994',\n",
       "    'fator_impacto_jcr': '2.8',\n",
       "    'ISSN': '00107824',\n",
       "    'titulo': 'Misoprostol: The experience of women in Fortaleza, Brazil',\n",
       "    'revista': 'Contraception (Stoneham)',\n",
       "    'autores': 'COELHO, Helana Lutéscia1994TEIXEIRA, A. C. A.; COELHO, Helana Lutéscia ; CRUZ, Maria de Fátima ; GONZAGA, Sandra Luzia de P ; ARRAIS, P S ; LUCHINI, L ; VECCHIA, Carlo La ; TOGNONI, Gianni . ',\n",
       "    'data_issn': '00107824',\n",
       "    'DOI': 'http://dx.doi.org/10.1016/0010-7824(94)90084-1',\n",
       "    'Qualis': 'A1'},\n",
       "   {'ano': '1993',\n",
       "    'fator_impacto_jcr': '98.4',\n",
       "    'ISSN': '01406736',\n",
       "    'titulo': 'Misoprostol and illegal abortion in Fortaleza, Brazil',\n",
       "    'revista': 'Lancet (British edition)',\n",
       "    'autores': 'TEIXEIRA, A. C. A.1993TEIXEIRA, A. C. A.; COELHO, Helana Lutéscia ; SANTOS, Ana Paula ; FORTE, Eliane Barros ; MORAIS, Silvana Macedo ; VECCHIA, Carla La ; TOGNONI, Gianni . ',\n",
       "    'data_issn': '01406736',\n",
       "    'DOI': 'http://dx.doi.org/10.1016/0140-6736(93)91157-H',\n",
       "    'Qualis': 'A1'}],\n",
       "  'Livros publicados/organizados ou edições': {'1.': 'CARNEIRO, FERNANDO FERREIRA ; DANTAS, V. L. A. ;TEIXEIRA, ANA CLÁUDIA DE ARAÚJO. Vigia, Povo! Um Guia de Vigilância Popular em Saúde. 1. ed. Eusébio: Fiocruz Ceará, 2023. 157p .',\n",
       "   '2.': 'TEIXEIRA, A. C. A.; PULGA, V. L. (Org.) ; CASTRO, G. (Org.) . Trajetórias e aprendizados do curso de educação popular e promoção de territórios saudáveis na convivência com o semiárido. 1. ed. Porto Alegre: Rede Unida, 2022. v. 1. 184p .',\n",
       "   '3.': 'CARNEIRO, F. F.;PESSOA, V. M.;TEIXEIRA, A. C. A.. Campo, Floresta e Águas: práticas e saberes em saúde. 1. ed. Brasília: Editora UnB, 2017. v. 1. 464p .',\n",
       "   '4.': 'CASTRO, Gigi ;TEIXEIRA, A. C. A.; FAUSTINO, C ; MEIRELES, A. J. A. ; JOVENTINO, J. L. ; QUEIROZ, L. ; MCCABE, M. A. ; SANTOS, M. L. (Mentinha) ; TUPINAMBA, S. V. . Manguezais X Carcinicultura: lições aprendidas. 1. ed. Fortaleza: Expressão Gráfica, 2009. 130p .'},\n",
       "  'Capítulos de livros publicados': {'1.': 'TEIXEIRA, ANA CLÁUDIA DE ARAÚJO; BARBOSA, A. R. ; CARNEIRO, FERNANDO FERREIRA ; COSTA, L. A. ; FREIRE, T. M. L. ; FIUZA, T. M. ; KERR, L. R. F. S. . Vigilância Popular e Pesquisa Participativa: construção de saberes dialógicos. In: SASSI A. P. Fiuza. (Org.). Sociedade Brasileira de Medicina de Família e Comunidade. 17ed.Porto Alegre: ARTMED, 2022, v. 2, p. 85-102.',\n",
       "   '2.': 'TEIXEIRA, A. C. A.; DANTAS, V. L. A. ; PULGA, V. L. . O processo pedagógico do curso: diálogos entre a educação popular em saúde e a convivência com o semiárido. In: Ana Cláudia de Araújo Teixeira; Vanderléia Laodete Pulga; Gigi Castro. (Org.). Trajetórias e Aprendizados do Curso de Educação Popular e Promoção de Territórios Saudáveis na Convivência com o Semiárido. 1ed.Porto Alegre: Rede Unida, 2022, v. 1, p. 26-40.',\n",
       "   '3.': 'TEIXEIRA, ANA CLÁUDIA DE ARAÚJO; PULGA, V. L. ; CASTRO, G. . Encontros regionais e interestadual ? o que se pôde colher na articulação entre o desenho curricular, a diversidade de temas e o diálogo interdisciplinar. In: Ana Cláudia de Araújo Teixeira; Vanderléia Laodete Pulga; Gigi Castro. (Org.). Trajetórias e Aprendizados do Curso de Educação Popular e Promoção de Territórios Saudáveis na Convivência com o Semiárido. 1ed.Porto Alegre: Rede Unida, 2022, v. 1, p. 95-161.',\n",
       "   '4.': 'LIMA, M. C. ;TEIXEIRA, A. C. A.; BARROS, B. ; SAVASSI, L. C. M. ; ALMEIDA, M. M. ; SEGURA, M. C. ; PEREIRA, R. P. A. ; ANDERSON, M. I. P. . O fazer rural. In: Leonardo Cançado Monteiro Savassi; Magda moura de Almeida; Mayara Floss; Mônica Correia Lima. (Org.). Saúde no caminho da roça. 1ed.Rio de Janeiro: Editora Fiocruz, 2018, v. 1, p. 31-48.',\n",
       "   '5.': 'TEIXEIRA, A. C. A.; LINHARES, A. M. B. ;RIGOTTO, R. M.; PAULINO, A. G. L. . A afirmação da identidade de. In: Fernando Ferreira Carneiro; Vanira Matos Pessoa, Ana Cláudia de Araújo Teixeira. (Org.). Campo, Floresta e Águas: práticas e saberes em saúde. 1ed.Brasília: Editora UnB, 2017, v. 1, p. 369-405.',\n",
       "   '6.': 'RIGOTTO, R. M.; Andrezza Graziella Veríssimo Pontes ;Marcelo José Monteiro Ferreira;TEIXEIRA, A. C. A.;PESSOA, V. M.;ROSA, I. F.. Saúde do Trabalhador e a Questão Ambiental. In: Vera Lúcia Navarro; Edvânia Ângela de Souza Lourenço. (Org.). O Avesso do Trabalho III: saúde do trabalhador e questões contemporâneas. 164ed.São Paulo: Outras Expressões, 2013, v. 1, p. 1-143.',\n",
       "   '7.': 'TEIXEIRA, A. C. A.;PEQUENO, A. M. C. M.; ELLERY, A. E. L. ; BEZERRA, M. G. V. ;RIGOTTO, Raquel Maria;Marcelo José Monteiro Ferreira. Conceitos, Olhares e Primeiras Interpretações sobre o Problema em Estudo.. Agrotóxicos, Trabalho e Saúde.. : , 2011, v. , p. 35-70.',\n",
       "   '8.': 'PEQUENO, A. M. C. M.;TEIXEIRA, A. C. A.;Marcelo José Monteiro Ferreira; Fabíola Silva de Castro ; BRAGA, L. Q. V. ; MACIEL, R. H . Agronegocio, agricultura familiar, assentamento e comunidade agroecológic: Quem são estes Trabalhadores?. Agrotóxicos, Trabalho e Saúde.. : , 2011, v. , p. 273-295.',\n",
       "   '9.': 'Marcelo José Monteiro Ferreira;TEIXEIRA, A. C. A.;PEQUENO, A. M. C. M.. Centralidade do trabalho  e sua caracterização nos diferentes contextos de produção agrícola no Baixo Jaguaribe.. Agrotóxicos, Trabalho e Saúde.. : , 2011, v. , p. 296-318.',\n",
       "   '10.': 'TEIXEIRA, A. C. A.;PEQUENO, A. M. C. M.;Marcelo José Monteiro Ferreira;RIGOTTO, Raquel Maria; BRAGA, L. Q. V. ; Silva. M, L, V, . Tecendo aproximações entre o trabalho e a pluralidade da exposição dos trabalhadoresa agrotóxicos e fertilizantes no Baixo Jaguaribe.. Agrotóxicos, Trabalho e Saúde.. : , 2011, v. , p. 319-360.',\n",
       "   '11.': 'PESSOA, V. M.;RIGOTTO, Raquel Maria; XIMENES, Ana Carmem Rocha ;TEIXEIRA, A. C. A.; PINHEIRO, T. M. M. . As nossas necessidades de saúde no território de atuação da atenção primária à saúde no Baixo Valeno Jaguaribe-Ce e os desafios à política Pública de Saúde.. Agrotóxicos, Trabalho e Saúde.. : , 2011, v. , p. 549-583.',\n",
       "   '12.': 'RIGOTTO, Raquel Maria; MACIEL, R. H ; GODOY, M.G.C ;TEIXEIRA, A. C. A.; TOFOLI, L. F. ; CAVALCANTE, N. C. ; LOPES, C.H. . Trabalhadores dos CAPS no Ceará: condições organizacionais e seu impacto sobre a saúde. In: João Bosco Feitosa dos Santos. (Org.). Recursos Humanos em Saúde: Diagnósticos e reflexões. 1ed.Fortaleza: Editora da Universidade Estadual do Ceará, 2008, v. , p. 49-65.',\n",
       "   '13.': 'TEIXEIRA, A. C. A.. Concepções sobre a saúde e a doença: Uma viagem à Grécia Antiga. In: José Gerardo Vanconcelos, Andréa Pinheiro, Érica Atem. (Org.). Polifonias:vozes, olhares e registros na filosofia da educação. 23ed.Fortaleza: UFC, 2005, v. 274, p. 194-205.'},\n",
       "  'Resumos expandidos publicados em anais de congressos': {'1.': 'TEIXEIRA, A. C. A.;RIGOTTO, R. M.; ALVES, Pablo Araújo ;PESSOA, V. M.. Territorialização em saúde: contexto de risco, impactos ambientais, riscos e danos à saúde nas comunidades vulnerabilizadas no cenário de implantação da mineração de urânio e fosfato no Ceará. In: 2º Simpósio Brasileiro de Saúde e Ambiente, 2014, Belo Horizonte. Anais Eletrônicos do 2º Simpósio Brasileiro de Saúde e Ambiente, 2014.',\n",
       "   '2.': 'ALVES, Pablo Araújo ;TEIXEIRA, A. C. A.;PESSOA, V. M.. Vigilância Popular da Saúde em Contextos de Risco: O Caso da Mineração de Urânio e Fosfato no Ceará. In: 2º Simpósio Brasileiro de Saúde e Ambiente, 2014, Belo Horizonte. Anais Eletrônicos do 2º Simpósio Brasileiro de Saúde e Ambiente, 2014.',\n",
       "   '3.': 'HOEFEL, M. G. L. ;TEIXEIRA, A. C. A.; ALVES JR., R. ; SILVA, M. T. ; SEVERO, D. . Projeto Vidas Paralelas: contribuição para o fortalecimento da autonomia e identidade camponesa. In: 2º Simpósio Brasileiro de Saúde e Ambiente, 2014, Belo Horizonte. Anais Eletrônicos do 2º Simpósio Brasileiro de Saúde e Ambiente, 2014.',\n",
       "   '4.': 'TEIXEIRA, A. C. A.;ROSA, I. F.;RIGOTTO, R. M.; ALVES, Pablo Araújo ;Marcelo José Monteiro Ferreira;PESSOA, V. M.. Cartograia social potencializando el diálogo de saberes entre academia, movimientos sociales y comunidades: una estrategia de comunicación de riesgo bajo la perspectiva de promoción de la salud y la justicia ambiental. In: 5a. Conferência Lantinoamericana e 4a. Conferência Interamericana de Promoción de la Salud y Educación para la Salud, 2012, México. Presentación de Carteles, 2012.',\n",
       "   '5.': 'ALVES, Pablo Araújo ;TEIXEIRA, A. C. A.; ANDRADE, T. V. P. ; SILVA, E. C. ;RIGOTTO, R. M.;ROSA, I. F.;PESSOA, V. M.;Marcelo José Monteiro Ferreira. Planificación de la formación de base en las comunidades próximas a la mina de uranio de santa Quitéria, Ceará, Brasil: Temas y estrategias metodológicas bajo la perspectiva de la Promoción de la salud de la Justicia Ambiental. In: 5a. Conferência Lantinoamericana e 4a. Conferência Interamericana de Promoción de la Salud y Educación para la Salud, 2012, México. Presentación de Trabajos Orales, 2012.',\n",
       "   '6.': 'PESSOA, V. M.;RIGOTTO, R. M.;CARNEIRO, F. F.;TEIXEIRA, A. C. A.;Marcelo José Monteiro Ferreira. Las interrelaciones producción-ambiente y las perspectivas para levar a cabo la promoción da la salud, en el Sistema Único de Salud y comtribuir en la mejoría de la calidad de vida, en contextos de vulnerabilidad Socio ambiental. In: 5a. Conferência Lantinoamericana e 4a. Conferência Interamericana de Promoción de la Salud y Educación para la Salud, 2012, México. Presentación de Trabajos Orales, 2012.',\n",
       "   '7.': 'Marcelo José Monteiro Ferreira;PESSOA, V. M.;RIGOTTO, R. M.;TEIXEIRA, A. C. A.;CARNEIRO, F. F.. Integración de los conocimientos y las prácticas de las políticas de salud y educación: recreando estrategias que promuevan la salud en el contexto de la vulnerabilidad socio ambiental en el noroeste brasileño. In: 5a. Conferência Lantinoamericana e 4a. Conferência Interamericana de Promoción de la Salud y Educación para la Salud, 2012, México. Presentación de Trabajos Orales, 2012.'},\n",
       "  'Resumos publicados em anais de congressos': {'1.': 'TEIXEIRA, A. C. A.;RIGOTTO, R. M.; ALVES, Pablo Araújo ;PESSOA, V. M.;Marcelo José Monteiro Ferreira;ROSA, I. F.; DIAS, J. A. ; AGUIAR, A. C. P. ; Andrezza Graziella Veríssimo Pontes . O caso da Mineração de Urânio no Ceará: a perspectiva da vigilância de base territorial local. In: 10º Congresso Brasileiro de Saúde Coletiva - Saúde é Desenvolvimento: Ciência para a Cidadania, 2012, Porto Alegre. Anais Saúde Coletiva - ISSN 1413-8123, 2012.',\n",
       "   '2.': 'TEIXEIRA, A. C. A.;PEQUENO, A. M. C. M.;PESSOA, V. M.;CARNEIRO, F. F.;Marcelo José Monteiro Ferreira; BEZERRA, M. G. V. . Relato da Oficina de Trabalho Vigilância de Base Territorial Local: desafios e possibilidades. In: 10º Congresso Brasileiro de Saúde Coletiva - Saúde é Desenvolvimento: Ciência para a Cidadania, 2012, Porto Alegre. Anais Saúde Coletiva - ISSN 1413-8123, 2012.',\n",
       "   '3.': 'Silva. M, L, V, ; Andrezza Graziella Veríssimo Pontes ;TEIXEIRA, A. C. A.;Marcelo José Monteiro Ferreira;RIGOTTO, R. M.; SILVA, J. V. ; MELO, E. M. O. . O SUS e as implicações da mineração de urânio à saúde e ao ambiente. In: 10º Congresso Brasileiro de Saúde Coletiva - Saúde é Desenvolvimento: Ciência para a Cidadania, 2012, Porto Alegre. Anais Saúde Coletiva - ISSN 1413-8123.',\n",
       "   '4.': 'VASCONCELOS, D. P. E. ; PRADO, L. D ; ALMEIDA, M. B. A. L. ;ROSA, I. F.;RIGOTTO, Raquel Maria;TEIXEIRA, A. C. A.. Da escavação de poço a silicose: a saga de um sobrevivente na serra da Ibiapaba-CE. In: 10º Congresso Brasileiro de Saúde Coletiva - Saúde é Desenvolvimento: Ciência para a Cidadania, 2012, Porto Alegre. Anais Saúde Coletiva - ISSN 1413-8123, 2012.',\n",
       "   '5.': \"FROTA, M. C. ; MENEZES, J. A. S. ; JÚNIOR, F. M. L. P. ; VASCONCELOS, D. P. E. ; BRITO, A. V. ;TEIXEIRA, A. C. A.. História de pescador: por uma identidade histórica na conformação do 'ser saúde' do pescador artesanal. In: 10º Congresso Brasileiro de Saúde Coletiva - Saúde é Desenvolvimento: Ciência para a Cidadania, 2012, Porto Alegre. Anais Saúde Coletiva - ISSN 1413-8123, 2012.\",\n",
       "   '6.': 'FROTA, M. C. ; MORAIS, A. M. B. ; MENEZES, J. A. S. ; NUNES, C. F. O. ; VASCONCELOS, D. P. E. ;TEIXEIRA, A. C. A.. Atenção básica à saúde do pescador artesanal no município de Fortaleza/CE. In: 10º Congresso Brasileiro de Saúde Coletiva - Saúde é Desenvolvimento: Ciência para a Cidadania, 2012, Porto Alegre. Anais Saúde Coletiva - ISSN 1413-8123, 2012.',\n",
       "   '7.': 'Marcelo José Monteiro Ferreira;RIGOTTO, R. M.; Andrezza Graziella Veríssimo Pontes ;ROSA, I. F.;TEIXEIRA, A. C. A.;PESSOA, V. M.. Por uma orientação solidária da atividade universitária. In: 10º Congresso Brasileiro de Saúde Coletiva - Saúde é Desenvolvimento: Ciência para a Cidadania, 2012, Porto Alegre. Anais Saúde Coletiva - ISSN 1413-8123, 2012.',\n",
       "   '8.': 'ALVES, Pablo Araújo ;TEIXEIRA, A. C. A.;ROSA, I. F.;Marcelo José Monteiro Ferreira;PESSOA, V. M.;RIGOTTO, R. M.. Vigilância popular no contexto da implantação da mineração de urânio em Santa Quitéria - Ceará. In: 10º Congresso Brasileiro de Saúde Coletiva - Saúde é Desenvolvimento: Ciência para a Cidadania, 2012, Porto Alegre. Anais Saúde Coletiva - ISSN 1413-8123, 2012.',\n",
       "   '9.': 'TEIXEIRA, A. C. A.;RIGOTTO, Raquel Maria;LINHARES, Ângela Maria Bessa. A Trajetória metodológica trilhada para o estudo intitulado o trabalho no mangue nas tramas do ( Des)envolvimento e da (Des) ilusão com \" esse furação chamado carcinicultura\": Conflito socioambiental no cumbe, Aracati-Ce.. In: IV Congresso Ibero-americano de Pesquisa Qualitativa em Saúde., 2011, Fortaleza. Ciência e Saúde Coletiva (Impresso), 2010.',\n",
       "   '10.': 'FERREIRA, M. J. M. ;TEIXEIRA, A. C. A.;RIGOTTO, R. M.; CASTRO, G. ; VICENTE, L. ;PESSOA, V. M.; TEIXEIRA, M. M. . Mais que devolução de resultados, um horizonte de esperanças transformadoras.. In: V Congresso Brasileiro de Ciências Sociais e Humanas em Saúde, 2011, São Paulo. V Congresso Brasileiro de Ciências Sociais e Humanas em Saúde, 2011.',\n",
       "   '11.': 'TEIXEIRA, A. C. A.;RIGOTTO, Raquel Maria;LINHARES, Ângela Maria Bessa. As TRAMAS do (des)envolvimento e da (des)ilusão com ?esse furacão chamado carcinicultura?. In: IX Congresso Brasileiro de Saúde Coletiva, 2009, Recife, Brasil. Edição suplementar da Revista Ciência e Saúde Coletiva. Rio de Janeiro: Abrasco Livros, 2009.',\n",
       "   '12.': 'TEIXEIRA, A. C. A.;LINHARES, Ângela Maria Bessa;RIGOTTO, Raquel Maria. A vida no Cumbe, Aracati-CE: em meio ao conflito com a carcinicultura, a afirmação da identidade tradicional de ?povos do mangue?. In: IX Congresso Brasileiro de Saúde Coletiva, 2009, Recife, Brasil. Edição suplementar da Revista Ciência e Saúde Coletiva. Rio de Janeiro: Abrasco Livros, 2009.',\n",
       "   '13.': 'TEIXEIRA, A. C. A.;RIGOTTO, Raquel Maria;LINHARES, Ângela Maria Bessa. O Trabalho no mangue e o emprego na carcinicultura: significados construídos pelos catadores de caranguejo do Cumbe-Aracati-CE.. In: IX Congresso Brasileiro de Saúde Coletiva, 2009, Recife, Brasil. Edição suplementar da Revista Ciência e Saúde Coletiva. Rio de Janeiro: Abrasco Livros, 2009.',\n",
       "   '14.': 'PORTELA, G. A. ;TEIXEIRA, A. C. A.;RIGOTTO, Raquel Maria; SANTOS, A. L. ; ALEXANDRE, S. F. . O Processo de construção e desenvolvimento do curso de saúde do trabalhador para a atenção básica do SUS no Ceará: relato de experiência. In: IX Congresso Brasileiro de Saúde Coletiva, 2009, Recife, Brasil. Edição suplementar da Revista Ciência e Saúde Coletiva. Rio de Janeiro: Abrasco Livros, 2009.',\n",
       "   '15.': 'BEZERRA, M. G. V. ;RIGOTTO, R. M.;PESSOA, V. M.;TEIXEIRA, A. C. A.. Processos de desenvolvimento, transformações sócio-ambientais e as repercussões sobre a saúde: o olhar da comunidade de bolso em São Gonçalo do Amarante-CE. In: IX Congresso Brasileiro de Saúde Coletiva, 2009, Recife, Brasil. Edição suplementar da Revista Ciência e Saúde Coletiva. Rio de Janeiro: Abrasco Livros, 2009.',\n",
       "   '16.': 'RIGOTTO, Raquel Maria;LINHARES, Ângela Maria Bessa;TEIXEIRA, A. C. A.. O corpo disciplinado implícito nas concepções de saúde-doença: da teoria do germe à teoria da tríade ecológica. In: X Congresso Latino Americano de Medicina Social, IV Congresso Brasileiro de Ciências Sociais e Humanas em Saúde e XIV Congress of the International Association of Health Policy ? Equidade, Ética e Direito à Saúde: desafios à Saúde Coletiva na mundializaçã, 2007, Salvador. X Congresso Latino Americano de Medicina Social, IV Congresso Brasileiro de Ciências Sociais e Humanas em Saúde e XIV Congress of the International Association of Health Policy ? Equidade, Ética e Direito à Saúde: desafios à Saúde Coletiva na mundializaçã, 2007.',\n",
       "   '17.': 'TEIXEIRA, A. C. A.; CUNHA, Jane C L ; CASSIANO, J G S ; SÁ, H L C ; FALCÃO, Lucília M N . Desenho do Curriculo do Curso de Especialização em Vigilância Epidemiológica Fundamentado na Aprendizagem Baseada em Problemas. In: VII Congresso Brasileiro de Saúde Coletiva, 2003, Brasília. Livro de Resumos I do VII Congresso Brasileiro de Saúde Coletiva, 2003. v. 8. p. 353-353.',\n",
       "   '18.': 'TEIXEIRA, A. C. A.; CUNHA, Jane C L ; ALMEIDA, Paulo César de ; FALCÃO, Lucília Maria N . Pós-Graduação Lato Sensu em Vigilância Epidemiológica da Escola de Saúde Pública do Ceará. In: VII Congresso Brasileiro de Saúde Coletiva, 2003, Brasília. Livro de Resumos I do VII Congresso Brasileiro de Saúde Coletiva, 2003. v. 8. p. 350-350.',\n",
       "   '19.': 'TEIXEIRA, A. C. A.. Análise da Satisfação dos Participantes do Curso de Especialização em Assistência Farmacêutica: um instrumento de capacitação baseado na metodologia PBL. In: I Congresso Internacional de Medicamentos, 2002, João Pessoa. Anais do I Congresso Internacional de Medicamentos, 2002.',\n",
       "   '20.': 'TEIXEIRA, A. C. A.. Curso de Especialização em Assistência Farmacêutica: análise do desempenho e satisfação dos participantes a partir de instrumento de avaliação. In: I Congresso Internacional de Medicamentos, 2002, João Pessoa. Anais do I Congresso Internacional de Medicamentos, 2002.',\n",
       "   '21.': 'TEIXEIRA, A. C. A.. Desempenho da Entrevista Comparado à Contagem de Comprimidos na Determinação da Adesão ao Tratamento da Hipertensão em Pacientes de Ambulatório. In: V Congresso Brasileiro de Epidemiologia, 2002, Curitiba. Anais do V Congresso Brasileiro de Epidemiologia, 2002.',\n",
       "   '22.': 'TEIXEIRA, A. C. A.; SI, R C A ; NEVES, K R T ; CARLO, I C C . The Elaboration and Implementation of a Course for Pharmaceutical Assistants that Uses the Problem-Based Learning Method. In: International Conference on Challenges of Primary Care-Oriented Health Systems: innovations by educational institutions, health professions and health services, 2001, Londrina. Anals of the International Conference on Challenges of Primary Care-Oriented Health Systems: innovations by educational institutions, health professions and health services, 2001. p. 177-177.',\n",
       "   '23.': 'TEIXEIRA, A. C. A.; COELHO, Helena Lutéscia Luna ; MONTE, Cristina Maria Gomes Do . Fatores que Determinam a Adesão ao Tratamento Anti-Hipertensivo em Pacientes de Ambulatório. In: VI Congresso Brasileiro de Saúde Coletiva, 2000, Salvador. Livro de Resumos do VI Congresso Brasileiro de Saúde Coletiva, 2000. v. 5. p. 421-421.',\n",
       "   '24.': 'TEIXEIRA, A. C. A.; COELHO, Helena Lutéscia Luna ; MONTE, Cristina Maria Gomes Do . Adesão ao Tratamento da Hipertensão Arterial e seus Determinantes em Pacientes de Ambulatório. In: II Congresso Nacional da Sociedade Brasileira de Farmácia Hospitalar, 1998, Belo Horizonte. Anais do II Congresso Nacional da Sociedade Brasileira de Farmácia Hospitalar, 1998. p. 51-51.',\n",
       "   '25.': 'SOUSA, Domingos Sávio ; LACERDA, Elenice ; CARVALHO, Regina Maria Vale ;TEIXEIRA, A. C. A.; COELHO, Helena Lutéscia . Uso Racional de Medicamentos: atividades de educação popular com agentes de saúde e radialistas. In: II Congresso Nacional da Sociedade Brasileira de Farmácia Hospitalar, 1998, Belo Horizonte. Anais do II Congresso Nacional da Sociedade Brasileira de Farmácia Hospitalar, 1998. p. 52-52.',\n",
       "   '26.': 'TEIXEIRA, A. C. A.; SILVA, J A ; SERRA, G M C . Estudo Descritivo do Uso de Medicamentos na Unidade de Pacientes Externos do Hospital de Messejana, Fortaleza-CE. In: V Congresso Brasileiro de Saúde Coletiva, 1997, Águas de Lindóia. Livro de Resumos do V Congresso Brasileiro de Saúde Coletiva, 1997. p. 141-141.',\n",
       "   '27.': 'TEIXEIRA, A. C. A.; SILVA, J A ; MELO, e M ; BRASILEIRO, e B O . Perfil da Utilização de Medicamentos na Unidade de Pacientes Externos do Hospital de Messejana, Fortaleza-CE. In: I Seminário Brasileiro de Farmacoepidemiologia, 1996, Fortaleza. Anais do I Seminário Brasileiro de Farmacoepidemiologia, 1996. p. 32-33.',\n",
       "   '28.': 'SOUSA, D S C ; CARVALHO, R M V ;TEIXEIRA, A. C. A.; GONDIM, A P S ; COELHO, H L L . Programa Falando de Medicamentos: uma experiência em educação pública de medicamentos através do rádio. In: I Seminário Brasileiro de Farmacoepidemiologia, 1996, Fortaleza. Anais do I Seminário Brasileiro de Farmacoepidemiologia, 1996. p. 54-55.',\n",
       "   '29.': 'COELHO, H L ;TEIXEIRA, A. C. A.; SILVA, J A ; MELO, O F . Misoprostol e Clandestinidade em Fortaleza-CE. In: III Congresso Brasileiro, II Congresso Ibero Americano, I Congresso Latino Americano, 1ª Mostra de Tecnologia em Epidemiologia, 1995, Salvador. Anais do III Congresso Brasileiro, II Congresso Ibero Americano, I Congresso Latino Americano, 1ª Mostra de Tecnologia em Epidemiologia, 1995. p. 221-221.',\n",
       "   '30.': 'COELHO, H L L ;TEIXEIRA, A. C. A.; SILVA, C D C ; ARRAIS, P S D . Farmacoepidemiologia e Farmacovigilância, Nova Área de Ensino, Pesquisa e Extensão em Cursos de Farmácia: uma experiência. In: 1º Seminário Internacional de Farmacêuticos, 4º Congresso Brasileiro de Produtos Farmacêuticos, Cosméticos e Afins e 9º Congresso Paulista de Farmacêuticos, 1995, São Paulo. Anais do 1º Seminário Internacional de Farmacêuticos, 4º Congresso Brasileiro de Produtos Farmacêuticos, Cosméticos e Afins e 9º Congresso Paulista de Farmacêuticos, 1995. p. 48-48.',\n",
       "   '31.': 'TEIXEIRA, A. C. A.. Investigação do Uso de Medicamentos nos Serviços de Saúde no Município de Fortaleza. In: III Congresso Brasileiro de Vigilância de Medicamentos, 1995, São Paulo. Anais do III Congresso Brasileiro de Vigilância de Medicamentos, 1995. p. 40-40.',\n",
       "   '32.': 'COELHO, H L ; SOUSA, D S ; CRUZ, M F ;TEIXEIRA, A. C. A.; SILVA, C D ; ARRAIS, P S . Grupo de Prevenção ao Uso de Medicamentos: uma experiência positiva. In: 2º Congresso Mundial de Farmacêuticos de Expressão Portuguesa e 1º Congresso Brasileiro de Farmácia Magistral, 1994. Anais do 2º Congresso Mundial de Farmacêuticos de Expressão Portuguesa e 1º Congresso Brasileiro de Farmácia Magistral, 1994. p. 42-42.',\n",
       "   '33.': 'COELHO, H L L ; SOUSA, D S ; CRUZ, F ;TEIXEIRA, A. C. A.. Grupo de Prevenção ao Uso de Medicamentos: uma experiência. In: IV Congresso Brasileiro de Saúde Coletiva, 1994, Olinda. Anais do IV Congresso Brasileiro de Saúde Coletiva, 1994.',\n",
       "   '34.': 'COELHO, H L L ; BRASIL, R ; FARIAS, M ;TEIXEIRA, A. C. A.. Prevalência do Uso de Medicamentos por Mulheres na Idade Fértil em Fortaleza-CE. In: IX Reunião Anual da Federação de Sociedades de Biologia Experimental, 1994, Caxambú. Anais da IX Reunião Anual da Federação de Sociedades de Biologia Experimental, 1994. p. 161-161.',\n",
       "   '35.': 'COELHO, H L L ; BRASIL, R ; FARIAS, M ; TAVARES, M F Cruz A P ;TEIXEIRA, A. C. A.; REIS, A . Prevalência do Uso de Medicamentos por Mulheres na Idade Fértil em Fortaleza-CE. In: IX Reunião Anual da Federação de Sociedades de Biologia Experimental, 1994, Caxambú. Anais da IX Reunião Anual da Federação de Sociedades de Biologia Experimental, 1994. p. 161-161.',\n",
       "   '36.': 'COELHO, H L L ; BRAIL, Rita M C ; MARTINS, Alice M C ;TEIXEIRA, A. C. A.. Prevalência do Uso de Medicamentos por Mulheres em Idade Fértil no Município de Fortaleza. In: XII Encontro Universitário de Iniciação à Pesquisa e I Encontro de Pós-Graduação, 1993, Fortaleza. Anais do XII Encontro Universitário de Iniciação à Pesquisa e I Encontro de Pós-Graduação, 1993. p. 136-136.',\n",
       "   '37.': 'COELHO, H L L ;TEIXEIRA, A. C. A.; CRUZ, M F S ; GONZAGA, S L P ; LUCHINI, L . Uso do Misoprostol para Interrupção Voluntária da Gravidez:experiência das mulheres de Fortaleza I. Caracterização da Amostra. In: VIII Reunião da Federação de Sociedades de Biologia Experimental-FESBE, 1993, Caxambu. Anais da VIII Reunião da Federação de Sociedades de Biologia Experimental-FESBE, 1993. p. 244-244.',\n",
       "   '38.': 'COELHO, H L L ;TEIXEIRA, A. C. A.; SANTOS, A P C ; FORTE, e B ; LUCHINI, L . Uso do Misoprostol para Interrupção Voluntária da Gravidez: experiência das mulheres de Fortaleza II. Variáveis da Tentativa de Abortar. In: VIII Reunião da Federação de Sociedades de Biologia Experimental-FESBE, 1993, Caxambu. Anais da VIII Reunião da Federação de Sociedades de Biologia Experimental-FESBE, 1993. p. 244-244.',\n",
       "   '39.': 'COELHO, H L L ;TEIXEIRA, A. C. A.; VALE, R M G ; MONTE, M C A ; BARBOSA, M A . Uso do Misoprostol para Interrupção Voluntária da Gravidez: a experiência das mulheres de Fortaleza III. Opinião sobre o Aborto e o Misoprostol. In: VIII Reunião da Federação de Sociedades de Biologia Experimental-FESBE, 1993, Caxambu. Anais da VIII Reunião da Federação de Sociedades de Biologia Experimental-FESBE, 1993. p. 244-244.',\n",
       "   '40.': 'COELHO, H L L ; ARRAIS, P S D ; SANTOS, A P C ;TEIXEIRA, A. C. A.. Automedicação em Fortaleza-CE. In: VII Reunião da Federação de Sociedades de Biologia Experimental-FESBE, 1992. Anais da VII Reunião da Federação de Sociedades de Biologia Experimental-FESBE, 1992. p. 225-225.',\n",
       "   '41.': 'COELHO, H L L ;TEIXEIRA, A. C. A.; MORAIS, S M ; CRUZ, M F S ; GONZAGA, S L P . Evolução do Uso do Misoprostol em Fortaleza-CE: informação e proibição. In: VII Reunião da Federação de Sociedades de Biologia Experimental-FESBE, 1992. Anais da VII Reunião da Federação de Sociedades de Biologia Experimental-FESBE, 1992. p. 225-225.',\n",
       "   '42.': 'COELHO, H L L ;TEIXEIRA, A. C. A.; FORTE, Eliane Barros ; CRUZ, Maria de Fátima S ; GONZAGA, Sandra Luzia de P . Evolução do Uso do Misoprostol em Fortaleza-CE em 1991: informação e proibição. In: 44ª Reunião da Sociedade Brasileira para o Progresso da Ciência, 1992. Anais da 44ª Reunião da Sociedade Brasileira para o Progresso da Ciência, 1992. p. 764-765.',\n",
       "   '43.': 'TEIXEIRA, A. C. A.; CRUZ, Maria de Fátima S ; COELHO, Helena L L . Auto Administração de Misoprostol como Abortivo - A Experiência das Mulheres de Fortaleza. In: XI Encontro Universitário de Iniciação à Pesquisa, 1992, Fortaleza. Anais do XI Encontro Universitário de Iniciação à Pesquisa, 1992. p. 396-396.',\n",
       "   '44.': 'TEIXEIRA, A. C. A.; CRUZ, Maria de Fátima S ; COELHO, H L L . Evolução do Uso do Misoprostol em Fortaleza-CE. In: XI Encontro Universitário de Iniciação à Pesquisa, 1992, Fortaleza. Anais do XI Encontro Universitário de Iniciação à Pesquisa, 1992.',\n",
       "   '45.': 'COELHO, Helena L L ; SOUSA, Domingos Sávio C ;TEIXEIRA, A. C. A.; OLIVEIRA, Antônio Italo ; ARAÚJO, Júlio Maria L ; MISAGO, Chizuro ; FONSECA, Walter Victor C . Estudo Descritivo da Indicação de Medicamentos com Finalidade Abortiva em Famácias de Fortaleza-CE. In: 43ª Reunião da Sociedade Brasileira para o Progresso da Ciência, 1991. Anais da 43ª Reunião da Sociedade Brasileira para o Progresso da Ciência, 1991. p. 783-783.',\n",
       "   '46.': 'TEIXEIRA, A. C. A.; OLIVEIRA, Adolfo Ítalo P de ; SOUSA, Domingos Sávio de C ; COELHO, Helena L L . Indicação de Medicamentos com Finalidade Abortiva em Farmácias de Fortaleza. In: IX Encontro Universitário de Iniciação à Pesquisa, 1990, Fortaleza. Anais do IX Encontro Universitário de Iniciação à Pesquisa, 1990. p. 184-184.',\n",
       "   '47.': 'TEIXEIRA, A. C. A.; MENDONÇA, Vera Lúcia Maia ; BANDEIRA, Mary Anne Medeiros . Determinação do Teor de Alcalóides nas Folhas de Colônia (Alpinia Speciosa Schum), e Ensaios Farmacológicos para Verificação do Efeito Hipotensor. In: VIII Encontro Universitário de Iniciação à Pesquisa, 1989, Fortaleza. Anais do VIII Encontro Universitário de Iniciação à Pesquisa, 1989. p. 246-246.'},\n",
       "  'Resumos publicados em anais de congressos (artigos)': {},\n",
       "  'Apresentações de Trabalho': {'1.': 'TEIXEIRA, A. C. A.;CARNEIRO, F. F.; DIOGENES, S. S. ; SILVA, F. V. E. ; MAIA, A. S. ; ARAUJO, R. F. ; SILVA, L. R. C. . AGROECOLOGIA VERSUS AGRONEGÓCIO: UMA EXPERIÊNCIA DE VIGILÂNCIA POPULAR DA SAÚDE FRENTE AOS IMPACTOS DO AGRONEGÓCIO EM UMA REGIÃO DO SEMIÁRIDO CEARENSE, NORDESTE DO BRASIL. 2023. (Apresentação de Trabalho/Congresso).',\n",
       "   '2.': 'BARBOSA, A. R. ; FREIRE, T. M. L. ; KERR, L. R. F. S. ;TEIXEIRA, A. C. A.; NAYRA\\t ; RODRIGUES, L. S. ;CARNEIRO, F. F.. DESENVOLVIMENTO DE ESTRATÉGIAS DE VIGILÂNCIA POPULAR E CUIDADO EM SAÚDE JUNTO A UM GRUPO DE MULHERES EM SITUAÇÃO DE VULNERABILIDADE.. 2023. (Apresentação de Trabalho/Congresso).',\n",
       "   '3.': 'MENESES, M. N. ; SILVA, L. R. C. ;TEIXEIRA, ANA CLÁUDIA DE ARAÚJO;CARNEIRO, F. F.; DIOGENES, S. S. ; DANTAS, V. L. A. ; BARBOSA, F. ; BARROS, M. C. ; FERNANDES, R. W. T. ; ARJONA, F. B. S. . ENCONTROS QUE TECEM POSSIBILIDADES DE RE-EXISTIR: EXPERIÊNCIA FORMATIVA EM  VIGILÂNCIA POPULAR. 2022. (Apresentação de Trabalho/Congresso).',\n",
       "   '4.': 'PESSOA, V. M.;CARNEIRO, F. F.; ARRUDA, C. A. M. ;TEIXEIRA, ANA CLÁUDIA DE ARAÚJO; Et Al. . SERPOVOS - SAÚDE, CUIDADO E ECOLOGIA DE SABERES NAS PRÁTICAS DA ATENÇÃO  PRIMÁRIA À SAÚDE EM TERRITÓRIOS DO CAMPO, FLORESTAS E ÁGUAS. 2022. (Apresentação de Trabalho/Congresso).',\n",
       "   '5.': 'TEIXEIRA, ANA CLÁUDIA DE ARAÚJO;CARNEIRO, F. F.; FRANCA, P. R. B. ; DIOGENES, S. S. ; SILVA, L. R. C. ; SILVA, F. V. E. ; BARBOSA, A. R. ; BARBOSA, F. J. ; FERNANDES, R. W. T. ; FRANÇA, F. M. O. R. . LUTA PELA DEMARCAÇÃO DO TERRITÓRIO INDÍGENA ANACÉ E PRESERVAÇÃO DA ÁREA DE PROTEÇÃO AMBIENTAL DO LAGAMAR DO CAUÍPE, CEARÁ: UMA EXPERIÊNCIA DE  VIGILÂNCIA POPULAR. 2022. (Apresentação de Trabalho/Congresso).',\n",
       "   '6.': 'ARRUDA, C. A. M. ;PESSOA, V. M.; SANTOS, R. A. ;CARNEIRO, F. F.;TEIXEIRA, ANA CLÁUDIA DE ARAÚJO; NOBREGA, R. C. ; FERNANDES, R. W. T. ; SILVA, F. V. E. ; MARTINS, A. K. L. ; PINHEIRO, R. D. . SABERES, PRÁTICAS POPULARES E AGROECOLOGIA NA PROMOÇÃO DO BEM VIVER COMUNITÁRIO E DO CUIDADO EM SAÚDE, EM TERRITÓRIOS DAS POPULAÇÕES DO CAMPO  E DAS ÁGUAS NO NORDESTE. 2022. (Apresentação de Trabalho/Congresso).',\n",
       "   '7.': 'DIOGENES, S. S. ;CARNEIRO, F. F.;TEIXEIRA, A. C. A.; SILVA, L. R. C. ; SILVA, F. V. E. ; MACEDO, E. M. T. ; ANTONIA MARCIA\\t ; ARAUJO, R. F. ; MAIA, A. S. ; BATISTA, M. H. . LUTA CONTRA O AGRONEGÓCIO PELA A VIDA, ÁGUA E O BEM VIVER NO BAIXO JAGUARIBE, CEARÁ: UMA EXPERIÊNCIA DE VIGILÂNCIA POPULAR. 2022. (Apresentação de Trabalho/Congresso).',\n",
       "   '8.': 'CARNEIRO, F. F.; SILVA, L. R. C. ;TEIXEIRA, A. C. A.; GOMES, C. B. S. ; ALBUQUERQUE, O. C. ; VALE, M. E. P. . PESCADORAS E PESCADORES ARTESANAIS EM DEFESA DA VIDA E DO RIO JAGUARIBE:  UMA EXPERIÊNCIA EM VIGILÂNCIA POPULAR. 2022. (Apresentação de Trabalho/Congresso).',\n",
       "   '9.': 'BARBOSA, A. R. ; FREIRE, T. M. L. ; NAYRA\\t ;TEIXEIRA, A. C. A.; AGUIAR, I. W. O. ; KERR, L. R. F. S. ; KENDALL, B. C. ; FIUZA, T. M. ; SILVA, A. A. S. ;CARNEIRO, F. F.. ATENÇÃO PRIMÁRIA, CUIDADO E VIGILÂNCIA POPULAR DA SAÚDE EM TERRITÓRIO VULNERÁVEL DE FORTALEZA, CEARÁ. 2022. (Apresentação de Trabalho/Congresso).',\n",
       "   '10.': 'PESSOA, V. M.; BEZERRA, M. G. V. ;CARNEIRO, F. F.; SILVA, F. V. E. ; Santana. I,V,F, ; MARTINS, A. K. L. ;TEIXEIRA, ANA CLÁUDIA; SILVA, T. P. ; MELO, L. N. ; SILVA, E. M. . IDENTIDADES, MEMÓRIAS E PRÁTICAS DE CUIDADOS EM SAÚDE: OS DESAFIOS ATUAIS NA DEFESA DO DIREITO À SAÚDE E DA VIDA EM TERRITÓRIOS INDÍGENAS NO SERTÃO DO CEARÁ. 2022. (Apresentação de Trabalho/Congresso).',\n",
       "   '11.': 'CARNEIRO, F. F.;TEIXEIRA, ANA CLÁUDIA; SILVA, L. R. C. ; SILVA, F. V. E. ; DIOGENES, S. S. ; MENESES, M. N. ; DANTAS, V. L. A. ; DIAS, A. P. ; MACEDO, E. M. T. ; ARAUJO, R. F. . VIGILÂNCIA POPULAR DA SAÚDE, AMBIENTE E TRABALHO (VPSAT): COMUNIDADES, SUS E PESQUISADORES NA DEFESA DA VIDA DE POPULAÇÕES VULNERABILIZADAS POR MEIO DE UM PARTICIPATÓRIO. 2022. (Apresentação de Trabalho/Congresso).',\n",
       "   '12.': 'NOBREGA, R. C. ;TEIXEIRA, ANA CLÁUDIA;PESSOA, V. M.;CARNEIRO, F. F.; ARRUDA, C. A. M. ; MARTINS, A. K. L. . EXPERIÊNCIAS INOVADORAS DE CUIDADO EM SAÚDE EM TERRITÓRIOS DAS POPULAÇÕES DO CAMPO, FLORESTA E ÁGUAS DO CEARÁ. 2022. (Apresentação de Trabalho/Congresso).',\n",
       "   '13.': 'TEIXEIRA, A. C. A.;CARNEIRO, F. F.; NASCIMENTO, J. L. J. ; DIOGENES, S. S. ; SILVA, L. R. C. ; SILVA, F. V. E. ; BARBOSA, F. J. ; SILVA, R. G. ; ROCHA, C. R. ; SOUSA, L. S. . EXPERIÊNCIA DE VIGILÂNCIA POPULAR FRENTE AOS IMPACTOS CAUSADOS PELA CARCINICULTURA, EÓLICAS E CAGECE: O CASO DO QUILOMBO DO CUMBE, ARACATI-CE.. 2022. (Apresentação de Trabalho/Congresso).',\n",
       "   '14.': 'DUARTE, A. R. G. ; BATISTA, M. H. ;CARNEIRO, F. F.;TEIXEIRA, A. C. A.; MACEDO, E. M. T. . VIGILÂNCIA POPULAR DA SAÚDE, AMBIENTE E TRABALHO CONSTRUÇÃO DO PLANO DE AÇÃO DE DUAS EXPERIÊNCIAS NO VALE DO JAGUARIBE, CEARÁ.. 2022. (Apresentação de Trabalho/Outra).',\n",
       "   '15.': 'FREIRE, T. M. L. ; BARBOSA, A. R. ;TEIXEIRA, ANA CLÁUDIA DE ARAÚJO; NAYRA\\t ; FIUZA, T. M. ; KERR, L. R. F. S. ; RODRIGUES, L. S. ; ARAUJO, M. A. P. S. ; BEZERRA, M. C. L. ;CARNEIRO, F. F.. PESQUISA-AÇÃO COM ENFOQUE NA VIGILÂNCIA POPULAR E NO CUIDADO EM SAÚDE JUNTO A UM GRUPO DE MULHERES EM SITUAÇÃO DE VULNERABILIDADE NO CONTEXTO DA INICIAÇÃO CIENTÍFICA.. 2022. (Apresentação de Trabalho/Congresso).',\n",
       "   '16.': 'TEIXEIRA, A. C.; DANTAS, V. L. A. ; LIMA, G. M. C. ; LIMA, R. F. ;CARNEIRO, F. F.. EDUCAÇÃO POPULAR E CONVIVÊNCIA COM O SEMIÁRIDO: UMA EXPERIÊNCIA DE FORMAÇÃO VOLTADA PARA A PROMOÇÃO DE TERRITÓRIOS SAUDÁVEIS. 2019. (Apresentação de Trabalho/Congresso).',\n",
       "   '17.': 'RABELO, M. I. F. D. ;TEIXEIRA, A. C.; DANTAS, V. L. A. . Relato de Experiência: EDUCAÇÃO POPULAR EM SAÚDE VIVENCIADA NO SEMIÁRIDO DA REGIÃO JAGUARIBANA.. 2019. (Apresentação de Trabalho/Congresso).',\n",
       "   '18.': 'TEIXEIRA, A. C.;CARNEIRO, F. F.; ROCHA, B. T. G. ; ROCHA, S. E. ; FREITAS, F. S. ; NUTO, S. A. S. . MAPEAMENTO DOS CONTEXTOS QUE PROMOVEM E AMEAÇAM A SAÚDE E A VIDA DAS COMUNIDADES IMPACTADAS PELO COMPLEXO INDUSTRIAL E PORTUÁRIO DO PECÉM NO CEARÁ. 2018. (Apresentação de Trabalho/Congresso).',\n",
       "   '19.': 'SILVA, L. R. C. ;PESSOA, V. M.;TEIXEIRA, A. C.;CARNEIRO, F. F.; MEDEIROS, G. C. O. . ANÁLISE SISTEMÁTICA DOS ESTUDOS AMBIENTAIS COM DESTAQUE À SAÚDE DAS COMUNIDADES IMPACTADAS PELO POLO INDUSTRIAL E TECNOLÓGICO DA SAÚDE DE EUSÉBIO, CE. 2018. (Apresentação de Trabalho/Congresso).',\n",
       "   '20.': 'TEIXEIRA, A. C.;CARNEIRO, F. F.;PESSOA, V. M.. EXPERIÊNCIA DE CRIAÇÃO DA REDE SAÚDE, SANEAMENTO E DIREITOS HUMANOS NO SEMIÁRIDO BRASILEIRO.. 2017. (Apresentação de Trabalho/Congresso).',\n",
       "   '21.': 'TEIXEIRA, A. C.;PESSOA, V. M.;CARNEIRO, F. F.. O ESTADO NA IMPLANTAÇÃO DO PÓLO INDUSTRIAL E TECNOLÓGICO DA SAÚDE E DO PROGRAMA RENDA MÍNIMA EM EUSÉBIO/CE: QUEM PROMOVE DESENVOLVIMENTO COMUNITÁRIO?.. 2017. (Apresentação de Trabalho/Congresso).',\n",
       "   '22.': 'CARNEIRO, F. F.;PESSOA, V. M.;TEIXEIRA, A. C.. A GARANTIA DO DIREITO A SAÚDE DAS POPULAÇÕES DO CAMPO, DA FLORESTA E DAS ÁGUAS: DESAFIOS E PERSPECTIVAS PARA O ESTADO BRASILEIRO.. 2017. (Apresentação de Trabalho/Congresso).',\n",
       "   '23.': 'TEIXEIRA, A. C.;RIGOTTO, R. M.;PESSOA, V. M.. PESQUISA-PARTICIPANTE: ALTERNATIVA PARA A PRODUÇÃO COMPARTILHADA DE CONHECIMENTOS EM COMUNIDADES ATINGIDAS POR MINERAÇÃO DE URÂNIO E FOSFATO NO CEARÁ. 2015. (Apresentação de Trabalho/Comunicação).',\n",
       "   '24.': 'TEIXEIRA, A. C. A.;RIGOTTO, R. M.; ALVES, Pablo Araújo ;PESSOA, V. M.. Territorialização em saúde: contexto de risco, impactos ambientais, riscos e danos à saúde nas comunidades vulnerabilizadas no cenário de implantação da mineração de urânio e fosfato no Ceará. 2014. (Apresentação de Trabalho/Simpósio).',\n",
       "   '25.': 'ALVES, Pablo Araújo ;TEIXEIRA, A. C. A.;PESSOA, V. M.. Vigilância Popular da Saúde em Contextos de Risco: O Caso da Mineração de Urânio e Fosfato no Ceará. 2014. (Apresentação de Trabalho/Simpósio).',\n",
       "   '26.': 'HOEFEL, M. G. L. ;TEIXEIRA, A. C. A.; ALVES JR., R. ; SILVA, M. T. ; SEVERO, D. . Projeto Vidas Paralelas: contribuição para o fortalecimento da autonomia e identidade camponesa. 2014. (Apresentação de Trabalho/Simpósio).',\n",
       "   '27.': 'SAMPAIO, E. C. C. ; BRITO, M. K. M. ; ROCHA, R. D. ; MACHADO, J. M. H. ; CAMPELO, F. ;TEIXEIRA, A. C.. SITUAÇÃO DA VIGILÂNCIA EM SAÚDE DO TRABALHADOR NO BRASIL EM 2014. 2014. (Apresentação de Trabalho/Congresso).',\n",
       "   '28.': 'TEIXEIRA, A. C.;RIGOTTO, R. M.;PESSOA, V. M.. AS NECESSIDADES SOCIAIS E DE SAÚDE DE COMUNIDADES EM SITUAÇÃO DE CONFLITO E INJUSTIÇA AMBIENTAL: O CASO DA MINERAÇÃO DE URÂNIO E FOSFATO NO CEARÁ.. 2014. (Apresentação de Trabalho/Comunicação).',\n",
       "   '29.': 'FROTA, M. C. ;TEIXEIRA, A. C.; NUNES, B. S. ; COSTA, D. S. ; ALVES, Pablo Araújo . O USO DE MÍDIA ALTERNATIVA NA CONSTRUÇÃO DO PROCESSO DE RESISTÊNCIA DE COMUNIDADES DIANTE A POSSÍVEL EXPLORAÇÃO DA MINERAÇÃO DE URÂNIO E FOSFATO EM SANTA QUITÉRIA - CE. 2013. (Apresentação de Trabalho/Outra).',\n",
       "   '30.': 'COSTA, D. S. ; FROTA, M. C. ; NUNES, B. S. ;TEIXEIRA, A. C.; ALVES, Pablo Araújo . PAINEL ACADÊMICO POPULAR: UMA CONSTRUÇÃO COLETIVA DA RESISTÊNCIA NO CONTEXTO CONFLITO AMBIENTAL DA MINERAÇÃO DE URÂNIO EM SANTA QUITÉRIA CEARÁ. 2013. (Apresentação de Trabalho/Outra).',\n",
       "   '31.': 'ALVES, Pablo Araújo ;TEIXEIRA, A. C. A.; ANDRADE, T. V. P. ; SILVA, E. C. ;RIGOTTO, Raquel Maria;ROSA, I. F.;PESSOA, V. M.; FERREIRA, M. J. M. . Planificación de la formación de base en las comunidades próximas a la mina de uranio de santa Quitéria, Ceará, Brasil: Temas y estrategias metodológicas bajo la perspectiva de la Promoción de la salud de la Justicia Ambiental. 2012. (Apresentação de Trabalho/Congresso).',\n",
       "   '32.': 'PESSOA, V. M. ; Rigotto, R. M. ;CARNEIRO, F. F.;TEIXEIRA, A. C. A.;Marcelo José Monteiro Ferreira. Las interrelaciones producción-ambiente y las perspectivas para levar a cabo la promoción da la salud, en el Sistema Único de Salud y comtribuir en la mejoría de la calidad de vida, en contextos de vulnerabilidad Socio ambiental. 2012. (Apresentação de Trabalho/Congresso).',\n",
       "   '33.': 'Marcelo José Monteiro Ferreira; PESSOA, V. M. ; Rigotto, R. M. ;TEIXEIRA, A. C. A.;CARNEIRO, F. F.. Integración de los conocimientos y las prácticas de las políticas de salud y educación: recreando estrategias que promuevan la salud en el contexto de la vulnerabilidad socio ambiental en el noroeste brasileño.. 2012. (Apresentação de Trabalho/Congresso).',\n",
       "   '34.': 'TEIXEIRA, A. C. A.;ROSA, I. F.;RIGOTTO, Raquel Maria; ALVES, Pablo Araújo ;Marcelo José Monteiro Ferreira;PESSOA, V. M.. Cartograia social potencializando el diálogo de saberes entre academia, movimientos sociales y comunidades: una estrategia de comunicación de riesgo bajo la perspectiva de promoción de la salud y la justicia ambiental. 2012. (Apresentação de Trabalho/Congresso).',\n",
       "   '35.': 'TEIXEIRA, A. C. A.. Mineração de Urânio em Santa Quitéria. 2012. (Apresentação de Trabalho/Seminário).',\n",
       "   '36.': 'TEIXEIRA, A. C. A.;RIGOTTO, R. M.; ALVES, Pablo Araújo ;PESSOA, V. M.;Marcelo José Monteiro Ferreira;ROSA, I. F.; DIAS, J. A. ; AGUIAR, A. C. P. ; Andrezza Graziella Veríssimo Pontes . O caso da Mineração de Urânio no Ceará: a perspectiva da vigilância de base territorial local. 2012. (Apresentação de Trabalho/Congresso).',\n",
       "   '37.': 'TEIXEIRA, A. C. A.;PEQUENO, A. M. C. M.;PESSOA, V. M.;CARNEIRO, F. F.;Marcelo José Monteiro Ferreira; BEZERRA, M. G. V. . Relato da Oficina de Trabalho Vigilância de Base Territorial Local: desafios e possibilidades. 2012. (Apresentação de Trabalho/Congresso).',\n",
       "   '38.': 'VASCONCELOS, D. P. E. ;ROSA, I. F.;TEIXEIRA, A. C. A.;RIGOTTO, R. M.; CAMARA, L. M. C. . A adoção de metodologias problematizadoras no processo de ensino-aprendizagem de futuros profissionais de saúde. 2012. (Apresentação de Trabalho/Outra).',\n",
       "   '39.': 'NUNES, B. S. ;TEIXEIRA, A. C. A.; FROTA, M. C. ; ARAUJO, J. C. H. ;RIGOTTO, R. M.. A exploração da mina de urânio e fosfato em Santa Quitéria-CE sob as lentes da Avaliação de Equidade Ambiental. 2012. (Apresentação de Trabalho/Outra).',\n",
       "   '40.': 'AGUIAR, A. C. P. ;RIGOTTO, R. M.;ROSA, I. F.;TEIXEIRA, A. C. A.; ALVES, Pablo Araújo . I Jornada Antinuclear do Ceará: uma estratégia por justiça ambiental (Núcleo TRAMAS ? TRABALHO, MEIO AMBIENTE E SAÚDE PARA SUSTENTABILIDADE. 2012. (Apresentação de Trabalho/Outra).',\n",
       "   '41.': 'Holanda. R,S, ;ROSA, I. F.;TEIXEIRA, A. C. A.;RIGOTTO, R. M.. A estratégia de seminários como ferramenta de ensino-aprendizagem. 2012. (Apresentação de Trabalho/Outra).',\n",
       "   '42.': 'MONJE, M. M. ;RIGOTTO, R. M.;ROSA, I. F.;TEIXEIRA, A. C. A.. A relevância na monitoria da disciplina Saúde, Trabalho, Ambiente e Cultura das atividades de anamnese clínico-ocupacional junto a perícia médica do INSS. 2012. (Apresentação de Trabalho/Outra).',\n",
       "   '43.': 'DIAS, J. A. ;ROSA, I. F.;TEIXEIRA, A. C. A.;RIGOTTO, R. M.. Mineração de urânio no Ceará: matriz de Corvalán como ferramenta de ensino. 2012. (Apresentação de Trabalho/Outra).',\n",
       "   '44.': \"DIAS, J. A. ; AGUIAR, A. C. P. ; ALVES, Pablo Araújo ;TEIXEIRA, A. C. A.;RIGOTTO, R. M.. Oficina 'Justiça Ambiental, Exploração de Urânio e Monitoramento Comunitário de Radioatividade': relato da troca de experiência com Caetité-BA. 2012. (Apresentação de Trabalho/Outra).\",\n",
       "   '45.': 'RIBEIRO, J. M. ;RIGOTTO, R. M.;ROSA, I. F.;TEIXEIRA, A. C. A.. Temas emergentes em saúde do trabalhador e saúde ambiental: abordagem teórica e metodológica na graduação médica. 2012. (Apresentação de Trabalho/Outra).',\n",
       "   '46.': 'TEIXEIRA, A. C. A.. Direito ao desenvolvimento e modelo(s) energético(s): estudo de caso e análise. 2012. (Apresentação de Trabalho/Seminário).',\n",
       "   '47.': 'TEIXEIRA, A. C. A.. Experiencias de construção de Avaliações de Equidade Ambiental do Ceará: os desafios e possibilidades para promoção de equidade e justiça ambiental. 2012. (Apresentação de Trabalho/Outra).',\n",
       "   '48.': 'TEIXEIRA, A. C. A.. A mineração de urânio e fosfato no Ceará. 2012. (Apresentação de Trabalho/Outra).',\n",
       "   '49.': 'TEIXEIRA, A. C. A.;Marcelo José Monteiro Ferreira. Superando o distanciamento entre teoria e prática na formação médica. 2011. (Apresentação de Trabalho/Outra).',\n",
       "   '50.': 'Holanda. R,S, ;ROSA, I. F.;TEIXEIRA, A. C. A.. Avaliação quantitativa do conteúdo programático da disciplina Saúde, Trabalho, Ambiente e Cultura: a perspectiva do aluno do terceiro semestre da graduação médica da UFC. 2011. (Apresentação de Trabalho/Outra).',\n",
       "   '51.': 'Marcelo José Monteiro Ferreira;RIGOTTO, R. M.;TEIXEIRA, A. C. A.. A Incorporação da anamnese clínico-ocupacional para a formação médica. 2011. (Apresentação de Trabalho/Outra).',\n",
       "   '52.': 'Marcelo José Monteiro Ferreira;RIGOTTO, Raquel Maria;TEIXEIRA, A. C. A.. Incorporação do Roteiro para Estudo do Processo de Trabalho na Formação Médica. 2011. (Apresentação de Trabalho/Outra).',\n",
       "   '53.': 'TEIXEIRA, A. C. A.. Agroecologia como paradigma para o desenvolvimento rural. 2011. (Apresentação de Trabalho/Congresso).',\n",
       "   '54.': 'Marcelo José Monteiro Ferreira;RIGOTTO, Raquel Maria;TEIXEIRA, A. C. A.. Incorporação do roteiro para estudo do processo de trabalho na formação médica.. 2011. (Apresentação de Trabalho/Congresso).',\n",
       "   '55.': 'Marcelo José Monteiro Ferreira;RIGOTTO, Raquel Maria;TEIXEIRA, A. C. A.. Superando o distanciamento entre teoria e prática na formação médica.. 2011. (Apresentação de Trabalho/Congresso).',\n",
       "   '56.': 'TEIXEIRA, A. C. A.. Os riscos ambientais e o uso abusivo de agrotóxicos na Baixo Jaguaribe. 2011. (Apresentação de Trabalho/Outra).',\n",
       "   '57.': 'Marcelo José Monteiro Ferreira;RIGOTTO, Raquel Maria;TEIXEIRA, A. C. A.;PEQUENO, A. M. C. M.;ROSA, I. F.;PESSOA, V. M.; RIGOTTO, A. M. . Contribuições epistemológicas para o fortalecimento de uma ciência à favor da vida.. 2011. (Apresentação de Trabalho/Congresso).',\n",
       "   '58.': 'Marcelo José Monteiro Ferreira;PEQUENO, A. M. C. M.;TEIXEIRA, A. C. A.;RIGOTTO, Raquel Maria;ROSA, I. F.;PESSOA, V. M.. Epidemiologia e as politicas públicas de saúde. 2011. (Apresentação de Trabalho/Congresso).',\n",
       "   '59.': 'PEQUENO, A. M. C. M.;Marcelo José Monteiro Ferreira;TEIXEIRA, A. C. A.;RIGOTTO, Raquel Maria. Caracteristicas do modelo de produção e processo de trabalho de agricultores familiares camponeses em área de expansão do agronegócio da fruticultura irrigada no baixo jaguaribe.-ce. 2011. (Apresentação de Trabalho/Congresso).',\n",
       "   '60.': 'TEIXEIRA, A. C. A.;ROSA, I. F.; Moreira. A, E, M, ; Silva. M, L, V, ; Santana. I,V,F, ; Cláudia Ribeiro de Barros Leal ; Mendes. G, ;RIGOTTO, Raquel Maria; MEIRELES, A. J. A. . Uma experiência de comunicação de risco com a participação de comunidades situadas no entorno da mina de urânio em Santa Quitèria-Ce. A cartografia social potencializando a autonomia e o direito de saber das comunidades.. 2011. (Apresentação de Trabalho/Congresso).',\n",
       "   '61.': 'ROSA, I. F.;TEIXEIRA, A. C. A.;PEQUENO, A. M. C. M.;Marcelo José Monteiro Ferreira;RIGOTTO, Raquel Maria. Trabalhadores Rurais: Transformações nas caracteristicas demográficas e socioeconômicas no semi-árido do Baixo - Jaguaribe- Ce.. 2011. (Apresentação de Trabalho/Congresso).',\n",
       "   '62.': 'ROSA, I. F.;TEIXEIRA, A. C. A.;PEQUENO, A. M. C. M.;Marcelo José Monteiro Ferreira;RIGOTTO, Raquel Maria. Hábitos de vida diários dos trabalhadores do agronegócio, agricultores familiares e resistência do Baixo Jaguaribe- Ce.. 2011. (Apresentação de Trabalho/Congresso).',\n",
       "   '63.': 'TEIXEIRA, A. C. A.;RIGOTTO, Raquel Maria;LINHARES, Ângela Maria Bessa. A Trajetória metodológica trilhada para o estudo intitulado o trabalho no mangue nas tramas do ( Des)envolvimento e da (Des) ilusão com. 2011. (Apresentação de Trabalho/Congresso).',\n",
       "   '64.': 'Marcelo José Monteiro Ferreira;MARINHO, Alice Maria Pequeno;TEIXEIRA, A. C. A.;RIGOTTO, R. M.;ROSA, I. F.;PESSOA, V. M.. Medidas de controle de risco adotadas em área de uso de agrotoxicos no Baixo Jaguaribe: desvelando a realidade do trabalho entre números e falas.. 2011. (Apresentação de Trabalho/Congresso).',\n",
       "   '65.': 'TUPINAMBA, S. V. ;TEIXEIRA, A. C. A.; BARRETO, H. M. R. . Justiça Ambiental e conflito socioambiental. 2011. (Apresentação de Trabalho/Outra).',\n",
       "   '66.': 'TEIXEIRA, A. C. A.;RIGOTTO, R. M.;RIGOTTO, Raquel Maria; CASTRO, Gigi ; Silva. M, L, V, ;Marcelo José Monteiro Ferreira; Cláudia Ribeiro de Barros Leal ; Maiana Maia Teixeira . A sistematização da pesquisa Estudo epidemiológico da população da região do Baixo Jaguaribe exposta à contaminação ambiental em área de uso de agrotóxicos : construindo estratégias de comunicação de risco. 2010. (Apresentação de Trabalho/Simpósio).',\n",
       "   '67.': 'TEIXEIRA, A. C. A.; Gomes. L,G,A, ; Filho. E,B,B,F, ;MARINHO, Alice Maria Pequeno. Desenho do currículo do curso de especialização em vigilância em saúde ambiental em competências: A Experiência da escola de saúde pública do ceará. 2010. (Apresentação de Trabalho/Simpósio).',\n",
       "   '68.': 'TEIXEIRA, A. C. A.;RIGOTTO, R. M.;LINHARES, Ângela Maria Bessa. As TRAMAS do (des)envolvimento e da (des)ilusão com ?esse furacão chamado carcinicultura?. 2009. (Apresentação de Trabalho/Congresso).',\n",
       "   '69.': 'TEIXEIRA, A. C. A.;LINHARES, Ângela Maria Bessa;RIGOTTO, Raquel Maria. A vida no Cumbe, Aracati-CE: em meio ao conflito com a carcinicultura, a afirmação da identidade tradicional de ?povos do mangue?.. 2009. (Apresentação de Trabalho/Congresso).',\n",
       "   '70.': 'TEIXEIRA, A. C. A.;RIGOTTO, Raquel Maria;LINHARES, Ângela Maria Bessa. O trabalho no mangue e o emprego na carcinicultura: significados construídos pelos catadores de caranguejo do Cumbe-Aracati-CE.. 2009. (Apresentação de Trabalho/Congresso).',\n",
       "   '71.': 'PORTELA, G. A. ;TEIXEIRA, A. C. A.;RIGOTTO, R. M.; SANTOS, A. L. ; ALEXANDRE, S. F. . O processo de construção e desenvolvimento do curso de saúde do trabalhador para a atenção básica do SUS no Ceará: relato de experiência. 2009. (Apresentação de Trabalho/Congresso).',\n",
       "   '72.': 'BEZERRA, M. G. V. ;RIGOTTO, Raquel Maria;PESSOA, V. M.;TEIXEIRA, A. C. A.. Processos de desenvolvimento, transformações sócio-ambientais e as repercussões sobre a saúde: o olhar da comunidade de bolso em São Gonçalo do Amarante-CE. 2009. (Apresentação de Trabalho/Congresso).',\n",
       "   '73.': 'Rigotto, R. M. ; BRAGA, L. Q. V. ; PESSOA, V. M. ; SANTIAGO, A. V. ; MOREIRA, A. E. M. M. ;TEIXEIRA, A. C. A.. Indígenas em trabalho escravo? Integrando saberes, produção de conhecimento e fortalecimento comunitário na coordenação entre universidade e movimentos sociais. 2009. (Apresentação de Trabalho/Congresso).',\n",
       "   '74.': 'RIGOTTO, Raquel Maria; BRAGA, L. Q. V. ;PESSOA, V. M.; Santiago. A,V, ; Moreira. A, E, M, ;TEIXEIRA, A. C. A.. Indígenas em trabalho escravo? Integrando saberes, produção de conhecimento e fortalecimento comunitário na cooperação entre universidade e movimentos sociais.. 2009. (Apresentação de Trabalho/Congresso).',\n",
       "   '75.': 'RIGOTTO, Raquel Maria;TEIXEIRA, A. C. A.. A contribuição da disciplina Saúde, Trabalho, Ambiente e Cultura na formação do médico numa perspectiva da promoção da saúde. ABRASCO. 2007. (Apresentação de Trabalho/Congresso).',\n",
       "   '76.': 'TEIXEIRA, A. C. A.;LINHARES, Ângela Maria Bessa;RIGOTTO, Raquel Maria. O corpo disciplinado implícito nas concepções de saúde-doença: da teoria do germe à teoria da tríade ecológica. 2007. (Apresentação de Trabalho/Congresso).',\n",
       "   '77.': 'GODOY, M.G.C ;RIGOTTO, Raquel Maria; MACIEL, R.H.M.O ;TEIXEIRA, A. C. A.; LOPES, C.H. . Condições organizacionais e saúde mental dos trabalhadores dos CAPS do Ceará. ABRASCO. 2007. (Apresentação de Trabalho/Congresso).',\n",
       "   '78.': 'TEIXEIRA, A. C. A.;RIGOTTO, Raquel Maria. O paradigma da promoção da saúde: avanços e limitações nos marcos do capitalismo. 2006. (Apresentação de Trabalho/Congresso).',\n",
       "   '79.': 'ANDRADE, Maria das Graças Monteiro de Sales ;TEIXEIRA, A. C. A.. Avaliação das Momografias do Serviço Público no Município de Fortaleza. 2004. (Apresentação de Trabalho/Simpósio).',\n",
       "   '80.': 'SOUSA, Liduina Virginio de ; NORÕES, Gláucia Maria Reis de ; OLIVEIRA, Diana Carmem Almeida Nunes de ;TEIXEIRA, A. C. A.. Seminário de Promoção da Saúde: perfil dos atores e participantes, envolvidos na construção da intersetorialidade. 2004. (Apresentação de Trabalho/Simpósio).',\n",
       "   '81.': 'SOUSA, Liduina Virginio de ; NORÕES, Glaúcia Maria Reis de ; CRUZ, Sérgio Murilo Martins ; OLIVEIRA, Diana Carmem Almeida Nunes de ;TEIXEIRA, A. C. A.. Avaliação das Ações de Vigilância da Qualidade de Água para Consumo Humano do Ceará: 2002 a 2004. 2004. (Apresentação de Trabalho/Simpósio).',\n",
       "   '82.': 'CASTRO, Maria Goretti Gurgel Mota de ; CRUZ, Sérgio Murilo Martins ; OLIVEIRA, Diana Carmem Almeida Nunes de ; SOUZA, Liduina Virgínio de ; NORÕES, Glaúcia Maria Reis ;TEIXEIRA, A. C. A.. Melioidose no Ceará: a identificação da Burkholderia Pseudomalley no ambiente. 2004. (Apresentação de Trabalho/Outra).',\n",
       "   '83.': 'MONTE, Viviane Gomes ; CASTRO, Maria Goretti Gurgel Mota de ; OLIVEIRA, Diana Carmem Almeida Nunes de ; SOUSA, Liduina Virginio de ; NORÕES, Glaúcia Maria Reis de ;TEIXEIRA, A. C. A.. Vigilância Ambiental da Melioidose, Ceará, 2004. 2004. (Apresentação de Trabalho/Simpósio).',\n",
       "   '84.': 'NORÕES, Gláucia Maria Reis de ; ALMEIDA, Gilson Holanda ;MARINHO, Alice Maria Pequeno;TEIXEIRA, A. C. A.; OLIVEIRA, Diana Carmem Almeida Nunes de ; CASTRO, Maria Goretti Gurgel Mota de ; SOUZA, Liduina Virginio de ; GUERINO, Selma Simões ; CRUZ, Sérgio Murilo Martins ; MONTE, Viviane Gomes ; SILVEIRA, Francisca Lucília Costa . Fórum Temático: articulação de ações intersetoriais para a promoção do uso sustentável da água. 2004. (Apresentação de Trabalho/Simpósio).',\n",
       "   '85.': 'TEIXEIRA, A. C. A.; OLIVEIRA, Diana Carmem de A N ; NORÕES, Gláucia Reis de ; CASTRO, Maria Goretti G M ; SOUSA, Liduina Virginio de ; GUERINO, Selma Simões ; MONTE, Viviane Gomes ; CRUZ, Sérgio Murilo M ; FIGUEREDO, Ana Maria A . Oficina de Integração e Articulação das Comissões Intersetoriais de Saúde, Ambiente e Trabalho: construindo estratégias da ação intersetorial no Ceará. 2004. (Apresentação de Trabalho/Simpósio).',\n",
       "   '86.': 'TEIXEIRA, A. C. A.; OLIVEIRA, Diana Carmem Almeida Nunes ; NORÕES, Gláucia Maria Reis . Metodologia da Investigação Científica: Importância para a produção de conhecimento no âmbito da Vigilância Sanitária, Controle e Avaliação em Saúde. 2004. (Apresentação de Trabalho/Simpósio).',\n",
       "   '87.': 'NEVES, Kelly Rose Tavares ;TEIXEIRA, A. C. A.; SÁ, Ricardo Carvalho de Azevedo e ; CARLOS, Isabel Cristina Cavalcanti ; MOTA, Daniel Marques . Curso de Especialização em Assistência Farmacêutica: Análise do Desempenho e Satisfação dos Participantes a partir de Instrumentos de Avaliação. 2002. (Apresentação de Trabalho/Congresso).',\n",
       "   '88.': 'TEIXEIRA, A. C. A.. Mais que devolução de resultados, um horizonte de espernças transformadoras.. 2001. (Apresentação de Trabalho/Congresso).',\n",
       "   '89.': 'TEIXEIRA, A. C. A.. Fatores que determinam a adesão ao tratamento anti-hipertensivo em pacientes de ambulatório. 2000. (Apresentação de Trabalho/Congresso).',\n",
       "   '90.': 'TEIXEIRA, A. C. A.; COELHO, Helena Lutéscia ; MONTE, Cristina Maria Gomes Do . Adesão ao Tratamento de Hipertensão Arterial e seus Determinantes em pacientes de Ambulatório. 1998. (Apresentação de Trabalho/Congresso).',\n",
       "   '91.': 'TEIXEIRA, A. C. A.; SILVA, J A da ; SERRA, G M C . Estudo descritivo do uso de medicamentos na unidade de pacientes externos do Hospital de Messejana, Fortaleza-CE, Hosp. de Messejana. 1997. (Apresentação de Trabalho/Congresso).',\n",
       "   '92.': 'TEIXEIRA, A. C. A.; SILVA, J A ; MELO, e M ; BRASILEIRO, e B O . Perfil da Utilização de Medicamentos na Unidade de Pacientes Externos do Hospital de Messejana, Fortaleza-CE. 1996. (Apresentação de Trabalho/Seminário).',\n",
       "   '93.': 'TEIXEIRA, A. C. A.. Investigação do Uso de Medicamentos nos Serviços de Saúde do Município de Fortaleza-CE. 1996. (Apresentação de Trabalho/Seminário).',\n",
       "   '94.': 'TEIXEIRA, A. C. A.; SILVA, J A ; MELO, e M ; BRASILEIRO, e B O . Perfil da utilização de medicamentos na unidade de pacientes externos do Hospital de Messejana, Fortaleza-CE. 1996. (Apresentação de Trabalho/Seminário).',\n",
       "   '95.': 'COELHO, H L L ;TEIXEIRA, A. C. A.; SILVA, J A ; MELO, O F . Misoprostol e Clandestinidade em Fortaleza-CE. 1995. (Apresentação de Trabalho/Congresso).',\n",
       "   '96.': 'TEIXEIRA, A. C. A.. Farmacoepidemiologia e Farmacovigilância , nova área de ensino, pesquisa e extensão em cursos de Farmácia: uma experiência. 1995. (Apresentação de Trabalho/Congresso).',\n",
       "   '97.': 'TEIXEIRA, A. C. A.. Prevalência do Uso de Medicamentos por Mulheres em Idade Fértil no Município de Fortaleza. 1993. (Apresentação de Trabalho/Outra).',\n",
       "   '98.': 'COELHO, H C L ;TEIXEIRA, A. C. A.; SANTOS, A P C ; FORTE, e B ; LUCHINI, L S . Uso do Misoprostol para Interrupção Voluntária da Gravidez: a experiência das mulheres de Fortaleza, parte I. 1993. (Apresentação de Trabalho/Congresso).',\n",
       "   '99.': 'COELHO, H L L ;TEIXEIRA, A. C. A.; VALE, R M G ; MONTE, M C A ; BARBOSA, M A . Uso do Misoprostol para Interrupção Voluntária da Gravidez: a experiência das mulheres de Fortaleza, parte II. 1993. (Apresentação de Trabalho/Congresso).',\n",
       "   '100.': 'COELHO, H L L ;TEIXEIRA, A. C. A.; CRUZ, A C A ; GONZAGA, M F S ; LUCHINI, S L P . Uso do Misoprostol para Interrupção Voluntária da Gravidez: a experiência das mulheres de Fortaleza I. Caracterização da Amostra. 1993. (Apresentação de Trabalho/Outra).',\n",
       "   '101.': 'COELHO, H L L ;TEIXEIRA, A. C. A.; VALE, R M G ; MONTE, M C A ; BARBOSA, M A . Uso do Misoprostol para Interrupção Voluntária da Gravidez: a experiência das mulheres de Fortaleza II. Variáveis da Tentativa de Abortar. 1993. (Apresentação de Trabalho/Outra).',\n",
       "   '102.': 'TEIXEIRA, A. C. A.; COELHO, H L L ; VALE, R M G ; MONTE, M C A ; BARBOSA, M A . Uso do Misoprostol para Interrupção Voluntária da Gravidez: a experiência das mulheres de Fortaleza III. Opinião sobre o Aborto e o Misoprostol. 1993. (Apresentação de Trabalho/Outra).',\n",
       "   '103.': 'COELHO, Helena Lutéscia Luna ;TEIXEIRA, A. C. A.; VALE, R M G ; MONTE, M C A ; BARBOSA, M A . Uso do Misoprostol para Interrupção Voluntária da Gravidez: a experiência das mulheres de Fortaleza, parte III,. 1993. (Apresentação de Trabalho/Congresso).',\n",
       "   '104.': 'COELHO, H L L ; ARRAIS, P S D ; SANTOS, A P C ;TEIXEIRA, A. C. A.. Automedicação em Fortaleza-CE. 1992. (Apresentação de Trabalho/Outra).',\n",
       "   '105.': 'COELHO, H L L ;TEIXEIRA, A. C. A.; MORAIS, S M ; CRUZ, M F S ; GONZAGA, S L P . Evolução do Uso do Misoprostol em Fortaleza-CE: informação e proibição. 1992. (Apresentação de Trabalho/Outra).',\n",
       "   '106.': 'COELHO, H L L ;TEIXEIRA, A. C. A.; MORAIS, S M ; CRUZ, M F S ; GONZAGA, S L P . Evolução do Uso do Misoprostol em Fortaleza-CE: informação e proibição. 1992. (Apresentação de Trabalho/Outra).',\n",
       "   '107.': 'TEIXEIRA, A. C. A.. Auto-Administração de Misoprostol como Abortivo - A Experiência das Mulheres de Fortaleza. 1992. (Apresentação de Trabalho/Outra).',\n",
       "   '108.': 'TEIXEIRA, A. C. A.. Indicação de Medicamentos com Finalidade Abortiva em Farmácias de Fortaleza. 1990. (Apresentação de Trabalho/Outra).',\n",
       "   '109.': 'TEIXEIRA, A. C. A.. Determinação do Teor Alcalóides nas Folhas de Colonia (Alinia Speciosa Schum) e Ensaios Farmacológicos para Verificação do Efeito Hipotensor. 1989. (Apresentação de Trabalho/Outra).'},\n",
       "  'Outras produções bibliográficas': {'1.': 'TEIXEIRA, A. C.; ALVES, Pablo Araújo . Nas TRAMAS da pesquisa-ação in Silva, E. C.; Barros, M. R. S. (Orgs) - No Ceará a peleja da vida contra o urânio.\\n\\t\\t\\t\\t\\t\\tFortaleza-CE 2013 (Desenvolvimento de material didático ou instrucional - Almanaque educativo).',\n",
       "   '2.': 'CASTRO, G. ; ROCHA, Mayara Melo ;RIGOTTO, Raquel Maria; AGUIAR, A. C. P. ; MARINHO, A. M. C. P. ;TEIXEIRA, A. C. A.; ELLERY, A. E. L. ; LEAL, C. R. B. ; GADELHA, D. C. ;CARNEIRO, F. F.; PAULINO, F. A. ; PORTELA, G. A. ; SOUSA NETA, H. B. ;ROSA, I. F.; RAMERES, J. ; SAMPAIO, J. L. F. ; BRAGA, L. Q. V. ; AUGUSTO, L. G. S. ; TEIXEIRA, M. M. ; FERREIRA, M. J. M. . Almanaque do Baixo Jaguaribe ou TRAMAS para a afirmação do trabalho, meio ambiente e saúde para a sustentabilidade. Espressão Gráfica,\\n\\t\\t\\t\\t\\t\\t\\t 2012 (Desenvolvimento de material didático ou instrucional - Almanaque educativo).',\n",
       "   '3.': 'RIGOTTO, Raquel Maria;TEIXEIRA, A. C. A.. Desenvolvimento e Sustentabilidade socioambiental no campo, na cidade e na floresta.\\n\\t\\t\\t\\t\\t\\tRio de janeiro:\\n\\t\\t\\t\\t\\t\\t\\t\\tAbrasco Livros,  2009 (Cadeno de Textos da I Conferência Nacional de Saúde Ambiental).',\n",
       "   '4.': 'LOPES, Antonia Eufrasina Campelo ;TEIXEIRA, A. C. A.; GURGEL, Maria Lúcia Fernandes ; MIRANDA, Maria Do Ceu Carneiro de ; OLIVERIA, Maria Antonieta ; OLIVEIRA, Marilia Machado Luz ; MURTA, Regina Lúcia Gomes ; FREITAS, Rosa Maria Araujo ; MENEZES, Sylvana Macedo de Morais ; BRAGA, Wania Maria Souza ; JONCHERE, Kees de . Drug Use Evaluation in Health Services in Fortaleza, Brazil 1996 (Publicação em Boletim).'},\n",
       "  'Trabalhos técnicos': {'1.': 'TEIXEIRA, A. C. A.. Equipe de Revisão e Sistematização da Obra: EDUCAÇÃO POPULAR EM SAÚDE E A CONVIVÊNCIA COM O SEMIÁRIDO: DIÁLOGOS EM VERSO, PROSA E CENOPOESIA. 2023.',\n",
       "   '2.': 'TEIXEIRA, ANA CLÁUDIA DE ARAÚJO; BEZERRA, M. G. V. . Construindo Subsídios para Implementação da Unidade de  Conservação da Lagoa da Precabura. 2023.',\n",
       "   '3.': 'TEIXEIRA, A. C. A.. Equipe de Revisão e Sistematização da Obra: A PRODUÇÃO DE SABERES EMERGENTES NA INTERFACE ENTRE A EDUCAÇÃO POPULAR, SAÚDE E A CONVIVÊNCIA COM O SEMIÁRIDO. 2022.',\n",
       "   '4.': 'TEIXEIRA, A. C. A.. Equipe de Revisão e Sistematização da Obra: TRAJETÓRIAS E APRENDIZADOS DO CURSO DE EDUCAÇÃO POPULAR E PROMOÇÃO DE TERRITÓRIOS SAUDÁVEIS NA CONVIVÊNCIA COM O SEMIÁRIDO. 2022.',\n",
       "   '5.': 'TEIXEIRA, A. C. A.;CARNEIRO, F. F.; CORDEIRO, J. L. P. ; ORTRIZ, A. C. M. ; HACON, S. . Nota Técnica do Grupo Temático de Saúde e Ambiente daFiocruz Ceará - Assunto:Proteção eConservação da Lagoa da Precabura. 2019.',\n",
       "   '6.': 'ALEXANDRE, S. F. ;TEIXEIRA, A. C. A.;RIGOTTO, R. M.; FERREIRA, M. J. M. ; BEZERRA, H. ; JUSSARA, J. . Estudo de caso: anamnese clínico ocupacional com trabalhador do agronegócio. 2011.',\n",
       "   '7.': 'RIGOTTO, Raquel Maria; MEIRELES, A. J. A. ;TEIXEIRA, A. C. A.. Abordagem preliminar dos impactos da atividade de carcinicultura no modo de vida e na saúde da Comunidade de Curral Velho, em Acaraú/CE. 2009.',\n",
       "   '8.': 'TEIXEIRA, A. C. A.. Elaboração do Desenho do Curriculo do Curso de Saúde do Trabalhador para Atenção Básica de Saúde do Trabalhador. 2007.',\n",
       "   '9.': 'TEIXEIRA, A. C. A.. Elaboração do Desenho do Currículo do Curso Especialização em Saúde do Trabalhador e Saúde Ambiental.. 2007.',\n",
       "   '10.': 'TEIXEIRA, A. C. A.; SANTOS, A. L. ; PORTELA, G. A. ; ALEXANDRE, S. F. ;RIGOTTO, R. M.. O processo de construção e desenvolvimento do curso de saúde do trabalhador para a atenção básica do SUS no Ceará: relato de experiência. 2007.'},\n",
       "  'Entrevistas, mesas redondas, programas e comentários na mídia': {'1.': 'TEIXEIRA, A. C.. Programa Rádio Universitária. 2013.\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t(Programa de rádio ou TV/Entrevista).',\n",
       "   '2.': 'TEIXEIRA, A. C. A.; VILASBOAS, Z. ; BARBOSA, O. ; MENDONCA, L. ; VALENTIM, T. . 1a. Jornada Antinuclear do Ceará: o presente que temos em Caetité, o futuro que queremos em Santa Quitéria. 2012.\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t(Programa de rádio ou TV/Entrevista).',\n",
       "   '3.': 'TEIXEIRA, A. C. A.; SILVA, E. C. ; VILASBOAS, Z. ; BARBOSA, O. ; CARDOSO, F. ; MENDONCA, L. ; FAUSTINO, C. . 1a. Jornada Antinuclear do Ceará: o presente que temos em Caetité, o futuro que queremos em Santa Quitéria. 2012.\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t(Programa de rádio ou TV/Mesa redonda).',\n",
       "   '4.': 'TEIXEIRA, A. C. A.; ANDRADE, T. V. P. ; MENDONCA, L. ; CARDOSO, F. . 1a. Jornada Antinuclear do Ceará: o presente que temos em Caetité, o futuro que queremos em Santa Quitéria. 2012.\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t(Programa de rádio ou TV/Mesa redonda).',\n",
       "   '5.': 'TEIXEIRA, A. C. A.. Energia Nuclear: no Ceará, Jornada coloca em discussão os riscos que representam esses investimentos. 2012.\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t(Programa de rádio ou TV/Entrevista).',\n",
       "   '6.': 'TEIXEIRA, A. C. A.. Falando de Medicamentos. 1996.\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t(Programa de rádio ou TV/Outra).'},\n",
       "  'Demais tipos de produção técnica': {'1.': \"TEIXEIRA, A. C. A.. Coordenadora do Curso: 'Encontros que tecem possibilidades de re-existir: experiência formativa em Vigilância Popular',. 2022. (Curso de curta duração ministrado/Outra).\",\n",
       "   '2.': 'CARNEIRO, F. F.;TEIXEIRA, A. C.; DANTAS, V. L. A. ; LIMA, G. M. C. ; PEREIRA, A. M. G. ; LIMA, R. F. ;PESSOA, V. M.; SILVA, M. R. F. . Especialização e Aperfeiçoamento em Educação Popular e Promoção de Territórios Saudáveis na Convivência com o Semiárido. 2019. (Curso de curta duração ministrado/Especialização).',\n",
       "   '3.': 'TEIXEIRA, A. C.; DANTAS, V. L. A. . Planejamento: Módulo III: Educação Popular em Saúde e o Módulo VI: Construção Compartilhada do Conhecimento do  Curso de Especialização e Aperfeiçoamento em Educação Popular e Promoção de Territórios Saudáveis na Convivência com o Semiárido. 2019. (Curso de curta duração ministrado/Especialização).',\n",
       "   '4.': 'TEIXEIRA, A. C. A.;RIGOTTO, R. M.; CASTRO, G. ; SANTANA, L. M. ; PINHEIRO, C. . DE CAETITÉ (BA) A SANTA QUITÉRIA (CE): AS SAGAS DA EXPLORAÇÃO DO URÂNIO NO BRASIL.\\n\\t\\t\\t\\t\\t\\t2013. (Desenvolvimento de material didático ou instrucional - vídeo educacional).',\n",
       "   '5.': 'TEIXEIRA, A. C.;ROSA, I. F.. Módulo Introdutório à Saúde do Trabalhador do Curso de Aperfeiçoamento em Saúde do Trabalhador.\\n\\t\\t\\t\\t\\t\\t2013. (Desenvolvimento de material didático ou instrucional - Manual didático).',\n",
       "   '6.': 'TEIXEIRA, A. C.; Rigotto, R. M. . Territorialização em Saúde: estudo das relações produção, ambiente, saúde e cultura na atenção primária à saúde. 2013. (Relatório de pesquisa).',\n",
       "   '7.': 'TEIXEIRA, A. C.; ALVES, Pablo Araújo . Conflito ambiental relacionado à mineração de urânio e fosfato em Santa Quitéria in Dossiê IMPACTOS DOS GRANDES PROJETOS E MUDANÇAS CLIMÁTICAS NO CEARÁ. 2013.\\n\\t\\t\\t\\t\\t\\t(Dossiê).',\n",
       "   '8.': 'TEIXEIRA, A. C. A.. Projeto Vivências e Estágios na Realidade do Sistema Único de Saúde do Brasil (VER/SUS). 2013.\\n\\t\\t\\t\\t\\t\\t(Exposição Interativa).',\n",
       "   '9.': 'TEIXEIRA, A. C. A.;RIGOTTO, R. M.. Territorialização em Saúde: estudo das relações produção, ambiente, saúde e cultura na atenção primária ä saúde. 2012. (Relatório de pesquisa).',\n",
       "   '10.': 'TEIXEIRA, A. C. A.. Curso Justiça Ambiental. 2011. (Curso de curta duração ministrado/Outra).',\n",
       "   '11.': 'TEIXEIRA, A. C. A.;RIGOTTO, R. M.. Territorialização em Saúde: estudo das relações produção, ambiente, saúde e cultura na atenção primária ä saúde. 2011. (Relatório de pesquisa).',\n",
       "   '12.': 'MARINHO, Alice Maria Pequeno;TEIXEIRA, A. C. A.; SANTIAGO, J. C. L. C. ; GOMES, L. G. A. ; MELO, M. L. B. . Módulo VI ? Vigilância em Saúde Ambiental: Modelos e Formas de Atuação - Unidade I - Qualidade da Água.\\n\\t\\t\\t\\t\\t\\t2010. (Desenvolvimento de material didático ou instrucional - Manual do III Curso de Especialização em Vigilância em Saúde Ambiental).',\n",
       "   '13.': 'TEIXEIRA, A. C. A.;MARINHO, Alice Maria Pequeno;ROSA, I. F.; GOMES, L. G. A. . Módulo VI ? Vigilância em Saúde Ambiental: Modelos e Formas de Atuação - Unidade II - Qualidade da água e populações expostas a solo contaminado.\\n\\t\\t\\t\\t\\t\\t2010. (Desenvolvimento de material didático ou instrucional - Manual do III Curso de Especialização em Vigilância em Saúde Ambiental).',\n",
       "   '14.': 'TEIXEIRA, A. C. A.;MARINHO, Alice Maria Pequeno; GOMES, L. G. A. ; SERAFIM, F. G. . Módulo VI ? Vigilância em Saúde Ambiental: Modelos e Formas de Atuação - Unidade III -Populações expostas a solo contaminado e à poluição do ar.\\n\\t\\t\\t\\t\\t\\t2010. (Desenvolvimento de material didático ou instrucional - Manual do III Curso de Especialização em Vigilância em Saúde Ambiental).',\n",
       "   '15.': 'TEIXEIRA, A. C. A.;ROSA, I. F.;PESSOA, V. M.; CARVALHO, Q. C. M. ;MARINHO, Alice Maria Pequeno. Módulo I - Produção, Ambiente e Saúde - Unidade I - Introdução ao Estudo das Relações Produção, Ambiente e Saúde.\\n\\t\\t\\t\\t\\t\\t2010. (Desenvolvimento de material didático ou instrucional - Manual do I Curso de Especialização em Saúde do Trabalhador).',\n",
       "   '16.': 'TEIXEIRA, A. C. A.. Modelos de Estudos em Farmacoepidemiologia. 2009. (Curso de curta duração ministrado/Especialização).',\n",
       "   '17.': 'TEIXEIRA, A. C. A.. Introdução à Epidemiologia: história, fundamentos, medidas de frequência de doenças, indicadores de saúde, medidas de associações. 2009. (Curso de curta duração ministrado/Especialização).',\n",
       "   '18.': 'TEIXEIRA, A. C. A.. Especialização em Farmácia Hospitalar. 2009. (Curso de curta duração ministrado/Especialização).',\n",
       "   '19.': 'TEIXEIRA, A. C. A.. Especialização em Farmácia Hospitalar. 2009. (Curso de curta duração ministrado/Especialização).',\n",
       "   '20.': 'TEIXEIRA, A. C. A.. Curso de Pós Graduação Lato Sensu em Saude do Trabalhador. 2009. (Curso de curta duração ministrado/Outra).',\n",
       "   '21.': 'MARINHO, Alice Maria Pequeno;TEIXEIRA, A. C. A.; BARREIRA FILHO, E. B. ; GOMES, L. G. A. . Módulo I ? Políticas de Saúde: a construção da Política de Saúde Ambiental no Brasil.\\n\\t\\t\\t\\t\\t\\t2009. (Desenvolvimento de material didático ou instrucional - Manual do III Curso de Especialização em Vigilância em Saúde Ambiental).',\n",
       "   '22.': 'PEQUENO, A. M. C. M.;TEIXEIRA, A. C. A.; BARREIRA FILHO, E. B. ; GOMES, L. G. A. ; SCHAUMANN, G. P. . Módulo II ? Planejamento e Gestão de Serviços de Vigilância em Saúde Ambiental.\\n\\t\\t\\t\\t\\t\\t2009. (Desenvolvimento de material didático ou instrucional - Manual do III Curso de Especialização em Vigilância em Saúde Ambiental).',\n",
       "   '23.': 'TEIXEIRA, A. C. A.; BARREIRA FILHO, E. B. ; GOMES, L. G. A. ;MARINHO, Alice Maria Pequeno. Módulo III ? As Inter-Relações Ambiente e Saúde no Território.\\n\\t\\t\\t\\t\\t\\t2009. (Desenvolvimento de material didático ou instrucional - Manual do III Curso de Especialização em Vigilância em Saúde Ambiental).',\n",
       "   '24.': 'MARINHO, Alice Maria Pequeno;TEIXEIRA, A. C. A.; BARREIRA FILHO, E. B. ; GOMES, L. G. A. . Módulo IV ? Vigilância em Saúde Ambiental: objeto e campo de atuação.\\n\\t\\t\\t\\t\\t\\t2009. (Desenvolvimento de material didático ou instrucional - Manual do III Curso de Especialização em Vigilância em Saúde Ambiental).',\n",
       "   '25.': 'MARINHO, Alice Maria Pequeno;TEIXEIRA, A. C. A.; BARREIRA FILHO, E. B. ; ALONZO, H. G. A. ; GOMES, L. G. A. . Módulo V ? Epidemiologia e Toxicologia Aplicada à Vigilância em Saúde Ambiental - Unidade I - Epidemiologia Aplicada à Vigilância em Saúde Ambiental.\\n\\t\\t\\t\\t\\t\\t2009. (Desenvolvimento de material didático ou instrucional - Manual do III Curso de Especialização em Vigilância em Saúde Ambiental).',\n",
       "   '26.': 'TEIXEIRA, A. C. A.; BARREIRA FILHO, E. B. ; SANTIAGO, J. C. L. C. ; GOMES, L. G. A. . Módulo V ? Epidemiologia e Toxicologia Aplicada à Vigilância em Saúde Ambiental - Unidade II - Toxicologia Aplicada à Vigilância em Saúde Ambiental.\\n\\t\\t\\t\\t\\t\\t2009. (Desenvolvimento de material didático ou instrucional - Manual do III Curso de Especialização em Vigilância em Saúde Ambiental).',\n",
       "   '27.': 'MARINHO, Alice Maria Pequeno; LEMOS, A. F. ;TEIXEIRA, A. C. A.; GOMES, L. G. A. ; MELO, M. L. B. . Módulo V ? Epidemiologia e Toxicologia Aplicada à Vigilância em Saúde Ambiental - Unidade III - Avaliação e Comunicação de Risco.\\n\\t\\t\\t\\t\\t\\t2009. (Desenvolvimento de material didático ou instrucional - Manual do III Curso de Especialização em Vigilância em Saúde Ambiental).',\n",
       "   '28.': 'RIGOTTO, Raquel Maria;TEIXEIRA, A. C. A.; MEIRELES, A. J. A. ; NOGUEIRA, F. N. A. . Impactos do processo produtivo e das transformações ambientais da carcinicultura sobre a saúde humana no Baixo Jaguaribe, Ceará/Brasil. 2008. (Relatório de pesquisa).',\n",
       "   '29.': 'TEIXEIRA, A. C. A.. Saúde do Trabalhador para a Atenção Básica em Saúde. 2007. (Curso de curta duração ministrado/Outra).',\n",
       "   '30.': 'RIGOTTO, Raquel Maria; MACIEL, R. H ; GODOY, M. G. C. ;TEIXEIRA, A. C. A.; LOPES, C. H. ; TOFOLI, L. F. ; CAVALCANTE, N. C. . Análise das condições organizacionais e de seu impacto sobre a saúde dos trabalhadores dos Centros de Atenção Psicossocial do Ceará. 2007. (Relatório de pesquisa).',\n",
       "   '31.': 'TEIXEIRA, A. C. A.. Integração à Prática Farmacêutica I. 2007.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '32.': 'TEIXEIRA, A. C. A.. Desenvolvimento social, econômico, institucional e sustentabilidade. 2007.\\n\\t\\t\\t\\t\\t\\t(Palestras ministradas).',\n",
       "   '33.': 'TEIXEIRA, A. C. A.. Ministrou aula sobre Práticas Integrativas e Comportamentares em saúde e sua Inserção na Atenção Básica. 2006.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '34.': 'TEIXEIRA, A. C. A.. Curso de Especialiazação em Gestão da Assistência Farmacêutica. 2004. (Curso de curta duração ministrado/Especialização).',\n",
       "   '35.': 'TEIXEIRA, A. C. A.. Palestrante do tema Apresentando Informações em Saúde aos alunos do Curso de Especialização em Assistência Farmacêutica ? Turma 2003. 2004.\\n\\t\\t\\t\\t\\t\\t(Palestras ministradas).',\n",
       "   '36.': 'TEIXEIRA, A. C. A.. Ministrou o módulo Introdução à Farmacoepidemiologia para os alunos do Curso de Especialização em Assist Farmacêutica - Turma 2004. 2004.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '37.': 'TEIXEIRA, A. C. A.. Palestrante do tema Farmacoepidemiologia - Estudo de Utilização de Medicamentos aos alunos do Curso de Especialização em Vigilância Sanitária? Turma 2003. 2004.\\n\\t\\t\\t\\t\\t\\t(Palestras ministradas).',\n",
       "   '38.': 'TEIXEIRA, A. C. A.. Ministrou o módulo Metodologia da Investigação Científica para os alunos do Curso de Especialização em Assist Farmacêutica - Turma 2004. 2004.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '39.': 'TEIXEIRA, A. C. A.. Ministrou o módulo Metodologia da Investigação Científica para os alunos do Curso de Especialização em Vigilância Sanitária - Turma 2004. 2004.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '40.': 'TEIXEIRA, A. C. A.. Ministrou Seminário sobre Metodologia Científica, no Curso de Mestrado em Ciências Farmacêuticas. 2003.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '41.': 'TEIXEIRA, A. C. A.. Palestrante do tema O Processo de Elaboração de uma Pesquisa aos alunos do Curso de Especialização em Assistência Farmacêutica. 2003.\\n\\t\\t\\t\\t\\t\\t(Palestras ministradas).',\n",
       "   '42.': 'TEIXEIRA, A. C. A.. Palestrante do tema Introdução a Farmacoepidemiologia: conceito, histórico, objetivos e aplicações, aos alunos do Curso de Especialização em Assistência Farmacêutica? Turma 2003. 2003.\\n\\t\\t\\t\\t\\t\\t(Palestras ministradas).',\n",
       "   '43.': 'TEIXEIRA, A. C. A.. Metodologia da Investigação Científica. 2003.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '44.': 'TEIXEIRA, A. C. A.; COELHO, H L L ; MONTE, C M G . Desempenho da entrevista comparado à contagem de comprimidos na determinação da adesão ao tratamento farmacológico da hipertensão em pacientes de ambulatório. 2002.\\n\\t\\t\\t\\t\\t\\t(Palestras ministradas).',\n",
       "   '45.': 'TEIXEIRA, A. C. A.. Palestrante sobre o tema Métodos Epidemiológicos aos alunos do Curso de Especialização em Gestão de Sistemas Locais de Saúde. 2002.\\n\\t\\t\\t\\t\\t\\t(Palestra proferida).',\n",
       "   '46.': 'TEIXEIRA, A. C. A.. Ministrou aula sobre o tema Farmacoepidemiologia aos alunos do I Curso de Direito Sanitário da Secretaria da Saúde do Estado do Ceará. 2002.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '47.': 'TEIXEIRA, A. C. A.. Metodologia da Investigação Científica. 2002.\\n\\t\\t\\t\\t\\t\\t(Exposição Interativa).',\n",
       "   '48.': 'TEIXEIRA, A. C. A.. Palestrante sobre o tema Modelos de Estudos em Farmacologia aos alunos do Curso de Especialização em Assistência Farmacêutica. 2002.\\n\\t\\t\\t\\t\\t\\t(Palestras ministradas).',\n",
       "   '49.': 'TEIXEIRA, A. C. A.. Palestrante sobre o tema Fundamentos Básicos da Farmacoepidemiologia aos alunos do Curso  Básico em Assistência Farmacêutica. 2001.\\n\\t\\t\\t\\t\\t\\t(Palestras ministradas).',\n",
       "   '50.': 'TEIXEIRA, A. C. A.. Seminário Farmacoepidemiologia. 2001.\\n\\t\\t\\t\\t\\t\\t(Palestras ministradas).',\n",
       "   '51.': 'TEIXEIRA, A. C. A.. Palestrante do tema Farmacoepidemiologia aos alunos do Curso de Especialização em Saúde da Família. 2001.\\n\\t\\t\\t\\t\\t\\t(Palestras ministradas).',\n",
       "   '52.': 'TEIXEIRA, A. C. A.. Aula ministrada sobre Metodologia da Investigação Científica aos alunos do Curso de Especialização em Gerenciamento e Atenção Farmacêutica. 2001.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '53.': 'TEIXEIRA, A. C. A.. Conferencia proferida sobre o tema Aderência ao Tratamento Medicamentoso aos participantes da III Jornada Farmacêutica do Hospital Geral de Fortaleza. 2000.\\n\\t\\t\\t\\t\\t\\t(Conferência proferida).',\n",
       "   '54.': 'TEIXEIRA, A. C. A.. Palestrante sobre o tema Farmacoepidemiologia aos alunos do Curso de Especialização em Vigilância Sanitária. 2000.\\n\\t\\t\\t\\t\\t\\t(Palestras ministradas).',\n",
       "   '55.': 'TEIXEIRA, A. C. A.. Aula ministrada sobre Metodologia da Investigação Científica aos alunos do Curso de Especialização em Farmácia Hospitalar. 2000.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '56.': 'TEIXEIRA, A. C. A.. Aula ministrada sobre Economia e Administração de Empresas Farmacêuticas aos alunos do Curso de Medicina. 2000.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '57.': 'TEIXEIRA, A. C. A.. Aulas ministradas sobre os temas Métodos Analíticos à Farmacoepidemiologia e Preparação de Protocolos para EUMs aos alunos do Curso de Espec em Farmácia. 1999.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '58.': 'TEIXEIRA, A. C. A.. Aula ministrada sobre o tema Investigação em Farmacoepidemiologia: a pergunta de partida aos alunos da disciplina de Farmacoepidemiologia do Curso de Farmácia. 1999.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '59.': 'TEIXEIRA, A. C. A.. Aula ministrada sobre o tema Preparação de Protocolos para EUMs aos alunos do Curso de Farmácia. 1999.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '60.': 'TEIXEIRA, A. C. A.. Facilitadora da Oficina de Educação Popular para o Uso Adequado de Medicamentos desenvolvida pelo GPUIM. 1999.\\n\\t\\t\\t\\t\\t\\t(Facilitadora).',\n",
       "   '61.': 'TEIXEIRA, A. C. A.. Aula ministrada sobre o tema Farmácia Ambulatorial aos alunos do Curso Básico de Farmácia Hospitalar. 1997.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '62.': 'TEIXEIRA, A. C. A.. Palestrante sobre o tema Orientação Farmacêutica aos alunos da disciplina Estágio Curricular Supervisionado do Curso de Farmácia. 1997.\\n\\t\\t\\t\\t\\t\\t(Palestras ministradas).',\n",
       "   '63.': 'TEIXEIRA, A. C. A.. I Curso de Atualização em Farmácia. 1996. .',\n",
       "   '64.': 'TEIXEIRA, A. C. A.. Presidente da Conferência Proposta de Modernização de Farmácia Ambulatorial de Messejana. 1996.\\n\\t\\t\\t\\t\\t\\t(Presidente de Conferência).',\n",
       "   '65.': 'TEIXEIRA, A. C. A.. Expositora da Mesa Redonda Produção de Conhecimento na Área de Assistência Farmacêutica: Parceria Ensino x Serviço. 1996.\\n\\t\\t\\t\\t\\t\\t(Expositora de Mesa Redonda).',\n",
       "   '66.': 'TEIXEIRA, A. C. A.. Expositora do tema Aspectos da Política de Medicamentos no Brasil aos alunos do I Curso de Atualização em Farmácia. 1996.\\n\\t\\t\\t\\t\\t\\t(Expositora).',\n",
       "   '67.': 'TEIXEIRA, A. C. A.. Aula ministrada sobre Promoção do Uso Racional de Medicamentos aos alunos da disciplina de Farmacoepidemiologia e Farmacovigilância do Curso de Especialização em Farmácia. 1996.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '68.': 'TEIXEIRA, A. C. A.. Aula ministrada sobre Saúde Pública aos alunos do Curso de Medicina. 1996.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '69.': 'TEIXEIRA, A. C. A.. Epidemiologia Especial. 1996.\\n\\t\\t\\t\\t\\t\\t(Aula ministrada).',\n",
       "   '70.': 'TEIXEIRA, A. C. A.. Instrutora do Curso de Noções Básicas sobre Medicamentos. 1995. (Curso de curta duração ministrado/Outra).',\n",
       "   '71.': 'COELHO, Helena Lutéscia ;TEIXEIRA, A. C. A.. Auto administração de misoprostol como abortivo - a experiência das mulheres em Fortaleza. 1992. (Relatório de pesquisa).'},\n",
       "  'Demais trabalhos': {'1.': 'TEIXEIRA, A. C. A.. Articulação Anti-Nuclear Brasileira (Atividades Comunítárias). 2011 (Atividades Comunítárias) .',\n",
       "   '2.': 'TEIXEIRA, A. C. A.. Articulação Anti-nuclear do Ceará (Atividades Comunítárias). 2011 (Atividades Comunitárias) .',\n",
       "   '3.': 'TEIXEIRA, A. C. A.. Delegada  da 1ª Conferência Estadual de Saúde Ambiental (Atividades Comunítárias). 2009 (Delegada) .',\n",
       "   '4.': 'TEIXEIRA, A. C. A.. Relatora da 1ª Conferência Estadual de Saúde Ambiental (Atividades Comunítárias). 2009 (Relatora) .',\n",
       "   '5.': 'TEIXEIRA, A. C. A.. Membro da Rede Brasileira de Justiça Ambiental. 2007 (Participação em Associações) .',\n",
       "   '6.': 'TEIXEIRA, A. C. A.. Membro do Grupo de Trabalho Combate ao Racismo Ambiental da Rede Brasileira de Justiça Ambiental.. 2007 (Participação em Associações) .',\n",
       "   '7.': 'TEIXEIRA, A. C. A.. Relatora de Síntese na 13ª Conferência Nacional de Saúde.. 2005 (Participação em Associações) .',\n",
       "   '8.': 'Rigotto, R. M. ;TEIXEIRA, A. C. A.;ROSA, I. F.; PESSOA, V. M. ; FERREIRA, M. J. M. ; Andrezza Graziella Veríssimo Pontes ; BRAGA, L. Q. V. ; ALVES, Pablo Araújo ; SILVA, Maria de Lourdes Vicente da ; FROTA, M. C. ; PAIXAO, D. ; ROCHA, Mayara Melo ; Maiana Maia Teixeira . Pesquisadora do Núcleo TRAMAS - Trabalho, Meio Ambiente e Saúde para a Sustentabilidade. 2004 (Participação em Associações) .',\n",
       "   '9.': 'TEIXEIRA, A. C. A.. Fórum em Defesa da Zona Costeira do Ceará (FDZCC).. 2004 (Atividades Comunitárias) .',\n",
       "   '10.': 'TEIXEIRA, A. C. A.. Seminário sobre Farmacoepidemiologia. 2001 (Instrutora) .',\n",
       "   '11.': 'TEIXEIRA, A. C. A.. Sócia ativa da Associação Brasileira de Saúde Coletiva- ABRASCO.. 2001 (Participação em Associações) .',\n",
       "   '12.': 'TEIXEIRA, A. C. A.. Facilitadora da Oficina de Educação Popular sobre o Uso Adequado de Medicamentos realizada em Itapipoca-CE. 1998 (Facilitadora) .',\n",
       "   '13.': 'TEIXEIRA, A. C. A.. Facilitadora da Oficina de Educação Popular sobre o Uso Adequado de Medicamentos realizada em Tauá-CE. 1998 (Facilitadora) .',\n",
       "   '14.': 'TEIXEIRA, A. C. A.. Facilitadora da Oficina de Educação Popular sobre o Uso Adequado de Medicamentos realizada no Crato-CE. 1998 (Facilitadora) .',\n",
       "   '15.': 'TEIXEIRA, A. C. A.. Facilitadora da Oficina de Educação Popular sobre o Uso Adequado de Medicamentos realizada em Iguatú-CE. 1998 (Facilitadora) .',\n",
       "   '16.': 'TEIXEIRA, A. C. A.. Facilitadora da Oficina de Educação Popular sobre o Uso Adequado de Medicamentos realizada em Quixadá-CE. 1998 (Facilitadora) .',\n",
       "   '17.': 'TEIXEIRA, A. C. A.. Facilitadora da Oficina de Educação Popular sobre o Uso Adequado de Medicamentos realizada em Sobral-CE. 1998 (Facilitadora) .',\n",
       "   '18.': 'TEIXEIRA, A. C. A.. Oficinas para Teste do Manual para Agentes de Saúde - Medicamentos realizadas em Fortaleza e Pedra Branca-CE. 1997 (Facilitadora) .',\n",
       "   '19.': 'TEIXEIRA, A. C. A.. Facilitadora da Oficina de Educação Popular sobre o Uso Adequado de Medicamentos realizada em Fortaleza-CE. 1997 (Facilitadora) .',\n",
       "   '20.': 'TEIXEIRA, A. C. A.. Facilitadora da Oficina de Educação Popular sobre o Uso Adequado de Medicamentos realizada em Limoeiro do Norte-CE. 1997 (Facilitadora) .',\n",
       "   '21.': 'COELHO, Helena Lutéscia Luna ;TEIXEIRA, A. C. A.; SOUSA, Domingos Sávio C ; ARRAIS, Paulo Sérgio ; Cunha. C,D, . Grupo de Prevenção ao Uso Indevido de Medicamentos. 1989 (Integrante do GPUIM) .'}},\n",
       " 'ProjetosPesquisa': [{'chave': '2021 - Atual',\n",
       "   'titulo_projeto': 'Participatório Nacional de Vigilância Popular da Saúde, Ambiente e Trabalho',\n",
       "   'descricao': 'Projeto certificado pelo(a) coordenador(a) Fernando Ferreira Carneiro em 24/06/2021.'}],\n",
       " 'ProjetosExtensão': [{'chave': '2023 - Atual',\n",
       "   'titulo_projeto': 'Apoio à implementação das Áreas de Proteção Ambiental da Lagoa da Precabura por meio de um Participatório em Saúde e Ecologia de Saberes',\n",
       "   'descricao': 'Descrição: Considerando a criação das Áreas de Proteção Ambiental (APA) da Lagoa da Precabura pelo governo do estado do Ceará e pelo município de Eusébio-CE e a instalação prevista do Distrito de Inovação em Saúde no território da APA, do qual a Fiocruz Ceará faz parte, o presente projeto ?Apoio à implementação das Áreas de Proteção Ambiental da Lagoa da Precabura por meio de um Participatório em Saúde e Ecologia de Saberes? busca a responsabilidade socioambiental e comprometida com a construção de um modelo de desenvolvimento tecnológico sustentável corroborado nas Teses e Diretrizes aprovadas no IX Congresso Interno e em consonância com a ?Estratégia Fiocruz para a Agenda 2030? que assinala o compromisso da instituição em tomar como referência para seu planejamento, definição de prioridades e estratégias, o documento resultado da Assembleia Geral das Nações Unidas realizada em 2015 que estabeleceu 17 Objetivos de Desenvolvimento Sustentável. O projeto tem os seguintes objetivos: Geral: Estruturar um Participatório em Saúde e Ecologia de Saberes para o desenvolvimento de pesquisas participativas, atividades formativas, e ações de promoção da saúde e do meio ambiente envolvendo tanto a sociedade quanto os trabalhadores da Fiocruz; e Específicos: 1. Desenvolver pesquisa participativa visando um diagnóstico socioambiental dos territórios de influências das Áreas de Proteção Ambiental da Lagoa da Precabura em sintonia com a Agenda 2030 e a implantação do Distrito de Inovação em Saúde de Eusébio, 2. Contribuir com a participação e apoio ao Conselho Gestor da APA da Lagoa da Precabura estadual, 3. Contribuir com o desenvolvimento do Planejo de Manejo da APA da Lagoa da Precabura estadual, 4. Desenvolver cursos livres visando a promoção da saúde e ambiente junto à sociedade cearense e especialmente com comunidades da área de influência das APAs, 5. Desenvolver Projeto de Terapia Comunitária para os trabalhadores da Fiocruz e comunidade, 6. Constituir espaço para os trabalhadores da Fiocruz Ceará e comunidade realizarem atividades coletivas visando a promoção da saúde na perspectiva de seu conceito ampliado, 7. Desenvolver pesquisa voltada para a Vigilância Popular da Saúde, Ambiente e Trabalho do território.Situação: Em andamento; Natureza: Extensão.Integrantes: Ana Cláudia de Araújo Teixeira - Coordenador / Fernando Ferreira Carneiro - Integrante / Vanira Matos Pessoa - Integrante / Flora Viana Eliseu da Silva - Integrante.'},\n",
       "  {'chave': '2010 - 2014',\n",
       "   'titulo_projeto': 'Projeto Vidas Paralelas - Populações do Campo',\n",
       "   'descricao': 'Descrição: Com base nos princípios da ecologia de saberes, na determinação social do processo saúde-doença, na perspectiva da justiça ambiental e da promoção da saúde, o Projeto Vidas Paralelas Populações do Campo (PVP Campo) vinculado ao Departamento de Saúde Coletiva da Faculdade de Ciências da Saúde da Universidade de Brasília, financiado pelo Ministério da Cultura (Brasil), organiza-se em articulação com diversos movimentos sociais do campo ? Movimento dos Trabalhadores Rurais Sem Terra, Movimento dos Pequenos Agricultores, Movimento das Mulheres Camponesas, Coordenação Nacional das Comunidades Quilombolas, Campanha Contra os Agrotóxicos e pela Vida ?, e populações do campo em situação de injustiça ambiental atingidas por projetos de desenvolvimento instalados em seus territórios. Nesse sentido, o PVP Campo objetiva, a partir da formação em audiovisual, retratar por meio de vídeos de curta duração, os contextos de risco e o cotidiano de vida e trabalho vivenciados por essas populações..Situação: Concluído; Natureza: Extensão.Integrantes: Ana Cláudia de Araújo Teixeira - Integrante / Maria da Graça Luderitz Hoefel - Coordenador / Ricardo Alves Jr. - Integrante / Juliane Peixoto - Integrante / Marciano Toledo Silva - Integrante / Denise Severo - Integrante.'},\n",
       "  {'chave': '2010 - 2011',\n",
       "   'titulo_projeto': 'Núcleo TRAMAS ? Trabalho, Meio Ambiente e Saúde para Sustentabilidade',\n",
       "   'descricao': 'Projeto certificado pelo(a) coordenador(a) Raquel Maria Rigotto em 18/02/2013.'}],\n",
       " 'ProjetosDesenvolvimento': [],\n",
       " 'ProjetosOutros': [{'chave': '2017 - Atual',\n",
       "   'titulo_projeto': 'Rede Saúde, Saneamento, Água e Direitos Humanos',\n",
       "   'descricao': 'Descrição: \\u200bA Rede Saúde, Saneamento, Água e Direitos Humanos (RESSADH) no Semiárido foi criada em março de 2017[1] a partir de uma articulação da Fiocruz Ceará em parceria com a Secretaria de Saúde do Estado (SESA-CE), o Instituto Federal do Ceará (IFCE), a Caritas Brasileira Regional Ceará, o Movimento dos Trabalhadores Rurais Sem Terra (MST) e o Centro de Estudos do Trabalho e de Assessoria ao Trabalhador (Cetra). O processo de criação da RESSADH considera o tema Saúde, Saneamento, Água e Direitos Humanos no contexto do modelo de desenvolvimento e dos processos de produção, os quais demandam elevado consumo de água, que se torna um agravante ao se considerar o contexto do semiárido. Além desse aspecto, aborda a tríplice epidemia de Dengue, Zika e Chikungunya e outras doenças, as quais estão relacionadas às condições ambientais, sociais e econômicas, destacando o saneamento como uma dimensão relevante. A RESSADH se propõe a atuar em formação, pesquisa e cooperação de modo articulado. Objetiva-se que o conhecimento, a tecnologia e a inovação produzidos no âmbito da RESSADH subsidiem a formulação de políticas públicas setoriais e intersetoriais promotoras de saúde e de qualidade de vida. Nesse sentido, as estratégias teórico-metodológicas participativas e os novos conhecimentos gerados poderão ser úteis no desenvolvimento de tecnologias sociais e estratégias de convivência com o semiárido, na elaboração de planos diretores de unidades de conservação, de terras indígenas, ribeirinhos e quilombolas, camponeses, de municípios, bacias hidrográficas e outras categorias de análise e gestão territorial.  [1] https://portal.fiocruz.br/pt-br/content/rede-sobre-saude-agua-e-direitos-humanos-e-criada-no-ceara.Situação: Em andamento; Natureza: Outra.Integrantes: Ana Cláudia de Araújo Teixeira - Coordenador / Fernando Ferreira Carneiro - Integrante / Vanira Matos Pessoa - Integrante.'}],\n",
       " 'Patentes e registros': {},\n",
       " 'Bancas': {'Participação em bancas de trabalhos de conclusão': {'1.': 'TEIXEIRA, A. C. A.; VERAS, V. S.; OLIVEIRA, M. A. A.. Banca Examinadora  - 29a Reunião Anual de Iniciação Científica da Fiocruz/RAIC: Thaís dos Santos Moreira- PIBIC FUNCAP. 2021. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '2.': 'TEIXEIRA, A. C. A.; VERAS, V. S.; OLIVEIRA, M. A. A.. Banca Examinadora - 29a Reunião Anual de Iniciação Científica da Fiocruz/RAIC:  Aline Camurça Mesquita PIBIC FUNCAP. 2021. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '3.': 'TEIXEIRA, A. C. A.; VERAS, V. S.; OLIVEIRA, M. A. A.. Banca Examinadora - 29a Reunião Anual de Iniciação Científica da Fiocruz/RAIC - Palmira da Conceição Alberto Tonet - PIBIC Fiocruz. 2021. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '4.': 'TEIXEIRA, A. C. A.; VERAS, V. S.; OLIVEIRA, M. A. A.. Banca Examinadora - 29a Reunião Anual de Iniciação Científica da Fiocruz/RAIC ? Tainá Amora Felix - PIBIC Fiocruz. 2021. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '5.': 'TEIXEIRA, A. C. A.; ARRUDA, C. A. M.;PESSOA, V. M.. Banca Examinadora  - 28a Reunião Anual de Iniciação Científica da Fiocruz/RAIC: GREYCEANNE CECILIA DUTRA BRITO - PIBIC.. 2020. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '6.': 'TEIXEIRA, A. C. A.; ARRUDA, C. A. M.;PESSOA, V. M.. Banca Examinadora - 28a Reunião Anual de Iniciação Científica da Fiocruz/RAIC ? JOAO PEDRO CARVALHO VERAS - PIBITI. 2020. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '7.': 'TEIXEIRA, A. C. A.;PESSOA, V. M.; TORRES FILHO, J.. Banca Examinadora - 25a Reunião Anual de Iniciação Científica da Fiocruz/RAIC: Desenvolvimento do perfil ecosociosanitário histórico da população na região do Maciço de Baturité ? CE: análise dos dados secundários sobre o câncer e os transtornos mentais - Francisca Manerlene Ferreira do Nascimento. 2017. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '8.': 'LIMA, A. E. F.;PESSOA, V. M.;TEIXEIRA, A. C. A.. Banca Examinadora - 25a Reunião Anual de Iniciação Científica da Fiocruz/RAIC - Identificar os contextos de vulnerabilidade e riscos socioambientais relacionados à exposição de agrotóxicos: Leidiane Marques Maciel. 2017. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '9.': 'TEIXEIRA, A. C. A.; LIMA, A. E. F.; BARBOSA, MARIA IDALICE SILVA. Banca Examinadora - 25a Reunião Anual de Iniciação Científica da Fiocruz/RAIC: Caracterização de Eusébio/CE como ?município-laboratório? da FIOCRUZ CEARÁ e de implantação do pólo industrial e tecnológico da saúde (PITS) - Natália Martins Bília. 2017. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '10.': 'TEIXEIRA, A. C. A.. Comissão de seleção do Mestrado em Saúde Pública. 2013. Universidade Federal do Ceará.',\n",
       "   '11.': 'TEIXEIRA, A. C. A.. Comissão de seleção do Mestrado em Saúde Pública. 2012. Universidade Federal do Ceará.',\n",
       "   '12.': 'TEIXEIRA, A. C. A.. Comissão de seleção do Mestrado em Saúde Pública. 2011. Universidade Federal do Ceará.'},\n",
       "  'Participação em bancas de comissões julgadoras': {'1.': 'TEIXEIRA, A. C. A.; VERAS, V. S.; OLIVEIRA, M. A. A.. Banca Examinadora  - 29a Reunião Anual de Iniciação Científica da Fiocruz/RAIC: Thaís dos Santos Moreira- PIBIC FUNCAP. 2021. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '2.': 'TEIXEIRA, A. C. A.; VERAS, V. S.; OLIVEIRA, M. A. A.. Banca Examinadora - 29a Reunião Anual de Iniciação Científica da Fiocruz/RAIC:  Aline Camurça Mesquita PIBIC FUNCAP. 2021. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '3.': 'TEIXEIRA, A. C. A.; VERAS, V. S.; OLIVEIRA, M. A. A.. Banca Examinadora - 29a Reunião Anual de Iniciação Científica da Fiocruz/RAIC - Palmira da Conceição Alberto Tonet - PIBIC Fiocruz. 2021. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '4.': 'TEIXEIRA, A. C. A.; VERAS, V. S.; OLIVEIRA, M. A. A.. Banca Examinadora - 29a Reunião Anual de Iniciação Científica da Fiocruz/RAIC ? Tainá Amora Felix - PIBIC Fiocruz. 2021. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '5.': 'TEIXEIRA, A. C. A.; ARRUDA, C. A. M.;PESSOA, V. M.. Banca Examinadora  - 28a Reunião Anual de Iniciação Científica da Fiocruz/RAIC: GREYCEANNE CECILIA DUTRA BRITO - PIBIC.. 2020. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '6.': 'TEIXEIRA, A. C. A.; ARRUDA, C. A. M.;PESSOA, V. M.. Banca Examinadora - 28a Reunião Anual de Iniciação Científica da Fiocruz/RAIC ? JOAO PEDRO CARVALHO VERAS - PIBITI. 2020. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '7.': 'TEIXEIRA, A. C. A.;PESSOA, V. M.; TORRES FILHO, J.. Banca Examinadora - 25a Reunião Anual de Iniciação Científica da Fiocruz/RAIC: Desenvolvimento do perfil ecosociosanitário histórico da população na região do Maciço de Baturité ? CE: análise dos dados secundários sobre o câncer e os transtornos mentais - Francisca Manerlene Ferreira do Nascimento. 2017. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '8.': 'LIMA, A. E. F.;PESSOA, V. M.;TEIXEIRA, A. C. A.. Banca Examinadora - 25a Reunião Anual de Iniciação Científica da Fiocruz/RAIC - Identificar os contextos de vulnerabilidade e riscos socioambientais relacionados à exposição de agrotóxicos: Leidiane Marques Maciel. 2017. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '9.': 'TEIXEIRA, A. C. A.; LIMA, A. E. F.; BARBOSA, MARIA IDALICE SILVA. Banca Examinadora - 25a Reunião Anual de Iniciação Científica da Fiocruz/RAIC: Caracterização de Eusébio/CE como ?município-laboratório? da FIOCRUZ CEARÁ e de implantação do pólo industrial e tecnológico da saúde (PITS) - Natália Martins Bília. 2017. Fundação Oswaldo Cruz - Fiocruz Ceará.',\n",
       "   '10.': 'TEIXEIRA, A. C. A.. Comissão de seleção do Mestrado em Saúde Pública. 2013. Universidade Federal do Ceará.',\n",
       "   '11.': 'TEIXEIRA, A. C. A.. Comissão de seleção do Mestrado em Saúde Pública. 2012. Universidade Federal do Ceará.',\n",
       "   '12.': 'TEIXEIRA, A. C. A.. Comissão de seleção do Mestrado em Saúde Pública. 2011. Universidade Federal do Ceará.'}},\n",
       " 'Orientações': [{'nome': 'Orientações',\n",
       "   'subsecoes': [{'nome': 'Orientações e supervisões concluídas',\n",
       "     'orientacoes': [{'tipo': 'Dissertação de mestrado',\n",
       "       'detalhes': 'George Bezerra Pinheiro. ?A Nossa Vida Estava Toda no Lugar? Desterritorialização e Impactos na Saúde Mental: O Caso da Comunidade Palmares Vila 2 Atingidas pelo Projeto da Barragem Lago de Fronteiras em Crateús-CE?,. 2022.  Dissertação  (Mestrado em MESTRADO PROFISSIONAL EM POLÍTICAS PÚBLICAS DE SAÚDE)  - Fundação Oswaldo Cruz - Fiocruz Ceará, . Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Dissertação de mestrado',\n",
       "       'detalhes': 'Renata Castelo da Nóbrega. ?Experiências  inovadoras de cuidado em saúde  em territórios das populações do campo, floresta e águas do Ceará.?,. 2022.  Dissertação  (Mestrado em Mestrado Profissional em Saúde da Família (RENASF))  - Fundação Oswaldo Cruz - Fiocruz Ceará, . Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Dissertação de mestrado',\n",
       "       'detalhes': 'Emmanuella Carvalho Fonseca. ANÁLISE DA VIGIL NCIA EM SAÚDE RELACIONADA À QUALIDADE DA ÁGUA PARA CONSUMO HUMANO NO MUNICíPIO DE CASCAVEL, CEARÁ.. 2020.  Dissertação  (Mestrado em Mestrado Profissional em Saúde da Família (RENASF))  - Fundação Oswaldo Cruz - Fiocruz Ceará, . Coorientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Dissertação de mestrado',\n",
       "       'detalhes': 'Pablo Araújo Alves. Vigilância Popular da Saúde: cartografia dos riscos e vulnerabilidades socioambientais no contexto de implantação da mineração de urânio e fosfato no Ceará. 2013.  Dissertação  (Mestrado em Mestrado em Saúde Pública)  - Universidade Federal do Ceará, . Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Dissertação de mestrado',\n",
       "       'detalhes': 'Maria das Graças Viana Bezerra. Do canto das nambus ao barulho do trem: transformações no modo de vida e na saúde na comunidade de bolso no complexo industrial e portuário do Pecém-CE.. 2010.  Dissertação  (Mestrado em Pós-Graduação em Saúde Pública)  - Universidade Federal do Ceará, . Coorientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Antônia Iara Chagas Martins. Um relato de experiência sobre o ?Projeto de tratamento biológico de água e efluentes da comunidade Santana da Cal, Canindé-Ceará. 2020. Monografia. (Aperfeiçoamento/Especialização em Ed Popular e Promoção de Territórios Saudáveis na Convivência com Semiárido)  - Fundação Oswaldo Cruz - Fiocruz Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Maria Dalvanir e Silva Duarte. A experimentação e inovação de uma unidade de produção familiar com a agroecologia e as tecnologias sociais de convivência com o semiárido, na comunidade Casa Forte ? Baracho ? Sobral-CE. 2020. Monografia. (Aperfeiçoamento/Especialização em Ed Popular e Promoção de Territórios Saudáveis na Convivência com Semiárido)  - Fundação Oswaldo Cruz - Fiocruz Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Manoela Cavalcanti Frota. Considerações sobre a relação entre a seca e suas repercussões na saúde. 2017. Monografia. (Aperfeiçoamento/Especialização em Residência Integrada em Saúde /RIS-ESP/CE)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Haiany Mirela da Silva Rodrigues. Estudo do absenteísmo na Coordenação de Atendimento de Pessoal do Ministério da Saúde no ano de 2014. 2015. Monografia. (Aperfeiçoamento/Especialização em Especialização em Políticas Públicas de Saúde e Gestão Participativa)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Patrícia Rodrigues de Almeida Leal. A produção do cuidado em saúde a partir das necessidades de saúde do território. 2015. Monografia. (Aperfeiçoamento/Especialização em Especialização em Políticas Públicas de Saúde e Gestão Participativa)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Pedro Henrique Mourão Silva. A importância dos serviços de apoio à amamentação no local de trabalho para a promoção da saúde da mulher trabalhadora que amamenta. 2015. Monografia. (Aperfeiçoamento/Especialização em Especialização em Políticas Públicas de Saúde e Gestão Participativa)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Antonia Nilza Bezerra Vieira. Saúde e Trabalho: As Implicações das LER-DORT na Vida das trabalhadoras que participam do Grupo de Apoio à Saúde do Trabalhador do CEREST-CE. 2010. Monografia. (Aperfeiçoamento/Especialização em Curso de Especialização em Saúde Pública)  - Universidade Estadual do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Georgiana de Alencar Portela. Modelos de atenção à saúde: iniciativa de prevenção de doenças e da promoção da saúde em plano de autogestão. 2007. Monografia. (Aperfeiçoamento/Especialização em Gestão de Sistemas e Serviços de Saúde)  - Escola de Saúde Pública. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Vanesca Fontenele Ribeiro. Adesão ao Tratamento Farmacológico dos Pacientes Diabéticos Tipo 2 Acompanhados no Centro Integrado de Diabetes e Hipertensão-Ce. 2004. Monografia. (Aperfeiçoamento/Especialização em Especialização em Assistência Farmacêutica)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Assunção Maria de Sá Nogueira. Mortalidade Infantil no Município de Tianguá no Ano de 2002. 2004. Monografia. (Aperfeiçoamento/Especialização em Especialização em Vigilância Epidemiológica)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Alessandra Enes Rocha. Análise da Qualidade das Prescrições Médicas do Hospital Universitário Presidente Dutra - HUUFMA São Luís/MA. 2002. 0 f. Monografia. (Aperfeiçoamento/Especialização em Assistência Farmacêutica)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Gláucia Veras Dias da Cunha. Evolução da Leishimaniose Tegumentar Americana no Município de Viçosa do Ceará no período de 1998 a 2002. 2002. Monografia. (Aperfeiçoamento/Especialização em Especialização em Vigilância Epidemiológica)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Geanne Maria Costa Torres. Fatores de Riscos a que estão Expostos os Portadores de Hipertensão Arterial Atendidos pelo Programa Saúde da Família do Bairro Aparecida, no Município de Campos Sales. 2002. Monografia. (Aperfeiçoamento/Especialização em Especialização em Vigilância Epidemiológica)  - Escola de Saúde Pública. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Luiza Cunha Saldanha Brito. Estudo Descritivo sobre a Utilização de Medicamentos pelos Pacientes Atendidos no Ambulatório do Hospital e Maternidade Adolfo Bezerra de Menezes do Município de Jaguaretama - Ceará. 2001. 0 f. Monografia. (Aperfeiçoamento/Especialização em Gestão de Sistemas Locais de Saúde)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Antonio Claudio Carlos de Freitas. Utilização do Diazepam pelos Pacientes do Hospital Dr. Amadeu Sá do Município de Euzébio-CE. 2001. 0 f. Monografia. (Aperfeiçoamento/Especialização em Assistência Farmacêutica)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Célia Maria Oliveira Alves. Utilização do Diazepam por Pacientes atendidos no Centro de Saúde do Município de Quixeré-CE. 2001. 0 f. Monografia. (Aperfeiçoamento/Especialização em Assistência Farmacêutica)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Clécia Maria de Santiago Moura. Dificuldades do Portador de Diabetes Mellitus na adesão ao Tratamento Farmacológico. 2001. 0 f. Monografia. (Aperfeiçoamento/Especialização em Saúde da Família)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Lairtes Morais Ferreira. Grau de Informação dos Pacientes Hipertensos atendidos no Ambulatório do Hospital Gonzaga Mota da Barra do Ceará sobre os Medicamentos Prescritos. 2001. 0 f. Monografia. (Aperfeiçoamento/Especialização em Assistência Farmacêutica)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Ìtalo Ney Bezerra Paulino. Avaliação da percepção dos Profissionais de Saúde que atuam na Atenção Primária do Município de Quixelô sobre a Importância da notificação de reações adversas à medicamentos. 2001. 0 f. Monografia. (Aperfeiçoamento/Especialização em Vigilância Sanitária)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Edna Lacerda Queiroz. As condições sanitárias das drogarias da periferia de Fortaleza/CE. 2001. 0 f. Monografia. (Aperfeiçoamento/Especialização em Vigilância Sanitária)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Ana Lídia Lima Solon. Dificuldades dos Municípios na Certificação das Ações de Epidemiologia e Controle de Doenças no Estado do Ceará. 2001. 0 f. Monografia. (Aperfeiçoamento/Especialização em Vigilância Epidemiológica)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Francisca Ivonete Granja Pinheiro. Características das Mulheres Portadoras do Papiloma Vítus Humano (HPV). 2001. 0 f. Monografia. (Aperfeiçoamento/Especialização em Vigilância Epidemiológica)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Elza Gadelha Lima. A Contribuição do Laboratório Central de Saúde Pública ao Sistema de Vigilância Epidemiológica no Ceará. 2001. 0 f. Monografia. (Aperfeiçoamento/Especialização em Vigilância Epidemiológica)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Ana Carmem Rocha Ximenes. A Descentralização das Ações de Vigilância Sanitária: Controle de Psicotrópicos de Acordo com a Portaria 344 de 1998 na 12ª Microrregião de Saúde - Sobral - CE. 2001. 0 f. Monografia. (Aperfeiçoamento/Especialização em Vigilância Sanitária)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Agerton Ribeiro do Vale Filho. Perfil dos Pacientes Atendidos no Ambulatório de HIV do Hospital Geral de Fortaleza. 2001. Monografia. (Aperfeiçoamento/Especialização em Especialização em Assistência Farmacêutica)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Germana Aquino de Oliveira Sátiro. Utilização do Diazepan pelos Clientes Atendidos na Unidade Mista São Bernardo pela Equipe PSF-1 (SEDE) do Município de Deputado Irapuan Pinheiro-Ce. 2001. Monografia. (Aperfeiçoamento/Especialização em Saúde da Família)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Kátia Gorette Silva Brito Peixoto. Fatores de Risco para Determinação em Crianças dos Seis aos Sessenta Meses de Idade. 2001. Monografia. (Aperfeiçoamento/Especialização em Especialização em Saúde da Família)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Rita Heroína Amorim Bem. As Condições Sanitárias das Farmácias do Município de Barbalha-Ce. 2001. Monografia. (Aperfeiçoamento/Especialização em Especialização em Vigilância Sanitária)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Francisco Airton Barreto Dantas. Uso do Xarope de Ararás no Tratamento e Prevenção das Crises de Asma em Crianças de 01 a 13 anos inscritas no Programa de Controle de Asma no Município de Maracanaú-CE. 2000. 0 f. Monografia. (Aperfeiçoamento/Especialização em Saúde da Família)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Geovania Vieira de Brito. Utilização de Plantas nos Cuidados Primários de Saúde pela Comunidade de Buíra em Viçosa do Ceará. 2000. 0 f. Monografia. (Aperfeiçoamento/Especialização em Saúde da Família)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Maria Socorro de Lucena. Avaliação do Ciclo Logístico da Assistência Farmacêutica no Município de Caririaçu-CE. 2000. 0 f. Monografia. (Aperfeiçoamento/Especialização em Gestão de Sistemas Locais de Saúde)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Julieta Gonçalves Ribeiro. Adesão ao TratamentoFarmacológico da Hipertensão Arterial em Pacientes Atendidos no Ambulatório do Hospital Municipal de Capistrano-CE. 1999. 0 f. Monografia. (Aperfeiçoamento/Especialização em Administração de Sistema Integral de Medicamentos)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'Pedro Henrique Silvano da Silva. As Bases Teórico-conceituais da Orientação ao Paciente: um estudo preliminar. 1999. 0 f. Monografia. (Aperfeiçoamento/Especialização em Administração de Sistema Integral de Medicamentos)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Monografia de conclusão de curso de aperfeiçoamento/especialização',\n",
       "       'detalhes': 'José Romério Rabêlo Ribeiro. Consumo de Psicofármacos em Morada Nova-CE. 1999. Monografia. (Aperfeiçoamento/Especialização em Administração de Sistema Integral de Medicamentos)  - Escola de Saúde Pública do Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Iniciação científica',\n",
       "       'detalhes': 'Aline De Melo Vieira. Caracterização dos aspectos sociais, sanitários, econômicos, ambientais e das estratégias de enfrentamento das arboviroses em região do semiárido brasileiro.. 2017. Iniciação Científica. (Graduando em PROGRAMA INSTITUCIONAL DE BOLSAS DE INICIAÇÃO CIENTÍFICA E TECNOLÓGICA ? FU)  - Fundação Oswaldo Cruz - Fiocruz Ceará, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Ayêska Haisa Alexandre de Lima. Programa Estágio - Mapeamento das formas sustentáveis de usufruto do território no semiárido do Ceará. 2020. Orientação de outra natureza. (Programa Estágio - Graduação)  - Fundação Oswaldo Cruz - Fiocruz Ceará. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Priscila Lie Fukushi. Caracterização das intoxicações por agrotóxicos de uso agrícola na população no Distrito Federal, no período de 2009 a 2012. 2014. Orientação de outra natureza. (Gestão de Saúde Coletiva)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Georgia Batista de Amorim. Caracterização das intoxicações por agrotóxicos de uso agrícola na população no Distrito Federal, no período de 2009 a 2012. 2014. Orientação de outra natureza. (Gestão de Saúde Coletiva)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Bianca Mendes Araújo. Caracterização das intoxicações por agrotóxicos de uso agrícola na população no Distrito Federal, no período de 2009 a 2012. 2014. Orientação de outra natureza. (Gestão de Saúde Coletiva)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Guilherme Martins. Caracterização das intoxicações por agrotóxicos de uso agrícola na população no Distrito Federal, no período de 2009 a 2012. 2014. Orientação de outra natureza. (Gestão de Saúde Coletiva)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Daniele Amorim Fernandes. Aspectos das condições socioambientais e sanitárias e as doenças infecciosas na Cidade Estrutural, Distrito Federal. 2014. Orientação de outra natureza. (Gestão de Saúde Coletiva)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Gabriely Mota Costa. Aspectos das condições socioambientais e sanitárias e as doenças infecciosas na Cidade Estrutural, Distrito Federal. 2014. Orientação de outra natureza. (Gestão de Saúde Coletiva)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Grayce Loren Cardoso dos Santos. Aspectos das condições socioambientais e sanitárias e as doenças infecciosas na Cidade Estrutural, Distrito Federal. 2014. Orientação de outra natureza. (Gestão de Saúde Coletiva)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Karina dos Santos Casado. Aspectos das condições socioambientais e sanitárias e as doenças infecciosas na Cidade Estrutural, Distrito Federal. 2014. Orientação de outra natureza. (Gestão de Saúde Coletiva)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Josélia de Souza Trindade. Relatório de Estágio Supervisionado 3. 2014. Orientação de outra natureza. (Gestão de Saúde Coletiva)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Jéssica Lima Pereira. Proposta de cartilha sobre o tema Trabalho Infantil. 2014. Orientação de outra natureza. (Gestão de Saúde Coletiva)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Fernanda Campelo Rosa. Monitoramento da oferta de ações de Vigilância em Saúde do Trabalhador pelos Centros de Referência em Saúde do Trabalhador. 2014. Orientação de outra natureza. (Gestão de Saúde Coletiva)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'},\n",
       "      {'tipo': 'Orientações de outra natureza',\n",
       "       'detalhes': 'Ádria Vanessa Torres Mendes. Incorporação da dimensão da saúde nos Termos de Referencia para elaboração de Estudos de Impacto Ambiental de grandes empreendimentos: relato da experiência no Estágio Supervisionado I. 2014. Orientação de outra natureza. (Gestão de Saúde Coletiva)  - Universidade de Brasília. Orientador: Ana Cláudia de Araújo Teixeira.'}]}]}],\n",
       " 'JCR2': [{'doi': 'http://dx.doi.org/10.1590/1413-81232023289.13142022',\n",
       "   'impact-factor': '1.1',\n",
       "   'original_title': 'Ciencia & Saude Coletiva (1413-8123)'},\n",
       "  {'doi': 'http://dx.doi.org/10.1590/1981-7746-ojs275'},\n",
       "  {'doi': 'http://dx.doi.org/10.3395/2317-269x.00775',\n",
       "   'impact-factor': '1.1',\n",
       "   'original_title': 'Ciencia & Saude Coletiva (1413-8123)'},\n",
       "  {'doi': 'http://dx.doi.org/10.1590/S1413-81232013000800009',\n",
       "   'impact-factor': '1.1',\n",
       "   'original_title': 'Ciencia & Saude Coletiva (1413-8123)'},\n",
       "  {'doi': 'http://dx.doi.org/10.1590/s1414-32832013005000004',\n",
       "   'impact-factor': '1.1',\n",
       "   'original_title': 'Ciencia & Saude Coletiva (1413-8123)'},\n",
       "  {'doi': 'http://dx.doi.org/10.1590/s1413-81232012000600017',\n",
       "   'impact-factor': '1.1',\n",
       "   'original_title': 'Ciencia & Saude Coletiva (1413-8123)'},\n",
       "  {'doi': 'http://dx.doi.org/10.1590/s0303-76572009000100005'},\n",
       "  {'doi': 'http://dx.doi.org/10.1016/0010-7824(94)90084-1',\n",
       "   'impact-factor': '1.1',\n",
       "   'original_title': 'Ciencia & Saude Coletiva (1413-8123)'},\n",
       "  {'doi': 'http://dx.doi.org/10.1016/0140-6736(93)91157-H',\n",
       "   'impact-factor': '1.1',\n",
       "   'original_title': 'Ciencia & Saude Coletiva (1413-8123)'}]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curriculos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criar Label 'Pesquisador' com as propriedades:\n",
      "  Label: 'Pesquisador'\n",
      "  {'Nome': 'Carla Freire Celedonio Fernandes', 'ID Lattes': '3922481112087617', 'Última atualização': '03/07/2024'}\n",
      "  {'Formação Acadêmica': ['Doutorado em Ciências Naturais (Dr. rer. nat.). Philipps-University Marburg, UNI MARBURG, Alemanha. com período co-tutela em Philipps-Universität Marburg (Orientador: Josef Kriegelstein). Título: Molecular characterization and expression of two new members of the SLC10 transporter family: SLC10A4 and SLC10A5, Ano de obtenção: 2007. Orientador: Joseph Krieglstein e Ernst Petzinger. Bolsista do(a): Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES, Brasil. Palavras-chave: SLC10 family; Membrane transporters. Grande área: Ciências da Saúde Grande Área: Ciências Biológicas / Área: Farmacologia / Subárea: Farmacologia Bioquímica e Molecular.', 'Graduação em Farmácia. Universidade Federal do Ceará, UFC, Brasil.']}\n",
      "  {'1.': 'Grande área: Ciências Biológicas / Área: Farmacologia / Subárea: Farmacologia Bioquímica e Molecular.', '2.': 'Grande área: Ciências da Saúde / Área: Farmácia.', '3.': 'Grande área: Ciências Biológicas / Área: Bioquímica.'}\n",
      "  ['Fundação Oswaldo Cruz - Unidade de Rondônia, Fiocruz Rondônia, Brasil.', 'Fundação Oswaldo Cruz - Unidade de Rondônia, Fiocruz Rondônia, Brasil.', 'Fundação Oswaldo Cruz - Unidade de Rondônia, Fiocruz Rondônia, Brasil.', 'Fundação Oswaldo Cruz - Unidade de Rondônia, Fiocruz Rondônia, Brasil.', 'Fundação Oswaldo Cruz - Unidade de Rondônia, Fiocruz Rondônia, Brasil.', 'Fundação Oswaldo Cruz - Unidade de Rondônia, Fiocruz Rondônia, Brasil.', 'Centro de Pesquisas em Medicina Tropical, CEPEM, Brasil.', 'Centro de Pesquisas em Medicina Tropical, CEPEM, Brasil.', 'Universidade Federal de Rondônia, UNIR, Brasil.', 'Universidade Federal de Rondônia, UNIR, Brasil.', 'Universidade Federal de Rondônia, UNIR, Brasil.', 'Instituto de Pesquisas em Patologias Tropicais de Rondônia, IPEPATRO, Brasil.', 'Instituto de Pesquisas em Patologias Tropicais de Rondônia, IPEPATRO, Brasil.', 'Instituto de Pesquisas em Patologias Tropicais de Rondônia, IPEPATRO, Brasil.', 'Faculdades Integradas Aparício Carvalho, FIMCA, Brasil.', 'Faculdades Integradas Aparício Carvalho, FIMCA, Brasil.', 'Instituto Nacional de Apoio a Vida, NATIVIDA, Brasil.', 'Justus-Liebig-Universität Gießen, JLUG, Alemanha.', 'Justus-Liebig-Universität Gießen, JLUG, Alemanha.', 'Justus-Liebig-Universität Giessen, JLUG, Alemanha.', 'Justus-Liebig-Universität Giessen, JLUG, Alemanha.', 'Farmácias Pague Menos, FPM, Brasil.', 'Farmácias Pague Menos, FPM, Brasil.', 'Instituto de prevenção a desnutrição e a excepcionalidade, IPREDE, Brasil.', 'Instituto de prevenção a desnutrição e a excepcionalidade, IPREDE, Brasil.', 'Farmácia Sabin, FS, Brasil.', 'Farmácia Sabin, FS, Brasil.', 'Farmácia Sabin, FS, Brasil.', 'Universidade Federal do Ceará, UFC, Brasil.', 'Universidade Federal do Ceará, UFC, Brasil.', 'Universidade Federal do Ceará, UFC, Brasil.', 'Universidade Federal do Ceará, UFC, Brasil.', 'Escola de Ensino Fundamental e Médio Walter de Sá Cavalcante, WSC, Brasil.', 'Embrapa RO, EMBRAPA%20RO, Brasil.', 'Embrapa RO, EMBRAPA%20RO, Brasil.', 'Fundação Oswaldo Cruz Ceará, FIOCRUZ CE, Brasil.']\n",
      "  [{'Descrição': 'Glicoproteínas', 'Detalhes': ''}, {'Descrição': 'Identificação, clonagem e caracterização de proteínas de membrana', 'Detalhes': ''}, {'Descrição': 'Identificação, clonagem e caracterização de proteínas de membrana', 'Detalhes': 'Objetivo: Identificar e caracterizar  carreadores de substâncias endógenas e xenobióticos.. Grande área: Ciências da Saúde Setores de atividade: Produtos e Processos Biotecnológicos Vinculados À Saúde Humana Ou dos Animais. Palavras-chave: Membrane transporters; SLC10 family.'}, {'Descrição': 'Identificação, clonagem e caracterização de proteínas carreadoras de membrana', 'Detalhes': 'Objetivo: Identificar e caracterizar  carreadores de substâncias endógenas e xenobióticos.. Grande área: Ciências da Saúde Setores de atividade: Produtos e Processos Biotecnológicos Vinculados À Saúde Humana Ou dos Animais. Palavras-chave: Membrane transporters; SLC10 family.'}, {'Descrição': 'Produção de imunobiológicos para tratamento e diagnóstico de doenças de importancia humana', 'Detalhes': 'Objetivo: Identificar e caracterizar  carreadores de substâncias endógenas e xenobióticos.. Grande área: Ciências da Saúde Setores de atividade: Produtos e Processos Biotecnológicos Vinculados À Saúde Humana Ou dos Animais. Palavras-chave: Membrane transporters; SLC10 family.'}, {'Descrição': 'Produção, purificação e caracterização de nanocorpos VHH por phage display a partir de camelídeos como ferramenta de diagnostico ou estratégia farmacológica para tratamento de doenças relacionadas a pobreza', 'Detalhes': 'Objetivo: Identificar e caracterizar  carreadores de substâncias endógenas e xenobióticos.. Grande área: Ciências da Saúde Setores de atividade: Produtos e Processos Biotecnológicos Vinculados À Saúde Humana Ou dos Animais. Palavras-chave: Membrane transporters; SLC10 family.'}, {'Descrição': 'Produção, purificação e caracterização de nanocorpos VHH por phage display a partir de camelídeos como ferramenta de diagnostico ou estratégia farmacológica para tratamento de doenças relacionadas a pobreza', 'Detalhes': 'Objetivo: Identificar e caracterizar  carreadores de substâncias endógenas e xenobióticos.. Grande área: Ciências da Saúde Setores de atividade: Produtos e Processos Biotecnológicos Vinculados À Saúde Humana Ou dos Animais. Palavras-chave: Membrane transporters; SLC10 family. Objetivo: Visando melhorar características farmacocinéticas, farmacodinâmicas e minimizar o aparecimento de reações de hipersensibilidade relacionadas ao uso clínico de imunoglobulinas, a engenharia molecular para produção de anticorpos vem buscando diminuir o tamanho das regiões de interação antígeno-anticorpo para fragmentos variáveis de cadeia única. Camelídeos produzem além de anticorpos convencionais, imunoglobulinas desprovidas de cadeia leve, onde as regiões para reconhecimento de antígenos podem ser reduzidas, de forma funcional, através da biotecnologia, a domínios únicos, denominados VHH ou nanocorpos. Estas estruturas possuem cerca de um décimo do peso molecular de anticorpos inteiros, estabilidade a variações de temperatura e pH, boa solubilidade, capacidade de neutralização viral, além de menor tempo de meia-vida quando comparados a anticorpos humanos ou murinos. Unindo as características apresentadas pelos nanocorpos à necessidade de desenvolvimento de outras estratégias farmacológicas para tratamento de doenças negligenciadas ou relacionadas a pobreza, a linha de pesquisa propõe a produção de nanocorpos através da tecnologia phage display, previamente padronizada em nosso laboratório, para uso em soroterapia ou diagnóstico de doenças de origem viral ou ainda neutralização de efeitos tóxicos ou necrose tecidual desencadeáveis por toxinas animais.. Grande área: Ciências Biológicas Grande Área: Ciências Biológicas / Área: Biologia Geral / Subárea: Biotecnologia. Setores de atividade: Atividades de atenção à saúde humana. Palavras-chave: VHH; phage display; nanocorpos; camelídeo.'}, {'Descrição': 'Pesquisa e Desenvolvimento de biofármacos, anticorpos terapeuticos', 'Detalhes': 'Objetivo: Identificar e caracterizar  carreadores de substâncias endógenas e xenobióticos.. Grande área: Ciências da Saúde Setores de atividade: Produtos e Processos Biotecnológicos Vinculados À Saúde Humana Ou dos Animais. Palavras-chave: Membrane transporters; SLC10 family. Objetivo: Visando melhorar características farmacocinéticas, farmacodinâmicas e minimizar o aparecimento de reações de hipersensibilidade relacionadas ao uso clínico de imunoglobulinas, a engenharia molecular para produção de anticorpos vem buscando diminuir o tamanho das regiões de interação antígeno-anticorpo para fragmentos variáveis de cadeia única. Camelídeos produzem além de anticorpos convencionais, imunoglobulinas desprovidas de cadeia leve, onde as regiões para reconhecimento de antígenos podem ser reduzidas, de forma funcional, através da biotecnologia, a domínios únicos, denominados VHH ou nanocorpos. Estas estruturas possuem cerca de um décimo do peso molecular de anticorpos inteiros, estabilidade a variações de temperatura e pH, boa solubilidade, capacidade de neutralização viral, além de menor tempo de meia-vida quando comparados a anticorpos humanos ou murinos. Unindo as características apresentadas pelos nanocorpos à necessidade de desenvolvimento de outras estratégias farmacológicas para tratamento de doenças negligenciadas ou relacionadas a pobreza, a linha de pesquisa propõe a produção de nanocorpos através da tecnologia phage display, previamente padronizada em nosso laboratório, para uso em soroterapia ou diagnóstico de doenças de origem viral ou ainda neutralização de efeitos tóxicos ou necrose tecidual desencadeáveis por toxinas animais.. Grande área: Ciências Biológicas Grande Área: Ciências Biológicas / Área: Biologia Geral / Subárea: Biotecnologia. Setores de atividade: Atividades de atenção à saúde humana. Palavras-chave: VHH; phage display; nanocorpos; camelídeo.'}, {'Descrição': 'Bioprospecção e bioensaios', 'Detalhes': 'Objetivo: Identificar e caracterizar  carreadores de substâncias endógenas e xenobióticos.. Grande área: Ciências da Saúde Setores de atividade: Produtos e Processos Biotecnológicos Vinculados À Saúde Humana Ou dos Animais. Palavras-chave: Membrane transporters; SLC10 family. Objetivo: Visando melhorar características farmacocinéticas, farmacodinâmicas e minimizar o aparecimento de reações de hipersensibilidade relacionadas ao uso clínico de imunoglobulinas, a engenharia molecular para produção de anticorpos vem buscando diminuir o tamanho das regiões de interação antígeno-anticorpo para fragmentos variáveis de cadeia única. Camelídeos produzem além de anticorpos convencionais, imunoglobulinas desprovidas de cadeia leve, onde as regiões para reconhecimento de antígenos podem ser reduzidas, de forma funcional, através da biotecnologia, a domínios únicos, denominados VHH ou nanocorpos. Estas estruturas possuem cerca de um décimo do peso molecular de anticorpos inteiros, estabilidade a variações de temperatura e pH, boa solubilidade, capacidade de neutralização viral, além de menor tempo de meia-vida quando comparados a anticorpos humanos ou murinos. Unindo as características apresentadas pelos nanocorpos à necessidade de desenvolvimento de outras estratégias farmacológicas para tratamento de doenças negligenciadas ou relacionadas a pobreza, a linha de pesquisa propõe a produção de nanocorpos através da tecnologia phage display, previamente padronizada em nosso laboratório, para uso em soroterapia ou diagnóstico de doenças de origem viral ou ainda neutralização de efeitos tóxicos ou necrose tecidual desencadeáveis por toxinas animais.. Grande área: Ciências Biológicas Grande Área: Ciências Biológicas / Área: Biologia Geral / Subárea: Biotecnologia. Setores de atividade: Atividades de atenção à saúde humana. Palavras-chave: VHH; phage display; nanocorpos; camelídeo.'}, {'Descrição': 'Pesquisa, Desenvolvimento de novas tecnologias para diagnósticos rápidos, miniaturizados, automação, moleculares, biossensores, nanotecnologias para diagnóstico;', 'Detalhes': 'Objetivo: Identificar e caracterizar  carreadores de substâncias endógenas e xenobióticos.. Grande área: Ciências da Saúde Setores de atividade: Produtos e Processos Biotecnológicos Vinculados À Saúde Humana Ou dos Animais. Palavras-chave: Membrane transporters; SLC10 family. Objetivo: Visando melhorar características farmacocinéticas, farmacodinâmicas e minimizar o aparecimento de reações de hipersensibilidade relacionadas ao uso clínico de imunoglobulinas, a engenharia molecular para produção de anticorpos vem buscando diminuir o tamanho das regiões de interação antígeno-anticorpo para fragmentos variáveis de cadeia única. Camelídeos produzem além de anticorpos convencionais, imunoglobulinas desprovidas de cadeia leve, onde as regiões para reconhecimento de antígenos podem ser reduzidas, de forma funcional, através da biotecnologia, a domínios únicos, denominados VHH ou nanocorpos. Estas estruturas possuem cerca de um décimo do peso molecular de anticorpos inteiros, estabilidade a variações de temperatura e pH, boa solubilidade, capacidade de neutralização viral, além de menor tempo de meia-vida quando comparados a anticorpos humanos ou murinos. Unindo as características apresentadas pelos nanocorpos à necessidade de desenvolvimento de outras estratégias farmacológicas para tratamento de doenças negligenciadas ou relacionadas a pobreza, a linha de pesquisa propõe a produção de nanocorpos através da tecnologia phage display, previamente padronizada em nosso laboratório, para uso em soroterapia ou diagnóstico de doenças de origem viral ou ainda neutralização de efeitos tóxicos ou necrose tecidual desencadeáveis por toxinas animais.. Grande área: Ciências Biológicas Grande Área: Ciências Biológicas / Área: Biologia Geral / Subárea: Biotecnologia. Setores de atividade: Atividades de atenção à saúde humana. Palavras-chave: VHH; phage display; nanocorpos; camelídeo.'}, {'Descrição': 'Bioprospecção e Bioensaios', 'Detalhes': 'Objetivo: Identificar e caracterizar  carreadores de substâncias endógenas e xenobióticos.. Grande área: Ciências da Saúde Setores de atividade: Produtos e Processos Biotecnológicos Vinculados À Saúde Humana Ou dos Animais. Palavras-chave: Membrane transporters; SLC10 family. Objetivo: Visando melhorar características farmacocinéticas, farmacodinâmicas e minimizar o aparecimento de reações de hipersensibilidade relacionadas ao uso clínico de imunoglobulinas, a engenharia molecular para produção de anticorpos vem buscando diminuir o tamanho das regiões de interação antígeno-anticorpo para fragmentos variáveis de cadeia única. Camelídeos produzem além de anticorpos convencionais, imunoglobulinas desprovidas de cadeia leve, onde as regiões para reconhecimento de antígenos podem ser reduzidas, de forma funcional, através da biotecnologia, a domínios únicos, denominados VHH ou nanocorpos. Estas estruturas possuem cerca de um décimo do peso molecular de anticorpos inteiros, estabilidade a variações de temperatura e pH, boa solubilidade, capacidade de neutralização viral, além de menor tempo de meia-vida quando comparados a anticorpos humanos ou murinos. Unindo as características apresentadas pelos nanocorpos à necessidade de desenvolvimento de outras estratégias farmacológicas para tratamento de doenças negligenciadas ou relacionadas a pobreza, a linha de pesquisa propõe a produção de nanocorpos através da tecnologia phage display, previamente padronizada em nosso laboratório, para uso em soroterapia ou diagnóstico de doenças de origem viral ou ainda neutralização de efeitos tóxicos ou necrose tecidual desencadeáveis por toxinas animais.. Grande área: Ciências Biológicas Grande Área: Ciências Biológicas / Área: Biologia Geral / Subárea: Biotecnologia. Setores de atividade: Atividades de atenção à saúde humana. Palavras-chave: VHH; phage display; nanocorpos; camelídeo.'}]\n",
      "  [{'chave': '2022 - Atual', 'titulo_projeto': 'Plataforma de desenvolvimento e engenharia de anticorpos e nanocorpos terapêuticos contra doenças infecciosas', 'descricao': 'Situação: Em andamento; Natureza: Pesquisa.Alunos envolvidos: Graduação: (2)  / Doutorado: (4) .Integrantes: Carla Freire Celedonio Fernandes - Coordenador / Soraya dos Santos Pereira - Integrante / Maribel E. F. Huacca - Integrante / Gilvan Pessoa Furtado - Integrante / Marcela Helena Gambim Fonseca - Integrante / Marcos Roberto Lourenzoni - Integrante / Raphael Trevizani - Integrante / Guilherme Graziany Camelo de Carvalho - Integrante / Mauricio Fraga van Tilburg - Integrante / Maria FRANCILENE Souza Silva - Integrante / Ana Carolina Matias Dinelly Pinto - Integrante.'}, {'chave': '2020 - Atual', 'titulo_projeto': 'Sequenciamento por NGS do repertório gênico de nanocorpos de Lama glama e utilização de ferramentas in silico para identificação de fragmentos anti-SARS-CoV-2 e vírus relacionados.', 'descricao': 'Descrição: A pandemia COVID-19, causada pelo coronavírus SARS-CoV-2,  superou a marca de 310.000 óbitos no mundo. Além da vacina, estratégias para controlar a doença envolvem o reposicionamento de drogas, uso de plasma de paciente convalescente e imunoterapia com anticorpos monoclonais e fragmentos, como os nanocorpos.  Nanocorpos são domínios variáveis de IgGs desprovidas de cadeia leve, presente em camelídeos. O seu  reduzido tamanho (15 kDa) contribui para a baixa imunogenicidade, biodistribuição, versatilidade de formulações (inclusive intranasal) e produção em sistema procarioto.  Nosso grupo possui experiência com a seleção de nanocorpos contra alvos virais. Este projeto visa sequenciar o repertório de bibliotecas de nanocorpos por NGS) e usar ferramentas de bioinformática estrutural com vistas a identificação de nanocorpos capazes de inibir SARS-CoV-2. O principal alvo viral para o estudo será a proteína Spike/domínio RDB, que media a invasão de células humanas via ligação ao receptor ECA2..Situação: Em andamento; Natureza: Pesquisa.Alunos envolvidos: Graduação: (1)  / Doutorado: (2) .Integrantes: Carla Freire Celedonio Fernandes - Coordenador / Soraya dos Santos Pereira - Integrante / Roberto Nicolete - Integrante / Rodrigo Guerino Stabeli - Integrante / Gilvan Pessoa Furtado - Integrante / Marcela Helena Gambim Fonseca - Integrante / Raphael Trevizani - Integrante / André Aguirre - Integrante / Anna Carolina Machado Marinho - Integrante / Darcio Ítalo Alves Teixeira - Integrante / Caio Vitor Oliveira Silva - Integrante / Andréa Queiroz Maranhão - Integrante / Marcelo Dias Baruffi - Integrante / Carlos Fuzo - Integrante / Elisa Maria de Sousa Russo - Integrante.'}, {'chave': '2020 - Atual', 'titulo_projeto': 'Desenvolvimento de protótipos de fármacos e nanocorpos de camelídeos para o tratamento da COVID-19: design & prospecção quântica e ensaios biológicos', 'descricao': 'Descrição: A emergência sanitária instalada no país, gerada pela pandemia COVID-19, instiga a busca por estratégias científicas para mitigação dos danos provocados pela doença aos diversos setores da sociedade. A doença causada pelo coronavírus, SARS-CoV-2, alcançou 216 países/territórios, onde cerca de 20 milhões de pessoas foram notificadas com a infecção. Dessas, mais de 740.000 foram a óbito. No Brasil, o número de casos chega a mais de 3,2 milhões de pessoas, com número de óbitos superior a 105.000. No Ceará, 194.000 casos foram confirmados, e destes mais de 8.000 foram a óbito. Diante o quadro, é de extrema relevância a geração de insumos farmacêuticos ativos (IFA) e a formação de recursos humanos capazes de auxiliar no enfrentamento da doença. A biodiversidade brasileira, dada a sua riqueza, se apresenta como fonte promissora para a prospecção e o desenvolvimento de moléculas com potencial terapêutico. Como forma de potencializar a eficácia e reduzir a toxicidade dessas moléculas, nanocorpos de camelídeos, pelo tamanho e versatilidade biotecnológica do insumo, têm se mostrado como uma ferramenta excelente para neutralização viral ou no endereçamento de fármacos. A presente proposta objetiva desenvolver insumos farmacêuticos ativos (IFA) antivirais empregando como alvo a neutralização viral (proteína Spike (S) do SARS-CoV-2) e a replicação viral (protease Mpro e Polimerase RNA-dependente - RdRp). Para tanto, serão investigados protótipos, moléculas naturais e sintéticas, e nanocorpos de camelídeos (anti-SARS-CoV-2) para o tratamento de COVID-19. As moléculas líderes serão associadas aos nanocorpos com vistas ao direcionamento ao alvo terapêutico (molécula+anti-SARS-CoV-2). Para identificação das moléculas capazes de inibir o SARS-CoV-2 (replicação viral) serão utilizadas ferramentas de biologia computacional e ensaios biológicos in vitro. Como ponto de partida para os ensaios in silico, moléculas candidatas, como chalconas, cumarinas, alcaloides e glucosídeos fenólicos, foram selecionadas de acordo com evidências científicas e/ou resultados preliminares do grupo. A partir dos ensaios in silico, utilizando alvos virais que participam da replicação viral (protease Mpro e RdRp), serão selecionadas 10 moléculas promissoras para avaliação da citotoxicidade (teste MTT e citometria de fluxo) e atividade antiviral in vitro (células Vero). Destas, as 3 (três) que demonstrarem melhor resposta em relação ao fármaco de referência (remdesivir) serão submetidas aos ensaios in vivo (toxicidade dose única e doses repetidas). Nanocorpos de Lama glama, selecionados contra a proteína recombinante S e o seu domínio RBD (atraente alvo por permitir a interação do vírus com o receptor humano da enzima conversora da angiotensina 2, localizado em células epiteliais alveolares), serão utilizados. Após expressão e purificação, esses serão avaliados quanto a pureza, reatividade, especificidade, afinidade e estrutura. Ainda, serão investigados livres e associados às moléculas com atividade antiviral visando a vetorização e consequente potencialização do efeito, ao neutralizar o vírus e inibir sua replicação. O nanocorpo anti-SARS-CoV-2 associado à molécula antiviral será avaliado quanto a sua segurança pré-clínica em camundongos. O projeto permitirá a estruturação da Plataforma de Desenvolvimento de Nanocorpos de Camelídeos e Insumos Farmacêuticos Ativos (PlatNIF) no Ceará, constituída por institutos de pesquisa, IESs e instituições públicas de saúde do Estado. Além disso, contribuirá com a formação de capital intelectual e processos de inovação em saúde a fim de incrementar o portfólio de produtos da Fiocruz, ou de empresas interessadas no desenvolvimento, bem como fomentar o Polo Industrial e Tecnológico de Saúde em Eusébio - CE..Situação: Em andamento; Natureza: Pesquisa.Alunos envolvidos: Graduação: (1)  / Doutorado: (4) .Integrantes: Carla Freire Celedonio Fernandes - Coordenador / Cléberson de Freitas Fernandes - Integrante / Rodrigo Guerino Stabeli - Integrante / Soraya dos Santos Pereira - Integrante / Claudia Nunes Duarte dos Santos - Integrante / Roberto Nicolete - Integrante / Luzia Kalyne Almeida Moreira Leal - Integrante / Darcio Ítalo Alves Teixeira - Integrante / Edy Sousa de Brito - Integrante / Kirley Marques Canuto - Integrante / Guilherme Julião Zocolo - Integrante / Natália Florêncio Martins - Integrante / Paulo Riceli Vasconcelos Ribeiro - Integrante / Lorena Mara Alexandre e Silva - Integrante / José Roberto Vieira Junior - Integrante / Anna Carolina Machado Marinho - Integrante / Brunheld Maia Dutra - Integrante / Lívia Coelho de Assis - Integrante / Camila Sillos Rosas Brisighello - Integrante / Maria Izabel Florindo Guedes - Integrante / Josimar de Oliveira Eloy - Integrante / Valder Nogueira Freire - Integrante / Talita Magalhães Rocha - Integrante / Alexandre Magno Rodrigues Teixeira - Integrante / Carlos Emidio Sampaio Nogueira - Integrante / Henrique Douglas Melo Coutinho - Integrante / Irwin Rose Alencar de Menezes - Integrante / Pesquisador PrincipalLarissa Deadame de Figueiredo Nicolete - Integrante / Hélcio Silva dos Santos - Integrante / Paulo Nogueira Bandeira - Integrante / Francisco Franciné Maia Júnior - Integrante / José Junior Alves da Silva - Integrante / Pablo Abreu de Morais - Integrante / Danúbio Andrade Bezerra Farias - Integrante / Elza Gadelha Lima - Integrante.Financiador(es): Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico - Auxílio financeiro.'}, {'chave': '2017 - 2022', 'titulo_projeto': 'Nanocorpos de Camelídeos para fins terapêuticos e diagnósticos: consolidação de uma plataforma de desenvolvimento de insumos biotecnológicos aplicados a saúde humana e agropecuária.', 'descricao': 'Descrição: A alta especificidade e afinidade para com antígenos fazem dos anticorpos ferramentas úteis a pesquisa biomédica, ao diagnóstico de patologias e ao desenvolvimento de biofármacos. Dispositivos point of care utilizam antígenos e/ou anticorpos específicos para o diagnóstico de patologias na pespectiva de reduzir o tempo e o custo do procedimento e, ao mesmo tempo, garantir a confiança do resultado. Quando o objetivo é a terapia passiva, além das preparações policlonais e de imunoglobulinas quiméricas ou humanizadas, estratégias para obtenção de fragmentos de anticorpos são empregadas para melhorar características farmacocinéticas e minimizar o aparecimento de reações adversas. Em adição aos anticorpos convencionais, compostos por cadeias leves e pesadas, camelídeos produzem imunoglobulinas G funcionais desprovidas de cadeia leve. A região de reconhecimento antigênico é formada pelo domínio único denominado VHH ou nanocorpo. Com 14 kDa, nanocorpos alcançam epítopos fracamente antigênicos, possuem baixa imunogenicidade, alta solubilidade, estabilidade a variações de pH e temperatura, importante para aplicação em campo, e reduzido custo de produção. Apresentáveis como monômeros ou estruturas multiméricas, são insumos biotecnológicos versáteis, atrativos ao desenvolvimento de biossensores ou imunoterápicos. Para a consolidação de uma Plataforma de Desenvolvimento de Nanocorpos de Camelídeos, que permeie a saúde humana e a agropecuária, o nosso grupo selecionou o envenamento ofídico e a babesiose como provas de conceito. A ampla distribuição e complexidade do envevenamento ofídico, destaca o agravo como problema global de saúde pública. Além dos danos relacionados a morbidade, cerca de 90.000 pessoas/ano morrem após esses acidentes. No Brasil, os acidentes com serpentes envolvem com maior frequência espécies do gênero Bothrops, seguidos de Crotalus. A região Norte é responsável por cerca de 30% das notificações. O diagnóstico do envenenamento ofídico é clínico-epidemiológico e os antivenenos são produzidos pela imunização de grandes animais com venenos selecionados. Apesar de efetiva para danos sistêmicos, a soroterapia não inibe de forma eficaz os danos locais causados pelo envenenamento e pode desencadear reações adversas. Adicionalmente, o custo para manutenção de animais, a dificuldade para produção homogênea dos imunobiológicos e a instabilidade dos produtos, instigam a busca de produtos ou processos inovadores. Para o estudo serão utilizados nanocorpos previamente selecionados contra fosfolipases A2 botrópicas - BthTX-I, BthTX-II - e crotálicas - crotoxina e monômeros CA e CB -, além de metaloproteases. As infecções por hemoparasitas do gênero Babesia spp. se destacam por promoverem reduções drásticas na produtividade dos rebanhos bovinos leiteiros e de corte. Assim, o diagnóstico precoce das babesioses é imprescindível, tendo em vista a baixa eficácia do tratamento e a redução dos prejuízos econômicos ocasionados. Tradicionalmente, o diagnóstico da parasitemia é realizado por meio do exame microscópico de esfregaços sanguíneos colhidos de vasos periféricos, testes sorológicos, os quais são muito utilizados em levantamentos epidemiológicos e, de técnicas moleculares baseadas na amplificação e quantificação do DNA do parasita em amostras de sangue. A demora e o alto custo das técnicas moleculares tornam necessária a busca por métodos alternativos para o diagnóstico rápido da patologia. Para isso, foram selecionados alvos moleculares do parasita (MSA-2c e RAP-1) relacionados ao processo de invasão do parasita nos eritrócitos de animais infectados. Somando as características apresentadas pelos nanocorpos à necessidade de desenvolvimento de ferramentas alternativas ao tratamento e ao diagnóstico do envenenamento ofídico, bem como ao diagnóstico da babesiose em bovinos, o presente projeto propõe a caracterização in silico, in vitro e.Situação: Concluído; Natureza: Pesquisa.Alunos envolvidos: Graduação: (5)  / Mestrado acadêmico: (3)  / Doutorado: (4) .Integrantes: Carla Freire Celedonio Fernandes - Coordenador / André Luis Coelho da Silva - Integrante / Soraya dos Santos Pereira - Integrante / Anderson Makoto Kayano - Integrante / luciana gato brito - Integrante / Leandro Soares Moreira Dill - Integrante / Rodrigo Guerino Stabeli - Integrante / Maribel E. F. Huacca - Integrante / Nilson Ivo Tonin Zanchin - Integrante / Gilvan Pessoa Furtado - Integrante / João Hermínio Martins da Silva - Integrante / Marcela Helena Gambim Fonseca - Integrante / Marcos Roberto Lourenzoni - Integrante / Raphael Trevizani - Integrante / Bruno Anderson Matias da Rocha - Integrante.Financiador(es): Fundação de Amparo ao Desenvolvimento das Ações Científicas e Tecnológicas - Auxílio financeiro.'}, {'chave': '2017 - 2019', 'titulo_projeto': 'Desenvolvimento de insumos biotecnológicos com vistas ao incremento da soroterapia e à construção de dispositivos de diagnóstico para o envenenamento ofídico', 'descricao': 'Descrição: O projeto aguarda aprovação de todos os aspectos éticos e legais para início das atividades..Situação: Desativado; Natureza: Pesquisa.Alunos envolvidos: Mestrado acadêmico: (2) Doutorado: (4) .Integrantes: Carla Freire Celedonio Fernandes - Coordenador / Cléberson de Freitas Fernandes - Integrante / Nidiane Dantas Reis - Integrante / Fernando Berton Zanchi - Integrante / Soraya dos Santos Pereira - Integrante / Juliana Pavan Zuliani - Integrante / Marcos Barros Luiz - Integrante / Anderson Makoto Kayano - Integrante / Leonardo de Azevedo Calderon - Integrante / Leandro Soares Moreira Dill - Integrante / Rodrigo Guerino Stabeli - Integrante / maribel elizabeth Funes Huacca - Integrante / Marcela Cristina de Souza Silva - Integrante / Naan Rodrigues Gonçalves - Integrante / erika andrea bastos soares - Integrante / Michelle Suelen Silva - Integrante / Braz Junior Campos Farias - Integrante / Eduardo Honda - Integrante / Luiz Adroaldo Armanini Tagliani - Integrante / Sérgio Basano - Integrante.Financiador(es): Fundação de Amparo ao Desenvolvimento das Ações Científicas e Tecnológicas - Auxílio financeiro.'}, {'chave': '2014 - 2018', 'titulo_projeto': 'Nanocorpos de camelídeos como ferramenta para a construção de dispositivos de diagnóstico e alternativa ao tratamento do envenenamento ofídico', 'descricao': 'Situação: Concluído; Natureza: Pesquisa.Alunos envolvidos: Mestrado acadêmico: (5) Doutorado: (2) .Integrantes: Carla Freire Celedonio Fernandes - Coordenador / Nidiane Dantas Reis - Integrante / Fernando Berton Zanchi - Integrante / Rodrigo Guerino Stabeli - Integrante / Soraya dos Santos Pereira - Integrante / Juliana Pavan Zuliani - Integrante / Ricardo de Godoi Matos Ferreira - Integrante / Michele Suelen S. Moreias - Integrante / Andreimar M. Soares - Integrante / Michele Pereira da Silva - Integrante / Marcos Barros Luiz - Integrante / Anderson Makoto Kayano - Integrante / Leonardo de Avezedo Calderon - Integrante / Marcos Roberto de Mattos Fontes - Integrante / Leandro Soares Moreira Dill - Integrante / SETÚBAL, SULAMITA S. - Integrante / Rudson J Holanda - Integrante / Luiz Hildebrando Pereira da Silva - Integrante / maribel elizabeth Funes Huacca - Integrante / Marcela Cristina de Souza Silva - Integrante / Naan Rodrigues Gonçalves - Integrante / erika andrea bastos soares - Integrante.'}, {'chave': '2013 - Atual', 'titulo_projeto': 'Prospecção e validação de substâncias bioativas como estratégia de controle para fitopatógenos', 'descricao': 'Descrição: O controle de agentes patogênicos que atacam diferentes culturas vem sendo realizado ao longo dos anos pela aplicação de agroquímicos. O uso destes produtos, embora eficiente em sua maioria, pode causar sérios danos a saúde do trabalhador e ao ambiente, além de desencadear resistência de microrganismos. Assim, a busca por ferramentas alternativas capazes de controlar o crescimento de fitopatógenos e que apresentem segurança ao homem é essencial para redução da manipulação e aplicação de produtos de alta toxicidade. Ensaios utilizando diferentes toxinas animais e extratos e óleos essenciais de plantas vêm demonstrando atividade bactericida e/ou fungicida pela capacidade de impedir ou retardar a penetração dos microrganismos em plantas, por meio da ativação de mecanismos de defesa, como também pela ação direta sobre o patógeno. Unindo a riqueza de biodiversidade presente no Bioma Amazônia à necessidade de desenvolvimento de protótipos biotecnológicos para tratamento de doenças de plantas, o presente trabalho propõe avaliar e caracterizar a atividade farmacológica de venenos e toxinas ofídicas, além de extratos totais e frações ativas de espécies do gênero Piper, em ensaios in vitro e em casa de vegetação, relacionada ao controle de fitopatógenos de importância para a agricultura, como Colletotrichum gloeosporioides, Fusarium oxysporum, Hemileia vastatrix, Mycosphaerella fijiensis, Rhizoctonia solani, Ralstonia solanacearum, e Xanthomonas campestris pv. Campestris, com foco em cultivos conduzidos pela Agricultura Familiar..Situação: Em andamento; Natureza: Pesquisa.Alunos envolvidos: Graduação: (2)  / Mestrado acadêmico: (1) .Integrantes: Carla Freire Celedonio Fernandes - Integrante / Cléberson de Freitas Fernandes - Coordenador / Nidiane Dantas Reis - Integrante / José Roberto Vieira Júnior - Integrante / Rodrigo Guerino Stabeli - Integrante / Andreimar M. Soares - Integrante / Leonardo de Avezedo Calderon - Integrante / fabio da silva barbieri - Integrante / luciana gato brito - Integrante / Rodrigo Barros Rocha - Integrante / Cesar Augusto Domingues Teixeira - Integrante / MOREIRA-DILL, LEANDRO S. - Integrante / José Nilton Medeiros Costa - Integrante / Rita de Cássia Alves - Integrante / Tamiris Chaves Freire - Integrante / Aline Souza da Fonseca - Integrante.Financiador(es): Conselho Nacional de Desenvolvimento Científico e Tecnológico - Auxílio financeiro.'}, {'chave': '2013 - Atual', 'titulo_projeto': 'Avaliação da atividade de biomoléculas provenientes da fauna e flora amazônica para o controle de parasitas de interesse pecuário', 'descricao': 'Descrição: Diversos estudos têm evidenciado que o Brasil possui as melhores características para dominar o mercado exportador de produtos de origem animal. A disponibilidade de áreas agricultáveis aliadas à abundância de água doce são fatores determinantes para colocar o Brasil em posição de destaque no mercado internacional de produtos de origem animal. Contudo, em diversos fóruns do setor, o tema referente à qualidade dos produtos pecuários e a garantia sanitária dos mesmos são colocados como barreiras não tarifárias que impeditivas para que o país se coloque como exportador de produtos pecuários de qualidade superior. A partir de demandas do mercado comprador, em especial dos países importadores, o Brasil iniciou a busca pela garantia da inocuidade de seus produtos pecuários, onde também inclui-se as cadeias da pecuária de corte e da pecuária de leite. Conceitos relacionados diretamente ao uso racional de fármacos parasiticidas devem também ser considerados como importantes no que tange à promoção da qualidade do leite e da carne que é ofertada ao mercado consumidor e, essa situação só pode ser vislumbrada com a utilização de princípios ativos eficientes para o controle das parasitoses que acometem os rebanhos pecuários. Dentre os mais importantes problemas para os sistemas pecuários bovídeos, as parasitoses mostram-se como fator limitante para a máxima produtividade dos rebanhos. A eficácia das bases parasiticidas de uso veterinário na atualidade representa um desafio para o controle das populações de artrópodes e hematozoários de interesse pecuários devido a emergência e a fixação nas populações parasitárias de genótipos resistentes as bases parasiticidas disponíveis para comercialização. O processo de descoberta e desenvolvimento de fármacos é complexo, longo e de alto custo, tendo suas raízes profundamente ligadas às inovações científicas e tecnológicas. Em nenhum lugar do mundo existem mais espécies de flora e fauna do que na Amazônia, tanto em termos de espécies habitando a região como um todo, como coexistindo em um mesmo ponto. Esse imenso patrimônio genético, já escasso nos países desenvolvidos, tem na atualidade valor econômico-estratégico inestimável em várias atividades, mas é no campo do desenvolvimento de novos medicamentos onde reside sua maior potencialidade. A terapêutica moderna, composta por medicamentos com ações específicas sobre receptores, enzimas e canais iônicos, não teria sido possível sem a contribuição dos produtos naturais, notadamente das plantas superiores, das toxinas animais e dos microrganismos. A procura por novas moléculas eficientes para o controle da parasitoses mostra-se premente e alinhada as diretrizes da produção de alimentos seguros, uma vez que a ocorrência de infestações e infecções por parasitas pecuários representam importantes eventos que expõem os sistemas de produção pecuários a perigos químicos devido a necessidade da utilização de fármacos parasiticidas. Neste sentido, a presente proposta buscará avaliar a eficiência de biomoléculas de origem animal e vegetal oriundas da biodiversidade Amazônica para o controle da infecção por hematozoários e da infestação por artrópodes parasitas de interesse pecuário de grande importância em sistemas de produção de bovinos e bubalinos..Situação: Em andamento; Natureza: Pesquisa.Integrantes: Carla Freire Celedonio Fernandes - Integrante / Cléberson de Freitas Fernandes - Integrante / José Roberto Vieira Júnior - Integrante / Valdir Alves Facundo - Integrante / Rodrigo Guerino Stabeli - Integrante / Andreimar M. Soares - Integrante / Leonardo de Avezedo Calderon - Integrante / fabio da silva barbieri - Integrante / luciana gato brito - Coordenador / Rodrigo Barros Rocha - Integrante / MOREIRA-DILL, LEANDRO S. - Integrante / Márcia Cristina de Sena Oliveira - Integrante / Mauricio Reginaldo Alves dos Santos - Integrante / Júlio Sancho L T Militão - Integrante / Andrina Guimarães Silva Braga - Integrante / Gil Valdo José da Silva - Integrante / Valdemar Lacerda Junior - Integrante / Renato Abreu Lima - Integrante / Renata Reis da Silva - Integrante / Antonio Xavier do Nascimento - Integrante / Ivanete Ferreira da Silva - Integrante / Ana Paula Leite da Silva - Integrante.'}, {'chave': '2012 - 2016', 'titulo_projeto': 'Nanocorpos de Camelídeos como Ferramenta de Diagnóstico ou Estratégia Farmacológica para Tratamento de Doenças Relacionadas a Pobreza', 'descricao': 'Descrição: Visando melhorar características farmacocinéticas, farmacodinâmicas e minimizar o aparecimento de reações de hipersensibilidade relacionadas ao uso clínico de imunoglobulinas, a engenharia molecular para produção de anticorpos vem buscando diminuir o tamanho das regiões de interação antígeno-anticorpo para fragmentos variáveis de cadeia única. Camelídeos produzem além de anticorpos convencionais, imunoglobulinas desprovidas de cadeia leve, onde as regiões para reconhecimento de antígenos podem ser reduzidas, de forma funcional, através da biotecnologia, a domínios únicos, denominados VHH ou nanocorpos. Estas estruturas possuem cerca de um décimo do peso molecular de anticorpos inteiros, estabilidade a variações de temperatura e pH, boa solubilidade, capacidade de neutralização viral, além de menor tempo de meia-vida quando comparados a anticorpos humanos ou murinos. Unindo as características apresentadas pelos nanocorpos à necessidade de desenvolvimento de outras estratégias farmacológicas para tratamento de doenças negligenciadas ou relacionadas a pobreza, o presente trabalho propõe a produção de nanocorpos através da tecnologia phage display, previamente padronizada em nosso laboratório, para uso em soroterapia ou diagnóstico de doenças de origem viral como febre amarela ou raiva, ou ainda neutralização de efeitos miotóxicos ou necrose tecidual desencadeáveis por toxinas ofídicas..Situação: Concluído; Natureza: Pesquisa.Alunos envolvidos: Graduação: (2)  / Mestrado acadêmico: (4)  / Doutorado: (2) .Integrantes: Carla Freire Celedonio Fernandes - Integrante / Fernando Berton Zanchi - Integrante / Rodrigo Guerino Stabeli - Coordenador / Soraya dos Santos Pereira - Integrante / Luis Hildebrando Pereira da Silva - Integrante / Andreimar M. Soares - Integrante / Claudia Nunes Duarte dos Santos - Integrante / Marcos Roberto de Mattos Fontes - Integrante.Financiador(es): Conselho Nacional de Desenvolvimento Científico e Tecnológico - Auxílio financeiro.'}, {'chave': '2012 - 2015', 'titulo_projeto': 'Desenvolvimento de phage display em fagos de bactérias encontradas em mosquitos como ferramenta nanotecnológica para o bloqueio de transmissão de malária', 'descricao': 'Descrição: Propomos este projeto para o desenvolvimento de ferramentas nanobiotecnológicas utilizando fagos filamentosos em phage display para bloquear o parasita da malária no mosquito vetor e consequentemente controlar sua transmissão. Utilizaremos como modelo o fago Pf3 de Pseudomonas aeruginosa e um peptídeo conhecidamente ativo no bloqueio do desenvolvimento do parasita da malária no mosquito. Este modelo servirá como prova de conceito do desenvolvimento tecnológico aqui proposto e a qual denominamos ?fagotransgênesis?. Desde que a tecnologia de phage display é bastante caracterizada somente em fagos da bactéria Escherichia coli, desenvolveremos métodos para obter expressão de peptídeos em fagos de outras bactérias, levando em conta as similaridades das estruturas genômicas dos fagos filamentosos. Testaremos a sobrevivência de fagos no trato digestivo de mosquito, desde que a biologia de fagos neste ambiente é desconhecida. Apesar de ser o organismo biológico mais numeroso no planeta não há ainda descrição de isolamento de fagos em artrópodes. Igualmente, desde que o fago ideal é aquele possivelmente encontrado naturalmente no trato digestivo do mosquito, pesquisaremos sequências de fagos neste ambiente do inseto utilizando as últimas tecnologias highthroughput de sequenciamento. Utilizaremos igualmente técnicas de biologia molecular e bacteriologia clássica para o isolamento dos possíveis fagos visualizados por sequenciamento. Com a implementação deste projeto esperamos formar recursos humanos locais de alta capacidade científica e técnica, voltados não só para tecnologias de ponta mas igualmente para problemas específicos da região Amazônica..Situação: Concluído; Natureza: Pesquisa.Alunos envolvidos: Doutorado: (4) .Integrantes: Carla Freire Celedonio Fernandes - Integrante / Rodrigo Guerino Stabeli - Coordenador / Soraya dos Santos Pereira - Integrante / Luis Hildebrando Pereira da Silva - Integrante / Alexandre de Almeida e Silva - Integrante / Leonardo de Avezedo Calderon - Integrante / Ricardo de Godoi Mattos Ferreira - Integrante / Najla Benevides Matos - Integrante / Luiz Shozo Ozaki - Integrante / Tony Katsuragawa - Integrante.Financiador(es): Conselho Nacional de Desenvolvimento Científico e Tecnológico - Auxílio financeiro.'}, {'chave': '2012 - Atual', 'titulo_projeto': 'PROSPECÇÃO, CARACTERIZAÇÃO E PROTOTIPAGEM DE AGENTES DE AÇÃO ANTIMALÁRICA E INSETICIDA A PATIR DA BIODIVERSIDADE DA AMAZÔNIA LEGAL', 'descricao': 'Descrição: O presente projeto tem o objetivo identificar, isolar e produzir, em nível experimental, protótipos de novas drogas derivadas de moléculas de origem vegetal, animal e microbiana oriundas da biodiversidade brasileira, ou especificamente sintetizadas, ativas contra alvos moleculares de vias metabólicas particulares de parasitas da malária humana e de seus respectivo vetor inseto, o Anopheles darlingi. O material de partida para o screening serão extratos de plantas, secreções e venenos de animais, metabólitos secretados por microrganismos endofíticos e actinomicetos oriundos em sua maioria da biodiversidade amazônica, e também compostos produzidos ou modificados por síntese química. A atividade anti-parasitária e/ou anti-vetorial dos produtos desenvolvidos, sejam estes obtidos por purificação clássica, detectados e isolados por ressonância plasmônica de superfície (SPR), ou sintetizados, serão analisados in vitro na forma livre ou nanoencapsulados utilizando-se modelos de culturas celulares, e em infecções experimentais in vivo, quando testados contra os parasitas, ou contra os vetores respectivos em suas diferentes fases de desenvolvimento. Paralelamente serão realizados experimentos de screenig virtual, utilizando ferramentas de docking e dinâmica molecular para identificação de substâncias capazes de interagir com o centro ativo de enzimas identificadas com alvos validados, onde serão utilizadas estruturas moleculares conhecidas depositadas em bancos de dados disponíveis, que serão adquiridas por fornecedores comerciais para realização de testes quanto suas reais capacidades de inibição das enzimas e potencial diminuição/eliminação da parasitemia em ensaios in vitro e in vivo. Os protótipos serão desenvolvidos a partir das moléculas líderes indentificadas que servirão de base para a síntese de compostos químicos variantes que também serão testados quanto suas atividades anti-parasitárias. Em resumo, o objetivo da presente proposta é gerar protótipos anti-maláricos e inseticidas devidamente estudados em fase pré-clínica e registrados em patentes que possam ser encaminhados a ensaios de caracterização toxicológica e futuros testes clínicos, gerando consequentemente conhecimento e difusão tecnológica com viabilidade econômica para o país.Situação: Em andamento; Natureza: Pesquisa.Alunos envolvidos: Mestrado acadêmico: (5) Doutorado: (5) .Integrantes: Carla Freire Celedonio Fernandes - Integrante / Valdir Alves Facundo - Integrante / Fernando Berton Zanchi - Integrante / Pietro Ciancaglini - Integrante / Rodrigo Guerino Stabeli - Coordenador / Alexandre de Almeida e Silva - Integrante / Juliana Pavan Zuliani - Integrante / Andreimar M. Soares - Integrante / Ricardo de Godoi Mattos Ferreira - Integrante / Najla Benevides Matos - Integrante / Roberto Nicolete - Integrante / Giselle Martins Gonçalves - Integrante / Patricia Soares - Integrante.Financiador(es): Conselho Nacional de Desenvolvimento Científico e Tecnológico - Auxílio financeiro.'}, {'chave': '2011 - 2014', 'titulo_projeto': 'Ampliação do Laboratório de Pesquisa para Produção de Imunobiológicos Voltados a Doenças de Relevância Humana do Complexo IPEPATRO-CEPEM-FIOCRUZ', 'descricao': 'Descrição: Dotar o laboratório de produção de imunobiológicos de melhor aporte tecnológico para realizar pesquisas, capacitar recursos humanos e difundir conhecimentos em Rondônia na área de biologia molecular focada no isolamento e caracterização de moléculas de nanoanticorpos importantes em soroterapia e/ou diagnóstico em doenças negligenciadas..Situação: Concluído; Natureza: Pesquisa.Alunos envolvidos: Graduação: (2)  / Mestrado acadêmico: (1)  / Doutorado: (2) .Integrantes: Carla Freire Celedonio Fernandes - Coordenador / Cléberson de Freitas Fernandes - Integrante / Rodrigo Guerino Stabeli - Integrante / Luis Hildebrando Pereira da Silva - Integrante / Marivaldo Rodrigues Figueiro - Integrante.Financiador(es): Conselho Nacional de Desenvolvimento Científico e Tecnológico - Auxílio financeiro.'}]\n",
      "  [{'chave': '2013 - Atual', 'titulo_projeto': 'Seminário: Ciência, Saúde e Esporte', 'descricao': 'Descrição: Projeto de divulgação científica O Governo Federal realiza anualmente a Semana Nacional de Ciência e Tecnologia (SNCTI) objetivando a socialização do conhecimento científico e tecnológico gerado nas instituições de ensino e pesquisa do país. A semana visa estimular e integrar jovens estudantes do ensino médio e de graduação na construção do setor no Brasil. Assim, são propostos palestras, eventos de eventos de iniciação científica, de pós-graduação, semanas temáticas nas escolas, workshops, portas abertas das instituições do segmento, entre outras atividades, como formas de difusão da ciência. A cada ano temas distintos são colocados em pauta. Em 2013, o tema estimulará discussões relacionadas ao desenvolvimento da ciência no país, bem como de tecnologias e protótipos aplicáveis à saúde. O tema esporte será evidenciado ainda, visto que o Brasil vem sediando eventos esportivos mundiais. Em Rondônia, as ações da SNCTI contarão com uma atividade integradora denominada I Exposição de Ciência, Tecnologia e Inovação de Rondônia, que proporcionará divulgação das atividades desenvolvidas pelas diferentes instituições que fazem ciência, tecnologia e inovação no Estado. A execução do evento envolverá diversos setores da administração pública e privada, proporcionando à comunidade geral momentos de lazer, aprendizagem, negócios e outros, promovendo a popularização da ciência no estado. Durante o evento serão realizadas palestras sócio-educativas e empreendedoras, propiciando o amadurecimento do senso crítico da população local..Situação: Em andamento; Natureza: Extensão.Integrantes: Carla Freire Celedonio Fernandes - Coordenador.Financiador(es): Conselho Nacional de Desenvolvimento Científico e Tecnológico - Auxílio financeiro.'}, {'chave': '2012 - 2013', 'titulo_projeto': 'Debatendo Economia verde, sustentabilidade e erradicação da pobreza em Rondônia', 'descricao': 'Descrição: Projeto de apoio a divulgação científica A semana nacional de ciência e tecnologia, coordenada pelo Ministério de Ciência, Tecnologia e Inovação, vem ocorrendo no Brasil desde 2004 promovendo e estimulando a difusão dos conhecimentos científicos gerados pelas instituições de pesquisa/tecnológicas a sociedade. No corrente ano, cujo tema a ser discutido será: ?Economia verde, sustentabilidade e erradicação da pobreza?, o Estado de Rondônia, através da Secretaria de Estado de Planejamento, em parceria com a Fundação Oswaldo Cruz - Rondônia, Centro de Pesquisa em Medicina Tropical, Empresa Brasileira de Pesquisa Agropecuária - Embrapa, Universidade Federal de Rondônia, Faculdades Integradas Aparício Carvalho, Faculdade São Lucas e Faculdade Tecnológica São Mateus propõe a realização de uma atividade integradora relacionada ao tema, além de diversos eventos para debater e divulgar estratégias a partir da ciência para desenvolvimento e garantia de uma economia verde, sustentabilidade e erradicação da pobreza no país. A semana nacional de ciência e tecnologia, coordenada pelo Ministério de Ciência, Tecnologia e Inovação, vem ocorrendo no Brasil desde 2004 promovendo e estimulando a difusão dos conhecimentos científicos gerados pelas instituições de pesquisa/tecnológicas a sociedade. No corrente ano, cujo tema a ser discutido será: ?Economia verde, sustentabilidade e erradicação da pobreza?, o Estado de Rondônia, através da Secretaria de Estado de Planejamento, em parceria com a Fundação Oswaldo Cruz - Rondônia, Centro de Pesquisa em Medicina Tropical, Empresa Brasileira de Pesquisa Agropecuária - Embrapa, Universidade Federal de Rondônia, Faculdades Integradas Aparício Carvalho, Faculdade São Lucas e Faculdade Tecnológica São Mateus propõe a realização de uma atividade integradora relacionada ao tema, além de diversos eventos para debater e divulgar estratégias a partir da ciência para desenvolvimento e garantia de uma economia verde, sustentabilidade e erradicação da pobreza no país..Situação: Concluído; Natureza: Extensão.Integrantes: Carla Freire Celedonio Fernandes - Coordenador.Financiador(es): Conselho Nacional de Desenvolvimento Científico e Tecnológico - Auxílio financeiro.'}, {'chave': '2011 - 2012', 'titulo_projeto': 'Seminário mudanças climáticas e ações em Rondônia', 'descricao': 'Descrição: Projeto vinculado ao PROGRAMA ESPECIAL DE EDUCACAO EM CIENCIA E TECNOLOGIA  Durante a Semana Nacional de Ciência e Tecnologia, Rondônia propõe, através de instituições de pesquisa e Secretaria de Estado do Planejamento e Coordenação Geral, realizar atividades simultâneas que visam a apresentação, discussão e divulgação dos resultados das pesquisas desenvolvidas no Estado. Para tanto, serão realizadas palestras, oficinas e visitas técnicas a centros de pesquisa, bem como avaliações do processo de desenvolvimento das atividades científicas no Âmbito do Programa de infra-estrutura para Jovens Pesquisadores ? PPP e Programa de Desenvolvimento Científico Regional ? DCR. Como ação central das atividades, será realizado o SEMINÁRIO MUDANÇAS CLIMÁTICAS E AÇÕES EM RONDÔNIA, em acordo com o tema proposto pela Semana Nacional do corrente ano. Além dos eventos previstos para a capital, serão realizadas diversas atividades no interior do Estado, tais como: Palestras, debates, mini-cursos por ocasião das visitas aos Centros de Difusão do Acqua Viva Rede UNIR..Situação: Concluído; Natureza: Extensão.Integrantes: Carla Freire Celedonio Fernandes - Coordenador.Financiador(es): Conselho Nacional de Desenvolvimento Científico e Tecnológico - Auxílio financeiro.'}]\n",
      "  []\n",
      "  {'Patente': {'1': {'ano': 2007, 'texto': 'Petzinger, E. ; GEYER, J. ;FERNANDES, C.F.; GERSTBERGER ; RAFALZIK, S. . Protein P4 as a marker protein for cholinergic neurons of the CNS. 2007, Alemanha.Patente: Patente no Exterior. Número do registro: WO2008104151A1, título: \"Protein P4 as a marker protein for cholinergic neurons of the CNS\" Depósito: 28/02/2007Instituição(ões) financiadora(s): Justus-Liebig Universität.', 'autores': []}}}\n",
      "  {'Participação em bancas de trabalhos de conclusão': {'1.': 'FERNANDES,C.F.C.. Comitê Institucional do Processo Seleção Edital 2013-2014 PIBIC FIOCRUZ. 2013. Fundação Oswaldo Cruz - Unidade Rondônia.', '2.': 'FERNANDES,C.F.C.. Processo de Seleção PIBIC Fiocruz, Edital 2012-2013. 2012. Fundação Oswaldo Cruz.', '3.': 'FERNANDES,C.F.C.. Comissao Tecnico Científica. 2011. Fundação Oswaldo Cruz - Unidade de Rondônia.', '4.': 'FERNANDES,C.F.C.. Comissão Avaliadora no IV Encontro de Iniciação Científica. 2010.'}, 'Participação em bancas de comissões julgadoras': {'1.': 'FERNANDES,C.F.C.. Comitê Institucional do Processo Seleção Edital 2013-2014 PIBIC FIOCRUZ. 2013. Fundação Oswaldo Cruz - Unidade Rondônia.', '2.': 'FERNANDES,C.F.C.. Processo de Seleção PIBIC Fiocruz, Edital 2012-2013. 2012. Fundação Oswaldo Cruz.', '3.': 'FERNANDES,C.F.C.. Comissao Tecnico Científica. 2011. Fundação Oswaldo Cruz - Unidade de Rondônia.', '4.': 'FERNANDES,C.F.C.. Comissão Avaliadora no IV Encontro de Iniciação Científica. 2010.'}}\n",
      "  [{'nome': 'Orientações', 'subsecoes': [{'nome': 'Orientações e supervisões em andamento', 'orientacoes': [{'tipo': 'Tese de doutorado', 'detalhes': 'Anna Carolina Machado Marinho. Expressão, caracterização estrutural e ensaios de estabilidade de nanocorpos e construtos relacionados, com vistas ao incremento da soroterapia antiofídica. Início: 2020. Tese (Doutorado em Doutorado em Ciências Farmacêuticas da Universidade Federal do Ceará)  - Fundação Oswaldo Cruz CE. (Orientador).'}]}, {'nome': 'Orientações e supervisões concluídas', 'orientacoes': [{'tipo': 'Dissertação de mestrado', 'detalhes': 'Nairo Brilhante da Silva. CONSTRUÇÃO DE BIOSSENSORES A PARTIR DE VHHs DE CAMELÍDEO E NANOPARTÍCULAS DE OURO COLOIDAL COM VISTAS AO DESENVOLVIMENTO DE DISPOSITIVOS DE DIAGNÓSTICO RÁPIDO PARA O ENVENENAMENTO POR SERPENTES DO GÊNERO Crotalus. 2019.  Dissertação  (Mestrado em Biologia Experimental)  - Universidade Federal de Rondônia, . Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Dissertação de mestrado', 'detalhes': 'Erika Andrea B Soares. Nanocorpos de camelídeos como ferramenta para a construção de dispositivos de diagnóstico ofídico. 2017.  Dissertação  (Mestrado em Biologia Experimental)  - Universidade Federal de Rondônia, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Dissertação de mestrado', 'detalhes': 'Braz Junior C Farias. Anticorpos de camelídeos como ferramenta biotecnológica para neutralização de toxinas de serpentes. 2017.  Dissertação  (Mestrado em Biologia Experimental)  - Universidade Federal de Rondônia, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Dissertação de mestrado', 'detalhes': 'Michelle Suelen da Silva Morais. Caracterização de nanocorpos de camelídeos ativos contra a proteína N de hantavirus. 2016.  Dissertação  (Mestrado em Biologia Experimental)  - Universidade Federal de Rondônia, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Dissertação de mestrado', 'detalhes': 'Naan Gonçalves Rodrigues. Nanocorpos de camelídeos ativos contra toxinas do veneno da serpente Crotalus durissus terrificus. 2015.  Dissertação  (Mestrado em Biologia Experimental)  - Universidade Federal de Rondônia, Conselho Nacional de Desenvolvimento Científico e Tecnológico. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Dissertação de mestrado', 'detalhes': 'Marcela Cristina Souza da Silva. Produção e caracterização parcial de nanocorpos de Lama glama ativos contra a BjussuMP-II, uma metaloprotease isolada do veneno da serpente Bothrops jararacussu. 2015.  Dissertação  (Mestrado em Biologia Experimental)  - Universidade Federal de Rondônia, Conselho Nacional de Desenvolvimento Científico e Tecnológico. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Dissertação de mestrado', 'detalhes': 'Marcos Barros Luiz. Produção e caracterização  parcial de nanocorpos do tipo VHH ativos contra toxina crotoxina  da serpente Crotalus durissus terrificus. 2014.  Dissertação  (Mestrado em Biologia Experimental)  - Universidade Federal de Rondônia, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Dissertação de mestrado', 'detalhes': 'Michele Pereira da Silva. Caracterização parcial de nanocorpos ativos contra toxinas isoladas do veneno de Bothrops jararacussu. 2014.  Dissertação  (Mestrado em Biologia Experimental)  - Universidade Federal de Rondônia, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Dissertação de mestrado', 'detalhes': 'Nidiane Dantas Reis Prado. Produção e caracterização molecular de nanocorpos de Lama glama (VHH) ativos contra toxinas da serpente Bothrops jararacussu. 2013.  Dissertação  (Mestrado em Biologia Experimental)  - Universidade Federal de Rondônia, . Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Tese de doutorado', 'detalhes': 'Brunheld Maia Dutra. Construções diméricas de anticorpos de domínio único (VHH) como potencial insumo biotecnológico para o tratamento do envenenamento botrópico. 2024. Tese  (Doutorado em Turma Especial de Doutorado em Biotecnologia e Saúde/PGBCM)  - Fundação Oswaldo Cruz CE, . Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Tese de doutorado', 'detalhes': 'Lívia Coelho de Assis. Desenvolvimento de insumos biotecnológicos, baseados em nanocorpos de camelídeo e molécula sintética, para o SARS-CoV-2 agente etiológico da Covid-19. 2024. Tese  (Doutorado em Turma Especial de Doutorado em Biotecnologia e Saúde/PGBCM)  - Fundação Oswaldo Cruz CE, . Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Tese de doutorado', 'detalhes': 'Vívian Magalhães Brandão dos Santos. Desenvolvimento de fragmentos de anticorpos de domínio unico com vistas ao tratamento da Leishmaniose tegumentar. 2023. Tese  (Doutorado em Doutorado da Rede de Biotecnologia da Região Nordeste)  - Fundação Oswaldo Cruz CE, . Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Tese de doutorado', 'detalhes': 'Angela Donato Maia Malaquias. DESENVOLVIMENTO DE NANOCORPO DE CAMELÍDEO DO TIPO VHH ? Fc ANTI-FOSFOLIPASES BOTRÓPICAS (bthtx-I E II) EM SISTEMA VEGETAL COM VISTAS A NEUTRALIZAÇÃO DE TOXINAS DE SERPENTES DO GÊNERO Bothrops. 2023. Tese  (Doutorado em Doutorado da Rede de Biotecnologia da Região Nordeste)  - Fundação Oswaldo Cruz CE, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Tese de doutorado', 'detalhes': 'Marcos Barros Luiz. ANTICORPOS DE DOMÍNIO ÚNICO, DO TIPO VHH, CONTRA CROTOXINA DO VENENO DE Crotalus durissus terrificus: FERRAMENTAS BIOTECNOLÓGICAS PARA O DIAGNÓSTICO OU TRATAMENTO DO ENVENENAMENTO CROTÁLICO.. 2018. Tese  (Doutorado em Biologia Experimental)  - Universidade Federal de Rondônia, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Tese de doutorado', 'detalhes': 'Nidiane Dantas Reis Prado. Nanocorpos de camelídeos como estratégia farmacológica para neutralização de toxinas de Bothrops jararacussu. 2017. Tese  (Doutorado em Biologia Experimental)  - Universidade Federal de Rondônia, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Tese de doutorado', 'detalhes': 'Soraya dos Santos Pereira. Seleção e caracterização de nanocorpos de camelídeos contra o antígeno recombinante do segmento-S de Hantavirus, cepa Araucária: Um protótipo para diagnóstico alternativo de infecções por Hantavirus. 2013. Tese  (Doutorado em Biologia Experimental)  - Universidade Federal de Rondônia, . Coorientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Trabalho de conclusão de curso de graduação', 'detalhes': 'IDÊ COELHO ROCHA OLIVEIRA. ATENÇÃO FARMACÊUTICA NA AUTOMEDICAÇÃO DOMICILIAR. 2010. Trabalho de Conclusão de Curso. (Graduação em Farmácia)  - Faculdades Integradas Aparício Carvalho. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Trabalho de conclusão de curso de graduação', 'detalhes': 'CLEICIA PATRÍCIA CORDEIRO. USO FARMACOLÓGICO DA SITUBRAMINA NO TRATAMENTO DA OBESIDADE: REVISÃO DE LITERATURA. 2010. Trabalho de Conclusão de Curso. (Graduação em Farmácia)  - Faculdades Integradas Aparício Carvalho. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Trabalho de conclusão de curso de graduação', 'detalhes': 'Renata Ferreira Quinquin. Ocorrência de intoxicações por medicamentos na cidade de Porto Velho. 2010. Trabalho de Conclusão de Curso. (Graduação em Farmácia)  - Faculdades Integradas Aparício Carvalho. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Trabalho de conclusão de curso de graduação', 'detalhes': 'Anderson de Oliveira Guedes. Uso indiscriminado de medicamentos pela população do município de Porto Velho, um impacto na saúde pública. 2010. Trabalho de Conclusão de Curso. (Graduação em Farmácia)  - Faculdades Integradas Aparício Carvalho. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Trabalho de conclusão de curso de graduação', 'detalhes': 'Ermeson Dias santana. DEFICIÊNCIA DA ATENÇÃO FARMACÊUTICA EM FARMÁCIAS COMUNITÁRIAS DA ZONA LESTE DA CIDADE DE PORTO VELHO-RO. 2010. Trabalho de Conclusão de Curso. (Graduação em Farmácia)  - Faculdades Integradas Aparício Carvalho. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Trabalho de conclusão de curso de graduação', 'detalhes': 'Elber Rogério Jucá da Silva. IMPORTÂNCIA E BENEFÍCIOS DA IMPLANTAÇÃO DA FARMÁCIA CLÍNICA NO HOSPITAL JOÃO PAULO II EM PORTO VELHO ? RO. 2010. Trabalho de Conclusão de Curso. (Graduação em Farmácia)  - Faculdades Integradas Aparício Carvalho. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Trabalho de conclusão de curso de graduação', 'detalhes': 'Mariana Martins Chaves. Influência da atenção farmacêutica na interação prescritor e paciente diabético tipo II. 2009. Trabalho de Conclusão de Curso. (Graduação em Farmácia)  - Faculdades Integradas Aparício Carvalho. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Iniciação científica', 'detalhes': 'Nauanny Karem Rodrigues de Lima Silva. Padronização de ensaios de expressão e purificação de nanocorpos de camelídeo (VHH) ativos contra toxinas de serpentes dos gêneros Bothrops e Crotalus. 2015. Iniciação Científica - Fundação Oswaldo Cruz - Unidade Rondônia, Conselho Nacional de Desenvolvimento Científico e Tecnológico. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Iniciação científica', 'detalhes': 'Michelle Suelen da Silva Morais. Caracterização parcial de domínios de VHH de imunoglobulinas G de cadeia pesada de lama glama espefíficos para o vírus  rábico. 2013. Iniciação Científica. (Graduando em Ciências Biológicas)  - Centro Universitário São Lucas, Conselho Nacional de Desenvolvimento Científico e Tecnológico. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Iniciação científica', 'detalhes': 'Marta Gabriela Barbosa Sobreira Luz. Seleção e caracterização parcial de domínios VHH de imunoglobulinas G de cadeia pesada de lama glama ativos contra crotoxina, uma neurotoxina da serpente crotalus durissus terrificus. 2013. Iniciação Científica - Fundação Oswaldo Cruz - Unidade de Rondônia, Fundação Oswaldo Cruz - Unidade Rondônia. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Iniciação científica', 'detalhes': 'Shirlei Cristina Goes Silva. Seleção e caracterização parcial de domínios VHH de imunoglobulinas G de cadeia pesada de lama glama específicos para o vírus amarílico. 2012. Iniciação Científica - Fundação Oswaldo Cruz - Unidade de Rondônia. Orientador: Carla Freire Celedonio Fernandes.'}, {'tipo': 'Iniciação científica', 'detalhes': 'Suzannna Ribeiro Hassen. Purificação da Fração IgG e Produção de Anticorpos Policlonais Específicos Contra Imunoglobulinas G de Cadeia Pesada de Lama glama em Coelhos. 2010. Iniciação Científica - Instituto de Pesquisas em Patologias Tropicais de Rondônia. Orientador: Carla Freire Celedonio Fernandes.'}]}]}]\n",
      "  [{'doi': 'http://dx.doi.org/10.1016/j.drudis.2024.103967', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1007/s40291-024-00713-1', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.intimp.2024.112215', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.26668/businessreview/2024.v9i6.4695'}, {'doi': 'http://dx.doi.org/10.1016/j.toxicon.2024.107837', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.jviromet.2023.114787', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1007/s12033-023-00831-x', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.15406/japlr.2022.11.00399'}, {'doi': 'http://dx.doi.org/10.1155/2022/2748962', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1080/07391102.2022.2148128', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1080/07391102.2022.2107072', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.bbrc.2020.12.074', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.ijbiomac.2021.06.043', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1007/s40291-021-00533-7', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.ijbiomac.2021.10.126', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.ijbiomac.2020.10.031', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.ijbiomac.2020.10.062', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1590/s1678-3921.pab2020.v55.01756', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1002/jlb.ma1118-463r', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1590/0037-8682-0526-2018', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.3390/toxins10040142', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': None}, {'doi': 'http://dx.doi.org/10.1016/j.ijbiomac.2018.07.141', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.3389/fimmu.2017.00653', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.tiv.2017.02.003', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.ijbiomac.2017.05.076', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.ttbdis.2017.05.006', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.ijbiomac.2017.07.140', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1371/journal.pone.0151363', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.toxicon.2016.05.013', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.ijbiomac.2016.07.022', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1590/0037-8682-0195-2016', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.toxicon.2015.09.005', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1186/s12906-015-0948-1', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1155/2014/981923', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1155/2014/203639', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1155/2014/196754', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1155/2014/195356', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1155/2014/595186', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': None}, {'doi': None}, {'doi': 'http://dx.doi.org/10.1155/2014/950538', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1371/journal.pone.0108067', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': None}, {'doi': None}, {'doi': 'http://dx.doi.org/10.1016/j.neuroscience.2008.01.049', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1074/jbc.m702663200', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.ejcb.2007.06.001', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}, {'doi': 'http://dx.doi.org/10.1016/j.bbrc.2007.06.160', 'impact-factor': '6.5', 'original_title': 'Drug Discovery Today (1359-6446)'}]\n"
     ]
    }
   ],
   "source": [
    "k = 7\n",
    "print(f\"Criar Label 'Pesquisador' com as propriedades:\")\n",
    "print(f\"  Label: 'Pesquisador'\")\n",
    "subdict_identificacao = \n",
    "print(f\"  {curriculos[k].get('Identificação')}\")\n",
    "subdict_formacaoacademica = {'Formação Acadêmica': [x.get('Descrição') for x in curriculos[k].get('Formação').get('Acadêmica')]}\n",
    "print(f\"  {dict_formacaoacademica}\")\n",
    "print(f\"  {curriculos[k].get('Áreas')}\")\n",
    "print(f\"  {[x.get('Instituição') for x in curriculos[k].get('Atuação Profissional')]}\")\n",
    "print(f\"  {curriculos[k].get('Linhas de Pesquisa')}\")\n",
    "print(f\"  {curriculos[k].get('ProjetosPesquisa')}\")\n",
    "print(f\"  {curriculos[k].get('ProjetosExtensão')}\")\n",
    "print(f\"  {curriculos[k].get('ProjetosDesenvolvimento')}\")\n",
    "print(f\"  {curriculos[k].get('Patentes e registros')}\")\n",
    "print(f\"  {curriculos[k].get('Bancas')}\")\n",
    "print(f\"  {curriculos[k].get('Orientações')}\")\n",
    "print(f\"  {curriculos[k].get('JCR2')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traduzir títulos para inglês e gerar os embeedings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 05:52:25,486 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-09 05:52:25,487 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Processando currículos:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3089f8df26d8483f8986311c5a11f817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:   2%|▎         | 1/40 [00:00<00:32,  1.21it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c090a6b7cb9e4b2f9aacb56b1127612a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:   5%|▌         | 2/40 [00:03<01:08,  1.80s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e813d3faaa1b43fa9ae31a19a292f20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce78387b7ba485caff5fd7244a9d790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  10%|█         | 4/40 [00:08<01:23,  2.32s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5daea6b45514f6fbee5e80d9212f792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  12%|█▎        | 5/40 [00:09<01:09,  1.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31863138a75444b2ba5f45a7e6dbbffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  15%|█▌        | 6/40 [00:10<00:50,  1.48s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934914553cac4105a8742788f8bed7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d1c31faa374cc5abaf81aeaaacbfd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851ad7c948174aaf8409532e27e245b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b8717b6c1945da9f5b2b5441630ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  18%|█▊        | 7/40 [00:27<03:25,  6.21s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459a2b3751394dd28d3aed5c20e89093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fd0473ba2f49afbdf7b1ab8b27c1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  20%|██        | 8/40 [00:28<02:33,  4.80s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac2319e1ac4403d9027ea0c341a9baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a562d0c09e4945749fead68802c87901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ceccf63e37d480288413ec9da53af19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  25%|██▌       | 10/40 [00:29<01:24,  2.81s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3033201a9092450493d1e9690c8c6e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  28%|██▊       | 11/40 [00:30<01:07,  2.34s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3506542820454b66bf31499dfb93c073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5499713f65b74b388509fd741654807c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  30%|███       | 12/40 [00:31<00:54,  1.94s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77b9461282748e1828b70b3214acfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd12fce5d2f74ae998f18c77d61071da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80d2490fc2545178ebd252fec4eb0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  35%|███▌      | 14/40 [00:33<00:39,  1.54s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8e4f445931474b8ba9a0c335fec6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  38%|███▊      | 15/40 [00:33<00:30,  1.22s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049f3005c72e4a08b0d14321a5932e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8204fc8ff247d6b96c2151b0365cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89358a75cdca4bbf93e8c008a9c41596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  40%|████      | 16/40 [00:59<02:56,  7.35s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1656a8e94f4443a390670b2152454279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  42%|████▎     | 17/40 [01:02<02:25,  6.32s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e6aaec1cbb492c8a8b8b76fcef64fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  45%|████▌     | 18/40 [01:03<01:46,  4.82s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ba410dd3ea48188547ad34436dcfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3758e04c65a84530bffe93738ba9454a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490200a38b4d4bc9810267a26335e9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cba279786ea454e830a56d23a417da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  50%|█████     | 20/40 [01:27<02:40,  8.03s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549227c3d332467299f9c233868f9ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a20fff16044a83b79410f42321a269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81f22ad5c284f8b91d709cdd081ae8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b057b116934dd09c0993e8518b2d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  52%|█████▎    | 21/40 [01:32<02:15,  7.13s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8524a6cb47d64cd2b4df6dfd128a70b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ce1e853bde47dbb1c19dc9fe701531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3a6ba782c346ae9df6162f37c9ffb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  57%|█████▊    | 23/40 [01:37<01:29,  5.29s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1dffa9ab8b457189591cec1e0b5fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  60%|██████    | 24/40 [01:37<01:06,  4.18s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a115fc2e164777959398f9b4cbe2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c39c25891a4a29ba26357c56227d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709f12007c184a4b98eb2b53d9b0195b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  62%|██████▎   | 25/40 [02:05<02:30, 10.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b09aeffaaa4a9caa6410ae4e98834a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  65%|██████▌   | 26/40 [02:06<01:46,  7.60s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9415fb40e24a52ab793db5ada12511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1407f12a2d4432976ff708c01716d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fe00a7edee4eed9d411189c3904380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb1dde285954774941d6ef190baf30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409751755b1d4a2aa4677869f14025f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ec1b1cda7542e8a265e502639e7420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  70%|███████   | 28/40 [02:29<01:50,  9.24s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d76051f69804d8483ed3a8d28a586f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  72%|███████▎  | 29/40 [02:31<01:22,  7.50s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a22d7c85dd64a58ac9dcf7f4bebcf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  75%|███████▌  | 30/40 [02:33<01:02,  6.24s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37241bdbc83476581b19437334eebd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d3ccb535cd45049b96954459f61f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  78%|███████▊  | 31/40 [02:44<01:07,  7.48s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5000a5870c4c3ca6fc645b8da4ccea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  80%|████████  | 32/40 [02:45<00:44,  5.60s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91897a3a7a9437d9ffa464d5eb4d56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5ccedda20a42a8868a93b412d35bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  82%|████████▎ | 33/40 [02:47<00:32,  4.63s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444d10ec47954399961332ee7c42d02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar título: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddccc733677142668222ae04b653d048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33e3ee9a0164668aa15a07e153441ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  85%|████████▌ | 34/40 [02:48<00:22,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar título: Expecting value: line 1 column 1 (char 0)\n",
      "Erro ao processar título: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dbad0fbc9540d0a6241356faed8c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ea56de9fbd4ed2be43ea899141c7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar título: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e598198ee6b64a4bb4cb2cc53abcd8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1894fd6a7d3a43a3b40af7700a602497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  88%|████████▊ | 35/40 [03:11<00:45,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar título: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8936e72aa6934b7e92b6949659cb9afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  90%|█████████ | 36/40 [03:12<00:27,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar título: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe38a325336649ea92584c48761bbd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar título: Expecting value: line 1 column 1 (char 0)\n",
      "Erro ao processar título: Expecting value: line 1 column 1 (char 0)\n",
      "Erro ao processar título: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9854a5a58d1a47208a80af0bb2839d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  92%|█████████▎| 37/40 [03:30<00:30, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar título: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53da8675fc6b4e209e0e2f315ce334bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037c9951395f4ff682058b5cbb99bb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  95%|█████████▌| 38/40 [03:44<00:22, 11.26s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58a7e33657f4f7e95d1a9dcd2d6f6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos:  98%|█████████▊| 39/40 [03:44<00:08,  8.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe96799db344d2c912cae21e4adce73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando currículos: 100%|██████████| 40/40 [03:47<00:00,  5.69s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "\n",
    "# Instanciar o modelo de linguagem (use o mesmo modelo usado para os editais)\n",
    "modelo = SentenceTransformer(best_model_name)\n",
    "\n",
    "# Instanciar o tradutor\n",
    "tradutor = Translator()\n",
    "\n",
    "# Extrair os títulos dos artigos publicados, traduzir para inglês (se necessário) \n",
    "# e gerar embeddings em lotes, incluindo tratamento de erros e títulos vazios\n",
    "artigos_embeddings = []\n",
    "artigos_titulos = []\n",
    "batch_size = 32\n",
    "\n",
    "for curriculo in tqdm(curriculos, desc=\"Processando currículos\"):\n",
    "    artigos = curriculo.get('Produções', {}).get('Artigos completos publicados em periódicos', [])\n",
    "    for i in tqdm(range(0, len(artigos), batch_size), desc=\"Processando artigos\", leave=False):\n",
    "        batch = artigos[i: i + batch_size]\n",
    "        titulos_batch = [artigo.get('titulo') for artigo in batch]  # Manter todos os títulos, mesmo os vazios\n",
    "\n",
    "        # Criar lista de títulos traduzidos, incluindo títulos vazios e ignorando erros\n",
    "        titulos_en = []\n",
    "        for titulo in titulos_batch:\n",
    "            if titulo:\n",
    "                try:\n",
    "                    idioma = detect(titulo)\n",
    "                    if idioma != 'en':\n",
    "                        titulo = tradutor.translate(titulo, dest='en').text\n",
    "                except (JSONDecodeError, Exception) as e:  # Capturar todas as exceções\n",
    "                    print(f\"Erro ao processar título: {e}\")\n",
    "                    # Opções para tratar o erro:\n",
    "                    # 1. Usar o título original: titulo = titulo\n",
    "                    # 2. Usar um título vazio: titulo = \"\"\n",
    "                    # 3. Ignorar o título: continue\n",
    "                    continue  # Ignorar o título com erro\n",
    "            else:\n",
    "                titulo = \"\"  # Usar um título vazio se o título original for None\n",
    "\n",
    "            titulos_en.append(titulo)  # Adicionar o título (traduzido ou original) à lista\n",
    "\n",
    "        # Gerar embeddings para os títulos em inglês em lote\n",
    "        if titulos_en:\n",
    "            try:\n",
    "                embeddings = modelo.encode(titulos_en, convert_to_tensor=True, batch_size=batch_size)\n",
    "                artigos_embeddings.append(embeddings)\n",
    "            except Exception as e:\n",
    "                print(f\"  Erro ao gerar embeddings: {e}\")\n",
    "                print(f\"  Título com problema: {titulo}\")\n",
    "\n",
    "# Concatenar os embeddings em um único tensor\n",
    "if artigos_embeddings:\n",
    "    artigos_embeddings = torch.cat(artigos_embeddings, dim=0)\n",
    "    print(f\"{len(curriculos)} processados com total de {len(artigos_embeddings)} embeedings de artigos gerados.\")\n",
    "    print(\"Dimensões do tensor:\", artigos_embeddings.shape)\n",
    "else:\n",
    "    print(\"Nenhum embedding de artigo foi gerado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --force-reinstall googletrans==4.0.0-rc1 httpx==0.13.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes das traduções de frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funding_analyser import ENPreprocessor, BRPreprocessor\n",
    "\n",
    "# Criar uma instância do ENPreprocessor\n",
    "en_preprocessor = ENPreprocessor()\n",
    "\n",
    "# Frase em inglês para testar a tradução\n",
    "frase_portugues = \"Esta frase é em português.\"\n",
    "\n",
    "# Traduzir a frase para português\n",
    "traducao = en_preprocessor.translate_to_en([frase_portugues])[0]\n",
    "\n",
    "# Imprimir a tradução\n",
    "print(f\"Frase português: {frase_portugues}\")\n",
    "print(f\"Frase em inglês: {traducao}\")\n",
    "\n",
    "# Criar uma instância do BRPreprocessor\n",
    "br_preprocessor = BRPreprocessor()\n",
    "\n",
    "# Frase em inglês para testar a tradução\n",
    "frase_ingles = \"This is a test sentence.\"\n",
    "\n",
    "# Traduzir a frase para português\n",
    "traducao = br_preprocessor.translate_to_pt([frase_ingles])[0]\n",
    "\n",
    "# Imprimir a tradução\n",
    "print(f\"\\nFrase em inglês: {frase_ingles}\")\n",
    "print(f\"Frase português: {traducao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Preparar ambiente de desenvolvimento</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalações e testes de bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar bibliotecas de uso geral em cada ambiente virtual dos testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurar canais e repositórios de bibliotecas\n",
    "# %conda config --add channels pypi\n",
    "# %conda config --add channels nvidia\n",
    "\n",
    "## instalar Spacy e Xformers\n",
    "# %conda uninstall pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch-nightly -c nvidia\n",
    "# %conda install --force-reinstall pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch-nightly -c nvidia\n",
    "# %conda install xformers\n",
    "# %conda install plotly\n",
    "# %conda install seaborn\n",
    "# %conda install nltk\n",
    "# %conda install -c conda-forge wordcloud\n",
    "# %conda install -c conda-forge notebook ipywidgets\n",
    "\n",
    "## Bliliotecas só disponíveis no Pip\n",
    "# %pip install sentence_transformers\n",
    "# %pip install py-cpuinfo\n",
    "\n",
    "## Instalar e testar biblioteca de gráficos Altair e Vega Datasets\n",
    "# !pip3 install altair vega_datasets\n",
    "# !pip3 install altair_viewer\n",
    "# !pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teste de instalação da bilbioteca altair\n",
    "# import altair as alt\n",
    "# from vega_datasets import data\n",
    "\n",
    "# # Criar um gráfico de exemplo\n",
    "# chart = alt.Chart(data.cars()).mark_point().encode(\n",
    "#     x='Horsepower',\n",
    "#     y='Miles_per_Gallon',\n",
    "#     tooltip=['Horsepower', 'Miles_per_Gallon']\n",
    "# ).interactive()\n",
    "\n",
    "# chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Instalações em ambientes variados</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar CUDA e CUDA Toolkit no Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Procedimento normal de instalação em Windows\n",
    "## Criar novo ambiente virtual a partir de janela do jupyter notebook\n",
    "# !conda create -n pytorch_env python=3.11\n",
    "# !conda activate pytorch_env\n",
    "\n",
    "### Configurar as Variáveis de Ambiente em Windows:\n",
    "## Para configurar ambiente do conda corretamente no seu sistema. O caminho para o executável do conda (conda.exe) deve estar incluído na variável de ambiente PATH.\n",
    "\n",
    "# No Painel de Controle do Windows:\n",
    "# Abra o menu Iniciar e procure por \"variáveis de ambiente\".\n",
    "# Clique em \"Editar as variáveis de ambiente do sistema\".\n",
    "# Na aba \"Avançado\", clique em \"Variáveis de Ambiente\".\n",
    "\n",
    "## Em \"Variáveis do sistema\", encontre a variável \"Path\" e clique em \"Editar\".\n",
    "# Adicione os seguintes caminhos, se ainda não estiverem presentes (substitua \"seu_usuario\" pelo seu nome de usuário):\n",
    "# C:\\Users\\seu_usuario\\anaconda3\n",
    "# C:\\Users\\seu_usuario\\anaconda3\\Scripts\n",
    "# C:\\Users\\seu_usuario\\anaconda3\\Library\\bin\n",
    "\n",
    "# !conda init\n",
    "\n",
    "## A partir da janela do terminal\n",
    "# Atualizar o anaconda para sua versão mais recente\n",
    "# conda update -n base -c defaults conda\n",
    "# conda update -n base -c defaults conda --force-reinstall\n",
    "\n",
    "# Se precisar de uma versão específica basta determinar o número da versão, por exemplo para 24.5.0\n",
    "# conda install conda=24.5.0\n",
    "\n",
    "# Para interagir diretamente no PowerShell é preciso ter permissão para isso, verificar no terminal com\n",
    "# Get-ExecutionPolicy\n",
    "\n",
    "# Se a política for Restricted, altere-a para RemoteSigned ou Unrestricted com o seguinte comando:\n",
    "# Set-ExecutionPolicy RemoteSigned -Scope CurrentUser\n",
    "\n",
    "# Usando direto no terminal para ativar o ambiente\n",
    "# conda activate pytorch_env\n",
    "# conda deactivate pytorch_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instalações pontuais de pacotes extra requirements.txt\n",
    "# !python.exe -m pip install --upgrade pip\n",
    "# !python3 -m pip install --upgrade pip\n",
    "# !pip3 install -r requirements.txt\n",
    "# !pip install requests beautifulsoup4 selenium pandas gitpython\n",
    "\n",
    "### Complementar instalação de modelos multilingues no SpaCy\n",
    "## Baixar modelos de de linguagem específica do SpaCy\n",
    "# !python -m spacy download pt_core_news_lg\n",
    "\n",
    "## Verificar modelos e versões instaladas no SpaCy\n",
    "# !python -m spacy validate\n",
    "\n",
    "# %pip install --upgrade ipython\n",
    "\n",
    "### Instalar extensão no VScode para renderizar mermaid\n",
    "# https://marketplace.visualstudio.com/items?itemName=bierner.markdown-mermaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Em caso de necessidade de upgrade ou reinstalação do PyTorch\n",
    "# !pip3 install --upgrade torchvision\n",
    "\n",
    "### Em caso de conflitos entre pacotes que impedem instalação\n",
    "## Exemplo: Instalar xformers com pip pode dar conflito com a bilioteca TBB\n",
    "# %pip show xformers\n",
    "# print()\n",
    "# %pip show TBB\n",
    "# %pip uninstall TBB # Se não funcionar deletar manualmente a biblioteca será reinstalada com xformers\n",
    "# %pip install xformers\n",
    "\n",
    "### Outras resoluções de conflito entre pacotes\n",
    "## 1. Limpar cache e atualizar versão do pip\n",
    "# !pip cache purge\n",
    "# !python.exe -m pip install --upgrade pip\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "## 2. Em caso de conflitos persistentes forçar atualização ou reinstalação de pacotes\n",
    "# !pip install --upgrade bottleneck\n",
    "# !python.exe -m pip install --upgrade --force-reinstall omegaconf\n",
    "\n",
    "## 3. Instalar xformers com conda pode gerenciar melhor conflitos\n",
    "%conda config --add channels pytorch\n",
    "%conda update -n base -c defaults conda\n",
    "%conda install -c conda-forge xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar instalação do PyTorch\n",
    "# Deve retornar True se o CUDA estiver disponível e a versão do cuDNN instlada\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.is_available())\n",
    "    print(torch.backends.cudnn.version())\n",
    "else:\n",
    "    print(\"CUDA não está disponível.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar CUDA e CUDA Toolkit no WSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando se instala uma biblioteca no WSL, ela é instalada especificamente para o ambiente corrente. Isso permite ter uma versão de uma biblioteca instalada no Windows e uma versão diferente instalada no WSL, e elas não irão interferir uma na outra. Ao instalar bibliotecas no WSL, é recomendável usar o gerenciador de pacotes específico da distribuição (por exemplo, apt para distribuições baseadas em Debian/Ubuntu, pacman para Arch Linux, etc.). Isso garantirá que as bibliotecas sejam instaladas corretamente e que suas dependências sejam gerenciadas adequadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalar no sistema operacional PyTorch e CUDA Toolkit, acesse a página abeixo e escolha seus dados adequados\n",
    "\n",
    "https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No Terminal rodar com senha do sudo\n",
    "# sudo apt install python3-pip\n",
    "# sudo apt-get update\n",
    "# sudo apt-get upgrade\n",
    "\n",
    "## Depois siga as instruções oficiais da NVIDIA para instalar o CUDA Toolkit no WSL\n",
    "## Escolher a versão do CUDA que é compatível com a versão do PyTorch que você quer usar\n",
    "# https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=WSL-Ubuntu&target_version=2.0&target_type=deb_local\n",
    "\n",
    "## O conjunto de instruções para uma instalação típica do WSL com Ubuntu é mais ou menos o seguinte:\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\n",
    "# sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda-repo-wsl-ubuntu-12-6-local_12.6.0-1_amd64.deb\n",
    "# sudo dpkg -i cuda-repo-wsl-ubuntu-12-6-local_12.6.0-1_amd64.deb\n",
    "# sudo cp /var/cuda-repo-wsl-ubuntu-12-6-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "# sudo apt-get update\n",
    "# sudo apt-get -y install cuda-toolkit-12-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para inserir os comandos direto pelo notebook seguir os passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "password = getpass.getpass('Senha sudo: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo {password} | sudo -S rm -rf /tmp/*\n",
    "!echo {password} | sudo -S apt autoremove\n",
    "!echo {password} | sudo -S apt clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo {password} | sudo -S apt install nvidia-cuda-toolkit -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo {password} | sudo -S mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda-repo-wsl-ubuntu-12-6-local_12.6.0-1_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo {password} | sudo -S dpkg -i cuda-repo-wsl-ubuntu-12-6-local_12.6.0-1_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo {password} | sudo -S cp /var/cuda-repo-wsl-ubuntu-12-6-local/cuda-*-keyring.gpg /usr/share/keyrings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo {password} | sudo -S apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo {password} | sudo -S apt-get -y install cuda-toolkit-12-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar CUDA e CUDA Toolkit no Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar o Conda e driver da placa"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Para Linux, no terminal rodar:\n",
    "sudo apt update\n",
    "sudo apt install libopenblas-dev\n",
    "\n",
    "## Instalar o Miniconda e criar o ambiente para RAPIDS\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "bash Miniconda3-latest-Linux-x86_64.sh\n",
    "conda create -n rapids-24.10 -c rapidsai -c conda-forge -c nvidia rapids=24.10 python=3.12 'cuda-version>=12.0,<=12.5'\n",
    "\n",
    "nano ~/.bashrc\n",
    "    Adicione o caminho à variável PATH:\n",
    "        export PATH=\"$PATH:~/miniconda3/bin\"\n",
    "    Pressione Ctrl + X, depois Y e Enter para salvar as alterações.\n",
    "source ~/.bashrc\n",
    "\n",
    "sudo apt install nvidia-driver-535\n",
    "sudo reboot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar CUDA Toolkit no Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme comandos a ser gerados em: https://developer.nvidia.com/cuda-downloads\n",
    "\n",
    "Exemplo para Linux:\n",
    "\n",
    "https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=24.04&target_type=deb_local"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-ubuntu2404.pin\n",
    "sudo mv cuda-ubuntu2404.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "wget https://developer.download.nvidia.com/compute/cuda/12.6.2/local_installers/cuda-repo-ubuntu2404-12-6-local_12.6.2-560.35.03-1_amd64.deb\n",
    "sudo dpkg -i cuda-repo-ubuntu2404-12-6-local_12.6.2-560.35.03-1_amd64.deb\n",
    "sudo cp /var/cuda-repo-ubuntu2404-12-6-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "sudo apt-get update\n",
    "sudo apt-get -y install cuda-toolkit-12-6"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Instalar conforme comandos a ser gerados em: https://pytorch.org/get-started/locally/\n",
    "\n",
    "Exemplo Linux: \n",
    "\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch-nightly -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "password = getpass.getpass('Senha sudo: ')\n",
    "\n",
    "!echo {password} | sudo -S rm -rf /tmp/*\n",
    "!echo {password} | sudo -S apt autoremove\n",
    "!echo {password} | sudo -S apt clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo {password} | sudo -S apt install nvidia-cuda-toolkit -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar o Pytorch para Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar o Spacy e GoogleTranslate no WSL ou Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## No Terminal rodar com o Conda:\n",
    "# %conda install -c conda-forge spacy\n",
    "# %conda install -c conda-forge cupy\n",
    "# %conda install -c conda-forge spacy-transformers\n",
    "\n",
    "# ## Após isso rodar com o Pip:\n",
    "# %pip install spacy-lookups-data\n",
    "\n",
    "# ## Modelos mais precisos\n",
    "# # download dos modelos de linguagem\n",
    "# !python -m spacy download en_core_web_trf\n",
    "# !python -m spacy download xx_sent_ud_sm\n",
    "# !python -m spacy download pt_core_news_lg\n",
    "\n",
    "# ## Modelos menores\n",
    "# !python -m spacy download xx_ent_wiki_sm\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carregar modelos com load\n",
    "# spacy.load('pt_core_news_lg')\n",
    "# spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# %conda install -c conda-forge spacy\n",
    "# %conda install -c conda-forge cupy\n",
    "# %pip install spacy_transformers\n",
    "# %pip install spacy-lookups-data\n",
    "# %pip install editdistance\n",
    "# %pip install contextualSpellCheck\n",
    "# %pip install langdetect\n",
    "\n",
    "# %pip install cython\n",
    "\n",
    "# !python -m spacy download xx_ent_wiki_sm\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download pt_core_news_sm\n",
    "\n",
    "# %pip install --upgrade sentence-transformers\n",
    "# %pip install --upgrade --no-deps sentence-transformers\n",
    "\n",
    "# %pip install transformers -U\n",
    "# %pip install seaborn==0.12.2\n",
    "\n",
    "## Resolução de conflitos de versões entre transformers e sentence-transformers\n",
    "# %pip uninstall transformers -y\n",
    "# %pip install transformers==4.34.0 # downgarade\n",
    "\n",
    "# %pip install transformers -U # Não funcionou\n",
    "# %pip install tokenizers==0.15.2 # downgarade\n",
    "# %pip install pyspellchecker\n",
    "\n",
    "# %pip show spacy_transformers\n",
    "# %pip show sentence-transformers\n",
    "\n",
    "# %pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Não instalar em ambientes mais recentes:\n",
    "# %pip install google-cloud-translate\n",
    "# %pip install googletrans==4.0.0-rc1\n",
    "# %pip show googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ver pipelines intalados\n",
    "# !pip install -U spacy\n",
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testar insalação para utilizar GPU\n",
    "import spacy\n",
    "\n",
    "# spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/mak/miniconda3/envs/rapids-24.10/bin/python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.pipe_names)  # Should include 'spell_checker' if installed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_trf\n",
    "# !pip install transformers -U\n",
    "# !pip install spacy_transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar RAPIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No Terminal intalar o RAPIDS versões mais atualizada em https://docs.rapids.ai/install\n",
    "## Criar ambiente e instalar o RAPIDS\n",
    "# conda create -n rapids-24.08 -c rapidsai -c conda-forge -c nvidia rapids=24.08 python=3.11 'cuda-version>=12.0,<=12.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verificar intalação do RAPIDS NVIDIA\n",
    "%conda list rapids\n",
    "%conda list cugraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teste de cálculo em GPU\n",
    "import cudf\n",
    "print(cudf.Series([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reiniciar o Kernel e testar o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda list torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda list xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda list TBB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso CUDA ainda não disponível, verificar se há PyTorch CPU em vez de CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remover versões somente CPU para que seja possível CUDA ser habilitado\n",
    "# %conda uninstall pytorch torchvision torchaudio\n",
    "# %conda uninstall -y libtorch pytorch\n",
    "\n",
    "## Reinstalar versões compiladas para uso de GPU com CUDA\n",
    "## Estáveis\n",
    "%conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia\n",
    "## Ou Nightly\n",
    "# %conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch-nightly -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.randn(1, 1, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar PyTorch Geometric (pode conflitar PyTorch CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install pyg -c pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install pytorch-cluster -c pyg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testar o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se o compulador CUDA está instalado\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verificar se o driver da placa GPU está instalado\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar a lista de ambientes virtuais locais\n",
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Listar os kernels instalados no ambiente local\n",
    "!jupyter kernelspec list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Desinstalar um kernel com problemas\n",
    "# jupyter kernelspec uninstall nome_do_kernel_problemático \n",
    "# jupyter kernelspec install --user --name nome_do_seu_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar a lista de repositórios confiáveis onde buscar bibliotecas\n",
    "%conda config --show channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Capacidades CUDA disponíveis: {torch.backends.cudnn.version()}\")\n",
    "else:\n",
    "    print(\"CUDA não está disponível.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar bibliotecas otimizadas para Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar o cuDNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O cuDNN é uma biblioteca de rotinas otimizadas para Deep Learning, e é essencial para o PyTorch aproveitar ao máximo o potencial da sua GPU. \n",
    "\n",
    "Instalar a biblioteca cuDNN clicando no botão \"Download cuDNN Library\" de acordo com seu sistema operacional Windows ou Linux, Arquitetura x86_64, Versão Tarball e CUDA Version 12 \n",
    "\n",
    "https://developer.nvidia.com/cudnn\n",
    "\n",
    "Ou fazer o download com o comando:\n",
    "\n",
    "    wget https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/windows-x86_64/cudnn-windows-x86_64-9.2.1.18_cuda12-archive.zip\n",
    "\n",
    "Consultar compatibilidades em: \n",
    "\n",
    "https://docs.nvidia.com/deeplearning/cudnn/latest/reference/support-matrix.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar o cuDNN: Acesse a página de download do cuDNN no site da NVIDIA: https://developer.nvidia.com/cudnn\n",
    "\n",
    "1. Criar Conta (se necessário): Você precisará de uma conta de desenvolvedor da NVIDIA para fazer o download. Crie uma conta gratuitamente, se ainda não tiver uma.\n",
    "\n",
    "2. Escolher a versão do cuDNN que é compatível com o seu CUDA Toolkit 12.3. A página de download da NVIDIA fornecerá as opções corretas.\n",
    "\n",
    "3. Baixar o arquivo zip do cuDNN e extraia-o em um local de sua preferência.\n",
    "\n",
    "4. Copiar os seguintes arquivos da pasta extraída para a pasta de instalação do CUDA Toolkit:\n",
    "\n",
    "DLL:\n",
    "\n",
    "    Copie cudnn_cnn_infer64_8.dll (ou o nome do arquivo DLL correspondente à sua versão do cuDNN) para C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\bin.\n",
    "\n",
    "Lib:\n",
    "\n",
    "    Copie cudnn_adv_infer64_8.lib (ou arquivo .lib correspondente) para C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\lib\\x64.\n",
    "    Copie cudnn_adv_train64_8.lib (ou arquivo .lib correspondente) para C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\lib\\x64.\n",
    "    Copie cudnn_cnn_infer64_8.lib (ou arquivo .lib correspondente) para C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\lib\\x64.\n",
    "    Copie cudnn_cnn_train64_8.lib (ou arquivo .lib correspondente) para C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\lib\\x64.\n",
    "\n",
    "Include:\n",
    "    Copie cudnn.h para C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\include.\n",
    "\n",
    "5. Verificar a Instalação:\n",
    "\n",
    "Após copiar os arquivos, você pode verificar se o cuDNN foi instalado corretamente executando o seguinte código Python:\n",
    "\n",
    "    import torch\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(torch.backends.cudnn.version())\n",
    "    else:\n",
    "        print(\"CUDA não está disponível.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copiar arquivos contidos no arquivo zip do cuDNN baixado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da pasta bin do cuDNN:\n",
    "\n",
    "    cudnn_adv_infer64_9.dll\n",
    "    cudnn_cnn_infer64_9.dll\n",
    "\n",
    "Copiar para:\n",
    "\n",
    "    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\bin\n",
    "\n",
    "Da pasta include do cuDNN:\n",
    "\n",
    "    cudnn.h\n",
    "\n",
    "Copiar para:\n",
    "\n",
    "    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\include\n",
    "\n",
    "Da pasta lib/x64 do cuDNN:\n",
    "\n",
    "    Copie todos os arquivos .lib da pasta x64 do cuDNN para a pasta lib/x64 da sua instalação do CUDA:\n",
    "\n",
    "Copiar para:\n",
    "\n",
    "    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\lib\\x64\n",
    "\n",
    "Substitua v12.3 pela versão do CUDA que você tem instalada, se for diferente.\n",
    "\n",
    "Se a pasta lib/x64 do CUDA já contiver arquivos com o mesmo nome, você pode substituí-los pelos arquivos do cuDNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# cuda_bin_dir = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\bin\"\n",
    "# cuda_inc_dir = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\include\"\n",
    "# cuda_lib_dir = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\lib\\x64\"\n",
    "\n",
    "# arquivos_na_pasta = os.listdir(cuda_inc_dir)\n",
    "\n",
    "# for i in arquivos_na_pasta:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar compatibilidade Hardware/Software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install unidecode\n",
    "# %pip install --upgrade jupyter\n",
    "# %pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import pynvml\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from git import Repo\n",
    "from tqdm.notebook import tqdm # Importando tqdm do notebook\n",
    "from sentence_transformers import SentenceTransformer\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCHINDUCTOR_FREEZING'] = '1'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "\n",
    "## Sempre construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(str(root_folder), 'utils')\n",
    "folder_domain = os.path.join(str(root_folder), 'source', 'domain')\n",
    "folder_data_input = os.path.join(str(root_folder), '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(str(root_folder), '_data', 'out_json')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "from competence_extraction import HardwareEvaluator, ProcessingCapacityEstimator\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "\n",
    "# !pip3 install py-cpuinfo\n",
    "hardware_evaluator = HardwareEvaluator()\n",
    "hardware_evaluator.print_hardware_info()\n",
    "\n",
    "# Estimativas de throughput\n",
    "estimator = ProcessingCapacityEstimator(hardware_evaluator)\n",
    "num_samples = 1000\n",
    "instructions_per_sample = 100\n",
    "cpu_throughput = estimator.estimate_cpu_throughput(num_samples, instructions_per_sample)\n",
    "cpu_parallel_throughput = estimator.estimate_cpu_parallel_throughput(num_samples, instructions_per_sample)\n",
    "num_operations = 1000 * 1000\n",
    "gpu_parallel_throughput = estimator.estimate_gpu_parallel_throughput(num_operations, \"FLOPS\")\n",
    "\n",
    "print(f\"\\nThroughput teórico single-threading estimado da CPU: {int(cpu_throughput)//1000000000:4} bilhões de operações/s\")\n",
    "print(f\"Throughput teórico multi-threadings estimado da CPU: {int(cpu_parallel_throughput)//1000000000:4} bilhões de operações/s\")\n",
    "print(f\"Throughput teórico multi-threadings estimado da GPU: {int(gpu_parallel_throughput)//1000000000:4} bilhões de operações/s\")\n",
    "\n",
    "# Verificação de compatibilidade PyTorch-GPU\n",
    "hardware_evaluator.check_pytorch_gpu_compatibility()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectar hardware com nvidia-smi e pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --force-reinstall pynvml\n",
    "# import pynvml\n",
    "\n",
    "## Lista todos os atributos e métodos do módulo pynvml\n",
    "# all_attributes = dir(pynvml)\n",
    "\n",
    "## Filtra apenas os métodos (funções)\n",
    "# methods = [attr for attr in all_attributes if callable(getattr(pynvml, attr))]\n",
    "\n",
    "## Imprime os métodos\n",
    "# for method in methods:\n",
    "#     print(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=name,compute_cap\", \"--format=csv,noheader\"]).decode(\"utf-8\").strip()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import pynvml\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "def detect_platform():\n",
    "    \"\"\"\n",
    "    Detects the operating system platform where the code is running, \n",
    "    distinguishing between Windows, Linux, WSL, and macOS.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the operating system platform.\n",
    "    \"\"\"\n",
    "\n",
    "    system = platform.system()\n",
    "    if system == \"Linux\":\n",
    "        # Verifica se está rodando no WSL\n",
    "        if \"microsoft\" in platform.uname().release.lower():\n",
    "            return \"WSL\"\n",
    "        else:\n",
    "            return \"Linux\"\n",
    "    elif system == \"Windows\":\n",
    "        return \"Windows\"\n",
    "    elif system == \"Darwin\":\n",
    "        return \"macOS\"\n",
    "    else:\n",
    "        return \"Unknown\"  # Plataforma desconhecida\n",
    "\n",
    "def detect_gpu_driver_version():\n",
    "    \"\"\"\n",
    "    Detects the version of the installed GPU driver.\n",
    "\n",
    "    Returns:\n",
    "        str: The GPU driver version or an error message if the detection fails.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "        driver_version = pynvml.nvmlSystemGetDriverVersion()\n",
    "        pynvml.nvmlShutdown()\n",
    "        return driver_version  # No need to decode\n",
    "\n",
    "    except pynvml.NVMLError as error:\n",
    "        return f\"Erro ao obter a versão do driver da GPU: {error}\"\n",
    "\n",
    "def detect_pytorch_and_geometric_versions():\n",
    "    \"\"\"\n",
    "    Detecta as versões do PyTorch e PyTorch Geometric, se estiverem instalados.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Uma tupla contendo as versões do PyTorch e PyTorch Geometric como strings,\n",
    "               ou None se alguma das bibliotecas não estiver instalada.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "        # import torch_geometric\n",
    "\n",
    "        pytorch_version = torch.__version__\n",
    "        # pytorch_geometric_version = torch_geometric.__version__\n",
    "\n",
    "        # return pytorch_version, pytorch_geometric_version\n",
    "\n",
    "    except ImportError:\n",
    "        return None, None\n",
    "\n",
    "def detect_compatible_torch_versions():\n",
    "    \"\"\"\n",
    "    Detecta as versões do PyTorch compatíveis com a GPU instalada na máquina.\n",
    "\n",
    "    Returns:\n",
    "        list: Uma lista de strings contendo as versões mínimas do PyTorch \n",
    "              compatíveis com a compute capability da GPU.\n",
    "              Retorna uma lista vazia se a CUDA não estiver disponível.\n",
    "    \"\"\"\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA não está disponível. Não é possível verificar a compatibilidade com a GPU.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_compute_capability = f\"{pynvml.nvmlDeviceGetCudaComputeCapability(handle)[0]}.{pynvml.nvmlDeviceGetCudaComputeCapability(handle)[1]}\"\n",
    "        pynvml.nvmlShutdown()\n",
    "\n",
    "        # Dicionário de compatibilidade PyTorch-GPU (atualizado até agosto de 2024)\n",
    "        compatibility_dict = {\n",
    "            \"3.0\": (\"0.3.0\", \"Kepler\"),\n",
    "            \"3.5\": (\"0.4.0\", \"Kepler\"),\n",
    "            \"3.7\": (\"1.0.0\", \"Kepler\"),\n",
    "            \"5.0\": (\"1.2.0\", \"Maxwell\"),\n",
    "            \"5.2\": (\"1.3.0\", \"Maxwell\"),\n",
    "            \"6.0\": (\"1.4.0\", \"Pascal\"),\n",
    "            \"6.1\": (\"1.5.0\", \"Pascal\"),\n",
    "            \"7.0\": (\"1.6.0\", \"Volta\"),\n",
    "            \"7.5\": (\"1.7.0\", \"Turing\"),\n",
    "            \"8.0\": (\"1.8.0\", \"Ampere\"),\n",
    "            \"8.6\": (\"1.9.0\", \"Ampere\"),\n",
    "            \"8.9\": (\"1.12.0\", \"Ada Lovelace\"),\n",
    "            \"9.0\": (\"1.13.0\", \"Hopper\"),\n",
    "        }\n",
    "\n",
    "        compatible_versions = []\n",
    "        for compute_cap, (min_version, platform_name) in compatibility_dict.items():\n",
    "            if gpu_compute_capability >= compute_cap:\n",
    "                compatible_versions.append(min_version)\n",
    "\n",
    "        # Imprimir a plataforma detectada\n",
    "        if gpu_compute_capability in compatibility_dict:\n",
    "            detected_compatibility = compatibility_dict[gpu_compute_capability][0]\n",
    "            detected_platform = compatibility_dict[gpu_compute_capability][1]\n",
    "            print(f\"Computabilidade da GPU detectada: {detected_compatibility} | Plataforma NVIDIA {detected_platform}\")\n",
    "        else:\n",
    "            print(f\"Capacidade computacional da GPU detectada: {gpu_compute_capability}\") \n",
    "\n",
    "        return compatible_versions\n",
    "\n",
    "    except pynvml.NVMLError as error:\n",
    "        print(f\"Erro ao obter informações da GPU: {error}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_gpu_architecture():\n",
    "    \"\"\"\n",
    "    Obtém a arquitetura da GPU com base na compute capability.\n",
    "\n",
    "    Returns:\n",
    "        str: O nome da arquitetura da GPU ou \"Desconhecida\" se não for possível determinar.\n",
    "    \"\"\"\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        return \"Desconhecida\"  # CUDA não disponível\n",
    "\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        cc_major, cc_minor = pynvml.nvmlDeviceGetCudaComputeCapability(handle)\n",
    "        compute_capability = f\"{cc_major}.{cc_minor}\"\n",
    "        pynvml.nvmlShutdown()\n",
    "\n",
    "        # Dicionário para mapear compute capability para arquitetura (atualizado)\n",
    "        compute_cap_to_architecture = {\n",
    "            \"3.0\": \"Kepler\",\n",
    "            \"3.5\": \"Kepler\",\n",
    "            \"3.7\": \"Kepler\",\n",
    "            \"5.0\": \"Maxwell\",\n",
    "            \"5.2\": \"Maxwell\",\n",
    "            \"6.0\": \"Pascal\",\n",
    "            \"6.1\": \"Pascal\",\n",
    "            \"7.0\": \"Volta\",\n",
    "            \"7.5\": \"Turing\",       # como a RTX 2080 Ti\n",
    "            \"8.0\": \"Ampere\",       # como a RTX 3080\n",
    "            \"8.6\": \"Ampere\",       # como a A100\n",
    "            \"8.9\": \"Ada Lovelace\", # como a RTX 4060, 4070, 4080 e 4090\n",
    "            \"9.0\": \"Hopper\",       # como a H100\n",
    "            \"11.0\": \"Blackwell\",   # Substituir 11.0 pela compute capability real da Blackwell quando for divugada\n",
    "        }\n",
    "\n",
    "        return compute_cap_to_architecture.get(compute_capability, \"Desconhecida\")\n",
    "\n",
    "    except pynvml.NVMLError as error:\n",
    "        print(f\"Erro ao obter informações da GPU: {error}\")\n",
    "        return \"Desconhecida\"\n",
    "\n",
    "\n",
    "def get_num_sms_via_nvidia_smi():\n",
    "    \"\"\"\n",
    "    Gets the number of Streaming Multiprocessors (SMs) on the GPU using nvidia-smi.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of SMs or 0 if the information is not available or an error occurs\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=compute_cap\", \"--format=csv,noheader\"]).decode(\"utf-8\")\n",
    "        compute_cap = output.strip().split(\".\")[0]  # Extract the major compute capability version\n",
    "        return int(compute_cap) * 38  # Assuming 38 CUDA cores per SM\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter o número de SMs: {e}\")\n",
    "        return 0\n",
    "\n",
    "def get_tensor_cores_via_nvidia_smi():\n",
    "    \"\"\"\n",
    "    Gets the number of Tensor Cores by parsing nvidia-smi output and using \n",
    "    the `cuda_cores_per_sm` dictionary.\n",
    "\n",
    "    Returns:\n",
    "        int: The estimated number of Tensor Cores or 0 if the information is not available or an error occurs\n",
    "    \"\"\"\n",
    "    # Informações sobre CUDA Cores por SM para diferentes arquiteturas de GPU NVIDIA\n",
    "    cuda_cores_per_sm = {\n",
    "        \"Kepler\": 192,\n",
    "        \"Maxwell\": 128,\n",
    "        \"Pascal\": 128,\n",
    "        \"Volta\": 64,\n",
    "        \"Turing\": 64,\n",
    "        \"Ampere\": 64,\n",
    "        \"Ada Lovelace\": 128,\n",
    "        \"Hopper\": 128\n",
    "    }\n",
    "\n",
    "    # TO-DO: Pesquisar a quantidade de TensorCores de cada aruitetura\n",
    "    # Informações sobre Tensor Cores por SM para diferentes arquiteturas de GPU NVIDIA\n",
    "    tensor_cores_per_sm = {\n",
    "        \"Kepler\": 0,\n",
    "        \"Maxwell\": 0,\n",
    "        \"Pascal\": 4,\n",
    "        \"Volta\": 4,\n",
    "        \"Turing\": 4,\n",
    "        \"Ampere\": 8,\n",
    "        \"Ada Lovelace\": 12,\n",
    "        \"Hopper\": 12\n",
    "    }\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        return 0  # CUDA não disponível\n",
    "\n",
    "    try:\n",
    "        # Get GPU name and number of CUDA cores from nvidia-smi\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=name,compute_cap\", \"--format=csv,noheader\"]).decode(\"utf-8\")\n",
    "        gpu_name, compute_cap = output.strip().split(\",\")\n",
    "        cuda_cores = int(compute_cap.split('.')[0].strip())*32\n",
    "\n",
    "        # Identify the GPU architecture\n",
    "        architecture = None\n",
    "        for arch in cuda_cores_per_sm:\n",
    "            if arch.lower() in gpu_name.lower():\n",
    "                architecture = arch\n",
    "                break\n",
    "\n",
    "        if architecture:\n",
    "            # Estimate the number of SMs based on CUDA cores per SM\n",
    "            num_sms = cuda_cores // cuda_cores_per_sm[architecture]\n",
    "\n",
    "            # Estimate Tensor Cores (may not be accurate for all architectures)\n",
    "            if architecture in tensor_cores_per_sm:\n",
    "                tensor_cores = num_sms * tensor_cores_per_sm[architecture]\n",
    "            else:\n",
    "                tensor_cores = 0\n",
    "\n",
    "            return tensor_cores\n",
    "        else:\n",
    "            print(f\"Arquitetura da GPU '{gpu_name}' não reconhecida. Não foi possível determinar o número de Tensor Cores.\")\n",
    "            return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter informações da GPU usando nvidia-smi: {e}\")\n",
    "        return 0\n",
    "\n",
    "def get_tensor_cores():\n",
    "    \"\"\"\n",
    "    Obtém o número de Tensor Cores da GPU.\n",
    "\n",
    "    Returns:\n",
    "        int: Número de Tensor Cores ou 0 se a informação não estiver disponível ou ocorrer um erro.\n",
    "    \"\"\"\n",
    "    # Informações sobre CUDA Cores por SM para diferentes arquiteturas de GPU NVIDIA\n",
    "    cuda_cores_per_sm = {\n",
    "        \"Kepler\": 192,\n",
    "        \"Maxwell\": 128,\n",
    "        \"Pascal\": 128,\n",
    "        \"Volta\": 64,\n",
    "        \"Turing\": 64,\n",
    "        \"Ampere\": 64,\n",
    "        \"Ada Lovelace\": 128,\n",
    "        \"Hopper\": 128,\n",
    "        \"Blackwell\": 256 # Adicionado Blackwell, a arquitetura mais recente até agosto de 2024\n",
    "    }\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        return 0  # CUDA não disponível\n",
    "\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        # Obter o nome da GPU e a arquitetura\n",
    "        gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "\n",
    "        # Obter a arquitetura da GPU a partir da saída do nvidia-smi\n",
    "        architecture = get_gpu_architecture()\n",
    "        print(f\"Arquitetura: {architecture}\")\n",
    "\n",
    "        # Informações sobre Tensor Cores para diferentes arquiteturas de GPU NVIDIA\n",
    "        tensor_cores_per_sm = {\n",
    "            \"Volta\": 8,         # Tensor Cores Volta (V100, Titan V)\n",
    "            \"Turing\": 4,        # Tensor Cores Turing (RTX 20 series, Quadro RTX series)\n",
    "            \"Ampere\": 4,        # Tensor Cores Ampere (RTX 30 series, A100, etc.)\n",
    "            \"Ada Lovelace\": 4,  # Tensor Cores Ada Lovelace (RTX 40 series)\n",
    "            \"Hopper\": 2,        # Tensor Cores Hopper (H100)\n",
    "            \"Blackwell\": 8      # Estimativa de Tensor Cores baseada em rumores sobre Blackwell (B100/B200)\n",
    "        }\n",
    "\n",
    "        conf_tensor_cores = {\n",
    "            \"RTX 4070 Super\": 224, # 4th Gen tensor cores\n",
    "            \"RTX 4500\": 240, # 4th Gen tensor cores\n",
    "            \"RTX 4070 Ti Super\": 264, # 4th Gen tensor cores\n",
    "            \"RTX 4080 Super\": 320, # 4th Gen tensor cores\n",
    "            \"RTX 4090\": 512, # 4th Gen tensor cores\n",
    "            \"RTX 4090 D\": 456, # 4th Gen tensor cores\n",
    "            \"RTX 5000\": 400, # 4th Gen tensor cores\n",
    "            \"RTX 6000\": 568, # 4th Gen tensor cores\n",
    "            \"A100\": 512, # tensor cores\n",
    "            \"H100\": 512, # tensor cores\n",
    "             \n",
    "        }\n",
    "        try:\n",
    "            pynvml.nvmlInit()\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "            cc_major, cc_minor = pynvml.nvmlDeviceGetCudaComputeCapability(handle)\n",
    "            try:\n",
    "                num_sms = pynvml.nvmlDeviceGetVgpuMetadata(handle)\n",
    "            except:\n",
    "                num_sms = get_num_sms_via_nvidia_smi()\n",
    "            print(f\"Total de núcleos: {num_sms}\")\n",
    "        except Exception as e:\n",
    "            print('Erro ao tentar obter a quantidade de SMs')\n",
    "            print(e)\n",
    "            return 0\n",
    "\n",
    "        if architecture and num_sms:\n",
    "            total_cuda_cores = int(num_sms) * cuda_cores_per_sm[architecture] \n",
    "            \n",
    "            # Estimar o número de Tensor Cores (pode não ser preciso para todas as arquiteturas)\n",
    "            archs = [x.lower() for x in tensor_cores_per_sm.keys()]\n",
    "            if architecture.lower() in archs:\n",
    "                tensor_cores = int(num_sms) * tensor_cores_per_sm[architecture]\n",
    "            else:\n",
    "                # Para arquiteturas sem Tensor Cores dedicados, assumimos que não há\n",
    "                tensor_cores = 0  \n",
    "\n",
    "            return tensor_cores\n",
    "        else:\n",
    "            print(f\"Arquitetura da GPU '{architecture}' não reconhecida. Não foi possível determinar o número de Tensor Cores.\")\n",
    "            return 0\n",
    "\n",
    "    except pynvml.NVMLError as error:\n",
    "        print(f\"Erro ao obter informações da GPU: {error}\")\n",
    "        return 0\n",
    "\n",
    "    finally:\n",
    "        pynvml.nvmlShutdown()\n",
    "\n",
    "def get_num_sms():\n",
    "    \"\"\"\n",
    "    Obtém o número estimado de Streaming Multiprocessors (SMs) na GPU.\n",
    "\n",
    "    Returns:\n",
    "        int: O número estimado de SMs ou 0 se a informação não estiver disponível ou ocorrer um erro.\n",
    "    \"\"\"\n",
    "    # Informações sobre CUDA Cores por SM para diferentes arquiteturas de GPU NVIDIA\n",
    "    cuda_cores_per_sm = {\n",
    "        \"Kepler\": 192,\n",
    "        \"Maxwell\": 128,\n",
    "        \"Pascal\": 128,\n",
    "        \"Volta\": 64,\n",
    "        \"Turing\": 64,\n",
    "        \"Ampere\": 64,\n",
    "        \"Ada Lovelace\": 128,\n",
    "        \"Hopper\": 128\n",
    "    }\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        return 0  # CUDA não disponível\n",
    "\n",
    "    total_cuda_cores=0\n",
    "    \n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "\n",
    "        # Obter o número total de CUDA cores\n",
    "        try:\n",
    "            # total_cuda_cores = pynvml.nvmlDeviceGetNumGpuCores(handle)\n",
    "            # print(f\"Quantidade Total de núcleos: {total_cuda_cores}\")\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(f\"Não foi possível determinar a quantidade Total de núcleos da GPU '{gpu_name}' com pynvml.\")\n",
    "            print(f\"Usando estimativa com nvidia_smi.\")\n",
    "            total_cuda_cores = get_num_sms_via_nvidia_smi()\n",
    "            return total_cuda_cores\n",
    "\n",
    "        # Identificar a arquitetura da GPU\n",
    "        architecture = get_gpu_architecture()\n",
    "\n",
    "        if architecture:\n",
    "            # Consultar o dicionário para obter o número de CUDA cores por SM\n",
    "            cuda_cores_per_sm = cuda_cores_per_sm.get(architecture)\n",
    "\n",
    "            if cuda_cores_per_sm:\n",
    "                num_sms = total_cuda_cores // cuda_cores_per_sm\n",
    "                return num_sms\n",
    "            else:\n",
    "                print(f\"Arquitetura da GPU '{gpu_name}' não reconhecida. Não foi possível determinar o número de SMs.\")\n",
    "                return 0\n",
    "        else:\n",
    "            print(f\"Não foi possível determinar a arquitetura da GPU '{gpu_name}'. Não foi possível determinar o número de SMs.\")\n",
    "            return 0\n",
    "\n",
    "    except pynvml.NVMLError as error:\n",
    "        print(f\"Erro ao obter informações da GPU: {error}\")\n",
    "        return 0\n",
    "\n",
    "    finally:\n",
    "        pynvml.nvmlShutdown()\n",
    "\n",
    "\n",
    "def print_gpu_driver_info():\n",
    "    \"\"\"\n",
    "    Imprime informações detalhadas sobre o driver da GPU e a(s) GPU(s) instalada(s).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"-q\"]).decode(\"utf-8\")\n",
    "        lines = output.splitlines()\n",
    "        sections = ['==============NVSMI LOG==============','PCIe Generation','Link Width','Bridge Chip','FB Memory Usage',\n",
    "                    'BAR1 Memory Usage','Utilization','Temperature','GPU Power Readings','Clocks','Max Clocks','Voltage']\n",
    "        interests = ['Driver Version','CUDA Version','Attached GPUs','Product Name','Product Architecture',\n",
    "                    'MultiGPU Board', 'Total', 'Free', 'Gpu', 'Memory','Graphics','SM','Memory','Video','Max','Current',\n",
    "                    'Tx Throughput','Rx Throughput','Power Draw','Max Power Limit','GPU Current Temp','GPU Max Operating Temp']\n",
    "\n",
    "        info_dict = {}\n",
    "        current_section = None\n",
    "        key=None\n",
    "        val=None\n",
    "\n",
    "        for i in lines:\n",
    "            if i == \"\" or i is None:\n",
    "                continue\n",
    "            if \":\" not in i and i is not None:\n",
    "                section = i.strip()\n",
    "                if section in sections:\n",
    "                    if section == \"==============NVSMI LOG==============\":\n",
    "                        section = 'General_Data'\n",
    "                    current_section = section\n",
    "                    info_dict[current_section] = {}  # Criar a estrutura aninhada para seção\n",
    "                    # print('-'*75)\n",
    "                    # print(f\"Seção: {section}\") # DEBUG\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                try:\n",
    "                    key = i.split(':',1)[0].strip()\n",
    "                    val = i.split(':',1)[1].strip()\n",
    "                    if val != \"N/A\" and val != \"Not Active\" and key in interests:\n",
    "                    # if val != \"N/A\" and val != \"Not Active\":\n",
    "                        # print(f\"Seção: {current_section:30} | Chave: {key:30} | Valor: {val}\") # DEBUG\n",
    "                        info_dict[current_section][key] = val\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro na linha {i} da seção {current_section} | Erro: {e}\")\n",
    "                    print(i)\n",
    "                    # print(f\"Seção: {current_section:30} | Chave: {key:30} | Valor: {val}\") # DEBUG\n",
    "                    continue\n",
    "\n",
    "        # Print the desired information, handling missing sections gracefully\n",
    "        print(\"\\nInformações do Driver e da(s) GPU(s):\")\n",
    "\n",
    "        list_keys=[]\n",
    "        [list_keys.extend(list(x.keys())) for x in info_dict.values()]\n",
    "\n",
    "        if 'Driver Version' in list_keys:\n",
    "            print(f\"  Driver Version: {info_dict['General_Data'].get('Driver Version')}\")\n",
    "        else:\n",
    "            print(\"  DriverVersion: Não encontrada\")\n",
    "\n",
    "        if 'CUDA Version' in list_keys:\n",
    "            print(f\"    CUDA Version: {info_dict['General_Data']['CUDA Version']}\")\n",
    "        else:\n",
    "            print(\"   CUDA Version: Não encontrada\")\n",
    "\n",
    "        if 'Attached GPUs' in list_keys:\n",
    "            gpu_count = int(info_dict['General_Data'].get('Attached GPUs'))\n",
    "        else:\n",
    "            gpu_count = 0\n",
    "            print(\"  Attached GPUs: Não encontrado\")\n",
    "\n",
    "        print(f\"   Attached GPUs: {gpu_count}\")\n",
    "\n",
    "        for i in range(gpu_count):\n",
    "            gpu_section = f\"GPU {i}\"\n",
    "            if gpu_section in info_dict:\n",
    "                print(f\"    GPU {i} - Product Name: {info_dict[gpu_section]['Product Name']}\")\n",
    "                print(f\"    GPU {i} - Product Architecture: {info_dict[gpu_section]['Architecture']}\")\n",
    "                print(f\"    GPU {i} - Accounting Mode Buffer Size: {info_dict[gpu_section]['Accounting Mode Buffer Size']}\")\n",
    "                print(f\"    GPU {i} - MultiGPU Board: {info_dict[gpu_section]['MultiGPU Board']}\")\n",
    "            else:\n",
    "                print(f\"    GPU {i} - Informações não encontradas\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter informações da GPU: {e}\")\n",
    "    \n",
    "    return info_dict\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "NVIDIA RTX A1000\n",
    "Graphics Processor\n",
    "GA107\n",
    " \n",
    "Cores\n",
    "2304\n",
    " \n",
    "TMUs\n",
    "72\n",
    " \n",
    "ROPs\n",
    "32\n",
    " \n",
    "Memory Size\n",
    "8 GB\n",
    " \n",
    "Memory Type\n",
    "GDDR6\n",
    " \n",
    "Bus Width\n",
    "128 bit\n",
    "\n",
    "NVIDIA GeForce RTX 4060 AD106\n",
    "Graphics Processor\n",
    "AD106\n",
    " \n",
    "Cores\n",
    "3072\n",
    " \n",
    "TMUs\n",
    "96\n",
    " \n",
    "ROPs\n",
    "48\n",
    " \n",
    "Memory Size\n",
    "8 GB\n",
    " \n",
    "Memory Type\n",
    "GDDR6\n",
    " \n",
    "Bus Width\n",
    "128 bit\n",
    "\n",
    "NVIDIA GeForce RTX 4070 SUPER\n",
    "Graphics Processor\n",
    "AD104\n",
    " \n",
    "Cores\n",
    "7168\n",
    " \n",
    "TMUs\n",
    "224\n",
    " \n",
    "ROPs\n",
    "80\n",
    " \n",
    "Memory Size\n",
    "12 GB\n",
    " \n",
    "Memory Type\n",
    "GDDR6X\n",
    " \n",
    "Bus Width\n",
    "192 bit\n",
    "\n",
    "NVIDIA RTX 2000 Ada Generation\n",
    "Graphics Processor\n",
    "AD107\n",
    " \n",
    "Cores\n",
    "2816\n",
    " \n",
    "TMUs\n",
    "88\n",
    " \n",
    "ROPs\n",
    "48\n",
    " \n",
    "Memory Size\n",
    "16 GB\n",
    " \n",
    "Memory Type\n",
    "GDDR6\n",
    " \n",
    "Bus Width\n",
    "128 bit\n",
    "\n",
    "NVIDIA GeForce RTX 5080\n",
    "Graphics Processor\n",
    "GB203\n",
    " \n",
    "Cores\n",
    "10752\n",
    " \n",
    "TMUs\n",
    "336\n",
    " \n",
    "ROPs\n",
    "128\n",
    " \n",
    "Memory Size\n",
    "16 GB\n",
    " \n",
    "Memory Type\n",
    "GDDR7\n",
    " \n",
    "Bus Width\n",
    "256 bit\n",
    "\n",
    "NVIDIA GeForce RTX 5090\n",
    "Graphics Processor\n",
    "GB202\n",
    " \n",
    "Cores\n",
    "20480\n",
    " \n",
    "TMUs\n",
    "640\n",
    " \n",
    "ROPs\n",
    "192\n",
    " \n",
    "Memory Size\n",
    "28 GB\n",
    " \n",
    "Memory Type\n",
    "GDDR7\n",
    " \n",
    "Bus Width\n",
    "448 bit\n",
    "\n",
    "NVIDIA H100 PCIe 80 GB\n",
    "Graphics Processor\n",
    "GH100\n",
    " \n",
    "Cores\n",
    "14592\n",
    " \n",
    "TMUs\n",
    "456\n",
    " \n",
    "ROPs\n",
    "24\n",
    " \n",
    "Memory Size\n",
    "80 GB\n",
    " \n",
    "Memory Type\n",
    "HBM2e\n",
    " \n",
    "Bus Width\n",
    "5120 bit\n",
    "\n",
    "NVIDIA H100 PCIe 96 GB\n",
    "Graphics Processor\n",
    "GH100\n",
    " \n",
    "Cores\n",
    "16896\n",
    " \n",
    "TMUs\n",
    "528\n",
    " \n",
    "ROPs\n",
    "24\n",
    " \n",
    "Memory Size\n",
    "96 GB\n",
    " \n",
    "Memory Type\n",
    "HBM3\n",
    " \n",
    "Bus Width\n",
    "5120 bit\n",
    "\n",
    "NVIDIA RTX 4000 Ada Generation\n",
    "Graphics Processor\n",
    "AD104\n",
    " \n",
    "Cores\n",
    "6144\n",
    " \n",
    "TMUs\n",
    "192\n",
    " \n",
    "ROPs\n",
    "80\n",
    " \n",
    "Memory Size\n",
    "20 GB\n",
    " \n",
    "Memory Type\n",
    "GDDR6\n",
    " \n",
    "Bus Width\n",
    "160 bit\n",
    "\n",
    "NVIDIA RTX 4500 Ada Generation\n",
    "Graphics Processor\n",
    "AD103\n",
    " \n",
    "Cores\n",
    "7680\n",
    " \n",
    "TMUs\n",
    "240\n",
    " \n",
    "ROPs\n",
    "80\n",
    " \n",
    "Memory Size\n",
    "24 GB\n",
    " \n",
    "Memory Type\n",
    "GDDR6\n",
    " \n",
    "Bus Width\n",
    "192 bit\n",
    "\n",
    "NVIDIA RTX 5000 Ada Generation\n",
    "Graphics Processor\n",
    "AD102\n",
    " \n",
    "Cores\n",
    "12800\n",
    " \n",
    "TMUs\n",
    "400\n",
    " \n",
    "ROPs\n",
    "176\n",
    " \n",
    "Memory Size\n",
    "32 GB\n",
    " \n",
    "Memory Type\n",
    "GDDR6\n",
    " \n",
    "Bus Width\n",
    "256 bit\n",
    "\n",
    "NVIDIA RTX 6000 Ada Generation\n",
    "Graphics Processor\n",
    "AD102\n",
    " \n",
    "Cores\n",
    "18176\n",
    " \n",
    "TMUs\n",
    "568\n",
    " \n",
    "ROPs\n",
    "192\n",
    " \n",
    "Memory Size\n",
    "48 GB\n",
    " \n",
    "Memory Type\n",
    "GDDR6\n",
    " \n",
    "Bus Width\n",
    "384 bit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpu_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_sms_via_nvidia_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tensor_cores_via_nvidia_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tensor_cores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_sms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_sms_via_nvidia_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tensor_cores = get_tensor_cores()\n",
    "if num_tensor_cores > 0:\n",
    "    print(f\"Número de Tensor Cores: {num_tensor_cores}\")\n",
    "else:\n",
    "    print(\"Não foi possível determinar o número de Tensor Cores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_name = detect_platform()\n",
    "print(f\"Plataforma de Sistema Operacional em uso: {platform_name}\")\n",
    "\n",
    "compatible_torch_versions = detect_compatible_torch_versions()\n",
    "if compatible_torch_versions:\n",
    "    print(f\"Versão mínima mais recente do PyTorch compatível: {compatible_torch_versions[-1]}\")\n",
    "else:\n",
    "    print(\"Não foi possível detectar versões compatíveis do PyTorch.\")\n",
    "\n",
    "driver_version = detect_gpu_driver_version()\n",
    "print(f\"Versão do driver da GPU instalado: {driver_version}\")\n",
    "\n",
    "# Imprimir o caminho completo para o interpretador Python em uso\n",
    "print(f\"\\nInterpretador Python: {sys.executable}\")\n",
    "\n",
    "# Imprimir o nome do ambiente conda ativo\n",
    "print(f\"Ambiente conda ativo: {os.environ.get('CONDA_DEFAULT_ENV')}\")\n",
    "\n",
    "pytorch_version, pytorch_geometric_version = detect_pytorch_and_geometric_versions() # type: ignore\n",
    "\n",
    "if pytorch_version and pytorch_geometric_version:\n",
    "    print(f\"Versão do PyTorch: {pytorch_version}\")\n",
    "    print(f\"Versão do PyTorch Geometric: {pytorch_geometric_version}\")\n",
    "else:\n",
    "    if not pytorch_version:\n",
    "        print(\"          PyTorch não está instalado no ambiente ativo.\")\n",
    "    if not pytorch_geometric_version:\n",
    "        print(\"PyTorch Geometric não está instalado no ambiente ativo.\")\n",
    "\n",
    "info_dict = print_gpu_driver_info()\n",
    "info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keys=[]\n",
    "[list_keys.extend(list(x.keys())) for x in info_dict.values()]\n",
    "list_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict['General_Data'].get('Attached GPUs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "output = subprocess.check_output([\"nvidia-smi\", \"-q\"]).decode(\"utf-8\")\n",
    "lines = output.splitlines()\n",
    "sections = ['==============NVSMI LOG==============','PCIe Generation','Link Width','Bridge Chip','FB Memory Usage',\n",
    "            'BAR1 Memory Usage','Utilization','Temperature','GPU Power Readings','Clocks','Max Clocks','Voltage']\n",
    "interests = ['Driver Version','CUDA Version','Attached GPUs','Product Name','Product Brand','Product Architecture',\n",
    "             'MultiGPU Board', 'Total', 'Free', 'Gpu', 'Memory','Graphics','SM','Memory','Video','Max','Current',\n",
    "             'Tx Throughput','Rx Throughput','Power Draw','Max Power Limit','GPU Current Temp','GPU Max Operating Temp']\n",
    "for i in lines:\n",
    "    if i == \"\":\n",
    "        continue\n",
    "    if \":\" not in i:\n",
    "        section = i.strip()\n",
    "        if section in sections:\n",
    "            print('-'*75)\n",
    "            print(f\"Seção: {section}\")\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        try:\n",
    "            key = i.split(':',1)[0].strip()\n",
    "            val = i.split(':',1)[1].strip()\n",
    "            if val != \"N/A\" and val != \"Not Active\" and key in interests:\n",
    "            # if val != \"N/A\" and val != \"Not Active\":\n",
    "                print(f\"{key:30} | {val}\")\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Implementar Kernel CUDA</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A implementação do kernel CUDA para a vetorização de competências depende da escolha de arquitetura para o modelo. Para utilizar os embeedinds pré-treinados do modelo paraphrase-multilingual-mpnet-base-v2, foi preciso implementar um kernel otimizado que executa as seguintes etapas:\n",
    "\n",
    "1. Tokenizar: Dividir cada frase de competência em tokens (palavras ou subpalavras).\n",
    "\n",
    "2. Vetorizar: Converter cada token em um vetor de embedding usando o modelo pré-treinado.\n",
    "\n",
    "3. Codificar Sequência: Processar a sequência de embeddings usando as camadas do modelo (transformadores, camadas de pooling, etc.).\n",
    "\n",
    "4. Agregar (opcional): Se necessário, agregar os embeddings da sequência em um único vetor representativo da competência (por exemplo, usando a média ou o máximo dos embeddings).\n",
    "\n",
    "5. Armazenar embeedings: Armazenar os embeddings resultantes na memória da GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estratégias de Otimização de espaço de memória da GPU:\n",
    "\n",
    "Reduzir Dimensionalidade dos Embeddings: Para modelos com dimensões de embedding muito grandes (ex: 768), é possível otimizar cálculos usando modelos menores (ex: 128 ou 256). Isso reduz significativamente a quantidade de memória utilizada pela GPU. Modelos de menor dimensionalidade estão disponíveis no Hugging Face Model Hub, como, por exemplo, o \"paraphrase-MiniLM-L6-v2\" ou \"all-MiniLM-L6-v2\".\n",
    "\n",
    "Limpeza de Cache e Sincronização: Limpar o cache da GPU (torch.cuda.empty_cache()) e sincronizar operações CUDA (torch.cuda.synchronize()) é uma boa prática antes de cada iteração do loop no trecho \"for model_name in model_names:\"\n",
    "\n",
    "Processar em CPU: Se mesmo a redução do tamanho do modelo e do lote não for suficiente, pode-se processar os embeddings na CPU, o que torna execução mais lenta, mas evita a maioria dos erros de acesso à memória da GPU.\n",
    "\n",
    "Divisão dos Dados em Subconjuntos: Se você tiver um grande número de pesquisadores, divida os dados em subconjuntos menores e processe cada subconjunto separadamente.\n",
    "\n",
    "Uso de torch.utils.checkpoint: O PyTorch oferece a função torch.utils.checkpoint que permite trocar memória por tempo de computação. Ao ativar o checkpointing, o PyTorch recalculará partes do grafo computacional durante a retropropagação, em vez de armazenar todos os tensores intermediários na memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download pt_core_news_lg\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GPU e CPU: A GPU (Unidade de Processamento Gráfico) é especializada em cálculos paralelizáveisos ideal para treinar modelos de aprendizado de máquina. \n",
    "Já a CPU (Unidade Central de Processamento) é mais versátil e lida com diversas tarefas, incluindo a execução do seu código.\n",
    "\n",
    "Transferência GPU/RAM: Para usar GPU durante o treinamento, é necessário mover os dados (tensores) para a memória da GPU.\n",
    "No entanto, outras bibliotecas, como o NumPy e o scikit-learn, geralmente esperam que os dados estejam na memória principal (RAM) acessível à CPU.\n",
    "Método .cpu(): O método .cpu() copia o tensor da memória da GPU para a memória principal, permitindo que você o use com o NumPy e outras bibliotecas que não suportam diretamente tensores na GPU.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pynvml\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from tqdm import tqdm_notebook\n",
    "from git import Repo\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from competence_extraction import HardwareEvaluator, ProcessingCapacityEstimator\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def recommend_batch_size(model, model_name, X, evaluator, estimator, batch_sizes=[8, 16, 32, 64, 128]):\n",
    "    results = {}\n",
    "    gpu_memory_usage = []\n",
    "    gpu_utilization = []\n",
    "\n",
    "    # Inicializa o pynvml para monitorar a GPU\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    \n",
    "    for batch_size in tqdm(batch_sizes, desc=f\"Modelo: {model_name}\"):\n",
    "        print(f\"\\nBenchmarking e interpretação para batch size = {batch_size}\")\n",
    "\n",
    "        # Benchmark na CPU\n",
    "        cpu_time = evaluator.benchmark_model(model, X, 'cpu', batch_size=batch_size)\n",
    "\n",
    "        # Benchmark na GPU (se disponível) e transferência de dados\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            try:\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "                    gpu_time = evaluator.benchmark_model(model, X, device, batch_size=batch_size)\n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA out of memory\" in str(e):\n",
    "                    print(f\"Erro de memória da GPU para o modelo {model_name} com batch size {batch_size}.\")\n",
    "                    gpu_time = float('inf')\n",
    "                    transfer_results = {batch_size: {'cpu_to_gpu': 0, 'gpu_to_cpu': 0}}\n",
    "                else:\n",
    "                    # Plotar o trace até o momento do erro\n",
    "                    print(f\"Erro ao processar o modelo {model} com batch size {batch_size}: {e}\")\n",
    "                    print(\"Trace até o momento do erro:\")\n",
    "                    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10)) # type: ignore\n",
    "                    raise e  # Repassar o erro após exibir o trace\n",
    "\n",
    "            # Extrair dados de tracing do profiler\n",
    "            trace_events = prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10) # type: ignore\n",
    "            print(trace_events)\n",
    "            # Gerar gráfico de tracing (exemplo)\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(trace_events.columns[0], trace_events.columns[1])\n",
    "            plt.xlabel(\"Evento\")\n",
    "            plt.ylabel(\"Tempo (ms)\")\n",
    "            plt.title(f\"Tracing da GPU para {model_name} (batch size={batch_size})\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            gpu_time = None\n",
    "            transfer_results = {batch_size: {'cpu_to_gpu': 0, 'gpu_to_cpu': 0}}\n",
    "\n",
    "        # Monitorar GPU\n",
    "        gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)  # Em MB\n",
    "        gpu_utilization.append(pynvml.nvmlDeviceGetUtilizationRates(handle).gpu)\n",
    "\n",
    "        # Cálcular tempo total (incluindo transferência)\n",
    "        total_cpu_time = cpu_time\n",
    "        total_gpu_time = gpu_time + transfer_results[batch_size]['cpu_to_gpu'] + transfer_results[batch_size]['gpu_to_cpu'] if gpu_time is not None else float('inf')\n",
    "\n",
    "        best_device = \"gpu\" if total_gpu_time < total_cpu_time else \"cpu\"\n",
    "        best_time = min(total_cpu_time, total_gpu_time)\n",
    "\n",
    "        results[batch_size] = {\n",
    "            'cpu_time': cpu_time,\n",
    "            'gpu_time': gpu_time,\n",
    "            'transfer_time': transfer_results[batch_size]['cpu_to_gpu'] + transfer_results[batch_size]['gpu_to_cpu'],\n",
    "            'total_cpu_time': total_cpu_time,\n",
    "            'total_gpu_time': total_gpu_time,\n",
    "            'best_device': best_device,\n",
    "            'best_time': best_time,\n",
    "        }\n",
    "\n",
    "    # Finalizar o pynvml\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "    # Criar DataFrame com os resultados\n",
    "    df = pd.DataFrame.from_dict(results, orient='index')\n",
    "    df['batch_size'] = df.index\n",
    "\n",
    "    # Visualizar com Altair\n",
    "    chart = alt.Chart(df.melt('batch_size', var_name='metric', value_name='time')).mark_line(point=True).encode(\n",
    "        x='batch_size:Q',\n",
    "        y='time:Q',\n",
    "        color='metric:N',\n",
    "        tooltip=['batch_size', 'metric', 'time']\n",
    "    ).properties(\n",
    "        title=f'Tempos de Processamento para o Modelo {model}'\n",
    "    ).interactive()\n",
    "\n",
    "    chart.save(f'tempos_processamento_{model.replace(\"/\", \"_\")}.json')\n",
    "\n",
    "    # Encontrar o melhor batch size\n",
    "    best_batch_size = df['best_time'].idxmin()\n",
    "\n",
    "    print(f\"\\nRecomendação para o modelo {model}:\")\n",
    "    print(f\"  Melhor batch size: {best_batch_size}\")\n",
    "    print(f\"  Dispositivo recomendado: {df.loc[best_batch_size, 'best_device']}\")\n",
    "    print(f\"  Tempo de processamento estimado: {df.loc[best_batch_size, 'best_time']:.4f} segundos por amostra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from git import Repo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from competence_extraction import CompetenceExtraction\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "pathfilename = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not os.path.exists(pathfilename):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {pathfilename}\")\n",
    "\n",
    "try:\n",
    "    with open(pathfilename, \"r\") as f:\n",
    "        curricula_data = json.load(f)\n",
    "except json.JSONDecodeError:\n",
    "    raise ValueError(f\"Erro ao decodificar o arquivo JSON: {pathfilename}\")\n",
    "\n",
    "competence_extractor = CompetenceExtraction(curricula_file=pathfilename)\n",
    "\n",
    "# Lista de modelos a serem avaliados\n",
    "model_names = [\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"sentence-transformers/LaBSE\",\n",
    "    \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "]\n",
    "\n",
    "curricula_file = os.path.join(str(root_folder), filename)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA não está disponível. Verifique a instalação e configuração da CUDA.\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "# Sincronizar as operações CUDA\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Limpar o cache da GPU\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "classifier = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "\n",
    "# Definir o estimador (modelo de classificação)\n",
    "estimator = LogisticRegression() \n",
    "\n",
    "# Benchmark e interpretação para diferentes tamanhos de lote\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name, device=device) # type: ignore\n",
    "        model, X, y = classifier.prepare_data_for_classification(model)\n",
    "\n",
    "        # Recomendar o tamanho de lote ideal\n",
    "        try:\n",
    "            recommend_batch_size(model, model_name, X, classifier, estimator, batch_sizes)\n",
    "        except:\n",
    "            print(\"Não foi possível recomendar um tamanho de bath_size automaticamente\")\n",
    "            continue\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(f\"Erro de memória da GPU para o modelo {model_name}. Tente reduzir o tamanho do lote ou usar um modelo menor.\")\n",
    "        else:\n",
    "            print(f\"Erro ao processar o modelo {model_name}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratar erros comuns durante processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O erro mais comum ao rodar modelos na GPU tende a ser \"CUDA error: an illegal memory access was encountered\" indica que a GPU ainda está encontrando problemas para processar a quantidade de dados ou a complexidade do modelo. Como queremos rodar modelos locais, com limitação de hardware, devemos focar em estratégias para otimizar o uso da GPU e contornar esse problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Dados do Software\")\n",
    "print(f\"  Versão do PyTorch: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"     Versão do CUDA: {torch.__version__}\")\n",
    "    print(f\"    Versão do cuDNN: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"    CUDA disponível: {torch.cuda.is_available()}\")\n",
    "else:\n",
    "    print(\"CUDA não disponível, não está configurado corretamente.\")\n",
    "\n",
    "print(f\"\\nDados do Compilador CUDA\")\n",
    "!nvcc --version\n",
    "\n",
    "print(f\"\\nDetalhes do PyTorch\")\n",
    "!pip3 show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDetalhes da Memória ocupada na GPU\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_HOME\"] = \"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.3\"\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Alinhar competências às necessidades do CEIS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executar tarefas do modelo em Suporte ao PDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisar dos dados de fomento - fase exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install pytorch::faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Carregar o arquivo CSV de editais em um dataframe cuDF\n",
    "df_fomento = cudf.read_csv('df_fomento_geral.csv')\n",
    "\n",
    "# Carregar os dados dos currículos dos pesquisadores (assumindo que já estejam em um dataframe cuDF)\n",
    "df_curriculos = cudf.read_json('docents_dict_list.json', lines=True)  # Ajuste conforme a estrutura do seu JSON\n",
    "\n",
    "# Inicializar o modelo de embeddings de texto\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2').to('cuda')\n",
    "\n",
    "# Gerar embeddings para os detalhes dos editais\n",
    "edital_embeddings = model.encode(df_fomento['detalhes'].tolist(), convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "# Criar um índice Faiss para busca eficiente\n",
    "index = faiss.IndexFlatL2(edital_embeddings.shape[1])\n",
    "index.add(edital_embeddings, 3) ## Verificar do que trata o segundo parâmetro ao certo\n",
    "\n",
    "def recomendar_fomento(competencias_pesquisador):\n",
    "    # Gerar embeddings para as competências do pesquisador\n",
    "    competencias_embedding = model.encode(competencias_pesquisador, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # Realizar a busca de similaridade no índice Faiss\n",
    "    # Buscar os 10 editais mais similares\n",
    "    D, I = index.search(competencias_embedding.reshape(1, -1), 10, 3)  ## Verificar do que trata o segundo parâmetro ao certo\n",
    "\n",
    "    # Retornar os editais recomendados\n",
    "    return df_fomento.iloc[I[0]]\n",
    "\n",
    "# Exemplo de uso\n",
    "for _, pesquisador in df_curriculos.iterrows(): # type: ignore\n",
    "    competencias = pesquisador['competencias']  # Assumindo que as competências estejam em uma coluna 'competencias'\n",
    "    recomendacoes_fomento = recomendar_fomento(competencias)\n",
    "    print(f\"Recomendações de fomento para o pesquisador {pesquisador['nome']}:\")\n",
    "    print(recomendacoes_fomento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar classes do pacote\n",
    "from research_process_automation import QuestionFormulation, InteractiveFeedback, QuestionCriteriaEvaluator\n",
    "\n",
    "# Criar instância da classe QuestionFormulation\n",
    "question_formulator = QuestionFormulation()\n",
    "\n",
    "# Solicitar informações do usuário\n",
    "question_formulator.input_ideas()\n",
    "\n",
    "# Gerar a pergunta de pesquisa\n",
    "research_question = question_formulator.generate_question()\n",
    "print(\"PASSO 01: Questão de pesquisa\")\n",
    "print(research_question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolução de entidades com Ontologias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "class BioPortalAPI:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = '6e1ec84f-53df-45a3-9115-25d245caefb1'\n",
    "        self.base_url = \"https://data.bioontology.org/api\"\n",
    "\n",
    "    def search_entities(self, query, ontologies=None, max_results=10):\n",
    "        \"\"\"\n",
    "        Realiza uma busca por entidades no BioPortal.\n",
    "\n",
    "        Args:\n",
    "            query: Termo de busca.\n",
    "            ontologies: Lista de acrônimos de ontologias (opcional).\n",
    "            max_results: Número máximo de resultados (opcional).\n",
    "\n",
    "        Returns:\n",
    "            Lista de dicionários com informações sobre as entidades encontradas.\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/search?q={query}&apikey={self.api_key}\"\n",
    "        if ontologies:\n",
    "            url += f\"&ontologies={','.join(ontologies)}\"\n",
    "        if max_results:\n",
    "            url += f\"&pagesize={max_results}\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"collection\"]\n",
    "        else:\n",
    "            raise Exception(f\"Erro na requisição: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Listar competências de cada currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from competence_extraction import CompetenceExtraction\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "pathfilename = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not os.path.exists(pathfilename):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {pathfilename}\")\n",
    "\n",
    "try:\n",
    "    with open(pathfilename, \"r\") as f:\n",
    "        curricula_data = json.load(f)\n",
    "except json.JSONDecodeError:\n",
    "    raise ValueError(f\"Erro ao decodificar o arquivo JSON: {pathfilename}\")\n",
    "\n",
    "competence_extractor = CompetenceExtraction(curricula_file=pathfilename)\n",
    "\n",
    "# Extrair e imprimir as competências de cada pesquisador\n",
    "for researcher_index, researcher_data in enumerate(curricula_data[:6]):\n",
    "    competences = competence_extractor.extract_competences(researcher_data)\n",
    "    processed_competences = competence_extractor.preprocess_competences(competences)\n",
    "    print(f\"\\nCompetências do pesquisador {researcher_index + 1}:\")\n",
    "    for competence in processed_competences:\n",
    "        print(f\"- {competence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Visualizar áreas de pesquisa de cada currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "import os\n",
    "import warnings\n",
    "from git import Repo\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "from tqdm.notebook import tqdm # Importando tqdm do notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def load_curricula():\n",
    "    with open(curricula_file, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extrair_areas(areas_dict):\n",
    "    lista_grdareas = []\n",
    "    lista_areas = []\n",
    "    lista_subareas = []\n",
    "    # Expressão regular corrigida para extrair as áreas\n",
    "    pattern = r'Grande área:\\s*(.*?)\\s*/\\s*Área:\\s*(.*?)\\s*(?:/ Subárea:\\s*(.*?)\\s*)?\\.'\n",
    "\n",
    "    for _, valor in areas_dict.items():\n",
    "        match = re.search(pattern, valor)\n",
    "        if match:\n",
    "            areas = {\n",
    "                'Grande Área': match.group(1).strip() if match.group(1) else None , \n",
    "                'Área': match.group(2).strip() if match.group(2) else None ,\n",
    "                'Subárea': match.group(3).strip() if match.group(3) else None  \n",
    "            }\n",
    "            lista_grdareas.append(areas.get('Grande Área'))\n",
    "            lista_areas.append(areas.get('Área'))\n",
    "            lista_subareas.append(areas.get('Subárea'))\n",
    "\n",
    "    return {'Grande Áreas': lista_grdareas, 'Áreas': lista_areas, 'Subáreas': lista_subareas}\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curricula_file = os.path.join(str(root_folder), filename)\n",
    "\n",
    "curricula_data = load_curricula()\n",
    "\n",
    "for researcher_data in curricula_data:\n",
    "    areas =  researcher_data.get('Áreas')\n",
    "    print(extrair_areas(areas).get('Áreas'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extrair competências do currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Verificar se a CUDA está disponível\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA não está disponível.\")\n",
    "\n",
    "# Carregar um modelo pré-treinado\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v2')\n",
    "\n",
    "# Dados de exemplo\n",
    "sentences = [\"Esta é uma frase de teste.\", \"Outra frase de exemplo.\"]\n",
    "\n",
    "# Codificar as frases\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "from tqdm.notebook import tqdm # Importando tqdm do notebook\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Lista de modelos a serem avaliados\n",
    "model_names = [\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"sentence-transformers/LaBSE\",\n",
    "    \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "]\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curricula_file = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# Dados de validação (exemplo)\n",
    "validation_data = {\n",
    "    'similar': [\n",
    "        (\"machine learning\", \"deep learning\"),\n",
    "        (\"biologia molecular\", \"genética\")\n",
    "    ],\n",
    "    'dissimilar': [\n",
    "        (\"machine learning\", \"medicina\"),\n",
    "        (\"biologia molecular\", \"finanças\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA não está disponível. Verifique a instalação e configuração da CUDA.\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "# Limpar o cache da GPU\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Sincronizar as operações CUDA\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Verificar se o arquivo de currículos existe\n",
    "if not os.path.exists(curricula_file):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {curricula_file}\")\n",
    "\n",
    "# Escolha do tipo de avaliação\n",
    "use_cross_validation = True  # ou False\n",
    "classifier_name = \"LogisticRegression\"  # ou \"MultinomialNB\", \"SVC\", \"RandomForestClassifier\"\n",
    "\n",
    "# Criar o avaliador e comparar os modelos\n",
    "evaluator = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "\n",
    "# Usar validação cruzada com Regressão Logística default\n",
    "# results = evaluator.evaluate_models(validation_data, use_cross_validation=True)\n",
    "\n",
    "# Usar divisão em treinamento e teste com SVM\n",
    "results = evaluator.evaluate_models(validation_data, use_cross_validation=False, classifier_name=\"SVC\")\n",
    "\n",
    "comparator = ModelComparator(results)\n",
    "best_model, best_score = comparator.get_best_model()\n",
    "\n",
    "if best_model is not None:\n",
    "    print(f\"\\nO melhor modelo é: {best_model} com pontuação de {best_score:.4f}\")\n",
    "elif not results:\n",
    "    print(\"\\nNenhum modelo foi avaliado com sucesso.\")\n",
    "else:\n",
    "    print(\"\\nNão foi possível determinar o melhor modelo.\")\n",
    "\n",
    "# Visualizar os resultados\n",
    "visualizer = PlotlyResultVisualizer(results)\n",
    "visualizer.plot_similarity_distributions()\n",
    "if not use_cross_validation: # Plota a acurácia apenas se não for validação cruzada\n",
    "    visualizer.plot_accuracy_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import silhouette_score\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "\n",
    "def evaluate_embeddings(X, y, metric=cosine_similarity, n_splits=5):\n",
    "    scores = defaultdict(list)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "        y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "        for i, area in enumerate(y_test):\n",
    "            area_idx = [j for j, a in enumerate(y_train) if a == area]\n",
    "            competence_embeddings = [X_train[j] for j in area_idx]\n",
    "            similarities = metric([X_test[i]], competence_embeddings)\n",
    "            scores['intra_class'].append(np.mean(similarities))\n",
    "            dissimilar_idx = [j for j, a in enumerate(y_train) if a != area]\n",
    "            dissimilar_embeddings = [X_train[j] for j in dissimilar_idx]\n",
    "            similarities = metric([X_test[i]], dissimilar_embeddings)\n",
    "            scores['inter_class'].append(np.mean(similarities))\n",
    "\n",
    "        scores['silhouette'].append(silhouette_score(X_train, y_train, metric=metric)) # type: ignore\n",
    "\n",
    "    results = {k: np.mean(v) for k, v in scores.items()}\n",
    "    results['std_dev_intra_class'] = np.std(scores['intra_class'])\n",
    "    results['std_dev_inter_class'] = np.std(scores['inter_class'])\n",
    "    results['std_dev_silhouette'] = np.std(scores['silhouette'])\n",
    "    return results\n",
    "\n",
    "model_names = [\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"sentence-transformers/LaBSE\",\n",
    "    \"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "    # Adicione outros modelos aqui\n",
    "]\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curricula_file = os.path.join(str(root_folder), filename)\n",
    "\n",
    "classifier = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "\n",
    "# Defina o dispositivo (GPU se disponível, caso contrário CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Especifique a GPU correta (e.g., \"cuda:0\") se tiver várias\n",
    "    print(\"Usando GPU para processamento.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA não disponível, usando CPU.\")\n",
    "\n",
    "# Loop para avaliar os modelos\n",
    "for model_name in model_names:\n",
    "    # Limpeza de cache da GPU\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Redução do tamanho do lote\n",
    "    embeddings = model.encode(processed_competences, convert_to_tensor=True, batch_size=32)  # Tente um valor menor\n",
    "\n",
    "    # Carregue o modelo com o dispositivo especificado\n",
    "    model = SentenceTransformer(model_name, device=device) # type: ignore\n",
    "    X, y = classifier.prepare_data_for_classification(model) # type: ignore\n",
    "    score = evaluate_embeddings(X, y)\n",
    "    print(f\"Modelo: {model_name}, Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Escolher melhor vetorização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação individual do modelo de embeedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import spacy\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from competence_extraction import CompetenceExtraction\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "pathfilename = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not os.path.exists(pathfilename):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {pathfilename}\")\n",
    "\n",
    "try:\n",
    "    with open(pathfilename, \"r\") as f:\n",
    "        curricula_data = json.load(f)\n",
    "except json.JSONDecodeError:\n",
    "    raise ValueError(f\"Erro ao decodificar o arquivo JSON: {pathfilename}\")\n",
    "\n",
    "competence_extractor = CompetenceExtraction(curricula_file=pathfilename)\n",
    "\n",
    "# Extrair e vetorizar as competências de todos os pesquisadores\n",
    "all_competences = []\n",
    "for researcher_index, researcher_data in enumerate(curricula_data):  # Itera sobre a lista\n",
    "    competences = competence_extractor.extract_competences(researcher_data)\n",
    "    processed_competences = competence_extractor.preprocess_competences(competences)\n",
    "    all_competences.extend(processed_competences)\n",
    "\n",
    "# Vetorizar as competências em lote\n",
    "competence_vectors = competence_extractor.vectorize_competences(all_competences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(competence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Suponha que você tenha uma lista de pares de competências semelhantes e não semelhantes\n",
    "similar_pairs    = [(\"machine learning\", \"deep learning\"), (\"biologia molecular\", \"genética\")]\n",
    "dissimilar_pairs = [(\"machine learning\", \"medicina\"), (\"biologia molecular\", \"finanças\")]\n",
    "\n",
    "# Calcule a similaridade de cosseno entre os embeddings dos pares\n",
    "for pair in similar_pairs + dissimilar_pairs:\n",
    "    embedding1 = competence_extractor.vectorize_competences([pair[0]])\n",
    "    embedding2 = competence_extractor.vectorize_competences([pair[1]])\n",
    "    similarity = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "    print(f\"Similaridade entre '{pair[0]}' e '{pair[1]}': {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação comparativa entre os modelos de embeedings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas de Avaliação: As métricas utilizadas (similaridade média e acurácia) outras métricas de acordo com necessidade.\n",
    "\n",
    "Dados de Validação: Os dados de validação fornecidos são apenas exemplos. \n",
    "\n",
    "Os dados precisam ser reais e representativos para avaliar os modelos de forma adequada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lista de modelos a serem avaliados (atualizada)\n",
    "# model_names = [\"sentence-transformers/distiluse-base-multilingual-cased-v2\", \n",
    "#                \"bert-base-multilingual-cased\", \n",
    "#                \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"]\n",
    "\n",
    "# model_names = [\n",
    "#     \"sentence-transformers/distiluse-base-multilingual-cased-v2\", \n",
    "#     \"bert-base-uncased\",  # Modelo compatível com xFormers\n",
    "#     \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "# ]\n",
    "\n",
    "# Lista de modelos a serem avaliados\n",
    "# model_names = [\"distiluse-base-multilingual-cased-v2\",\n",
    "#                 \"bert-base-multilingual-cased\", \n",
    "#                 \"paraphrase-multilingual-mpnet-base-v2\"] ## Nome antigo não mais disponível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpustat\n",
    "\n",
    "gpu_stats = gpustat.GPUStatCollection.new_query()\n",
    "print(gpu_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CUDA_LAUNCH_BLOCKING=1  # No Linux/macOS\n",
    "!set CUDA_LAUNCH_BLOCKING=1  # No Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Limpeza de memória após a avaliação de cada modelo\n",
    "# del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from git import Repo\n",
    "\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "from tqdm.notebook import tqdm # Importando tqdm do notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Lista de modelos a serem avaliados\n",
    "model_names = [\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"sentence-transformers/LaBSE\",\n",
    "    \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "]\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curricula_file = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# Dados de validação (exemplo)\n",
    "validation_data = {\n",
    "    'similar': [\n",
    "        (\"machine learning\", \"deep learning\"),\n",
    "        (\"biologia molecular\", \"genética\")\n",
    "    ],\n",
    "    'dissimilar': [\n",
    "        (\"machine learning\", \"medicina\"),\n",
    "        (\"biologia molecular\", \"finanças\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Verificar se o arquivo de currículos existe\n",
    "if not os.path.exists(curricula_file):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {curricula_file}\")\n",
    "\n",
    "# Escolha do tipo de avaliação\n",
    "use_cross_validation = True  # ou False\n",
    "classifier_name = \"LogisticRegression\"  # ou \"MultinomialNB\", \"SVC\", \"RandomForestClassifier\"\n",
    "\n",
    "# Criar o avaliador e comparar os modelos\n",
    "evaluator = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "\n",
    "# Usar validação cruzada com Regressão Logística default\n",
    "# results = evaluator.evaluate_models(validation_data, use_cross_validation=True)\n",
    "\n",
    "# Usar divisão em treinamento e teste com SVM\n",
    "# results = evaluator.evaluate_models(validation_data, use_cross_validation=False, classifier_name=\"SVC\")\n",
    "results = evaluator.evaluate_models(validation_data, use_cross_validation=False, classifier_name=\"MultinomialNB\")\n",
    "\n",
    "comparator = ModelComparator(results)\n",
    "best_model, best_score = comparator.get_best_model()\n",
    "\n",
    "if best_model is not None:\n",
    "    print(f\"\\nO melhor modelo é: {best_model} com pontuação de {best_score:.4f}\")\n",
    "elif not results:\n",
    "    print(\"\\nNenhum modelo foi avaliado com sucesso.\")\n",
    "else:\n",
    "    print(\"\\nNão foi possível determinar o melhor modelo.\")\n",
    "\n",
    "# Visualizar os resultados\n",
    "visualizer = PlotlyResultVisualizer(results)\n",
    "visualizer.plot_similarity_distributions()\n",
    "if not use_cross_validation: # Plota a acurácia apenas se não for validação cruzada\n",
    "    visualizer.plot_accuracy_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from git import Repo\n",
    "\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "# from tqdm.notebook import tqdm # Importando tqdm do notebook\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# # Lista de modelos a serem avaliados\n",
    "# model_names = [\n",
    "#     \"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "#     \"bert-base-multilingual-cased\",\n",
    "#     \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "# ]\n",
    "\n",
    "# Lista de modelos a serem avaliados (atualizada)\n",
    "model_names = [\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"sentence-transformers/LaBSE\",\n",
    "    \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "]\n",
    "\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curricula_file = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# Dados de validação (exemplo)\n",
    "validation_data = {\n",
    "    'similar': [\n",
    "        (\"machine learning\", \"deep learning\"),\n",
    "        (\"biologia molecular\", \"genética\")\n",
    "    ],\n",
    "    'dissimilar': [\n",
    "        (\"machine learning\", \"medicina\"),\n",
    "        (\"biologia molecular\", \"finanças\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Verificar se o arquivo de currículos existe\n",
    "if not os.path.exists(curricula_file):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {curricula_file}\")\n",
    "\n",
    "# Criar o avaliador e comparar os modelos\n",
    "evaluator = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "results = evaluator.evaluate_models(validation_data)\n",
    "\n",
    "comparator = ModelComparator(results)\n",
    "best_model, best_score = comparator.get_best_model()\n",
    "\n",
    "if best_model is not None:\n",
    "    print(f\"\\nO melhor modelo é: {best_model} com pontuação de {best_score:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNão foi possível determinar o melhor modelo devido à falta de dados para validação cruzada.\")\n",
    "\n",
    "# Visualizar os resultados\n",
    "visualizer = PlotlyResultVisualizer(results)\n",
    "visualizer.plot_similarity_distributions()\n",
    "visualizer.plot_accuracy_comparison()  # Comentar esta linha se não houver resultados de validação cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking em Gerar Embeedings de Competências em PDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lime\n",
    "\n",
    "# # Lista de modelos a serem avaliados\n",
    "# model_names = [\n",
    "#     \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "#     \"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "#     \"sentence-transformers/LaBSE\"\n",
    "# ]\n",
    "\n",
    "# # Caminho para o arquivo de currículos\n",
    "# repo = Repo(search_parent_directories=True)\n",
    "# root_folder = repo.working_tree_dir\n",
    "# filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "# curricula_file = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# # Dispositivo de processamento\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Instanciando as classes de avaliação\n",
    "# hardware_evaluator = HardwareEvaluator()\n",
    "# estimator = ProcessingCapacityEstimator(hardware_evaluator)\n",
    "# classifier = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "\n",
    "# # Tamanhos de lote para testar\n",
    "# batch_sizes = [8, 16, 32, 64, 128]\n",
    "\n",
    "# # Dicionário para armazenar os resultados da avaliação\n",
    "# results = {}\n",
    "\n",
    "# # Realizar benchmarks e recomendações\n",
    "# for model_name in model_names:\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "#         torch.cuda.synchronize()\n",
    "#         print('='*125)\n",
    "#         print(f'MODELO PRÉ-TREINADO: {model_name}')\n",
    "#         try:\n",
    "#             # Limpar a memória da GPU antes de carregar o modelo\n",
    "#             torch.cuda.empty_cache() \n",
    "\n",
    "#             # Monitorar o uso de memória antes e depois de carregar o modelo\n",
    "#             print(f\"Memória ocupada na GPU  antes de carregar o modelo: {np.round(torch.cuda.memory_allocated() / 1024**2,2):>8} MB\")\n",
    "#             model = SentenceTransformer(model_name, device=device) #type: ignore\n",
    "#             print(f\"Memória ocupada na GPU depois de carregar o modelo: {np.round(torch.cuda.memory_allocated() / 1024**2,2):>8} MB\")\n",
    "#             print('-'*125)\n",
    "\n",
    "#             # Preparar os dados para classificação (passe o objeto 'model')\n",
    "#             X, y = classifier.prepare_data_for_classification(model) # type: ignore\n",
    "\n",
    "#             # Converter X e y para arrays NumPy, movendo os tensores para a CPU primeiro\n",
    "#             if isinstance(X, list):\n",
    "#                 X = np.array([tensor.cpu().numpy() for tensor in X]) \n",
    "#             if isinstance(y, list):\n",
    "#                 y = np.array(y) \n",
    "\n",
    "#             # Dividir os dados em conjuntos de treinamento e teste\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    " \n",
    "#             # Escolha do classificador (adaptado para lidar com dados desbalanceados ou uma classe)\n",
    "#             unique_classes = np.unique(y_train)\n",
    "#             if len(unique_classes) == 1:\n",
    "#                 # Se houver apenas uma classe, use um algoritmo de detecção de anomalias\n",
    "#                 clf = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)  # Ou IsolationForest, LocalOutlierFactor\n",
    "#                 clf.fit(X_train)\n",
    "#                 y_pred = clf.predict(X_test)\n",
    "#                 # Converter as previsões para 0 (normal) ou 1 (anomalia)\n",
    "#                 y_pred = [0 if pred == 1 else 1 for pred in y_pred]\n",
    "#                 # Como estamos em um cenário de detecção de anomalias, \n",
    "#                 # assumimos que a classe minoritária (ou única) é a anomalia (classe 1)\n",
    "#                 y_test = [1 if label == unique_classes[0] else 0 for label in y_test] \n",
    "#             else:\n",
    "#                 # Se houver mais de uma classe, use um classificador robusto a dados desbalanceados\n",
    "#                 clf = LogisticRegression(class_weight='balanced', max_iter=1000)  # Ou outro classificador adequado\n",
    "#                 clf.fit(X_train, y_train)\n",
    "#                 y_pred = clf.predict(X_test)\n",
    "\n",
    "#             # Avaliar o modelo\n",
    "#             report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "#             # Explicabilidade com LIME\n",
    "#             explainer = lime.lime_text.LimeTextExplainer(class_names=clf.classes_)\n",
    "#             idx = 0  # Escolha um exemplo para explicar\n",
    "#             exp = explainer.explain_instance(X_test[idx], clf.predict_proba, num_features=10)\n",
    "#             print(f\"Explicação para a instância {idx}:\")\n",
    "#             print(exp.as_list())\n",
    "\n",
    "#             # Armazenar os resultados\n",
    "#             results[model_name] = {\n",
    "#                 'report': report,\n",
    "#                 'explainer': explainer  # Armazene o objeto explainer para uso posterior\n",
    "#             }\n",
    "#         except RuntimeError as e:\n",
    "#             if \"CUDA out of memory\" in str(e):\n",
    "#                 print(f\"Erro de memória da GPU para o modelo {model_name}. Tente reduzir o tamanho do lote ou usar um modelo menor.\")\n",
    "#             else:\n",
    "#                 print(f\"\\n  Erro ao processar o modelo {model_name}\")\n",
    "#                 print(f\"  {e}\")\n",
    "#     else:\n",
    "#         print('GPU não configurada corretamente, CUDA indisponível')\n",
    "\n",
    "# if results:\n",
    "#     # Comparar os modelos e escolher o melhor\n",
    "#     best_model = max(results, key=lambda k: results[k]['report']['macro avg']['f1-score'])\n",
    "#     print(f\"\\nO melhor modelo é: {best_model}\")\n",
    "#     print(results[best_model]['report'])\n",
    "# else:\n",
    "#     print('Não foi possível ler os resultados para realizar as comparações de desempenho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gml_benchmark\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import time\n",
    "\n",
    "# # Carregar os dados do currículo usando sua classe CompetenceExtraction\n",
    "# repo = Repo(search_parent_directories=True)\n",
    "# root_folder = repo.working_tree_dir\n",
    "# filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "# pathfilename = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# # Verificar se o arquivo existe\n",
    "# if not os.path.exists(pathfilename):\n",
    "#     raise FileNotFoundError(f\"Arquivo JSON não encontrado: {pathfilename}\")\n",
    "\n",
    "# try:\n",
    "#     with open(pathfilename, \"r\") as f:\n",
    "#         curricula_data = json.load(f)\n",
    "# except json.JSONDecodeError:\n",
    "#     raise ValueError(f\"Erro ao decodificar o arquivo JSON: {pathfilename}\")\n",
    "\n",
    "# competence_extractor = CompetenceExtraction(curricula_file=pathfilename)\n",
    "# curricula_data = competence_extractor.load_curricula()\n",
    "# competences = []\n",
    "# for researcher_data in curricula_data:\n",
    "#     competences.extend(competence_extractor.extract_competences(researcher_data))\n",
    "# processed_competences = competence_extractor.preprocess_competences(competences)\n",
    "\n",
    "# # Defina o número de clusters desejado para a classificação\n",
    "# num_clusters = 5  # Ou ajuste conforme necessário\n",
    "\n",
    "# # Defina a operação de benchmarking (geração de embeddings e classificação)\n",
    "# def embedding_generation_and_classification(graph, model, data, num_clusters):\n",
    "#     embeddings = gml_benchmark.generate_embeddings(model, data)\n",
    "\n",
    "#     if isinstance(graph, torch_geometric.data.Data):\n",
    "#         graph.x = embeddings\n",
    "#     elif isinstance(graph, dgl.DGLGraph):\n",
    "#         graph.ndata['feat'] = embeddings\n",
    "\n",
    "#     node_similarity = gml_benchmark.calculate_node_similarity(embeddings)\n",
    "#     edge_similarity = gml_benchmark.calculate_edge_similarity(graph)\n",
    "\n",
    "#     node_labels = gml_benchmark.classify_nodes(embeddings, num_clusters)\n",
    "#     edge_labels = gml_benchmark.classify_edges(edge_similarity, num_clusters)\n",
    "\n",
    "# # Lista de modelos a serem avaliados (mesma do código anterior)\n",
    "# model_names = [\n",
    "#     \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "#     \"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "#     \"sentence-transformers/LaBSE\"\n",
    "# ]\n",
    "\n",
    "# # Dispositivo de processamento\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Benchmarking\n",
    "# results = {}\n",
    "# for model_name in model_names:\n",
    "#     print('-' * 125)\n",
    "#     print(f'MODELO PRÉ-TREINADO: {model_name}')\n",
    "\n",
    "#     try:\n",
    "#         # Carregar o modelo pré-treinado\n",
    "#         model = SentenceTransformer(model_name, device=device) # type: ignore\n",
    "\n",
    "#         # Criar os grafos (implemente as funções de criação de grafo)\n",
    "#         pyg_graph = gml_benchmark.create_pytorch_geometric_graph([], [])  # Substitua [] pelos seus dados de aresta\n",
    "#         dgl_graph = gml_benchmark.create_dgl_graph([], [])            # Substitua [] pelos seus dados de aresta\n",
    "\n",
    "#         # Realizar o benchmarking\n",
    "#         pyg_time = gml_benchmark.benchmark_operation(\n",
    "#             pyg_graph, \"PyTorch Geometric\",\n",
    "#             lambda graph: embedding_generation_and_classification(graph, model, processed_competences, num_clusters)\n",
    "#         )\n",
    "#         dgl_time = gml_benchmark.benchmark_operation(\n",
    "#             dgl_graph, \"DGL\",\n",
    "#             lambda graph: embedding_generation_and_classification(graph, model, processed_competences, num_clusters)\n",
    "#         )\n",
    "\n",
    "#         # Armazenar os resultados\n",
    "#         results[model_name] = {\"PyTorch Geometric\": pyg_time, \"DGL\": dgl_time}\n",
    "\n",
    "#     except RuntimeError as e:\n",
    "#         if \"CUDA out of memory\" in str(e):\n",
    "#             print(f\"Erro de memória da GPU para o modelo {model_name}. Tente reduzir o tamanho do lote ou usar um modelo menor.\")\n",
    "#         else:\n",
    "#             print(f\"Erro ao processar o modelo {model_name}: {e}\")\n",
    "\n",
    "# # Plotar os resultados\n",
    "# gml_benchmark.plot_benchmark_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gerar o modelo grafo inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx\n",
    "# !pip install matplotlib\n",
    "# !pip3 install selenium\n",
    "# !pip3 install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('../../templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from IPython.display import IFrame\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# from pathlib import Path\n",
    "# def find_repo_root(path='.', depth=10):\n",
    "#         ''' \n",
    "#         Busca o arquivo .git e retorna string com a pasta raiz do repositório.\n",
    "#         '''\n",
    "#         # Prevenir recursão infinita limitando a profundidade\n",
    "#         if depth < 0:\n",
    "#             return None\n",
    "#         path = Path(path).absolute()\n",
    "#         if (path / '.git').is_dir():\n",
    "#             return path\n",
    "#         # Corrigido para usar LattesScraper.find_repo_root para chamada recursiva\n",
    "#         return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "import os\n",
    "from git import Repo\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "# data_folder = os.path.join(str(root_folder),\"_data\",\"in_pdf\")\n",
    "\n",
    "# Criar o grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar nós (dominios, processos e entidades)\n",
    "dominios = [\"Pesquisar\", \"Desenvolver\", \"Inovar\"]\n",
    "processos = [\"P001\", \"P002\", \"P003\", \"P004\", \"P005\", \"P006\", \"P007\", \"P008\", \"P009\"]\n",
    "entidades = {\n",
    "    \"P001\": [\"Dores\", \"Desejos\", \"Desafios\"],\n",
    "    \"P002\": [\"Temas\", \"Tópicos\", \"Assuntos\"],\n",
    "    \"P003\": [\"Atitudes\", \"Experiências\", \"Habilidades\"],\n",
    "    \"P004\": [\"Papeis\", \"Tempo\", \"Orçamentos\"],\n",
    "    \"P005\": [\"Projetos\", \"Processos\", \"Programas\"],\n",
    "    \"P006\": [\"Ensaios\", \"Equipamentos\", \"Ambientes\"],\n",
    "    \"P007\": [\"Aplicação\", \"Solução\", \"Produto-Serviço\"],\n",
    "    \"P008\": [\"Modelos\", \"Protótipos\", \"Empreendimentos\"],\n",
    "    \"P009\": [\"Indicadores\", \"Evidências\", \"Mensuração\"]\n",
    "}\n",
    "\n",
    "# Criar visualização dos nós de acordo com a estrutura de dados\n",
    "for macroprocesso in dominios:\n",
    "    G.add_node(macroprocesso, type=\"macroprocesso\")\n",
    "\n",
    "for processo in processos:\n",
    "    G.add_node(processo, type=\"processo\")\n",
    "\n",
    "for processo, entidades_list in entidades.items():\n",
    "    for entidade in entidades_list:\n",
    "        G.add_node(entidade, type=\"entidade\")\n",
    "\n",
    "# Adicionar arestas (relacionamentos)\n",
    "for macroprocesso in dominios:\n",
    "    for i in range(1, 4):\n",
    "        G.add_edge(macroprocesso, f\"P00{i + 3*(dominios.index(macroprocesso))}\")\n",
    "\n",
    "for processo, entidades_list in entidades.items():\n",
    "    for entidade in entidades_list:\n",
    "        G.add_edge(processo, entidade)\n",
    "\n",
    "# (Opcional) Adicionar relacionamentos entre entidades, para formar Demanda, Faturamento, Lucro, Reinvestimento... etc\n",
    "# G.add_edge(\"Dores\", \"Desejos\")\n",
    "\n",
    "# Calcular distâncias, definir cores, tamanhos e tamanhos de fonte\n",
    "node_distances = {}\n",
    "for macroprocesso in dominios:\n",
    "    for node, distance in nx.shortest_path_length(G, source=macroprocesso).items():\n",
    "        node_distances[node] = distance\n",
    "\n",
    "cores_base = {\"macroprocesso\": \"#007BFF\", \"processo\": \"#28A745\", \"entidade\": \"#FFC107\"}\n",
    "node_colors = {}\n",
    "node_sizes = {}\n",
    "node_font_sizes = {}  # Dicionário para armazenar os tamanhos de fonte\n",
    "for node in G.nodes():\n",
    "    node_type = G.nodes[node][\"type\"]\n",
    "    cor_base = cores_base[node_type]\n",
    "    alpha = max(0, 255 - 25 * node_distances[node])\n",
    "    node_colors[node] = f\"{cor_base}{alpha:02X}\"\n",
    "\n",
    "    # Definir tamanhos e tamanhos de fonte com base na distância\n",
    "    tamanho_base = {\"macroprocesso\": 50, \"processo\": 30, \"entidade\": 15}\n",
    "    font_size_base = {\"macroprocesso\": 42, \"processo\": 28, \"entidade\": 18}  # Tamanhos de fonte iniciais\n",
    "    node_sizes[node] = tamanho_base[node_type] - 5 * node_distances[node]\n",
    "    node_font_sizes[node] = font_size_base[node_type] - node_distances[node]  # Reduzir 1 pixel por passo\n",
    "\n",
    "# Configurar o PyVis (notebook=False para renderizar na célula)\n",
    "net = Network(notebook=False, width=\"100%\", height=\"1200px\", bgcolor=\"#ffffff\", font_color=\"black\") # type: ignore\n",
    "net.barnes_hut()\n",
    "\n",
    "# Adicionar nós e arestas ao PyVis\n",
    "for node in G.nodes():\n",
    "    net.add_node(node, label=node, color=node_colors[node], title=G.nodes[node][\"type\"], \n",
    "                 size=node_sizes[node], font={\"size\": node_font_sizes[node], \"color\": \"black\"})  # Adicionar tamanho da fonte\n",
    "\n",
    "for edge in G.edges():\n",
    "    weight = 1\n",
    "    net.add_edge(*edge, value=weight)\n",
    "\n",
    "# Configurar o ForceAtlas2\n",
    "net.options.physics.solver = \"forceAtlas2Based\"\n",
    "net.options.physics.forceAtlas2Based = { # type: ignore\n",
    "    \"gravitationalConstant\": -50,\n",
    "    \"centralGravity\": 0.01,\n",
    "    \"springLength\": 100,\n",
    "    \"springConstant\": 0.08,\n",
    "    \"damping\": 0.4,\n",
    "    \"avoidOverlap\": 1\n",
    "}\n",
    "\n",
    "driver_path = None\n",
    "try:\n",
    "    # Caminho para o chromedriver no sistema local\n",
    "    if platform.system() == \"Windows\":\n",
    "        driver_path=os.path.join(str(root_folder),'chromedriver','chromedriver.exe')\n",
    "    else:\n",
    "        driver_path=os.path.join(str(root_folder),'chromedriver','chromedriver')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"Não foi possível estabelecer uma conexão, verifique o chromedriver\")\n",
    "    print(e)\n",
    "\n",
    "# print(driver_path)\n",
    "# service = Service(driver_path)\n",
    "# driver = webdriver.Chrome(service=service)\n",
    "# driver.get(\"grafo_interativo.html\")\n",
    "\n",
    "# Adicionar controles interativos (opcional)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "# Adicionar estilo inline para fundo branco\n",
    "net.html = net.html.replace(\"<body>\", '<body style=\"background-color: white;\">')\n",
    "\n",
    "# Salvar o HTML na pasta templates\n",
    "template_dir = os.path.join(str(root_folder),'templates')\n",
    "pathfilename = os.path.join(template_dir,\"grafo_interativo.html\")\n",
    "net.save_graph(pathfilename)\n",
    "\n",
    "print(f\"Grafo interativo salvo em: {pathfilename}\")\n",
    "# Renderizar na célula do Jupyter Notebook\n",
    "# net.show(\"'../../templates'grafo_interativo.html\")  # Definir fundo branco ao salvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Renderizar grafo no Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "net = Network(notebook=True, \n",
    "              width=\"100%\", \n",
    "              height=\"1200px\", \n",
    "              bgcolor=\"#ffffff\", \n",
    "              font_color=\"black\") # type: ignore\n",
    "\n",
    "IFrame(src='http://127.0.0.1:5000/grafo_interativo.html', width='100%', height='800px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renderizar grafo em HTML na célula do Jupyter Notebook\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=\"grafo_interativo.html\", width=\"100%\", height=\"600px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Tarefas para monitorar políticas públicas</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obter dados de Protocolos e Diretrizes Clínicas vigentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Protocolos clínicos vigentes no site do Ministério da Saúde\n",
    "\n",
    "Fonte de dados para extração de protocolos: https://www.gov.br/saude/pt-br/assuntos/pcdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Protocolos clínicos vigentes no site do CONITEC\n",
    "Fonte de dados para extração de protocolos: https://www.gov.br/conitec/pt-br/assuntos/avaliacao-de-tecnologias-em-saude/protocolos-clinicos-e-diretrizes-terapeuticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baixar Protocolos e Diretrizes Clínicas do Site do MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_protocol import SaudeGovDataExtractor\n",
    "\n",
    "extractor = SaudeGovDataExtractor(\"https://www.gov.br/saude/pt-br/assuntos/pcdt\")\n",
    "df_docs, sucessos, erros = extractor.download_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs[:60] # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitorar fluxos e fontes de dados no CONITEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fluxo elaboração/atualização de Protocolos e Diretrizes\n",
    "https://www.gov.br/conitec/pt-br/assuntos/avaliacao-de-tecnologias-em-saude/pcdt-em-elaboracao-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fluxo de Incorporação de Tecnologia no SUS\n",
    "https://www.gov.br/conitec/pt-br/assuntos/fluxo-de-incorporacao-de-tecnologias-no-sus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitorar Tecnologias discutidas no CONITEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gov.br/conitec/pt-br/assuntos/avaliacao-de-tecnologias-em-saude/monitoramento-de-tecnologias-em-saude#MHT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acompanhar Publicações Diretrizes Clínicas CONITEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gov.br/conitec/pt-br/assuntos/noticias/2024/abril/atualizacao-anual-de-diretrizes-clinicas-pelo-ministerio-da-saude-segue-criterios-de-priorizacao-e-considera-encaminhamento-de-areas-tecnicas\n",
    "\n",
    "https://www.gov.br/conitec/pt-br/@@search?SearchableText=Diretrizes%20Cl%C3%ADnicas\n",
    "\n",
    "https://www.gov.br/conitec/pt-br/midias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(str(path.parent), depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Sempre construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(str(base_repo_dir), 'utils')\n",
    "folder_domain = os.path.join(str(base_repo_dir), 'source', 'domain')\n",
    "folder_data_input = os.path.join(str(base_repo_dir), '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(str(base_repo_dir), '_data', 'out_json')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "from environment_setup import EnvironmentSetup\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from neo4j_persister import Neo4jPersister\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DictToHDF5, attribute_to_be_non_empty\n",
    "from extract_protocol import SaudeGovDataExtractor\n",
    "# from scraper_pasteur import PasteurScraper\n",
    "# from scraper_sucupira import SucupiraScraper\n",
    "# from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "\n",
    "t00 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from extract_protocol import SaudeGovDataExtractor\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "extractor = SaudeGovDataExtractor(\"https://www.gov.br/saude/pt-br/assuntos/pcdt\")\n",
    "links = extractor.get_links()\n",
    "if links:\n",
    "    print(f'{len(links)} links extraídos')\n",
    "\n",
    "## Visualizar links para documentos de protocolos e diretrizes\n",
    "# for link in links:\n",
    "#     filename = os.path.join(str(root_folder),\"_data\",\"in_pdf\", link.split(\"/\")[-1])\n",
    "#     if '#' in filename:\n",
    "#         filename = filename.split('#')[0]\n",
    "#     print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Tarefas para obter dados de Inovação no Mundo</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desempenho em Inovação no mundo e no Brasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medição da inovação em nível mundial é um campo importante para entender como diferentes nações estão progredindo em termos de capacidade e sucesso em inovação. Uma das iniciativas mais conhecidas neste campo é o Global Innovation Index (GII).\n",
    "\n",
    "O GII é um ranking anual publicado pela World Intellectual Property Organization (WIPO) em parceria com a INSEAD e outras instituições, como a Cornell University. Iniciado em 2007, o índice é baseado em dados subjetivos e objetivos obtidos de várias fontes, incluindo a International Telecommunication Union, o World Bank e o World Economic Forum. O GII classifica os países com base em dois sub-índices: o Innovation Input Index e o Innovation Output Index, compostos por cinco e dois pilares, respectivamente, cada um descrevendo um atributo da inovação.\n",
    "\n",
    "Além do GII, há outras iniciativas semelhantes, como o International Innovation Index, que medem o nível de inovação de um país. Este índice é produzido em conjunto pelo Boston Consulting Group (BCG), pela National Association of Manufacturers (NAM) e pelo Manufacturing Institute (MI), o afiliado de pesquisa apartidária da NAM.\n",
    "\n",
    "Cada um desses índices oferece uma perspectiva única sobre a inovação em nível de país, ajudando governos, formuladores de políticas e acadêmicos a entender as tendências de inovação e a identificar áreas para melhoria e investimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "for i in range(0,20):\n",
    "    print(math.factorial(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(1, 100, 100)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "# Valores de entrada (tamanho do problema) - ajustados\n",
    "n = np.linspace(1, 100, 100)\n",
    "\n",
    "# Cálculo das complexidades\n",
    "O_1 = np.ones(n.shape)\n",
    "O_log_n = np.log2(n)\n",
    "O_n = n\n",
    "O_n_log_n = n * np.log2(n)\n",
    "O_n_squared = n**2\n",
    "O_2_n = 2**n-1\n",
    "max_factorial_visible = 120  # Valor máximo para o cálculo do fatorial 6!=120\n",
    "O_n_factorial = np.array([math.factorial(int(i)) if i <= max_factorial_visible else np.nan for i in n])\n",
    "\n",
    "# Exibir O_n_factorial em notação científica\n",
    "for valor in O_n_factorial[:max_factorial_visible + 1]:\n",
    "    print(f\"{valor}\")\n",
    "\n",
    "# Criar um DataFrame para os dados a serem exibidos\n",
    "df = pd.DataFrame({'n': n[:max_factorial_visible + 1], 'O(n!)': O_n_factorial[:max_factorial_visible + 1]})\n",
    "\n",
    "# Criar o gráfico de linha\n",
    "chart = alt.Chart(df).mark_line(point=True).encode(  # Adicionamos point=True para mostrar os pontos\n",
    "    x=alt.X('n:Q', axis=alt.Axis(labelAngle=-45, format='.2f')),  # Formato com 4 casas decimais\n",
    "    y=alt.Y('O(n!)', scale=alt.Scale(type='log')),\n",
    "    tooltip=['n', 'O(n!)']\n",
    ").properties(\n",
    "    title='Valores de O(n!) até n = 20',\n",
    "    height=400,\n",
    "    width=800,\n",
    ").interactive()\n",
    "\n",
    "# Exibir o gráfico\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Valores de entrada (tamanho do problema) - ajustados\n",
    "n = np.linspace(1, 100, 100)\n",
    "\n",
    "# Cálculo das complexidades\n",
    "O_1 = np.ones(n.shape)\n",
    "O_log_n = np.log2(n)\n",
    "O_n = n\n",
    "O_n_log_n = n * np.log2(n)\n",
    "O_n_squared = n**2\n",
    "O_2_n = 2**n - 1\n",
    "\n",
    "# Calcular O(n!) até um ponto visível\n",
    "max_factorial = 100  # Valor máximo de n para o qual O(n!) é visível no gráfico\n",
    "O_n_factorial = np.array([math.factorial(int(i)) if i <= max_factorial_visible else np.nan for i in n])\n",
    "\n",
    "# Paleta de cores similar à imagem de referência\n",
    "colors = ['darkgreen', 'green', 'orange', 'gold', 'red', 'darkblue', 'purple']\n",
    "\n",
    "# Criar o gráfico Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar as curvas ao gráfico, usando cores personalizadas\n",
    "fig.add_trace(go.Scatter(x=n, y=O_1, mode='lines', name='O(1)', line=dict(color=colors[0], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_log_n, mode='lines', name='O(log n)', line=dict(color=colors[1], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n, mode='lines', name='O(n)', line=dict(color=colors[2], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_log_n, mode='lines', name='O(n log n)', line=dict(color=colors[3], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_squared, mode='lines', name='O(n²)', line=dict(color=colors[4], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_2_n, mode='lines', name='O(2ⁿ)', line=dict(color=colors[5], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n[:max_factorial + 1], y=O_n_factorial[:max_factorial + 1], mode='lines', name='O(n!)', line=dict(color=colors[6], width=2)))\n",
    "\n",
    "# Configuração do layout do gráfico\n",
    "fig.update_layout(\n",
    "    title='Comparação de Complexidades Algorítmicas (Big O)',\n",
    "    xaxis_title='Tamanho dos Dados de Entrada (n)',\n",
    "    yaxis_title='Tempo de Execução em escala logarítmica (em operações)',\n",
    "    yaxis_type='log',\n",
    "    yaxis=dict(range=[-0.04, 2]),  # Escala logarítmica com limite definido para y\n",
    "    xaxis=dict(range=[1, 20]),  # Escala linear com limite definido para x\n",
    "    showlegend=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(l=50, r=50, t=80, b=50),\n",
    ")\n",
    "\n",
    "# Função para calcular as posições das anotações\n",
    "def calculate_annotations(fig):\n",
    "    annotations = []\n",
    "    x_end = fig.layout.xaxis.range[1] * 0.95  \n",
    "    for i, trace in enumerate(fig.data):\n",
    "        # Encontrar o índice do valor de x mais próximo de x_end\n",
    "        idx = np.abs(trace.x - x_end).argmin()\n",
    "\n",
    "        # Se o valor de y for NaN, usar o último valor válido antes de x_end\n",
    "        if np.isnan(trace.y[idx]):\n",
    "            valid_indices = np.where(~np.isnan(trace.y) & (trace.x <= x_end))[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                idx = valid_indices[-1]\n",
    "\n",
    "        x = trace.x[idx]\n",
    "        y = trace.y[idx]\n",
    "\n",
    "        # Calcular a posição da anotação no eixo y, considerando a escala logarítmica, exceto para O(1)\n",
    "        if trace.name == 'O(n log n)':\n",
    "            y_annotation = 1.95\n",
    "        if trace.name == \"O(n)\":\n",
    "            y_annotation = 1.35\n",
    "            x_annotation = x\n",
    "        if trace.name == \"O(log n)\":\n",
    "            y_annotation = 0.7\n",
    "            x_annotation = x-0.4\n",
    "        if trace.name == \"O(1)\":\n",
    "            y_annotation = 0.05\n",
    "            x_annotation = x\n",
    "        if trace.name == \"O(n²)\":\n",
    "            y_annotation = 2\n",
    "            x_annotation = 10\n",
    "        if trace.name == \"O(2ⁿ)\":\n",
    "            y_annotation = 2\n",
    "            x_annotation = 6.7\n",
    "        if trace.name == \"O(n!)\":\n",
    "            y_annotation = 2\n",
    "            x_annotation = 5\n",
    "\n",
    "        annotations.append(\n",
    "            dict(\n",
    "                x=x_annotation,\n",
    "                y=y_annotation,\n",
    "                text=trace.name,\n",
    "                showarrow=False,\n",
    "                xanchor='left' if trace.name != 'O(n log n)' else 'center',  # Ajustar o alinhamento para O(n log n)\n",
    "                yanchor='middle',  # Centralizar o rótulo na vertical\n",
    "                font=dict(size=12, color=colors[i], family='Arial'),  # Usar a cor correspondente da paleta\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Calcular e adicionar as anotações\n",
    "fig.update_layout(annotations=calculate_annotations(fig))\n",
    "\n",
    "# Exibir o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Valores de entrada (tamanho do problema) - ajustados\n",
    "n = np.linspace(1, 100, 1000)\n",
    "\n",
    "# Cálculo das complexidades\n",
    "O_1 = np.ones(n.shape)\n",
    "O_log_n = np.log2(n)\n",
    "O_n = n\n",
    "O_n_log_n = n * np.log2(n)\n",
    "O_n_squared = n**2\n",
    "O_2_n = 2**n - 1\n",
    "\n",
    "# Cálculo da complexidade O(n!) para inteiros até max_factorial_visible\n",
    "max_factorial_visible = 100\n",
    "n_int = np.arange(1, max_factorial_visible + 1)  # Valores inteiros de 1 a max_factorial_visible\n",
    "O_n_factorial_int = np.array([math.factorial(i) for i in n_int])\n",
    "\n",
    "# Interpolação para valores fracionários\n",
    "O_n_factorial = np.array([math.gamma(i + 1) for i in n])\n",
    "\n",
    "# Paleta de cores similar à imagem de referência\n",
    "colors = ['darkgreen', 'green', 'gold', 'orange', 'red', 'darkblue', 'black']\n",
    "\n",
    "# Criar o gráfico Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar as curvas ao gráfico, usando cores personalizadas\n",
    "fig.add_trace(go.Scatter(x=n, y=O_1, mode='lines', name='O(1)', line=dict(color=colors[0], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_log_n, mode='lines', name='O(log n)', line=dict(color=colors[1], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n, mode='lines', name='O(n)', line=dict(color=colors[2], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_log_n, mode='lines', name='O(n log n)', line=dict(color=colors[3], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_squared, mode='lines', name='O(n²)', line=dict(color=colors[4], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_2_n, mode='lines', name='O(2ⁿ)', line=dict(color=colors[5], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n[:max_factorial_visible + 1], y=O_n_factorial[:max_factorial_visible + 1], \n",
    "    mode='lines', name='O(n!)', line=dict(color=colors[6], width=4)))\n",
    "\n",
    "# Configuração do layout do gráfico\n",
    "fig.update_layout(\n",
    "    title='Comparação de Complexidades Assintótica dos Algoritmos (Big O)',\n",
    "    xaxis_title='Tamanho dos Dados de Entrada (n)',\n",
    "    yaxis_title='Tempo de Execução em escala logarítmica (em operações)',\n",
    "    yaxis_type='log',\n",
    "    xaxis=dict(\n",
    "        range=[1, 20],          # Escala linear com limite definido para x\n",
    "        gridcolor='lightgray',  # Cor mais clara para a grade\n",
    "        gridwidth=0.25,         # Largura menor para a grade\n",
    "        showgrid=True,\n",
    "        griddash='dot',\n",
    "    ),  \n",
    "    yaxis=dict(\n",
    "        range=[-0.04, 2],       # Escala logarítmica com limite definido para y\n",
    "        gridcolor='lightgray',  \n",
    "        gridwidth=0.25,\n",
    "        showgrid=True,\n",
    "        griddash='dot',\n",
    "        # tickformat=\".0e\",     # Formatar os ticks em notação científica com expoente inteiro\n",
    "        # tickformat=\",d\"         # Formatar os ticks como números inteiros com separador de milhar\n",
    "        tickvals=[1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000],  # Valores dos ticks\n",
    "        ticktext=['1', '10', '100', '1,000', '10,000', '100,000', '1,000,000', '10,000,000', '100,000,000']  # Rótulos em números inteiros\n",
    "\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(l=25, r=50, t=80, b=50),\n",
    ")\n",
    "\n",
    "# Função para calcular as posições das anotações\n",
    "def calculate_annotations(fig):\n",
    "    annotations = []\n",
    "    x_end = fig.layout.xaxis.range[1] * 0.95  \n",
    "    for i, trace in enumerate(fig.data):\n",
    "        # Encontrar o índice do valor de x mais próximo de x_end\n",
    "        idx = np.abs(trace.x - x_end).argmin()\n",
    "\n",
    "        # Se o valor de y for NaN, usar o último valor válido antes de x_end\n",
    "        if np.isnan(trace.y[idx]):\n",
    "            valid_indices = np.where(~np.isnan(trace.y) & (trace.x <= x_end))[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                idx = valid_indices[-1]\n",
    "\n",
    "        x = trace.x[idx]\n",
    "        y = trace.y[idx]\n",
    "\n",
    "        # Calcular a posição da anotação no eixo y, considerando a escala logarítmica, exceto para O(1)\n",
    "        if trace.name == 'O(n log n)':\n",
    "            y_annotation = 1.95\n",
    "        if trace.name == \"O(n)\":\n",
    "            y_annotation = 1.4\n",
    "            x_annotation = x+0.75\n",
    "        if trace.name == \"O(log n)\":\n",
    "            y_annotation = 0.7\n",
    "            x_annotation = x+0.75\n",
    "        if trace.name == \"O(1)\":\n",
    "            y_annotation = 0.075\n",
    "            x_annotation = x+0.75\n",
    "        if trace.name == \"O(n²)\":\n",
    "            y_annotation = 1.95\n",
    "            x_annotation = 11.4\n",
    "        if trace.name == \"O(2ⁿ)\":\n",
    "            y_annotation = 1.95\n",
    "            x_annotation = 8\n",
    "        if trace.name == \"O(n!)\":\n",
    "            y_annotation = 1.95\n",
    "            x_annotation = 4.75         \n",
    "\n",
    "        annotations.append(\n",
    "            dict(\n",
    "                x=x_annotation,\n",
    "                y=y_annotation,\n",
    "                text=trace.name,\n",
    "                showarrow=False,\n",
    "                xanchor='right',\n",
    "                yanchor='middle',\n",
    "                font=dict(size=20, color='black', family='Arial'),  # Usar a cor correspondente da paleta\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Calcular e adicionar as anotações\n",
    "fig.update_layout(annotations=calculate_annotations(fig),\n",
    "    shapes=[\n",
    "        # Região verde (diagonal até um pouco acima de O(log n))\n",
    "        dict(\n",
    "            type=\"path\",\n",
    "            path=f\"M 0, {10**fig.layout.yaxis.range[0]} L 20, {O_log_n[-1]} L 20, {10**fig.layout.yaxis.range[0]} Z\", # type: ignore\n",
    "            fillcolor=\"lightgreen\",\n",
    "            opacity=0.3,\n",
    "            line_width=0,\n",
    "            layer='below' # para que as curvas fiquem por cima\n",
    "        ),\n",
    "        # Região amarela (diagonal até um pouco acima de O(n log n))\n",
    "        dict(\n",
    "            type=\"path\",\n",
    "            path=f\"M 0, {10**fig.layout.yaxis.range[0]} L 20, {O_n_log_n[-1]-5} L 20, {O_log_n[-1]} Z\", # type: ignore\n",
    "            fillcolor=\"yellow\",\n",
    "            opacity=0.3,\n",
    "            line_width=0,\n",
    "            layer='below' # para que as curvas fiquem por cima\n",
    "        ),\n",
    "        # Região vermelha (diagonal até o topo do gráfico)\n",
    "        dict(\n",
    "            type=\"path\",\n",
    "            # path=f\"M 0, {10**fig.layout.yaxis.range[0]} L 20, {10**fig.layout.yaxis.range[1]} L 0, {10**fig.layout.yaxis.range[1]} Z\",\n",
    "            path=f\"M 20, {O_n_log_n[-1] - 5} L 0, {10**(int(np.log10(O_n_log_n[-1])) + 1)} L 0, {10**fig.layout.yaxis.range[0]} Z\", # type: ignore\n",
    "            fillcolor=\"red\",\n",
    "            opacity=0.3,\n",
    "            line_width=0,\n",
    "            layer='below'  # para que as curvas fiquem por cima\n",
    "        ),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "# Exibir o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histórico no Global Innovation Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pathfilename = os.path.join(folder_data_input, 'gii_history_data.csv')\n",
    "df_gii = pd.read_csv(pathfilename)\n",
    "\n",
    "def calculate_percentages(df_gii):\n",
    "    df_avaliacao = pd.DataFrame(columns=['Ano', 'Participantes', 'Above Brazil', 'Below Brazil', 'Above Brazil (%)', 'Below Brazil (%)'])\n",
    "    \n",
    "    for year in df_gii['Ano'].unique():\n",
    "        df_year = df_gii[df_gii['Ano'] == year]\n",
    "        total_countries = df_year['Países Participantes'].values[0]\n",
    "        brazil_position = df_year['Colocação do Brasil'].values[0]\n",
    "        above_brazil = brazil_position - 1\n",
    "        below_brazil = total_countries - brazil_position\n",
    "        \n",
    "        above_percent = (above_brazil / total_countries) * 100\n",
    "        below_percent = (below_brazil / total_countries) * 100\n",
    "        \n",
    "        df_avaliacao = pd.concat([df_avaliacao, pd.DataFrame({'Ano': [year], 'Participantes': [total_countries], 'Above Brazil': [above_brazil], 'Below Brazil': [below_brazil], 'Above Brazil (%)': [above_percent], 'Below Brazil (%)': [below_percent]})], ignore_index=True)\n",
    "    \n",
    "    return df_avaliacao\n",
    "\n",
    "# Call the function to create df_avaliacao\n",
    "df_avaliacao = calculate_percentages(df_gii)\n",
    "df_avaliacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a figure with secondary y-axis for the line plots\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add stacked bars for 'Below Brazil (%)' and 'Above Brazil (%)'\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_avaliacao['Ano'], \n",
    "           y=df_avaliacao['Below Brazil (%)'], \n",
    "           name='Below Brazil (%)'),\n",
    "           secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_avaliacao['Ano'], \n",
    "           y=df_avaliacao['Above Brazil (%)'], \n",
    "           name='Above Brazil (%)'),\n",
    "           secondary_y=False,\n",
    ")\n",
    "\n",
    "# Correct the data label positions for each segment of the stacked bars\n",
    "for index, row in df_avaliacao.iterrows():\n",
    "    # Position for label of 'Above Brazil (%)'\n",
    "    position_above = row['Below Brazil (%)'] + row['Above Brazil (%)'] / 2\n",
    "    fig.add_annotation(\n",
    "        x=row['Ano'], y=position_above,\n",
    "        text=f\"{row['Above Brazil (%)']:.1f}%\",\n",
    "        showarrow=False, font=dict(color='white')\n",
    "    )\n",
    "\n",
    "    # Position for label of 'Below Brazil (%)'\n",
    "    position_below = row['Below Brazil (%)'] / 2\n",
    "    fig.add_annotation(\n",
    "        x=row['Ano'], y=position_below,\n",
    "        text=f\"{row['Below Brazil (%)']:.1f}%\",\n",
    "        showarrow=False, font=dict(color='white')\n",
    "    )\n",
    "\n",
    "# Update layout for stacked bars\n",
    "fig.update_layout(barmode='stack')\n",
    "\n",
    "# Add line for total number of participants\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_avaliacao['Ano'], \n",
    "               y=df_avaliacao['Participantes'], \n",
    "               name='Total Participants', \n",
    "               mode='lines+markers+text', \n",
    "               text=df_avaliacao['Participantes'], \n",
    "               textposition=\"top center\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add line for Brazil's performance, but on the primary y-axis\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_avaliacao['Ano'], \n",
    "               y=df_avaliacao['Below Brazil (%)'], \n",
    "               name='Brazil Performance', \n",
    "               mode='lines+markers', \n",
    "               line=dict(color='yellow', \n",
    "                         dash='dot')),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "# Update the bar colors\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Above Brazil (%)'),\n",
    "    marker=dict(color='orange')\n",
    ")\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Below Brazil (%)'),\n",
    "    marker=dict(color='blue')\n",
    ")\n",
    "\n",
    "# Update the line trace for total number of participants to have a thickness of 4\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Total Participants'),\n",
    "    line=dict(width=2)\n",
    ")\n",
    "\n",
    "# Update the line trace for Brazil's performance to have a thickness of 4\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Brazil Performance'),\n",
    "    line=dict(width=6)\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "vr_max = max(df_avaliacao['Participantes']) * 1.1\n",
    "fig.update_layout(\n",
    "    title='Performance Comparison: Brazil vs. Other Countries',\n",
    "    height=600,\n",
    "    yaxis=dict(title='Percentage', range=[0, vr_max]),  # Extending primary y-axis range\n",
    "    yaxis2=dict(title='Total Participants', overlaying='y', side='right', range=[0, vr_max])\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickvals=df_avaliacao['Ano'])\n",
    "\n",
    "# Re-render the chart\n",
    "fig.show(renderer=\"notebook\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Gerar análises no domínio PDI para o CEIS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Analisar similaridade tópicos - questões pesquisa</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(str(path.parent), depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(str(base_repo_dir), 'utils')\n",
    "folder_domain = os.path.join(str(base_repo_dir), 'source', 'domain')\n",
    "folder_data_input = os.path.join(str(base_repo_dir), '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(str(base_repo_dir), '_data', 'out_json')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "# from scraper_pasteur import PasteurScraper\n",
    "# from scraper_sucupira import SucupiraScraper\n",
    "# from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "from environment_setup import EnvironmentSetup\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from neo4j_persister import Neo4jPersister\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DictToHDF5, attribute_to_be_non_empty\n",
    "\n",
    "t00 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(folder_data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Carregar dados dos produtos prioritários, equipamentos e questões de pesquisa\n",
    "curr_pathfilename = os.path.join(folder_data_input,'docents_dict_list.json')\n",
    "with open(curr_pathfilename, 'r', encoding='utf-8') as f:\n",
    "    curriculos = json.load(f)\n",
    "\n",
    "prod_pathfilename = os.path.join(folder_data_output,'matriz_ceis.json')\n",
    "with open(prod_pathfilename, 'r', encoding='utf-8') as f:\n",
    "    matriz_produtos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(curriculos)} currículos carregados')\n",
    "# [x.get('Áreas') for x in curriculos]\n",
    "produtos = [produto.get('nome') for bloco in matriz_produtos.get('blocos', []) for produto in bloco.get('produtos', [])]\n",
    "print(f'{len(produtos)} produtos carregados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vega\n",
    "# !jupyter nbextension install --sys-prefix --py vega\n",
    "# !jupyter nbextension enable --sys-prefix --py vega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_validator import *\n",
    "\n",
    "# Exemplo de uso (substituir pelos dados reais)\n",
    "y_true = [[\"A\", \"B\"], [\"B\"], [\"A\", \"C\"]]\n",
    "y_pred = [[\"A\"], [\"B\", \"C\"], [\"A\", \"B\"]]\n",
    "classes = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "# Probabilidades aqui\n",
    "y_proba = [[0.8, 0.7, 0.3], [0.2, 0.9, 0.5], [0.6, 0.8, 0.4]]\n",
    "\n",
    "\n",
    "validar_modelo(y_true, y_pred, y_proba, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Criar grafo de conhecimento</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict = []\n",
    "equipamentos = [...]\n",
    "questoes_pesquisa = [...]\n",
    "produtos_prioritarios = []\n",
    "\n",
    "# Criar o grafo heterogêneo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar nós e arestas para pesquisadores\n",
    "for pesquisador in curriculos:\n",
    "    G.add_node(pesquisador['Identificação']['ID Lattes'], type=\"pesquisador\", **pesquisador)  # Adicionar atributos do currículo\n",
    "    \n",
    "    # Conectar pesquisador às suas áreas de atuação\n",
    "    for area in pesquisador['Áreas'].values():\n",
    "        G.add_node(area, type=\"area\")\n",
    "        G.add_edge(pesquisador['Identificação']['ID Lattes'], area)\n",
    "\n",
    "    # Conectar pesquisador às suas publicações\n",
    "    for publicacao in pesquisador['Produções']['Artigos completos publicados em periódicos']:\n",
    "        G.add_node(publicacao['titulo'], type=\"publicacao\")\n",
    "        G.add_edge(pesquisador['Identificação']['ID Lattes'], publicacao['titulo'])\n",
    "\n",
    "    # Conectar pesquisador a projetos\n",
    "    for pesquisador in curriculos:\n",
    "        for tipo_projeto in [\"ProjetosPesquisa\", \"ProjetosExtensão\", \"ProjetosDesenvolvimento\", \"ProjetosOutros\"]:\n",
    "            if tipo_projeto in pesquisador:\n",
    "                for projeto in pesquisador[tipo_projeto]:\n",
    "                    G.add_node(projeto['titulo_projeto'], type=\"projeto\")\n",
    "                    G.add_edge(pesquisador['Identificação']['ID Lattes'], projeto['titulo_projeto'])\n",
    "\n",
    "    # Conectar pesquisador a equipamentos\n",
    "    # (Assumindo que você tem uma lista de equipamentos mencionados nos currículos)\n",
    "    equipamentos_citados = []  # Preencha com os nomes dos equipamentos mencionados nos currículos\n",
    "    for pesquisador in curriculos:\n",
    "        for equipamento in equipamentos_citados:\n",
    "            if equipamento in pesquisador['Atuação Profissional'][0]['Descrição']:  # Exemplo: busca na descrição da atuação profissional\n",
    "                G.add_edge(pesquisador['Identificação']['ID Lattes'], equipamento)\n",
    "\n",
    "    # Conectar pesquisador a questões de pesquisa\n",
    "    # (Assumindo que você tem uma lista de questões de pesquisa e um método para associá-las aos pesquisadores)\n",
    "    for pesquisador in curriculos:\n",
    "        for questao in questoes_pesquisa:\n",
    "            if pesquisador_tem_interesse_na_questao(pesquisador, questao):  # inferência do interesse\n",
    "                G.add_edge(pesquisador['Identificação']['ID Lattes'], questao['descricao']) # type: ignore\n",
    "\n",
    "    # TO-DO\n",
    "    def calculate_similarity(pesquisador, list_dict):\n",
    "        \n",
    "        similarity=0\n",
    "        return similarity\n",
    "    \n",
    "    # TO-DO\n",
    "    def extract_topicos(questao, topico_pesquisa):\n",
    "        \n",
    "        lista_topicos=[]\n",
    "        return lista_topicos    \n",
    "\n",
    "    # Função para inferir o interesse do pesquisador em uma questão\n",
    "    def pesquisador_tem_interesse_na_questao(pesquisador, questao):\n",
    "        # Analise o currículo do pesquisador (áreas de atuação, publicações, projetos, etc.)\n",
    "        # e compare com a descrição da questão de pesquisa para determinar o interesse\n",
    "        # Retorna True se houver interesse, False caso contrário\n",
    "        topicos_pesquisador = extract_topicos(pesquisador, list_dict)\n",
    "        interesses_pesquisador = []\n",
    "        flag_interesse = False\n",
    "        threshold = 0.8\n",
    "        for i in topicos_pesquisador:\n",
    "            similarity = calculate_similarity(questao, i)\n",
    "            if similarity >= threshold:\n",
    "                interesses_pesquisador.append(questao)\n",
    "                flag_interesse = True\n",
    "\n",
    "        return flag_interesse\n",
    "\n",
    "\n",
    "# Adicionar nós e arestas para produtos prioritários, equipamentos e questões de pesquisa\n",
    "for produto in produtos_prioritarios:\n",
    "    G.add_node(produto['nome'], type=\"produto\", **produto)  # Adicionar atributos do produto (nome, descrição, área, etc.)\n",
    "\n",
    "    # Conectar produto às suas áreas (assumindo que o produto tem uma lista de áreas)\n",
    "    for area in produto.get('areas', []):  # Usar get() para evitar KeyError se 'areas' não existir\n",
    "        G.add_edge(produto['nome'], area)\n",
    "\n",
    "# Adicionar nós e arestas para equipamentos\n",
    "for equipamento in equipamentos:\n",
    "    G.add_node(equipamento['nome'], type=\"equipamento\", **equipamento)  # type: ignore # Adicionar atributos do equipamento\n",
    "\n",
    "    # Conectar equipamento às suas áreas (assumindo que o equipamento tem uma lista de áreas)\n",
    "    for area in equipamento.get('areas', []): # type: ignore\n",
    "        G.add_edge(equipamento['nome'], area) # type: ignore\n",
    "\n",
    "# Adicionar nós e arestas para questões de pesquisa\n",
    "for questao in questoes_pesquisa:\n",
    "    G.add_node(questao['descricao'], type=\"questao_pesquisa\", **questao)  # type: ignore # Adicionar atributos da questão\n",
    "\n",
    "    # Conectar questão de pesquisa às suas áreas (assumindo que a questão tem uma lista de áreas)\n",
    "    for area in questao.get('areas', []): # type: ignore\n",
    "        G.add_edge(questao['descricao'], area) # type: ignore\n",
    "\n",
    "# Adicionar nós e arestas para equipamentos\n",
    "for equipamento in equipamentos:\n",
    "    G.add_node(equipamento['nome'], type=\"equipamento\", **equipamento)  # type: ignore # Adicionar atributos do equipamento\n",
    "\n",
    "    # Conectar equipamento às suas áreas (assumindo que o equipamento tem uma lista de áreas)\n",
    "    if 'areas' in str(equipamento):  # Verificar se o equipamento possui áreas associadas\n",
    "        for area in equipamento['areas']: # type: ignore\n",
    "            if area in G.nodes:  # Verificar se a área já existe no grafo\n",
    "                G.add_edge(equipamento['nome'], area) # type: ignore\n",
    "            else:\n",
    "                # Se a área não existir, você pode decidir se quer adicioná-la como um novo nó\n",
    "                # G.add_node(area, type=\"area\")  \n",
    "                # G.add_edge(equipamento['nome'], area)\n",
    "                print(f\"Área '{area}' não encontrada para o equipamento '{equipamento['nome']}'.\") # type: ignore\n",
    "    else:\n",
    "        print(f\"Equipamento '{equipamento['nome']}' não possui áreas associadas.\") # type: ignore\n",
    "\n",
    "# Adicionar nós e arestas para questões de pesquisa\n",
    "for questao in questoes_pesquisa:\n",
    "    G.add_node(questao['descricao'], type=\"questao_pesquisa\", **questao)  # type: ignore # Adicionar atributos da questão\n",
    "\n",
    "    # Conectar questão de pesquisa às suas áreas\n",
    "    if 'areas' in str(questao):  # Verificar se a questão possui áreas associadas\n",
    "        for area in questao['areas']: # type: ignore\n",
    "            if area in G.nodes:  # Verificar se a área já existe no grafo\n",
    "                G.add_edge(questao['descricao'], area) # type: ignore\n",
    "            else:\n",
    "                # Se a área não existir, você pode decidir se quer adicioná-la como um novo nó\n",
    "                # G.add_node(area, type=\"area\")  \n",
    "                # G.add_edge(questao['descricao'], area)\n",
    "                print(f\"Área '{area}' não encontrada para a questão de pesquisa '{questao['descricao']}'.\") # type: ignore\n",
    "    else:\n",
    "        print(f\"Questão de pesquisa '{questao['descricao']}' não possui áreas associadas.\") # type: ignore\n",
    "\n",
    "# Análise 1: Agrupamento de Pesquisadores\n",
    "# Extrair características textuais dos currículos (ex: usando TF-IDF)\n",
    "# Exemplo usando a descrição da formação acadêmica\n",
    "corpus = [pesquisador['Formação']['Acadêmica'][0]['Descrição'] for pesquisador in curriculos]  \n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Calcular similaridade entre pesquisadores (ex: usando cosseno)\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "\n",
    "# Aplicar algoritmo de agrupamento (ex: DBSCAN)\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=0.3, min_samples=2).fit(similarity_matrix)\n",
    "labels = clustering.labels_\n",
    "\n",
    "# Adicionar atributo 'cluster' aos nós dos pesquisadores\n",
    "for i, pesquisador in enumerate(curriculos):\n",
    "    G.nodes[pesquisador['Identificação']['ID Lattes']]['cluster'] = labels[i]\n",
    "\n",
    "\n",
    "# Análise 2: Agrupamento de Questões de Pesquisa\n",
    "# Extrair características textuais das questões de pesquisa (ex: usando TF-IDF)\n",
    "corpus_questoes = [questao['descricao'] for questao in questoes_pesquisa] # type: ignore\n",
    "vectorizer_questoes = TfidfVectorizer()\n",
    "X_questoes = vectorizer_questoes.fit_transform(corpus_questoes)\n",
    "\n",
    "# Calcular similaridade entre questões (ex: usando cosseno)\n",
    "similarity_matrix_questoes = cosine_similarity(X_questoes)\n",
    "\n",
    "# Aplicar algoritmo de agrupamento (ex: K-Means)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 5  # Defina o número de clusters desejado\n",
    "clustering_questoes = KMeans(n_clusters=n_clusters).fit(similarity_matrix_questoes)\n",
    "labels_questoes = clustering_questoes.labels_\n",
    "\n",
    "# Adicionar atributo 'cluster' aos nós das questões de pesquisa\n",
    "for i, questao in enumerate(questoes_pesquisa):\n",
    "    G.nodes[questao['descricao']]['cluster'] = labels_questoes[i] # type: ignore\n",
    "\n",
    "# Análise 3: Recomendação de Projetos (exemplo simplificado)\n",
    "def recomendar_projetos(pesquisador_id):\n",
    "    pesquisador_areas = list(G.neighbors(pesquisador_id))  # Obter áreas do pesquisador\n",
    "    projetos_recomendados = []\n",
    "    for projeto_id in G.nodes:\n",
    "        if G.nodes[projeto_id]['type'] == \"projeto\":\n",
    "            projeto_areas = list(G.neighbors(projeto_id))\n",
    "            if set(pesquisador_areas) & set(projeto_areas):  # Verificar se há áreas em comum\n",
    "                projetos_recomendados.append(projeto_id)\n",
    "    return projetos_recomendados\n",
    "\n",
    "# Análise 4: Detecção de Oportunidades\n",
    "def detectar_oportunidades(G, produtos_prioritarios, top_n=5):\n",
    "    areas_importantes = {}\n",
    "    for produto in produtos_prioritarios:\n",
    "        for area in G.neighbors(produto['nome']):\n",
    "            areas_importantes[area] = areas_importantes.get(area, 0) + 1\n",
    "\n",
    "    # Ponderar pela concentração de pesquisadores e questões de pesquisa\n",
    "    for area in areas_importantes:\n",
    "        pesquisadores_na_area = len([n for n in G.neighbors(area) if G.nodes[n]['type'] == \"pesquisador\"])\n",
    "        questoes_na_area = len([n for n in G.neighbors(area) if G.nodes[n]['type'] == \"questao_pesquisa\"])\n",
    "        areas_importantes[area] *= (pesquisadores_na_area + questoes_na_area)\n",
    "\n",
    "    # Ordenar áreas por importância\n",
    "    areas_importantes = dict(sorted(areas_importantes.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return list(areas_importantes.keys())[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "print(stopwords.words('portuguese'))\n",
    "print(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade httpx\n",
    "# !pip install --upgrade httpcore\n",
    "# !pip install --upgrade googletrans==4.0.0-rc1\n",
    "# !pip install deep-translator\n",
    "\n",
    "import nltk\n",
    "print(nltk.data.find(\"corpora/stopwords\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_path = nltk.data.find(\"corpora/stopwords\")\n",
    "stopwords_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from deep_translator import GoogleTranslator  # Import the Googletrans library\n",
    "\n",
    "def identify_researcher_topics(data, fields, translate=False, target_language='en'):\n",
    "    \"\"\"\n",
    "    Identifies the top 3 research topics of a researcher based on their Lattes CV data.\n",
    "\n",
    "    Args:\n",
    "        data: A dictionary containing the researcher's Lattes CV data.\n",
    "        fields: A list of field names to be used for topic identification.\n",
    "\n",
    "    Returns:\n",
    "        A list of the top 3 research topics.\n",
    "    \"\"\"\n",
    "\n",
    "    def translate_text(text, target_language):\n",
    "        \"\"\"\n",
    "        Translates the text to the target language using Google Translate.\n",
    "        \"\"\"\n",
    "        translated = GoogleTranslator(source='auto', target=target_language).translate(text)\n",
    "        return translated\n",
    "\n",
    "    def extract_text_from_fields(data, fields, corpus):\n",
    "        \"\"\"\n",
    "        Recursively extracts text from the specified fields in the data dictionary.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if key in fields:\n",
    "                    if isinstance(value, list):\n",
    "                        for item in value:\n",
    "                            corpus.append(item.get(\"Descricao\", \"\") + item.get(\"titulo\", \"\"))\n",
    "                    else:\n",
    "                        corpus.append(str(value))  # Convert non-string values to string\n",
    "                else:\n",
    "                    extract_text_from_fields(value, fields, corpus)\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                extract_text_from_fields(item, fields, corpus)\n",
    "\n",
    "    # def preprocess_text(text, target_language='en'):\n",
    "    #     \"\"\"\n",
    "    #     Preprocesses the text by tokenizing, removing stopwords and proper nouns, translating, and removing punctuation.\n",
    "    #     \"\"\"\n",
    "    #     # Tokenize the text\n",
    "    #     words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    #     # Remove stopwords in the original language (if available)\n",
    "    #     source_language = detect(text)\n",
    "    #     try:\n",
    "    #         words = [word for word in words if word not in stopwords.words(source_language) and not word.istitle()]\n",
    "    #     except OSError:\n",
    "    #         print(f\"Warning: Stopwords not found for language '{source_language}'. Skipping stopword removal.\")\n",
    "\n",
    "    #     # Translate the text (if it's not already in the target language)\n",
    "    #     if target_language != 'auto' and source_language != target_language:\n",
    "    #         translator = GoogleTranslator(source=source_language, target=target_language)\n",
    "    #         text = translator.translate(text)\n",
    "\n",
    "    #         # Tokenize the translated text\n",
    "    #         words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    #         # Remove stopwords in the target language\n",
    "    #         words = [word for word in words if word not in stopwords.words(target_language)]\n",
    "\n",
    "    #     # Remove punctuation and numbers\n",
    "    #     words = [re.sub(r'[^\\w\\s]', '', word) for word in words if not word.isdigit()]\n",
    "\n",
    "    #     return \" \".join(words)\n",
    "\n",
    "    def preprocess_text(text, target_language='en'):\n",
    "        \"\"\"\n",
    "        Preprocesses the text by tokenizing, removing stopwords and proper nouns, translating, and removing punctuation.\n",
    "        \"\"\"\n",
    "        # Tokenize the text\n",
    "        words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "        # Remove stopwords in the original language (if available)\n",
    "        source_language = detect(text)\n",
    "        stopwords_path = nltk.data.find(\"corpora/stopwords\")\n",
    "        # stopwords_path = os.path.join(os.getenv('APPDATA'), 'Roaming', 'nltk_data', 'corpora', 'stopwords')  # Get stopwords path\n",
    "        if os.path.exists(os.path.join(str(stopwords_path), source_language)):\n",
    "            with open(os.path.join(str(stopwords_path), source_language), 'r', encoding='utf-8') as f:\n",
    "                stop_words = set(f.read().splitlines())\n",
    "            words = [word for word in words if word not in stop_words and not word.istitle()]\n",
    "\n",
    "        # Translate the text (if it's not already in the target language)\n",
    "        if target_language != 'auto' and source_language != target_language:\n",
    "            translator = GoogleTranslator(source=source_language, target=target_language)\n",
    "            text = translator.translate(text)\n",
    "\n",
    "            # Tokenize the translated text\n",
    "            words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "            # Remove stopwords in the target language\n",
    "            with open(os.path.join(str(stopwords_path), target_language), 'r', encoding='utf-8') as f:\n",
    "                stop_words = set(f.read().splitlines())\n",
    "            words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Remove punctuation and numbers\n",
    "        words = [re.sub(r'[^\\w\\s]', '', word) for word in words if not word.isdigit()]\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "    # Concatenate text from specified fields\n",
    "    corpus = []\n",
    "    extract_text_from_fields(data, fields, corpus)\n",
    "\n",
    "    # Preprocess the corpus\n",
    "    corpus = [preprocess_text(text) for text in corpus]\n",
    "\n",
    "    # Vectorize the text using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Apply LDA for topic modeling\n",
    "    lda = LatentDirichletAllocation(n_components=7, random_state=0)\n",
    "    lda.fit(X)\n",
    "\n",
    "    # Get the top words for each topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words.append([feature_names[i] for i in topic.argsort()[:-4:-1]])\n",
    "\n",
    "    return top_words\n",
    "\n",
    "pathfilename = os.path.join(folder_data_input,'docents_dict_list.json')\n",
    "with open(pathfilename, 'r', encoding='utf-8') as file:\n",
    "    docents_data = json.load(file)\n",
    "    print(f'{len(docents_data)} currículos carregados')\n",
    "\n",
    "# fields_to_use = [\"Formação Acadêmica\", \"Atuação Profissional\", \"Produções\"]\n",
    "fields_to_use = [\"titulo\"]\n",
    "\n",
    "for researcher in docents_data:\n",
    "    topics = identify_researcher_topics(researcher, fields_to_use)\n",
    "    print(f\"Principais tópicos de interesse para {researcher['Identificação']['Nome']}:\")\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"  Tópico {i+1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "oportunidades = detectar_oportunidades(G, produtos_prioritarios)\n",
    "print(\"Áreas de oportunidade:\", oportunidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo\n",
    "import requests\n",
    "\n",
    "# Cabeçalhos para a requisição\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "import http.client\n",
    "import ssl\n",
    "\n",
    "# Cria um contexto SSL sem verificação de certificado\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "# conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\")\n",
    "\n",
    "conn.request(\"GET\", \"/cities/filters?language=en\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import pandas as pd\n",
    "import ssl\n",
    "\n",
    "# Cria um contexto SSL sem verificação de certificado (NÃO RECOMENDADO PARA PRODUÇÃO)\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "# Função para fazer a requisição com tratamento de erro RETORNA OBJETO BYTES\n",
    "# def fazer_requisicao(conn, endpoint, method=\"GET\", params=None, body=None):\n",
    "#     try:\n",
    "#         # Cabeçalhos para a requisição\n",
    "#         headers = {\n",
    "#             \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "#             \"Accept\": \"application/json\",\n",
    "#         }\n",
    "#         conn.request(method, endpoint, body=body, headers=headers)\n",
    "#         res = conn.getresponse()\n",
    "#         data = res.read()\n",
    "#         return json.loads(data.decode(\"utf-8\"))\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro na requisição: {e}\")\n",
    "#         return None\n",
    "\n",
    "# Função para fazer a requisição com tratamento de erro e decodificação de bytes\n",
    "def fazer_requisicao(conn, endpoint, method=\"GET\", params=None, body=None):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        }\n",
    "        conn.request(method, endpoint, body=body, headers=headers)\n",
    "        res = conn.getresponse()\n",
    "\n",
    "        # Lê os dados da resposta como bytes\n",
    "        data = res.read()  \n",
    "\n",
    "        # Decodifica os bytes para uma string UTF-8\n",
    "        data_str = data.decode(\"utf-8\")  \n",
    "\n",
    "        # Converte a string JSON para um dicionário Python\n",
    "        return json.loads(data_str)  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na requisição: {e}\")\n",
    "        return None\n",
    "\n",
    "# Conexão com a API\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "\n",
    "# Endpoint para obter os anos disponíveis\n",
    "anos_url = \"/cities/dates/years\"\n",
    "anos_response = fazer_requisicao(conn, anos_url)\n",
    "\n",
    "# Inicializa as variáveis com valores padrão\n",
    "ano_inicial = 1997  # Ano inicial padrão\n",
    "ano_final = 2024   # Ano final padrão (ou o ano atual)\n",
    "\n",
    "if anos_response:\n",
    "    if anos_response.get('success', False):  # Verifica se a requisição foi bem-sucedida\n",
    "        if 'data' in anos_response and 'min' in anos_response['data'] and 'max' in anos_response['data']:\n",
    "            ano_inicial = int(anos_response['data']['min'])\n",
    "            ano_final = int(anos_response['data']['max'])\n",
    "        else:\n",
    "            print(\"Erro: As chaves 'data', 'min' e/ou 'max' não foram encontradas na resposta da API.\")\n",
    "    else:\n",
    "        print(f\"Erro na requisição para obter os anos: {anos_response}\")\n",
    "else:\n",
    "    print(\"Erro na requisição para obter os anos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anos_response['data']['min']) # type: ignore\n",
    "print(anos_response['data']['max']) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.request(\"GET\", \"/cities/filters?language=en\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint para obter os valores do filtros disponíveis na API\n",
    "endpoint = \"/cities/filters\"\n",
    "filters_params = {\"language\": \"pt\"}\n",
    "filters_response = fazer_requisicao(conn, endpoint, params=filters_params)\n",
    "filters_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtros = [x['filter'] for x in filters_response.get('data').get('list')] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_options_filters(api_filter):\n",
    "    # Cria um contexto SSL sem verificação de certificado\n",
    "    context = ssl._create_unverified_context()\n",
    "    conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "    conn.request(\"GET\", f\"/general/filters/{api_filter}?language=pt\")\n",
    "\n",
    "    res = conn.getresponse()\n",
    "    # print(f'objeto       res: {type(res)}')\n",
    "    data = res.read()\n",
    "    # print(f'objeto      data: {type(data)}')\n",
    "    data_str = data.decode(\"utf-8\")\n",
    "    # print(f'objeto  data_str: {type(data_str)}')\n",
    "    data_json = json.loads(data_str)\n",
    "    # print(f'objeto data_json: {type(data_json)}')\n",
    "    try:\n",
    "        results = [x.get('text') for x in data_json.get('data')[0]]\n",
    "        print(f'{len(results):>4} resultados para filtro {api_filter}')\n",
    "    except Exception as e:\n",
    "        print(f'     Erro ao buscar dados para filtro {api_filter}: {e}')\n",
    "    return [results][0]\n",
    "\n",
    "todos_campos_filtro={}\n",
    "for n,i in enumerate(filtros):\n",
    "    try:\n",
    "        todos_campos_filtro[i] = get_options_filters(i)\n",
    "    except Exception as e:\n",
    "        print(f'     Filtro {i} não disponível na API. Erro: {e}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicos=[]\n",
    "[unicos.append(x) for x in todos_campos_filtro['economicBlock'] if x not in unicos]\n",
    "unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicos=[]\n",
    "[unicos.append(x) for x in todos_campos_filtro['state'] if x not in unicos]\n",
    "unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_interesse = [\n",
    "    'VI - Produtos das indústrias químicas ou indústrias conexas', \n",
    "    'XVIII - Instrumentos e aparelhos de ótica, fotografia ou cinematografia, medida, controle ou de precisão; Instrumentos e aparelhos médico-cirúrgicos; Relógios e aparelhos semelhantes; Instrumentos musicais; Suas partes e acessórios',\n",
    "    'XX - Mercadorias e produtos diversos',\n",
    "    'XXII - Transações especiais'\n",
    "    ]\n",
    "\n",
    "chapter_interesse = [\n",
    "    '28 - Produtos químicos inorgânicos; compostos inorgânicos ou orgânicos de metais preciosos, de elementos radioativos, de metais das terras raras ou de isótopos',\n",
    "    '29 - Produtos químicos orgânicos',\n",
    "    '30 - Produtos farmacêuticos',\n",
    "    '35 - Matérias albuminóides; produtos à base de amidos ou de féculas modificados; colas; enzimas',\n",
    "    '38 - Produtos diversos das indústrias químicas',\n",
    "    ]\n",
    "\n",
    "heading_interesse = [\n",
    "    '2801 - Flúor, cloro, bromo e iodo',\n",
    "    '2802 - Enxofre sublimado ou precipitado; enxofre coloidal',\n",
    "    '2803 - Carbono (negros-de-carbono e outras formas não compreendidas em outras posições)',\n",
    "    '2804 - Hidrogénio, gases raros e outros elementos não metálicos',\n",
    "    '2805 - Metais alcalinos ou alcalino-terrosos; metais de terras raras, escândio e ítrio, mesmo misturados ou ligados entre si; mercúrio',\n",
    "    '2806 - Cloreto de hidrogénio (ácido clorídrico); ácido clorossulfúrico',\n",
    "    '2807 - Ácido sulfúrico e ácido sulfúrico fumante (oleum)',\n",
    "    '2808 - Ácido nítrico; ácidos sulfonítricos',\n",
    "    '2809 - Pentóxido de difosfóro; ácido fosfórico; ácidos polifosfóricos, de constituição química definida ou não',\n",
    "    '2810 - Óxidos de boro; ácidos bóricos',\n",
    "    '2811 - Outros ácidos inorgânicos e outros compostos oxigenados inorgânicos dos elementos não metálicos',\n",
    "    '2812 - Halogenetos e oxialogenetos dos elementos não metálicos',\n",
    "    '2813 - Sulfuretos dos elementos não metálicos; trissulfureto de fósforo comercial',\n",
    "    '2814 - Amoníaco anidro ou em solução aquosa (amónia)',\n",
    "    '2815 - Hidróxido de sódio (soda cáustica); hidróxido de potássio (potassa cáustica); peróxidos de sódio ou de potássio',\n",
    "    '2816 - Hidróxido e peróxido de magnésio; óxidos, hidróxidos e peróxidos, de estrôncio ou de bário',\n",
    "    '2817 - Óxido de zinco; peróxido de zinco',\n",
    "    '2818 - Corindo artificial, quimicamente definido ou não; óxido de alumínio; hidróxido de alumínio',\n",
    "    '2819 - Óxidos e hidróxidos de crómio',\n",
    "    '2820 - Óxidos de manganés',\n",
    "    '2821 - Óxidos e hidróxidos de ferro; terras corantes contendo, em peso, 70\\xa0% ou mais de ferro combinado, expresso em Fe2O3',\n",
    "    '2822 - Óxidos e hidróxidos de cobalto, inclusive os comerciais',\n",
    "    '2823 - Óxidos de titânio',\n",
    "    '2824 - Óxidos de chumbo; mínio (zarcão) e mínio-laranja (mine-orange)',\n",
    "    '2825 - Hidrazina e hidroxilamina, e seus sais inorgânicos; outras bases inorgânicas; outros óxidos, hidróxidos e peróxidos, de metais',\n",
    "    '2826 - Fluoretos; fluorossilicatos, fluoroaluminatos e outros sais complexos de flúor',\n",
    "    '2827 - Cloretos, oxicloretos e hidroxicloretos; brometos e oxibrometos; iodetos e oxiiodetos',\n",
    "    '2828 - Hipocloritos; hipoclorito de cálcio comercial; cloritos; hipobromitos',\n",
    "    '2829 - Cloratos e percloratos; bromatos e perbromatos; iodatos e periodatos',\n",
    "    '2830 - Sulfuretos; polissulfuretos, de constituição química definida ou não',\n",
    "    '2831 - Ditionites e sulfoxilatos',\n",
    "    '2832 - Sulfitos; tiosulfatos',\n",
    "    '2833 - Sulfatos; alúmenes; peroxosulfatos (persulfatos)',\n",
    "    '2834 - Nitritos; nitratos',\n",
    "    '2835 - Fosfinatos (hipofosfitos), fosfonatos (fosfitos) e fosfatos; polifosfatos, de constituição química definida ou não:',\n",
    "    '2836 - Carbonatos; peroxocarbonatos (percarbonatos); carbonato de amónio comercial contendo carbamato de amónio',\n",
    "    '2837 - Cianetos, oxicianetos e cianetos complexos',\n",
    "    '2838 - Fulminatos, cianatos e tiocianatos',\n",
    "    '2839 - Silicatos; silicatos dos metais alcalinos comerciais',\n",
    "    '2840 - Boratos; peroxoboratos (perboratos)',\n",
    "    '2841 - Sais dos ácidos oxometálicos ou peroxometálicos',\n",
    "    '2842 - Outros sais dos ácidos ou peroxoácidos inorgânicos (incluindo aluminossilicatos de constituição química definida ou não), exceto azidas',\n",
    "    '2843 - Metais preciosos no estado coloidal; compostos inorgânicos ou orgânicos de metais preciosos, de constituição química definida ou não; amálgamas de metais preciosos',\n",
    "    '2844 - Elementos químicos radioactivos e isótopos radioactivos (incluídos os elementos químicos e isótopos cindíveis ou férteis), e seus compostos; misturas e resíduos contendo esses produtos',\n",
    "    '2845 - Isótopos não incluídos na posição\\xa02844; seus compostos inorgânicos ou orgânicos, de constituição química definida ou não',\n",
    "    '2846 - Compostos, inorgânicos ou orgânicos, dos metais das terras raras, de ítrio ou de escândio ou das misturas destes metais',\n",
    "    '2847 - Peróxido de hidrogênio (água oxigenada), mesmo solidificado com ureia',\n",
    "    '2848 - Fosfetos, exceto ferrofósforos, quimicamente definidos ou não',\n",
    "    '2849 - Carbonetos de constituição química definida ou não',\n",
    "    '2850 - Hidretos, nitretos, azidas, silicietos e boretos, quimicamente definidos ou não',\n",
    "    '2851 - Compostos inorgânicos nesoi: liq ar: amálgamas nesoi',\n",
    "    '2852 - Compostos, inorgânicos ou orgânicos, de mercúrio, de constituição química definida ou não, exceto as amálgamas',\n",
    "    '2853 - Outros compostos inorgânicos (incluídas as águas destiladas, de condutibilidade ou de igual grau de pureza); ar líquido (incluído o ar líquido cujos gases raros foram eliminados); ar comprimido; amálgamas, exceto de metais preciosos.',\n",
    "    '2901 - Hidrocarbonetos acíclicos',\n",
    "    '2902 - Hidrocarbonetos cíclicos',\n",
    "    '2903 - Derivados halogenados dos hidrocarbonetos',\n",
    "    '2904 - Derivados sulfonados, nitrados ou nitrosados dos hidrocarbonetos, mesmo halogenados',\n",
    "    '2905 - Álcoois acíclicos e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2906 - Álcoois cíclicos e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2907 - Fenóis; fenóis-álcoois',\n",
    "    '2908 - Derivados halogenados, sulfonados, nitrados ou nitrosados dos fenóis ou dos fenóis-álcoois',\n",
    "    '2909 - Éteres, éteres-álcoois, éteres-fenóis, éteres-álcoois-fenóis, peróxidos de álcoois, peróxidos de éteres, peróxidos de cetonas (de constituição química definida ou não), e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2910 - Epóxidos, epoxi-álcoois, epoxi-fenóis e epoxi-éteres, com três átomos no ciclo, e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2911 - Acetais, semi-acetais, mesmo contendo outras funções oxigenadas, e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2912 - Aldeídos, mesmo contendo outras funções oxigenadas; polímeros cíclicos dos aldeídos; paraformaldeído',\n",
    "    '2913 - Derivados halogenados, sulfonados, nitrados ou nitrosados dos produtos da posição 2912',\n",
    "    '2914 - Cetonas e quinonas, mesmo contendo outras funções oxigenadas, e seus derivados halogenados, sulfonados, nitratos ou nitrosados',\n",
    "    '2915 - Ácidos monocarboxílicos acíclicos saturados e seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2916 - Ácidos monocarboxílicos acíclicos não saturados e ácidos monocarboxílicos cíclicos, seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2917 - Ácidos policarboxílicos, seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2918 - Ácidos carboxílicos contendo funções oxigenadas suplementares e seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2919 - Ésteres fosfóricos e seus sais, incluindo os lactofosfatos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2920 - Ésteres de outros ácidos inorgânicos de não-metais (exceto os ésteres de halogenetos de hidrogénio) e seus sais; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2921 - Compostos de função amina',\n",
    "    '2922 - Compostos aminados de funções oxigenadas',\n",
    "    '2923 - Sais e hidróxidos de amónio quaternários; lecitinas e outros fosfoaminolípidos, de constitução química definida ou não',\n",
    "    '2924 - Compostos de função carboxiamida; compostos de função amida do ácido carbónico',\n",
    "    '2925 - Compostos de função carboxiimida (incluindo a sacarina e seus sais) ou de função imina',\n",
    "    '2926 - Compostos de função nitrilo',\n",
    "    '2927 - Compostos diazóicos, azóicos e azóxicos',\n",
    "    '2928 - Derivados orgânicos da hidrazina e hidroxilamina',\n",
    "    '2929 - Compostos de outras funções azotadas (nitrogenadas)',\n",
    "    '2930 - Tiocompostos orgânicos',\n",
    "    '2931 - Outros compostos organo-inorgânicos',\n",
    "    '2932 - Compostos heterocíclicos exclusivamente de hetero-átomo(s) de oxigénio',\n",
    "    '2933 - Compostos heterocíclicos, exclusivamente de hetero-átomo(s) de azoto (nitrogénio)',\n",
    "    '2934 - Ácidos nucleicos e seus sais, de constituição química definida ou não; outros compostos heterocíclicos',\n",
    "    '2935 - Sulfonamidas',\n",
    "    '2936 - Provitaminas e vitaminas, naturais ou sintéticas (incluídos os concentrados naturais), bem como os seus derivados utilizados principalmente como vitaminas, misturados ou não entre si, mesmo em quaisquer soluções',\n",
    "    '2937 - Hormonas, prostaglandinas, tromboxanos e leucotrienos, naturais ou reproduzidos por síntese; seus derivados e análogos estruturais, incluindo os polipéptidos de cadeia modificada, utilizados principalmente como hormonas',\n",
    "    '2938 - Heterósidos, naturais ou sintéticos, seus sais, éteres, ésteres e outros derivados',\n",
    "    '2939 - Alcalóides vegetais, naturais ou sintéticos, seus sais, éteres, ésteres e outros derivados',\n",
    "    '2940 - Açúcares quimicamente puros, exceto sacarose, lactose, maltose, glicose e frutose; seus éteres e ésteres e seus sais',\n",
    "    '2941 - Antibióticos',\n",
    "    '2942 - Outros compostos orgânicos',\n",
    "    '3001 - Glândulas e outros órgãos para usos opoterápicos, dessecados, mesmo em pó; extractos de glândulas ou de outros órgãos ou das suas secreções, para usos opoterápicos; heparina e seus sais; outras substâncias humanas ou animais preparadas para fins terapêuti',\n",
    "    '3002 - Sangue humano; sangue animal preparado para usos terapêuticos, profilácticos ou de diagnóstico; anti-soros, outras fracções do sangue, produtos imunológicos modificados, mesmo obtidos por via biotecnológica; vacinas, toxinas, culturas de microrganismos (e',\n",
    "    '3003 - Medicamentos (exceto os produtos das posições\\xa03002, 3005\\xa0ou\\xa03006) constituídos por produtos misturados entre si, preparados para fins terapêuticos ou profilácticos, mas não apresentados em doses nem acondicionados para venda a retalho',\n",
    "    '3004 - Medicamentos (exceto os produtos das posições\\xa03002, 3005\\xa0ou\\xa03006) constituídos por produtos misturados ou não misturados, preparados para fins terapêuticos ou profilácticos, apresentados em doses (incluindo os destinados a serem administrados por via sub',\n",
    "    '3005 - Pastas (ouates), gazes, ataduras e artigos análogos (por exemplo: pensos, esparadrapos, sinapismos), impregnados ou recobertos de substâncias farmacêuticas ou acondicionados para venda a retalho para usos medicinais, cirúrgicos, dentários ou veterinários',\n",
    "    '3006 - Preparações e artigos farmacêuticos indicados na Nota\\xa04\\xa0do presente capítulo',\n",
    "    '3501 - Caseínas, caseinatos e outros derivados das caseínas; colas de caseína',\n",
    "    '3502 - Albuminas (incluídos os concentrados de várias proteínas de soro de leite, contendo, em peso calculado sobre matéria seca, mais de\\xa080\\xa0% de proteínas do soro de leite), albuminatos e outros derivados das albuminas',\n",
    "    '3503 - Gelatinas e seus derivados; ictiocola e outras colas de origem animal, exceto cola de caseína',\n",
    "    '3504 - Peptonas e seus derivados; outras matérias protéicas e seus derivados; pó de peles',\n",
    "    '3505 - Dextrina e outros amidos e féculas modificados (por exemplo: amidos e féculas pré-gelatinizados ou esterificados); colas à base de amidos ou de féculas, de dextrina ou de outros amidos ou féculas modificados',\n",
    "    '3506 - Colas e outros adesivos preparados, não especificados nem compreendidos em outras posições; produtos de qualquer espécie utilizados como colas ou adesivos, acondicionados para venda a retalho como colas ou adesivos, com peso líquido não superior a\\xa01\\xa0kg',\n",
    "    '3507 - Enzimas; enzimas preparadas não especificadas nem compreendidas em outras posições',\n",
    "    '3821 - Meios de cultura preparados para o desenvolvimento e a manutenção de microrganismos (incluindo os vírus e os organismos similares) ou de células vegetais, humanas ou animais',\n",
    "    '3822 - Reagentes de diagnóstico ou de laboratório, em qualquer suporte ou preparados, exceto os das posições 3002 ou 3006; materiais de referência certificados',\n",
    "    '3823 - Ácidos gordos monocarboxílicos industriais; óleos ácidos de refinação; alcoóis gordos industriais',\n",
    "    '3824 - Aglutinantes preparados para moldes ou para núcleos de fundição; produtos químicos e preparações das indústrias químicas ou das indústrias conexas (incluídos os constituídos por misturas de produtos naturais), não especificados nem compreendidos noutras p',\n",
    "    '3825 - Produtos residuais das indústrias químicas ou das indústrias conexas, não especificados nem compreendidos em outras posições; resíduos municipais; lamas de depuração; outros resíduos mencionados na Nota\\xa06 do presente capítulo',\n",
    "    '3901 - Polímeros de etileno, em formas primárias',\n",
    "    '3902 - Polímeros de propileno ou de outras olefinas, em formas primárias',\n",
    "    '3903 - Polímeros de estireno, em formas primárias',\n",
    "    '3904 - Polímeros de cloreto de vinilo ou de outras olefinas halogenadas, em formas primárias',\n",
    "    '3905 - Polímeros de acetato de vinilo ou de outros ésteres de vinilo, em formas primárias; outros polímeros de vinilo, em formas primárias',\n",
    "    '3906 - Polímeros acrílicos, em formas primárias',\n",
    "    '3907 - Poliacetais, outros poliéteres e resinas epóxidas, em formas primárias; policarbonatos, resinas alquídicas, poliésteres alílicos e outros poliésteres, em formas primárias',\n",
    "    '3908 - Poliamidas em formas primárias',\n",
    "    '3909 - Resinas amínicas, resinas fenólicas e poliuretanos, em formas primárias',\n",
    "    '3910 - Silicones, em formas primárias',\n",
    "    '8417 - Fornos industriais ou de laboratório, incluídos os incineradores, não elétricos',\n",
    "    '8418 - Refrigeradores, congeladores (freezers) e outro material, máquinas e aparelhos para a produção de frio, com equipamento eléctrico ou outro; bombas de calor, excluídas as máquinas e aparelhos de ar condicionado da posição 8415',\n",
    "    '8419 - Aparelhos e dispositivos, mesmo aquecidos electricamente (exceto fornos e outros aparelhos da posição 8514), para tratamento de matérias por meio de operações que impliquem mudança de temperatura, tais como o aquecimento, cozimento, torrefacção, destilaç',\n",
    "    '8420 - Calandras e laminadores, exceto os destinados ao tratamento de metais ou vidro, e seus cilindros',\n",
    "    '8421 - Centrifugadores, incluídos os secadores centrífugos, aparelhos para filtrar ou depurar líquidos ou gases',\n",
    "    '8423 - Aparelhos e instrumentos de pesagem, incluídas as básculas e balanças para verificar peças fabricadas, excluídas as balanças sensíveis a pesos não superiores a 5 cg; pesos para quaisquer balanças',\n",
    "    '8471 - Máquinas automáticas para processamento de dados e suas unidades; leitores magnéticos ou ópticos, máquinas para registar dados em suporte sob forma codificada, e máquinas para processamento desses dados, não especificadas nem compreendidas em outras posiç',\n",
    "    '8472 - Outras máquinas e aparelhos de escritório [por exemplo: duplicadores hectográficos ou a stencil, máquinas para imprimir endereços, distribuidores automáticos de papel-moeda, máquinas para seleccionar, contar ou empacotar moedas, afiadores (apontadores) me',\n",
    "    '9011 - Microscópios ópticos, incluídos os microscópios para fotomicrografia, cinefotomicrografia ou microprojecção',\n",
    "    '9012 - Microscópios, exceto ópticos; difractógrafos',\n",
    "    '9013 - Dispositivos de cristais líquidos que não constituam artigos compreendidos mais especificamente em outras posições; lasers, exceto díodos laser; outros aparelhos e instrumentos de óptica, não especificados nem compreendidos em outras posições do presente',\n",
    "    '9016 - Balanças sensíveis a pesos >= 5 cg, com ou sem pesos',\n",
    "    '9021 - Artigos e aparelhos ortopédicos, incluídas as cintas e fundas médico-cirúrgicas e as muletas; talas, goteiras e outros artigos e aparelhos para fracturas; artigos e aparelhos de prótese; aparelhos para facilitar a audição dos surdos e outros aparelhos par',\n",
    "    '9022 - Aparelhos de raios X e aparelhos que utilizem as radiações alfa, beta ou gama, mesmo para usos médicos, cirúrgicos, odontológicos ou veterinários, incluídos os aparelhos de radiofotografia ou de radioterapia, os tubos de raios X e outros dispositivos gera',\n",
    "    '9023 - Instrumentos, aparelhos e modelos, concebidos para demonstração (por exemplo, no ensino e nas exposições), não suscetíveis de outros usos',\n",
    "    '9024 - Máquinas e aparelhos para ensaios de dureza, tracção, compressão, elasticidade e de outras propriedades mecânicas de materiais (por exemplo: metais, madeira, têxteis, papel, plásticos)',\n",
    "    '9025 - Densímetros, areómetros, pesa-líquidos e instrumentos flutuantes semelhantes, termómetros, pirómetros, barómetros, higrómetros e psicrómetros, registadores ou não, mesmo combinados entre si',\n",
    "    '9026 - Instrumentos e aparelhos para medida ou controlo do caudal (vazão), do nível, da pressão ou de outras características variáveis dos líquidos ou gases (por exemplo: medidores de caudal, indicadores de nível, manómetros, contadores de calor), exceto os ins',\n",
    "    '9027 - Instrumentos e aparelhos para análises físicas ou químicas (por exemplo: polarímetros, refractómetros, espectrómetros, analisadores de gases ou de fumos); instrumentos e aparelhos para ensaios de viscosidade, porosidade, dilatação, tensão superficial ou s',\n",
    "    '9028 - Contadores de gases, de líquidos ou de electricidade, incluídos os aparelhos para a sua aferição',\n",
    "    '9029 - Outros contadores (por exemplo: contadores de voltas, contadores de produção, taxímetros, totalizadores de caminho percorrido, podómetros); indicadores de velocidade e tacómetros, exceto os das posições 9014 ou 9015; estroboscópios',\n",
    "    '9030 - Osciloscópios, analisadores de espectro e outros instrumentos e aparelhos para medida ou controlo de grandezas elétricas; instrumentos e aparelhos para medida ou detecção de radiações alfa, beta, gama, X, cósmicas ou outras radiações ionizantes',\n",
    "    '9031 - Instrumentos, aparelhos e máquinas de medida ou controlo, não especificados nem compreendidos em outras posições do presente capítulo; projectores de perfis',\n",
    "    '9032 - Instrumentos e aparelhos para regulação ou controlo, automáticos',\n",
    "    '9033 - Partes e acessórios não especificados nem compreendidos noutras posições do presente Capítulo, para máquinas, aparelhos, instrumentos ou artigos do Capítulo 90',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "conn.request(\"GET\", \"/tables/ncm/02042200\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "conn.request(\"GET\", \"/tables/ncm?language=pt\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesse = []\n",
    "# f = filtros[0]\n",
    "# print(f)\n",
    "# for n,i in enumerate(todos_campos_filtro.get(f)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Endpoint para obter os valores por filtro disponíveis na API\n",
    "# # Lista os valores disponíveis para um filtro específico. Exemplos de como acessar os valores possíveis por diferentes filtros:\n",
    "# # Países: /general/filters/country?language=pt\n",
    "# # Blocos Econômicos: /general/filters/economicBlock?language=pt\n",
    "# # Seções (do Sistema Harmonizado - SH): /general/filters/section?language=pt\n",
    "# # NCM (Nomenclatura Comum do Mercosul): /general/filters/ncm?language=pt\n",
    "\n",
    "# import http.client\n",
    "\n",
    "# # Cria um contexto SSL sem verificação de certificado\n",
    "# context = ssl._create_unverified_context()\n",
    "# conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "# conn.request(\"GET\", \"/general/filters/heading?language=pt\")\n",
    "\n",
    "# res = conn.getresponse()\n",
    "# print(f'objeto       res: {type(res)}')\n",
    "# data = res.read()\n",
    "# print(f'objeto      data: {type(data)}')\n",
    "# data_str = data.decode(\"utf-8\")\n",
    "# print(f'objeto  data_str: {type(data_str)}')\n",
    "# data_json = json.loads(data_str)\n",
    "# print(f'objeto data_json: {type(data_json)}')\n",
    "\n",
    "# Lista de campos no filtro heading\n",
    "# heading_list = [y.get('text') for y in data_json.get('data')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de campos no filtro chapter\n",
    "campos = ['country', 'economicBlock', 'state', 'city', 'heading', 'chapter', 'section']\n",
    "filter='economicBlock'\n",
    "api_filters = ['']\n",
    "list_country = get_options_filters('country')\n",
    "list_economicBlock = get_options_filters('economicBlock')\n",
    "list_state = get_options_filters('state')\n",
    "list_heading = get_options_filters('heading')\n",
    "list_chapter = get_options_filters('chapter')\n",
    "list_section = get_options_filters('section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Inicializa a lista de NCMs de saúde com valores padrão\n",
    "# ncms_saude = []  # Lista vazia caso a requisição falhe\n",
    "\n",
    "# if ncm_response:\n",
    "#     ncms = ncm_response.json()\n",
    "#     # Filtrar os NCMs relacionados à saúde (exemplo)\n",
    "#     ncms_saude = [ncm['id'] for ncm in ncms if ncm['desc'].startswith(\"Medicamentos\")]\n",
    "\n",
    "# # Endpoint para consulta dos dados\n",
    "# consulta_url = \"/cities\"\n",
    "\n",
    "# # Parâmetros da consulta (exemplo)\n",
    "# params = {\n",
    "#     \"flow\": [\"export\", \"import\"],\n",
    "#     \"monthDetail\": False,\n",
    "#     \"period\": {\"from\": f\"{ano_inicial}-01\", \"to\": f\"{ano_final}-12\"},\n",
    "#     \"filters\": [{\"filter\": \"ncm\", \"values\": ncms_saude}],\n",
    "#     \"details\": [\"ncm\"],\n",
    "#     \"metrics\": [\"metricFOB\"],\n",
    "# }\n",
    "\n",
    "# # Realiza a consulta\n",
    "# response = fazer_requisicao(conn, consulta_url, params=params)\n",
    "\n",
    "# if response:\n",
    "#     data = response\n",
    "#     df = pd.DataFrame(data['data'])\n",
    "\n",
    "#     # Calcula o déficit por ano e NCM\n",
    "#     df_pivot = df.pivot_table(\n",
    "#         index=[\"year\", \"ncm\"], columns=\"flow\", values=\"metricFOB\", aggfunc=\"sum\"\n",
    "#     )\n",
    "#     df_pivot[\"deficit\"] = df_pivot[\"import\"] - df_pivot[\"export\"]\n",
    "\n",
    "#     # Salva os resultados em um arquivo CSV\n",
    "#     df_pivot.to_csv(\"deficit_balanca_comercial_saude.csv\")\n",
    "\n",
    "#     print(\"Dados salvos com sucesso!\")\n",
    "\n",
    "# # Fecha a conexão\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Inicializa a lista de NCMs de saúde com valores padrão\n",
    "# # com lista vazia caso a requisição falhe\n",
    "# ncms_saude = []  \n",
    "# ncm_response = []\n",
    "\n",
    "# if ncm_response:\n",
    "#     ncms = ncm_response.json()\n",
    "#     # Filtrar os NCMs relacionados à saúde (exemplo)\n",
    "#     ncms_saude = [ncm['id'] for ncm in ncms if ncm['desc'].startswith(\"Medicamentos\")]\n",
    "#     print(ncms_saude)\n",
    "\n",
    "# # Endpoint para consulta dos dados\n",
    "# consulta_url = \"https://api-comexstat.mdic.gov.br/cities\"\n",
    "\n",
    "# # Parâmetros da consulta (exemplo)\n",
    "# params = {\n",
    "#     \"flow\": [\"export\", \"import\"],\n",
    "#     \"monthDetail\": False,\n",
    "#     \"period\": {\"from\": f\"{ano_inicial}-01\", \"to\": f\"{ano_final}-12\"},\n",
    "#     \"filters\": [{\"filter\": \"ncm\", \"values\": ncms_saude}],\n",
    "#     \"details\": [\"ncm\"],\n",
    "#     \"metrics\": [\"metricFOB\"],\n",
    "# }\n",
    "\n",
    "# # Realiza a consulta\n",
    "# response = fazer_requisicao(consulta_url, json=params)\n",
    "\n",
    "# if response:\n",
    "#     data = response.json()\n",
    "#     df = pd.DataFrame(data)\n",
    "\n",
    "#     # # Calcula o déficit por ano e NCM\n",
    "#     # df_pivot = df.pivot_table(\n",
    "#     #     index=[\"coAno\", \"ncm\"], columns=\"flow\", values=\"metricFOB\", aggfunc=\"sum\"\n",
    "#     # )\n",
    "#     # df_pivot[\"deficit\"] = df_pivot[\"import\"] - df_pivot[\"export\"]\n",
    "\n",
    "#     # # Salva os resultados em um arquivo CSV\n",
    "#     # df_pivot.to_csv(\"deficit_balanca_comercial_saude.csv\")\n",
    "\n",
    "#     # print(\"Dados salvos com sucesso!\")\n",
    "# for i in df['data'].items():\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncm_response = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Gerar ilustrações para modelos pré-treinados</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir parâmetros adequado para trabalhar com tensores na GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O parâmetro return_tensors=\"pt\" em funções como o tokenizer do Hugging Face Transformers indica que você deseja que o tokenizador retorne os resultados da tokenização como tensores PyTorch.\n",
    "\n",
    "PyTorch é uma biblioteca popular de aprendizado de máquina que usa tensores como sua estrutura de dados principal.\n",
    "Por que usar return_tensors=\"pt\"?\n",
    "\n",
    "Tensores são estruturas de dados multidimensionais semelhantes a arrays NumPy, mas com a capacidade de serem processados ​​em GPUs para acelerar cálculos numéricos.\n",
    "\n",
    "A maioria dos modelos do Hugging Face Transformers são implementados em PyTorch. Ao usar return_tensors=\"pt\", você garante que os resultados da tokenização estejam no formato correto para serem alimentados diretamente nesses modelos.\n",
    "\n",
    "Se houver uma GPU dsiponivel, os tensores PyTorch podem ser movidos para a GPU para aproveitar sua capacidade de processamento paralelo e acelerar os cálculos.\n",
    "\n",
    "Alternativas\n",
    "\n",
    "return_tensors=\"tf\": Retorna os resultados como tensores do TensorFlow, outra biblioteca popular de aprendizado de máquina.\n",
    "\n",
    "return_tensors=\"np\": Retorna os resultados como arrays NumPy, que são úteis para algumas operações de pré-processamento ou análise, mas geralmente não são tão eficientes em GPUs quanto os tensores PyTorch.\n",
    "\n",
    "return_tensors=None (padrão): Retorna os resultados como listas de Python, que são mais fáceis de entender e manipular, mas podem ser menos eficientes para alimentar modelos de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"This is a test sentence.\"\n",
    "\n",
    "# Tokenização com tensores PyTorch\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence1 = \"This is a test sentence.\"\n",
    "sentence2 = \"How are you?\"\n",
    "\n",
    "# Tokenização com duas sentenças\n",
    "inputs = tokenizer([sentence1, sentence2], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "for i,j in inputs.items():\n",
    "    print(f\"{i:>15}: {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_pt\", model=\"unicamp-dl/translation-en-pt-t5\")\n",
    "print(translator.tokenizer.model_max_length) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificar pilelines do Spacy para PLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Carregar o modelo en_core_web_trf\n",
    "nlp_en = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Imprimir os nomes dos pipes disponíveis no modelo\n",
    "print(nlp_en.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificar informações básicas para modelos do Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Input [Input]\n",
    "    end\n",
    "\n",
    "    subgraph Embeddings\n",
    "        word_embeddings([\"word_embeddings\"])\n",
    "        position_embeddings([\"position_embeddings\"])\n",
    "        token_type_embeddings([\"token_type_embeddings\"])\n",
    "        LayerNorm([\"LayerNorm\"])\n",
    "        Dropout([\"Dropout\"])\n",
    "    end\n",
    "    \n",
    "    Input --> Embeddings\n",
    "\n",
    "    subgraph Encoder [Encoder]\n",
    "        BertLayers[12 x BertLayer]\n",
    "    end\n",
    "\n",
    "    Embeddings --> Encoder\n",
    "\n",
    "    subgraph Pooler [Pooler]\n",
    "        dense(( Linear ))\n",
    "        activation(( Tanh ))\n",
    "    end\n",
    "\n",
    "    Encoder --> Pooler\n",
    "    \n",
    "    subgraph BertLayer\n",
    "        BertAttention\n",
    "        BertIntermediate\n",
    "        BertOutput\n",
    "        LayerNorm_output([\"LayerNorm\"])\n",
    "        Dropout_output([\"Dropout\"])\n",
    "    end\n",
    "\n",
    "    subgraph BertAttention\n",
    "        SelfAttention\n",
    "        output\n",
    "    end\n",
    "\n",
    "    BertLayer --> BertAttention\n",
    "    BertLayer --> BertIntermediate\n",
    "    \n",
    "    subgraph BertSelfAttention\n",
    "        query(( Linear ))\n",
    "        key(( Linear ))\n",
    "        value(( Linear ))\n",
    "        Dropout([\"Dropout\"])\n",
    "    end\n",
    "\n",
    "    BertAttention --> BertSelfAttention\n",
    "\n",
    "    subgraph BertSelfOutput\n",
    "        dense(( Linear ))\n",
    "        LayerNorm([\"LayerNorm\"])\n",
    "        Dropout([\"Dropout\"])\n",
    "    end\n",
    "\n",
    "    BertSelfAttention --> BertSelfOutput\n",
    "\n",
    "    subgraph BertIntermediate\n",
    "        dense(( Linear ))\n",
    "        intermediate_act_fn(( GELUActivation ))\n",
    "    end\n",
    "\n",
    "    BertSelfOutput --> BertIntermediate\n",
    "\n",
    "    subgraph BertOutput\n",
    "        dense(( Linear ))\n",
    "        LayerNorm([\"LayerNorm\"])\n",
    "        Dropout([\"Dropout\"])\n",
    "    end\n",
    "\n",
    "    %% BertLayers --> BertLayer\n",
    "    \n",
    "    BertAttention -.-> BertSelfOutput  \n",
    "    BertSelfOutput --> LayerNorm_output\n",
    "    LayerNorm_output --> Dropout_output\n",
    "    Dropout_output --> BertLayer \n",
    "    BertLayer -.-> BertOutput \n",
    "    \n",
    "    BertIntermediate --> GELUActivation\n",
    "    BertIntermediate --> BertOutput\n",
    "    BertSelfAttention --> query\n",
    "    BertSelfAttention --> key\n",
    "    BertSelfAttention --> value\n",
    "    BertSelfAttention --> Dropout\n",
    "    \n",
    "    Pooler --> Tanh\n",
    "    Tanh --> pooler_output(( pooler_output ))\n",
    "\n",
    "    BertOutput --> dense\n",
    "    BertOutput --> LayerNorm\n",
    "    BertOutput --> Dropout\n",
    "    BertOutput --> last_hidden_state(( last_hidden_state ))\n",
    "    BertOutput --> hidden_states(( hidden_states ))\n",
    "    BertAttention --> attentions(( attentions ))\n",
    "\n",
    "    %% Embeddings - vertical arrangement\n",
    "    word_embeddings --> position_embeddings\n",
    "    position_embeddings --> token_type_embeddings\n",
    "    token_type_embeddings --> LayerNorm\n",
    "    LayerNorm --> Dropout\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir o nome de cada camada\n",
    "for name, module in model.named_modules():\n",
    "    print(name)\n",
    "\n",
    "# Acessar uma camada específica\n",
    "first_layer = model.get_encoder().layer[0]\n",
    "print(first_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mermaid_diagram_ok(model):\n",
    "    \"\"\"\n",
    "    Generates a Mermaid diagram representing the structure of a Hugging Face model, \n",
    "    collapsing repetitive layers into shapes with layer count and using rounded rectangles for internal layers.\n",
    "\n",
    "    Args:\n",
    "        model: The Hugging Face model to be analyzed.\n",
    "    \"\"\"\n",
    "\n",
    "    diagram = \"graph LR\\n\"\n",
    "    layer_counts = {}\n",
    "\n",
    "    root_nome_name = f\"{model.__class__.__name__}\"\n",
    "\n",
    "    # Adicionar o nó raiz que representa o modelo\n",
    "    diagram += f\"    {root_nome_name}{{ {root_nome_name} }}\\n\"\n",
    "\n",
    "    # Função recursiva para percorrer a estrutura do modelo \n",
    "    def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "        nonlocal diagram, layer_counts\n",
    "        for name, child_module in module.named_children():\n",
    "            current_layer_type = child_module.__class__.__name__\n",
    "            node_name = parent_name + \".\" + name if parent_name else name\n",
    "\n",
    "            ## Elementos filhos repetitivos dentro de um pai, mas precisa detectar dinamicamente o nome do elemento\n",
    "            layer_counts[current_layer_type] = layer_counts.get(current_layer_type, 0) + 1\n",
    "\n",
    "            if layer_counts[current_layer_type] == 1:\n",
    "                # Use rounded rectangle for single occurrences or non-repetitive layers\n",
    "                diagram += f\"    {node_name}{ (current_layer_type) }\\n\"\n",
    "                diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "            add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "    add_nodes_and_connections(model)\n",
    "\n",
    "    # Adicionar formas para camadas repetitivas com contagem e retângulos para camadas internas\n",
    "    named_children = list(model.named_children())\n",
    "    for layer_type, count in layer_counts.items():\n",
    "        if count > 1:\n",
    "            diagram += f\"    subgraph {layer_type}s\\n\"\n",
    "            diagram += f\"        {layer_type}x{count}[\\\"{count} x {layer_type}\\\"]\\n\"  \n",
    "            diagram += \"    end\\n\"\n",
    "            diagram += f\"    {model.__class__.__name__} --> {layer_type}s\\n\"\n",
    "\n",
    "            # Conectar a forma ao componente subsequente se não for o último filho\n",
    "            if count < len(named_children): \n",
    "                diagram += f\"    {layer_type}s --> {named_children[count][0]}\\n\"\n",
    "        else:\n",
    "            try:\n",
    "                parent_name = root_nome_name\n",
    "                first_child_name = layer_type\n",
    "                # add_nodes_and_connections(model)\n",
    "            except:\n",
    "                first_child_name = None\n",
    "            print(parent_name, first_child_name)\n",
    "\n",
    "    return diagram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "def generate_mermaid_diagram(model):\n",
    "    \"\"\"\n",
    "    Generates a Mermaid diagram representing the structure of a Hugging Face model, \n",
    "    collapsing repetitive layers into shapes with layer count and using rounded rectangles for internal layers.\n",
    "\n",
    "    Args:\n",
    "        model: The Hugging Face model to be analyzed.\n",
    "    \"\"\"\n",
    "\n",
    "    diagram = \"graph LR\\n\"\n",
    "    layer_counts = {}\n",
    "    parent_shapes = {}\n",
    "\n",
    "    # Add the root node representing the model\n",
    "    diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "    # Recursive function to traverse the model structure \n",
    "    def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "        nonlocal diagram, layer_counts, parent_shapes\n",
    "        for name, child_module in module.named_children():\n",
    "            current_layer_type = child_module.__class__.__name__\n",
    "\n",
    "            # Sanitize the node name more aggressively to be Mermaid-compatible\n",
    "            node_name = re.sub(r'[^a-zA-Z0-9_-]', '_', parent_name + \".\" + name if parent_name else name).strip()\n",
    "            # Replace consecutive underscores with a single underscore\n",
    "            node_name = re.sub(r'_+', '_', node_name)\n",
    "\n",
    "            layer_counts[node_name] = layer_counts.get(node_name, 0) + 1\n",
    "\n",
    "            # Always add nodes to the diagram\n",
    "            diagram += f\"    {node_name}(( {current_layer_type} ))\\n\"\n",
    "\n",
    "            # If this is the first child of its parent, create a subgraph for the parent\n",
    "            if layer_counts[node_name] == 1 and parent_name:\n",
    "                if parent_name not in parent_shapes:\n",
    "                    # Sanitize the parent name as well\n",
    "                    parent_name = re.sub(r'[^a-zA-Z0-9_-]', '_', parent_name).strip()\n",
    "                    parent_name = re.sub(r'_+', '_', parent_name)\n",
    "                    diagram += f\"    subgraph {parent_name}\\n\"\n",
    "                    parent_shapes[parent_name] = True\n",
    "\n",
    "            diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "            add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "            # If this is the last child of its parent, close the subgraph if it was opened\n",
    "            if layer_counts[node_name] == len(list(module.named_children())) and parent_name in parent_shapes:\n",
    "                diagram += \"    end\\n\"\n",
    "\n",
    "    add_nodes_and_connections(model)\n",
    "\n",
    "    # Add shapes for repetitive layers with count and rounded rectangles for internal layers\n",
    "    named_children = list(model.named_children())\n",
    "    for layer_type, count in layer_counts.items():\n",
    "        if count > 1:\n",
    "            parent_name = \".\".join(layer_type.split(\".\")[:-1])\n",
    "            # Sanitize the parent name\n",
    "            parent_name = re.sub(r'[^a-zA-Z0-9_-]', '_', parent_name).strip()\n",
    "            parent_name = re.sub(r'_+', '_', parent_name)\n",
    "\n",
    "            diagram = diagram.replace(f\"    subgraph {parent_name}\\n\", \n",
    "                                      f\"    subgraph {parent_name} [{count} x {layer_type.split('.')[-1]}]\\n\")\n",
    "\n",
    "            # Connect the grandparent to the shape\n",
    "            grandparent_name = \".\".join(parent_name.split(\".\")[:-1]) \n",
    "            if grandparent_name:\n",
    "                diagram += f\"    {grandparent_name} --> {parent_name}\\n\"\n",
    "\n",
    "            # Connect the shape to the subsequent component if it's not the last child\n",
    "            if any(name.startswith(parent_name + \".\") for name in layer_counts):\n",
    "                next_component = next(name for name in layer_counts if name.startswith(parent_name + \".\"))\n",
    "                diagram += f\"    {parent_name} --> {next_component}\\n\"\n",
    "\n",
    "    return diagram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar o código Mermaid\n",
    "mermaid_code = generate_mermaid_diagram_ok(model)\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar o código Mermaid\n",
    "mermaid_code = generate_mermaid_diagram(model)\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir o diagrama usando Markdown\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(f\"```mermaid\\n{mermaid_code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "graph TD\n",
    "    BertModel{{BertModel}} --> embeddings{{BertEmbeddings}} --> embeddings --> embeddings(BertEmbeddings) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "graph TD\n",
    "    BertModel{{BertModel}} --> embeddings{{BertEmbeddings}} --> embeddings --> embeddings((BertEmbeddings)) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "print(model)\n",
    "print()\n",
    "print(model.config)\n",
    "\n",
    "# Imprimir o nome de cada camada\n",
    "for name, module in model.named_modules():\n",
    "    print(name)\n",
    "\n",
    "# Acessar uma camada específica\n",
    "print(f\"\\nBusca por camada específica no modelo HF:\")\n",
    "camada = \"get_encoder\"\n",
    "try:\n",
    "    first_layer = model.camada().layer[0]\n",
    "    print(first_layer)\n",
    "except:\n",
    "    print(f\"  Modelo {model_name} não possui camada de {camada}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entradas de referência:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entradas de citações para referências:\n",
    "\n",
    "@book{gil2022comoelaborarprojetosdepesquisa,\n",
    "  title={Como Elaborar Projetos de Pesquisa},\n",
    "  author={Gil, Antonio Carlos},\n",
    "  edition={7},\n",
    "  publisher={Atlas},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "@book{popper1934logic,\n",
    "  title={A Lógica da Pesquisa Científica},\n",
    "  author={Popper, Karl},\n",
    "  year={1934}\n",
    "}\n",
    "\n",
    "@misc{IEP2023Popper,\n",
    "  author = {Internet Encyclopedia of Philosophy},\n",
    "  title = {Karl Popper: Political Philosophy},\n",
    "  year = {2023},\n",
    "  howpublished = {\\url{https://iep.utm.edu/popp-pol/}},\n",
    "  note = {Acesso em: 01/01/2024}\n",
    "}\n",
    "\n",
    "@phdthesis{Broderick1984,\n",
    "  author = {David Gregory Broderick},\n",
    "  title = {Objectivity: Thomas Aquinas and Karl Popper},\n",
    "  school = {Boston College},\n",
    "  year = {1984}\n",
    "}\n",
    "\n",
    "@article{CRYFUNavara2023,\n",
    "  author = {Grupo Ciencia, Razón y Fe (CRYF)},\n",
    "  title = {The Ethical Roots of Karl Popper's Epistemology},\n",
    "  journal = {Universidad de Navarra},\n",
    "  year = {2023},\n",
    "  url = {https://www.unav.edu/web/ciencia-razon-y-fe/poppers-epistemology}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bases de Dados:\n",
    "\n",
    "@misc{arxiv,\n",
    "  title = {{arXiv}},\n",
    "  url = {https://arxiv.org}\n",
    "}\n",
    "\n",
    "@misc{core,\n",
    "  title = {{CORE}},\n",
    "  url = {https://core.ac.uk}\n",
    "}\n",
    "\n",
    "@misc{doaj,\n",
    "  title = {{Directory of Open Access Journals (DOAJ)}},\n",
    "  url = {https://doaj.org}\n",
    "}\n",
    "\n",
    "@misc{googlescholar,\n",
    "  title = {{Google Scholar}},\n",
    "  url = {https://scholar.google.com}\n",
    "}\n",
    "\n",
    "@misc{openaire,\n",
    "  title = {{OpenAIRE}},\n",
    "  url = {https://www.openaire.eu}\n",
    "}\n",
    "\n",
    "@misc{pubmedcentral,\n",
    "  title = {{PubMed Central}},\n",
    "  url = {https://www.ncbi.nlm.nih.gov/pmc/}\n",
    "}\n",
    "\n",
    "@misc{ssrn,\n",
    "  title = {{Social Science Research Network (SSRN)}},\n",
    "  url = {https://www.ssrn.com}\n",
    "}\n",
    "\n",
    "@misc{scienceopen,\n",
    "  title = {{ScienceOpen}},\n",
    "  url = {https://www.scienceopen.com}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editoras com políticas de OA:\n",
    "\n",
    "@misc{springernature,\n",
    "  title = {{Springer Nature}},\n",
    "  url = {https://www.springernature.com}\n",
    "}\n",
    "\n",
    "@misc{oup,\n",
    "  title = {{Oxford University Press (OUP)}},\n",
    "  url = {https://academic.oup.com}\n",
    "}\n",
    "\n",
    "@misc{frontiers,\n",
    "  title = {{Frontiers}},\n",
    "  url = {https://www.frontiersin.org}\n",
    "}\n",
    "\n",
    "@misc{wiley,\n",
    "  title = {{Wiley}},\n",
    "  url = {https://www.wiley.com}\n",
    "}\n",
    "\n",
    "@misc{plos,\n",
    "  title = {{Public Library of Science (PLOS)}},\n",
    "  url = {https://www.plos.org}\n",
    "}\n",
    "\n",
    "@misc{hindawi,\n",
    "  title = {{Hindawi}},\n",
    "  url = {https://www.hindawi.com}\n",
    "}\n",
    "\n",
    "@misc{mdpi,\n",
    "  title = {{MDPI AG}},\n",
    "  url = {https://www.mdpi.com}\n",
    "}\n",
    "\n",
    "@misc{informa,\n",
    "  title = {{Informa PLC}},\n",
    "  url = {https://www.informa.com}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GII\n",
    "@misc{GII-WIPO,\n",
    "      title = {Global Innovation Index - WIPO Series},\n",
    "      abstract = {The Global Innovation Index (GII) ranks the innovation performance of some 131 countries and economies around the world, based on 80+ indicators. Co-published by WIPO, Cornell University and INSEAD, the report provides an annual ranking of the innovation capabilities and performance of economies around the world.},\n",
    "      {url = https://www.wipo.int/publications/en/series/index.jsp?id=129}\n",
    "}\n",
    "\n",
    "@misc{GII-2011,\n",
    "  title = {Global Innovation Index 2011 - Accelerating Growth and Development},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=274&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2012,\n",
    "  title = {Global Innovation Index 2012 - Stronger Innovation Linkages for Global Growth},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=247&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2013,\n",
    "  title = {Global Innovation Index 2013 - The Local Dynamics of Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=368&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2014,\n",
    "  title = {Global Innovation Index 2014 - The Human Factor in Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=3254&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2015,\n",
    "  title = {Global Innovation Index 2015 - Effective Innovation Policies for Development},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=3978&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2016,\n",
    "  title = {Global Innovation Index 2016 - Winning with Global Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4064&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2017,\n",
    "  title = {Global Innovation Index 2017 - Innovation Feeding the World},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4193&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2018,\n",
    "  title = {Global Innovation Index 2018 - Energizing the World with Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4330&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2019,\n",
    "  title = {Global Innovation Index 2019 - Creating Healthy Lives — The Future of Medical Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4434&plang=EN}\n",
    "}\n",
    "\n",
    "@article{40579,\n",
    "      author = {Cornell University.},\n",
    "      url = {http://tind.wipo.int/record/40579},\n",
    "      title = {Global Innovation Index 2019 - Executive version.},\n",
    "      abstract = {The Global Innovation Index 2019 provides detailed metrics  about the innovation performance of 129 countries and  economies around the world. Its 80 indicators explore a  broad vision of innovation, including political  environment, education, infrastructure and business  sophistication. The GII 2019 analyzes the medical  innovation landscape of the next decade, looking at how  technological and non-technological medical innovation will  transform the delivery of healthcare worldwide. It also  explores the role and dynamics of medical innovation as it  shapes the future of healthcare, and the potential  influence this may have on economic growth. Chapters of the  report provide more details on this year’s theme from  academic, business, and particular country perspectives  from leading experts and decision makers.},\n",
    "      doi = {https://doi.org/10.34667/tind.40579},\n",
    "      recid = {40579},\n",
    "      pages = {214 pages ;},\n",
    "}\n",
    "\n",
    "@article{35279,\n",
    "      url = {http://tind.wipo.int/record/35279},\n",
    "      title = {Índice Global de inovação de 2019 - PRINCIPAIS  RESULTADOS.},\n",
    "      abstract = {Criar Vidas Sadias - O Futuro da Inovação Médica.},\n",
    "      doi = {https://doi.org/10.34667/tind.35279},\n",
    "      recid = {35279},\n",
    "      pages = {20 pages ;},\n",
    "}\n",
    "\n",
    "@misc{GII-2020,\n",
    "  title = {Global Innovation Index 2020 - Who Will Finance Innovation?},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4514&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2021,\n",
    "  title = {Global Innovation Index 2021 - Tracking Innovation through the COVID-19 Crisis},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4560&plang=EN}\n",
    "}\n",
    "\n",
    "@article{46620,\n",
    "      url = {http://tind.wipo.int/record/46620},\n",
    "      title = {Índice Global de Inovação 2022 : Resumo executivo.},\n",
    "      abstract = {O Índice Global da Inovação 2022 (IGI) analisa as  tendências globais no campo da inovação em um cenário  marcado pela pandemia de COVID-19 em curso, por um  crescimento desacelerado da produtividade e pelo surgimento  de novos desafios. O IGI revela as economias mais  inovadoras do mundo, classificando o desempenho em  inovação de 132 economias, destacando seus pontos fortes  e fracos em matéria de inovação e identificando lacunas  em suas métricas de inovação. Esta edição de 2022 tem  como foco o efeito da inovação sobre a produtividade e o  bem-estar da sociedade ao longo das próximas décadas.},\n",
    "      doi = {https://doi.org/10.34667/tind.46620},\n",
    "      recid = {46620},\n",
    "      pages = {28 pages :},\n",
    "}\n",
    "\n",
    "@misc{GII-2022,\n",
    "  title = {Global Innovation Index 2022 - What is the future of innovation driven growth?},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4622&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2023,\n",
    "  title = {Global Innovation Index 2023},\n",
    "  url = {https://www.globalinnovationindex.org/gii-2023}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar ou apontar para local atual do Spacy para PLN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar instalação desnecessária quando o Spacy já está instalado no Windows onde o WSL está rodando, é possível apontar para o diretório do spaCy instalado no Windows a partir do WSL para evitar uma nova instalação e economizar espaço em disco. \n",
    "\n",
    "No WSL, o sistema de arquivos do Windows é montado em /mnt/c. Portanto, para navegar até o diretório c:\\.spacy no WSL, você pode mudar para o diretório com o seguinte comando no Terminal do WSL:\n",
    "\n",
    "Para que o comando import spacy funcione corretamente no WSL sem precisar reinstalar o spaCy, você deve colocar o link simbólico dentro do diretório onde o Python do WSL procura por pacotes instalados. Geralmente, esse diretório é:\n",
    "\n",
    "    /home/<seu_nome_de_usuario>/.local/lib/python<versão>/site-packages/\n",
    "\n",
    "Para criar um link simbólico no WSL no diretório adequado para apontar para o spaCy no Windows executamos o seguinte comando:\n",
    "\n",
    "Bash\n",
    "\n",
    "    ln -s /mnt/c/Users/<seu_nome_de_usuario>/.spacy /path/to/spacy/in/wsl\n",
    "    \n",
    "No meu caso aqui, para criar dentro do diretório do ambiente virtual que desejo usar o Spacy, por exemplo, ficou:\n",
    "\n",
    "    ln -s /mnt/c/.spacy /home/mak/miniconda3/envs/rapids-24.08/lib/python3.11/site-packages/spacy\n",
    "\n",
    "Para remover o link usar:\n",
    "\n",
    "    unlink /home/mak/miniconda3/envs/rapids-24.08/lib/python3.11/site-packages/spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import translate\n",
    "# def translate_pt_to_en(text):\n",
    "#     translate_client = translate.TranslationServiceClient()\n",
    "#     parent = \"projects/seu-projeto-id/locations/global\" \n",
    "#     response = translate_client.translate_text(\n",
    "#         request={\n",
    "#             \"parent\": parent,\n",
    "#             \"contents\": [text],\n",
    "#             \"mime_type\": \"text/plain\",\n",
    "#             \"source_language_code\": \"pt\",\n",
    "#             \"target_language_code\": \"en\",\n",
    "#         }\n",
    "#     )\n",
    "#     return response.translations[0].translated_text\n",
    "#\n",
    "# def translate_to_pt(text):\n",
    "#     translate_client = translate.TranslationServiceClient()\n",
    "#     parent = \"projects/seu-projeto-id/locations/global\"  # Substitua 'seu-projeto-id' pelo ID real do seu projeto\n",
    "#     response = translate_client.translate_text(\n",
    "#         request={\n",
    "#             \"parent\": parent,\n",
    "#             \"contents\": [text],\n",
    "#             \"mime_type\": \"text/plain\",\n",
    "#             \"target_language_code\": \"pt\",  # Traduzir para português\n",
    "#         }\n",
    "#     )\n",
    "#     return response.translations[0].translated_text\n",
    "\n",
    "    # # 0. Distribuição do número de palavras-chave por edital (neste caso é inútil pois montei com apenas uma palavra-chave)\n",
    "    # # Verificar valores únicos na coluna 'palavras-chave'\n",
    "    # print(df_fomento['palavras-chave'].unique().to_pandas()) \n",
    "\n",
    "    # # Normalizar caracteres Unicode e substituir valores especiais por NaN (on CPU)\n",
    "    # pd_series = df_fomento['palavras-chave'].to_pandas()\n",
    "    # pd_series = pd_series.astype(str).map(\n",
    "    #     lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # )\n",
    "    # pd_series = pd_series.str.replace(r'^\\s*$', '', regex=True) \n",
    "    # pd_series = pd_series.fillna('')\n",
    "\n",
    "    # # Aplicar str.split(',') e str.len() no Pandas\n",
    "    # pd_series_split = pd_series.str.split(',')\n",
    "    # pd_series_len = pd_series_split.str.len()\n",
    "\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # sns.histplot(pd_series_len, kde=True)  # Passar a Series do Pandas para o Seaborn\n",
    "    # plt.title('Distribuição do Número de Palavras-chave por Edital')\n",
    "    # plt.xlabel('Número de Palavras-chave')\n",
    "    # plt.ylabel('Frequência')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import cudf\n",
    "# import nltk\n",
    "# import torch\n",
    "# import spacy\n",
    "# import string\n",
    "# import logging\n",
    "# import unicodedata\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import contextualSpellCheck\n",
    "\n",
    "# from transformers.tokenization_utils_base import TruncationStrategy\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# from transformers import pipeline, TranslationPipeline\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from spacy.tokens import Doc, Token\n",
    "# from spacy.language import Language\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# from nltk.corpus import stopwords\n",
    "# from collections import Counter\n",
    "# from googletrans import Translator\n",
    "# from wordcloud import WordCloud\n",
    "# from langdetect import detect\n",
    "# from git import Repo\n",
    "# from tqdm.notebook import tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "# def detect_language(text):\n",
    "#     try:\n",
    "#         return detect(text)\n",
    "#     except langdetect.lang_detect_exception.LangDetectException:\n",
    "#         return 'unknown'\n",
    "\n",
    "# def translate_to_pt(texts):\n",
    "#     try:\n",
    "#         # Tradução usando o modelo Hugging Face\n",
    "#         inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "#         outputs = model.generate(**inputs)\n",
    "#         translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "#         return translations\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro na tradução: {e}\")\n",
    "#         return texts\n",
    "\n",
    "# # Função de pré-processamento otimizada para GPU\n",
    "# def gpu_preprocess_text(text):\n",
    "#     # Carregar as stopwords em inglês\n",
    "#     stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "#     # Adicionar as stopwords personalizadas em inglês\n",
    "#     stop_words_en.update([\"must\", \"due\", \"track\", \"may\", \"non\", \"year\", \"apply\", \"prepare\", \"era\", \"eligibility\",\n",
    "#                           \"funded value\", \"deadline\", \"application form\", \"description\", \"homepage\", \"Name\",\n",
    "#                           \"address\", \"phone\", \"Fax\", \"e-mail\", \"email\", \"contact\", \"home page\", \"home\", \"page\"])\n",
    "\n",
    "#     # Traduzir o texto para português (se necessário) em lote\n",
    "#     try:\n",
    "#         # logging.info(\"Traduzindo texto para o português (se necessário)...\")\n",
    "#         text_translated = translate_to_pt([text])[0] if detect_language(text) != 'pt' else text\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Erro na tradução: {e}\")\n",
    "#         return []\n",
    "\n",
    "#     # Converter para minúsculas e remover pontuação\n",
    "#     # logging.info(\"Limpando e normalizando o texto...\")\n",
    "#     text_translated = text_translated.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#     # Truncar o texto traduzido se for muito longo\n",
    "#     max_length = 512  # Ajuste conforme necessário\n",
    "#     text_translated = text_translated[:max_length]\n",
    "\n",
    "#     # Aplicar o corretor ortográfico e lematizar em inglês em lote (usando pipe do spaCy)\n",
    "#     # logging.info(\"Processando o texto com spaCy...\")\n",
    "#     with nlp_en.disable_pipes('ner'):  # Desabilitar NER para economizar memória da GPU\n",
    "#         docs = nlp_en.pipe([text_translated], batch_size=64) \n",
    "\n",
    "#     for doc in docs:\n",
    "#         words_en = [token.lemma_.lower() if token.text.lower() not in [\"institute\", \"institution\", \"institutional\"] else \"institution\"\n",
    "#                     for token in doc \n",
    "#                     if token.is_alpha and not token.is_stop and token.lemma_.lower() not in stop_words_en]\n",
    "\n",
    "#     return words_en  \n",
    "\n",
    "# # Configurar o logging (opcional)\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Load the transformer-based Portuguese and English models \n",
    "# nlp_pt = spacy.load(\"pt_core_news_sm\")  \n",
    "# nlp_en = spacy.load(\"en_core_web_trf\") \n",
    "\n",
    "# # Add the contextual spell checker to the English pipeline \n",
    "# contextualSpellCheck.add_to_pipe(nlp_en)\n",
    "\n",
    "# # Load the translation pipeline\n",
    "# translator = pipeline(\"translation\", model=\"unicamp-dl/translation-pt-en-t5\") \n",
    "\n",
    "# # Certifique-se de que `detect_language(text)` está definida em algum lugar do seu código\n",
    "# # Medir tempo para pré-processar (remover sw, traduzir para português, lematizar) sem cuGrpah mas já usando GPU\n",
    "# start_time = time.time()\n",
    "# # Aplicar a função de pré-processamento à coluna 'texto_para_embedding' em lotes, com barra de progresso\n",
    "# all_words = df_fomento['texto_para_embedding'].to_pandas().progress_apply(gpu_preprocess_text) \n",
    "# end_time = time.time()\n",
    "# time_com = end_time - start_time\n",
    "# print(f\"Tempos para pré-processar usando mais a GPU:\") \n",
    "# print(f\"Carregando o modelo diretamente na GPU: {time_com:>.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definir a função de pré-processamento \n",
    "# def br_preprocess_text(text):\n",
    "#     # Carregar as stopwords em português\n",
    "#     stop_words_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "#     # Adicionar as stopwords personalizadas em português\n",
    "#     stop_words_pt.update([\"deve\", \"devido\", \"acompanhar\", \"pode\", \"não\", \"ano\", \"aplicar\", \"preparar\", \"era\", \"elegibilidade\",\n",
    "#                        \"valorfinanciado\", \"datalimite\", \"formuláriodesolicitacao\", \"descrição\", \"homepage\", \"nome\",\n",
    "#                        \"endereço\", \"telefone\", \"fax\", \"e-mail\", \"contato\", \"home page\", \"casa\", \"página\"])\n",
    "\n",
    "#     # Traduzir o texto para português (se necessário)\n",
    "#     try:\n",
    "#         if detect_language(text) != 'pt':\n",
    "#             # logging.info(\"Traduzindo texto para o português...\")\n",
    "#             text_translated = translator(text, src_lang = \"auto\", tgt_lang=\"pt\")[0]['translation_text']\n",
    "#         else: \n",
    "#             text_translated = text \n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Erro na tradução: {e}\")\n",
    "#         return [] \n",
    "\n",
    "#     # Converter para minúsculas e remover pontuação\n",
    "#     # logging.info(\"Limpando e normalizando o texto...\")\n",
    "#     text_translated = text_translated.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#     # Truncar o texto traduzido se for muito longo\n",
    "#     max_length = 512 \n",
    "#     text_translated = text_translated[:max_length]\n",
    "\n",
    "#     # Aplicar o corretor ortográfico (se disponível para português)\n",
    "#     # logging.info(\"Aplicando o corretor ortográfico...\")\n",
    "#     # doc_pt_spell_check = nlp_pt(text_translated)\n",
    "#     # text_corrected = doc_pt_spell_check._.outcome_spellCheck \n",
    "\n",
    "#     # Lematizar em português\n",
    "#     # logging.info(\"Lematizando o texto...\")\n",
    "#     doc_pt = nlp_pt(text_translated)\n",
    "#     words_pt = [token.lemma_.lower() \n",
    "#                 for token in doc_pt \n",
    "#                 if token.is_alpha and not token.is_stop and token.lemma_.lower() not in stop_words_pt \n",
    "#                 and not (token.pos_ == \"PROPN\" and token.text.lower() not in stop_words_pt)]\n",
    "\n",
    "#     return words_pt\n",
    "\n",
    "# # Configurar o logging (opcional)\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Carregar o modelo de tradução e o tokenizador do Hugging Face\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/translation-pt-en-t5\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/translation-pt-en-t5\")\n",
    "\n",
    "# # Move the Hugging Face model to the GPU\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# if device == 'cuda':\n",
    "#     print(f\"Caregando modelo para GPU...\")\n",
    "# else:\n",
    "#     print(f\"GPU indisponível, usando aoenas CPU...\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Load the transformer-based English model\n",
    "# nlp_en = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# # Add the contextual spell checker to the pipeline\n",
    "# contextualSpellCheck.add_to_pipe(nlp_en)\n",
    "\n",
    "# # Medir tempo para pré-processar (remocer sw, traduzir para português, lematizar) sem cuGrpah mas já usando GPU\n",
    "# start_time = time.time()\n",
    "# # Aplicar a função de pré-processamento à coluna 'texto_para_embedding' em lotes\n",
    "# all_words = df_fomento['texto_para_embedding'].to_pandas().apply(br_preprocess_text)\n",
    "# end_time = time.time()\n",
    "# time_sem = end_time - start_time\n",
    "# print(f\"Tempos para pré-processar usando GPU somente indiretamente:\") \n",
    "# print(f\"Sem carregar o modelo diretamente na GPU: {time_sem:>.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import unicodedata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import contextualSpellCheck\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from spacy.tokens import Doc, Token\n",
    "from spacy.language import Language\n",
    "# from googletrans import Translator\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from git import Repo\n",
    "import logging\n",
    "\n",
    "# Configurar o logging (opcional)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# def translate_to_pt(text):\n",
    "#     translator = Translator()\n",
    "#     try:\n",
    "#         translation = translator.translate(text, dest='pt')\n",
    "#         return translation.text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro na tradução: {e}\")\n",
    "#         return text \n",
    "\n",
    "# def translate_pt_to_en(text):\n",
    "#     translator = Translator()\n",
    "#     try:\n",
    "#         translation = translator.translate(text, src='pt', dest='en')\n",
    "#         return translation.text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro na tradução: {e}\")\n",
    "#         return text\n",
    "\n",
    "# # Carregar os modelos de língua portuguesa e inglesa do spaCy\n",
    "# # nlp_pt = spacy.load('pt_core_news_sm')\n",
    "# # nlp_en = spacy.load('en_core_web_lg')\n",
    "\n",
    "# # Load the transformer-based English model\n",
    "# nlp_en = spacy.load(\"en_core_web_trf\") \n",
    "\n",
    "# # Add the contextual spell checker to the pipeline\n",
    "# contextualSpellCheck.add_to_pipe(nlp_en)\n",
    "\n",
    "# # Definir a função de pré-processamento (recebendo stop_words como argumento)\n",
    "# def cpu_preprocess_text(text):\n",
    "#     # Carregar as stopwords em inglês\n",
    "#     stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "#     # Adicionar as stopwords personalizadas em inglês\n",
    "#     stop_words_en.update([\"must\", \"due\", \"track\", \"may\", \"non\", \"year\", \"apply\", \"prepare\", \"era\", \"eligibility\",\n",
    "#                        \"funded value\", \"deadline\", \"application form\", \"description\", \"homepage\", \"Name\",\n",
    "#                        \"address\", \"phone\", \"Fax\", \"e-mail\", \"email\", \"contact\", \"home page\", \"home\", \"page\"])\n",
    "\n",
    "#     # Traduzir o texto de português para inglês\n",
    "#     try:\n",
    "#         logging.info(\"Pre-processar termos (traduzir para o inglês, corrigir ortografia e lematizar)...\")\n",
    "#         text_translated = translate_pt_to_en(text)\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Erro na tradução: {e}\")\n",
    "#         return []\n",
    "\n",
    "#     # Converter para minúsculas e remover pontuação\n",
    "#     text_translated = text_translated.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "#     # Truncate the translated text if it's too long\n",
    "#     max_length = 512  \n",
    "#     text_translated = text_translated[:max_length]\n",
    "\n",
    "#     # Aplicar o corretor ortográfico contextual\n",
    "#     doc_en_spell_check = nlp_en(text_translated)\n",
    "#     text_corrected = doc_en_spell_check._.outcome_spellCheck # Usando a extensão correta do contextualSpellCheck\n",
    "\n",
    "#     # Lematizar em inglês\n",
    "#     doc_en = nlp_en(text_corrected)\n",
    "#     words_en = [token.lemma_.lower() if token.text.lower() not in [\"institute\", \"institution\", \"institutional\"] else \"institution\"\n",
    "#                 for token in doc_en \n",
    "#                 if token.is_alpha and not token.is_stop and token.lemma_.lower() not in stop_words_en]\n",
    "\n",
    "#     return words_en\n",
    "\n",
    "# def analisar_dados_fomento(all_words):\n",
    "#     \"\"\"\n",
    "#     Realiza análises exploratórias nos dados de oportunidades de fomento.\n",
    "\n",
    "#     Args:\n",
    "#         arquivo_csv: Caminho para o arquivo CSV contendo os dados de fomento.\n",
    "#     \"\"\"\n",
    "#     # Contar a frequência das palavras\n",
    "#     word_counts = Counter(word for words in all_words for word in words)\n",
    "\n",
    "#     # Obter as palavras mais frequentes\n",
    "#     top_words = word_counts.most_common(20)\n",
    "\n",
    "#     # Plotar um gráfico de barras com as palavras mais frequentes\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.bar(*zip(*top_words))\n",
    "#     plt.title('Palavras Mais Frequentes (sem Stopwords e com Lematização)')\n",
    "#     plt.xlabel('Palavra')\n",
    "#     plt.ylabel('Frequência')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Criar uma nuvem de palavras\n",
    "#     wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)\n",
    "\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "#     # 2. Visualização dos embeddings em 2D usando PCA\n",
    "#     pca = PCA(n_components=2)\n",
    "#     embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "#     plt.title('Visualização dos Embeddings (PCA)')\n",
    "#     plt.xlabel('Componente Principal 1')\n",
    "#     plt.ylabel('Componente Principal 2')\n",
    "#     plt.show()\n",
    "\n",
    "#     # 3. Visualização dos embeddings em 2D usando t-SNE\n",
    "#     tsne = TSNE(n_components=2, perplexity=30, learning_rate=200)\n",
    "#     embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "#     plt.title('Visualização dos Embeddings (t-SNE)')\n",
    "#     plt.xlabel('Dimensão 1')\n",
    "#     plt.ylabel('Dimensão 2')\n",
    "#     plt.show()\n",
    "\n",
    "# analisar_dados_fomento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, HTML\n",
    "\n",
    "# def generate_mermaid_diagram_v0(model):\n",
    "#     \"\"\"\n",
    "#     Gera um diagrama Mermaid representando a estrutura de um modelo Hugging Face.\n",
    "\n",
    "#     Args:\n",
    "#         model: O modelo Hugging Face a ser analisado.\n",
    "\n",
    "#     Returns:\n",
    "#         Uma string contendo o código Mermaid para o diagrama.\n",
    "#     \"\"\"\n",
    "\n",
    "#     diagram = \"graph LR\\n\"\n",
    "\n",
    "#     # Adicionar o nó raiz representando o modelo\n",
    "#     diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "#     # Função recursiva para percorrer a estrutura do modelo e adicionar nós e conexões ao diagrama\n",
    "#     def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "#         nonlocal diagram\n",
    "#         for name, child_module in module.named_children():\n",
    "#             node_name = parent_name + \".\" + name if parent_name else name\n",
    "#             diagram += f\"    {node_name}{{ {child_module.__class__.__name__} }}\\n\"\n",
    "#             diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "#             add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "#     # Chamar a função recursiva para construir o diagrama\n",
    "#     add_nodes_and_connections(model)\n",
    "\n",
    "#     return diagram\n",
    "\n",
    "# def generate_mermaid_diagram_v1(model):\n",
    "#     \"\"\"\n",
    "#     Generates a Mermaid diagram representing the structure of a Hugging Face model, \n",
    "#     collapsing repetitive layers.\n",
    "\n",
    "#     Args:\n",
    "#         model: The Hugging Face model to be analyzed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     diagram = \"graph LR\\n\"\n",
    "#     prev_layer_type = None\n",
    "#     layer_count = 0\n",
    "\n",
    "#     # Add the root node representing the model\n",
    "#     diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "#     # Recursive function to traverse the model structure and add nodes and connections to the diagram\n",
    "#     def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "#         nonlocal diagram, prev_layer_type, layer_count\n",
    "#         for name, child_module in module.named_children():\n",
    "#             current_layer_type = child_module.__class__.__name__\n",
    "#             node_name = parent_name + \".\" + name if parent_name else name\n",
    "\n",
    "#             if current_layer_type == prev_layer_type:\n",
    "#                 layer_count += 1\n",
    "#             else:\n",
    "#                 if layer_count > 1:\n",
    "#                     diagram += f\"    {parent_name}.{prev_layer_type}_...(...):::same\\n\"  # Add ellipses for repetitive layers\n",
    "#                 prev_layer_type = current_layer_type\n",
    "#                 layer_count = 1\n",
    "#                 diagram += f\"    {node_name}{{ {current_layer_type} }}\\n\"\n",
    "#                 diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "#             add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "#         # Handle remaining repetitive layers at the end of a module\n",
    "#         if layer_count > 1:\n",
    "#             diagram += f\"    {parent_name}.{prev_layer_type}_...(...):::same\\n\"\n",
    "\n",
    "#     # Call the recursive function to build the diagram\n",
    "#     add_nodes_and_connections(model)\n",
    "\n",
    "#     # # Encapsulate the Mermaid code in an HTML block for rendering in Jupyter Notebook\n",
    "#     # html_code = f\"\"\"\n",
    "#     # <div class=\"mermaid\">\n",
    "#     # {diagram}\n",
    "#     # </div>\n",
    "#     # \"\"\"\n",
    "\n",
    "#     # # Display the diagram in the Jupyter Notebook cell\n",
    "#     # display(HTML(html_code))\n",
    "\n",
    "#     return diagram\n",
    "\n",
    "# def generate_mermaid_diagram_v2(model):\n",
    "#     \"\"\"\n",
    "#     Generates a Mermaid diagram representing the structure of a Hugging Face model, \n",
    "#     collapsing repetitive layers into shapes.\n",
    "\n",
    "#     Args:\n",
    "#         model: The Hugging Face model to be analyzed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     diagram = \"graph LR\\n\"\n",
    "#     prev_layer_type = None\n",
    "#     layer_count = 0\n",
    "#     shape_id = 0  # To keep track of unique shape IDs\n",
    "\n",
    "#     # Add the root node representing the model\n",
    "#     diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "#     # Recursive function to traverse the model structure and add nodes and connections to the diagram\n",
    "#     def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "#         nonlocal diagram, prev_layer_type, layer_count, shape_id\n",
    "#         for name, child_module in module.named_children():\n",
    "#             current_layer_type = child_module.__class__.__name__\n",
    "#             node_name = parent_name + \".\" + name if parent_name else name\n",
    "\n",
    "#             if current_layer_type == prev_layer_type:\n",
    "#                 layer_count += 1\n",
    "#             else:\n",
    "#                 if layer_count > 1:\n",
    "#                     # Create a shape to group repetitive layers\n",
    "#                     shape_id += 1\n",
    "#                     diagram += f\"    subgraph shape{shape_id} [{prev_layer_type} x {layer_count}]\\n\"\n",
    "#                     diagram += f\"        {parent_name}.{prev_layer_type}_0[\\\"{prev_layer_type} 0\\\"]\\n\"\n",
    "#                     diagram += f\"        {parent_name}.{prev_layer_type}_...[\\\" ... \\\"]\\n\"\n",
    "#                     diagram += f\"        {parent_name}.{prev_layer_type}_{layer_count - 1}[\\\"{prev_layer_type} {layer_count - 1}\\\"]\\n\"\n",
    "#                     diagram += \"    end\\n\"\n",
    "#                     diagram += f\"    {parent_name} --> shape{shape_id}\\n\"\n",
    "#                 else:\n",
    "#                     diagram += f\"    {node_name}{{ {current_layer_type} }}\\n\"\n",
    "#                     diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "#                 prev_layer_type = current_layer_type\n",
    "#                 layer_count = 1\n",
    "\n",
    "#             add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "#         # Handle remaining repetitive layers at the end of a module\n",
    "#         if layer_count > 1:\n",
    "#             shape_id += 1\n",
    "#             diagram += f\"    subgraph shape{shape_id} [{prev_layer_type} x {layer_count}]\\n\"\n",
    "#             diagram += f\"        {parent_name}.{prev_layer_type}_0[\\\"{prev_layer_type} 0\\\"]\\n\"\n",
    "#             diagram += f\"        {parent_name}.{prev_layer_type}_...[\\\" ... \\\"]\\n\"\n",
    "#             diagram += f\"        {parent_name}.{prev_layer_type}_{layer_count - 1}[\\\"{prev_layer_type} {layer_count - 1}\\\"]\\n\"\n",
    "#             diagram += \"    end\\n\"\n",
    "#             diagram += f\"    {parent_name} --> shape{shape_id}\\n\"\n",
    "\n",
    "#     # Call the recursive function to build the diagram\n",
    "#     add_nodes_and_connections(model)\n",
    "\n",
    "#     # # Encapsulate the Mermaid code in an HTML block for rendering in Jupyter Notebook\n",
    "#     # html_code = f\"\"\"\n",
    "#     # <div class=\"mermaid\">\n",
    "#     # {diagram}\n",
    "#     # </div>\n",
    "#     # \"\"\"\n",
    "\n",
    "#     # # Display the diagram in the Jupyter Notebook cell\n",
    "#     # display(HTML(html_code))\n",
    "\n",
    "#     return diagram\n",
    "\n",
    "# def generate_mermaid_diagram_v3_4_5(model):\n",
    "#     \"\"\"\n",
    "#     Generates a Mermaid diagram representing the structure of a Hugging Face model, \n",
    "#     collapsing repetitive layers into shapes with numbered instances.\n",
    "\n",
    "#     Args:\n",
    "#         model: The Hugging Face model to be analyzed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     diagram = \"graph LR\\n\"\n",
    "#     layer_counts = {}  # To keep track of layer counts for each type\n",
    "\n",
    "#     # Add the root node representing the model\n",
    "#     diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "#     # Recursive function to traverse the model structure and add nodes and connections to the diagram\n",
    "#     def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "#         nonlocal diagram, layer_counts\n",
    "#         for name, child_module in module.named_children():\n",
    "#             current_layer_type = child_module.__class__.__name__\n",
    "#             node_name = parent_name + \".\" + name if parent_name else name\n",
    "\n",
    "#             # Count occurrences of each layer type\n",
    "#             layer_counts[current_layer_type] = layer_counts.get(current_layer_type, 0) + 1\n",
    "\n",
    "#             # If this is the first occurrence of this layer type, add it to the diagram\n",
    "#             if layer_counts[current_layer_type] == 1:\n",
    "#                 diagram += f\"    {node_name}{{ {current_layer_type} }}\\n\"\n",
    "#                 diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "#             add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "#     # Call the recursive function to build the diagram\n",
    "#     add_nodes_and_connections(model)\n",
    "\n",
    "#     # Add shapes for repetitive layers\n",
    "#     named_children = list(model.named_children())\n",
    "#     for layer_type, count in layer_counts.items():\n",
    "#         if count > 1:\n",
    "#             diagram += f\"    subgraph {layer_type}s [x{count}]\\n\"\n",
    "#             for i in range(count):\n",
    "#                 diagram += f\"        {layer_type}_{i}{{ {layer_type} {i} }}\\n\"\n",
    "#             diagram += \"    end\\n\"\n",
    "#             # Connect the parent to the first layer in the shape \n",
    "#             diagram += f\"    {model.__class__.__name__} --> {layer_type}_0\\n\"\n",
    "            \n",
    "#             # Connect the last layer to the subsequent component only if it exists\n",
    "#             if count < len(named_children):  # Check if there's a subsequent component\n",
    "#                 diagram += f\"    {layer_type}_{count - 1} --> {named_children[count][0]}\\n\"\n",
    "\n",
    "#     # # Encapsulate the Mermaid code in an HTML block for rendering in Jupyter Notebook\n",
    "#     # html_code = f\"\"\"\n",
    "#     # <div class=\"mermaid\">\n",
    "#     # {diagram}\n",
    "#     # </div>\n",
    "#     # \"\"\"\n",
    "\n",
    "#     # # Display the diagram in the Jupyter Notebook cell\n",
    "#     # display(HTML(html_code))\n",
    "\n",
    "#     return diagram\n",
    "\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# def generate_mermaid_diagram(model):\n",
    "#     \"\"\"\n",
    "#     Generates a Mermaid diagram representing the structure of a Hugging Face model, \n",
    "#     collapsing repetitive layers into shapes with layer count and using rounded rectangles for internal layers.\n",
    "\n",
    "#     Args:\n",
    "#         model: The Hugging Face model to be analyzed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     diagram = \"graph LR\\n\"\n",
    "#     layer_counts = {}\n",
    "\n",
    "#     # Add the root node representing the model\n",
    "#     diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "#     # Recursive function to traverse the model structure \n",
    "#     def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "#         nonlocal diagram, layer_counts\n",
    "#         for name, child_module in module.named_children():\n",
    "#             current_layer_type = child_module.__class__.__name__\n",
    "#             node_name = parent_name + \".\" + name if parent_name else name\n",
    "\n",
    "#             layer_counts[current_layer_type] = layer_counts.get(current_layer_type, 0) + 1\n",
    "\n",
    "#             if layer_counts[current_layer_type] == 1:\n",
    "#                 # Use rounded rectangle for single occurrences or non-repetitive layers\n",
    "#                 diagram += f\"    {node_name}(( {current_layer_type} ))\\n\"\n",
    "#                 diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "#             add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "#     add_nodes_and_connections(model)\n",
    "\n",
    "#     # Add shapes for repetitive layers with count and rounded rectangles for internal layers\n",
    "#     named_children = list(model.named_children())\n",
    "#     for layer_type, count in layer_counts.items():\n",
    "#         if count > 1:\n",
    "#             diagram += f\"    subgraph {layer_type}s\\n\"\n",
    "#             diagram += f\"        {layer_type}x{count}[\\\"{count} x {layer_type}\\\"]\\n\"  \n",
    "#             diagram += \"    end\\n\"\n",
    "#             diagram += f\"    {model.__class__.__name__} --> {layer_type}s\\n\"\n",
    "            \n",
    "#             if count < len(named_children): \n",
    "#                 diagram += f\"    {layer_type}s --> {named_children[count][0]}\\n\"\n",
    "\n",
    "#     # # Encapsulate the Mermaid code in an HTML block for rendering in Jupyter Notebook\n",
    "#     # html_code = f\"\"\"\n",
    "#     # <div class=\"mermaid\">\n",
    "#     # {diagram}\n",
    "#     # </div>\n",
    "#     # \"\"\"\n",
    "\n",
    "#     # # Display the diagram in the Jupyter Notebook cell\n",
    "#     # display(HTML(html_code))\n",
    "    \n",
    "#     return diagram\n",
    "\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# def generate_mermaid_diagram(model):\n",
    "#     \"\"\"\n",
    "#     Generates a Mermaid diagram representing the structure of a Hugging Face model,\n",
    "#     collapsing repetitive layers into shapes with layer count and using rounded\n",
    "#     rectangles for internal layers.\n",
    "\n",
    "#     Args:\n",
    "#         model: The Hugging Face model to be analyzed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     diagram = \"graph LR\\n\"\n",
    "#     layer_counts = {}\n",
    "\n",
    "#     # Add the root node representing the model\n",
    "#     diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "#     # Recursive function to traverse the model structure\n",
    "#     def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "#         nonlocal diagram, layer_counts\n",
    "#         for name, child_module in module.named_children():\n",
    "#             current_layer_type = child_module.__class__.__name__\n",
    "#             node_name = parent_name + \".\" + name if parent_name else name\n",
    "\n",
    "#             layer_counts[current_layer_type] = layer_counts.get(current_layer_type, 0) + 1\n",
    "\n",
    "#             if layer_counts[current_layer_type] == 1:\n",
    "#                 # Use rounded rectangle for single occurrences or non-repetitive layers\n",
    "#                 diagram += f\"    {node_name}{ (current_layer_type) }\\n\"\n",
    "#                 diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "#             add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "#     add_nodes_and_connections(model)\n",
    "\n",
    "#     # Add shapes for repetitive layers with count and rounded rectangles for internal layers\n",
    "#     named_children = list(model.named_children())\n",
    "#     for layer_type, count in layer_counts.items():\n",
    "#         if count > 1:\n",
    "#             diagram += f\"    subgraph {layer_type}s\\n\"\n",
    "#             diagram += f\"        {layer_type}x{count}[\\\"{count} x {layer_type}\\\"]\\n\"  \n",
    "#             diagram += \"    end\\n\"\n",
    "#             diagram += f\"    {model.__class__.__name__} --> {layer_type}s\\n\"\n",
    "\n",
    "#             # Connect the shape to the subsequent component if it's not the last child\n",
    "#             if count < len(named_children): \n",
    "#                 diagram += f\"    {layer_type}s --> {named_children[count][0]}\\n\"\n",
    "\n",
    "#     # # Encapsulate the Mermaid code in an HTML block for rendering in Jupyter Notebook\n",
    "#     # html_code = f\"\"\"\n",
    "#     # <div class=\"mermaid\">\n",
    "#     # {diagram}\n",
    "#     # </div>\n",
    "#     # \"\"\"\n",
    "\n",
    "#     # # Display the diagram in the Jupyter Notebook cell\n",
    "#     # display(HTML(html_code))\n",
    "\n",
    "#     return diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versões antigas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from git import Repo\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from gml_unsupervised_learning_tools import DataPreprocessor\n",
    "# from gml_unsupervised_learning_tools import EmbeddingEvaluator\n",
    "# from funding_analyser import FundingEmbeddingGenerator\n",
    "\n",
    "# # Criar uma instância do FundingEmbeddingGenerator\n",
    "# embedding_generator = FundingEmbeddingGenerator()\n",
    "\n",
    "# try:\n",
    "#     # Criar a coluna 'texto_para_embedding' no dataframe df_fomento usando cuDF (se disponível)\n",
    "#     df_fomento = embedding_generator.create_embedding_column(use_cudf=True)\n",
    "# except:\n",
    "#     # Ou, criar a coluna 'texto_para_embedding' sem usar cuDF, usando apenas Pandas\n",
    "#     df_fomento = embedding_generator.create_embedding_column(use_cudf=False)\n",
    "\n",
    "# # Define the model names and the models you want to compare\n",
    "# model_names = [\n",
    "#     'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "#     'all-MiniLM-L6-v2'\n",
    "#     # Add more model names here if needed\n",
    "# ]\n",
    "\n",
    "# models = [\n",
    "#     SentenceTransformer(model_name)\n",
    "#     for model_name in model_names\n",
    "# ]\n",
    "\n",
    "# # Create an instance of EmbeddingEvaluator\n",
    "# benchmark = EmbeddingEvaluator(model_names, models, df_fomento) \n",
    "\n",
    "# # Gere o relatório de benchmarking\n",
    "# benchmark.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking de pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, cudf\n",
    "from funding_analyser import FundingEmbeddingGenerator\n",
    "\n",
    "df_fomento = pd.DataFrame()\n",
    "\n",
    "# Criar uma instância do EmbeddingGenerator\n",
    "embedding_generator = FundingEmbeddingGenerator()\n",
    "\n",
    "# Criar a coluna 'texto_para_embedding' no dataframe df_fomento usando cuDF (se disponível)\n",
    "try:\n",
    "    df_fomento = embedding_generator.create_embedding_column(use_cudf=True)\n",
    "except:\n",
    "    # Ou, criar a coluna 'texto_para_embedding' sem usar cuDF, usando apenas Pandas\n",
    "    df_fomento = embedding_generator.create_embedding_column(use_cudf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funding_analyser import ENPreprocessor\n",
    "\n",
    "# Criar instâncias do pré-processador\n",
    "en_preprocessor = ENPreprocessor()\n",
    "\n",
    "# Medir tempo pré-processar em lotes (remover sw, traduzir p/inglês, corrigir ortografia e lematizar)\n",
    "start_time = time.time()\n",
    "all_words_en = df_fomento['texto_para_embedding'].to_pandas().progress_apply(en_preprocessor.preprocess_text)  # type: ignore\n",
    "end_time = time.time()\n",
    "time_en = end_time - start_time\n",
    "print(f\"Tempo de execução da função en_preprocessor: {time_en:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funding_analyser import BRPreprocessor\n",
    "\n",
    "# Criar instância do pré-processador\n",
    "br_preprocessor = BRPreprocessor()\n",
    "\n",
    "# Medir tempo para pré-processar sem usar processamento em lotes (remover sw, traduzir p/português, lematizar)\n",
    "start_time = time.time()\n",
    "all_words_br = df_fomento['texto_para_embedding'].to_pandas().progress_apply(br_preprocessor.preprocess_text) # type: ignore\n",
    "end_time = time.time()\n",
    "time_br = end_time - start_time\n",
    "print(f\"Tempo de execução da função br_preprocess_text: {time_br:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in all_words_en[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in all_words_br[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking de geração de embeedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking - Geração de embeddings sem batches\n",
    "start_time = time.time()\n",
    "embeddings_sem_batch = embedding_generator.generate_embeddings(df_fomento)\n",
    "end_time = time.time()\n",
    "tempo_sem_batch = end_time - start_time\n",
    "print(f\"Tempo de execução sem batches: {tempo_sem_batch:.2f} segundos\")\n",
    "\n",
    "# Benchmarking - Geração de embeddings com batches\n",
    "start_time = time.time()\n",
    "embeddings_com_batch = embedding_generator.generate_embeddings_batch(df_fomento)\n",
    "end_time = time.time()\n",
    "tempo_com_batch = end_time - start_time\n",
    "print(f\"Tempo de execução com batches: {tempo_com_batch:.2f} segundos\")\n",
    "\n",
    "# Comparar os resultados (opcional)\n",
    "if np.allclose(embeddings_sem_batch, embeddings_com_batch):\n",
    "    print(\"Os embeddings gerados são iguais.\")\n",
    "else:\n",
    "    print(\"Os embeddings gerados são diferentes. Verifique a implementação.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir tempo de execução da função original\n",
    "start_time = time.time()\n",
    "all_words_original = df_fomento['texto_para_embedding'].to_pandas().apply(cpu_preprocess_text) # type: ignore\n",
    "original_time = time.time() - start_time\n",
    "print(f\"Tempo de execução da função original: {original_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir tempo de execução da função otimizada\n",
    "start_time = time.time()\n",
    "all_words_optimized = df_fomento['texto_para_embedding'].to_pandas().apply(gpu_preprocess_text) # type: ignore\n",
    "optimized_time = time.time() - start_time\n",
    "print(f\"Tempo de execução da função otimizada: {optimized_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# 1. Carregar o dataframe df_fomento (carregado e processado anteriormente)\n",
    "df_fomento = pdf\n",
    "\n",
    "# 2. Criar um grafo vazio\n",
    "G = nx.Graph()\n",
    "\n",
    "# 3. Obter todas as chaves únicas\n",
    "all_keys = set()\n",
    "for keys in df_fomento['detalhes'].map(lambda x: x.keys()):\n",
    "    all_keys.update(keys)\n",
    "\n",
    "# 4. Iterar sobre as linhas do dataframe\n",
    "for index, row in df_fomento.iterrows():\n",
    "    # 5. Criar um nó com o índice da linha como ID\n",
    "    G.add_node(index)\n",
    "\n",
    "    # 6. Adicionar as propriedades do dicionário ao nó\n",
    "    for key in all_keys:\n",
    "        G.nodes[index][key] = row['detalhes'].get(key, None)  # Usar None para chaves ausentes\n",
    "\n",
    "# 7. Exibir informações sobre o grafo\n",
    "print(\"Número de nós:\", G.number_of_nodes())\n",
    "print(\"Número de arestas:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fomento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Converter a coluna 'detalhes' para dicionários\n",
    "def convert_to_dict(text):\n",
    "    try:\n",
    "        return ast.literal_eval(text)\n",
    "    except ValueError:\n",
    "        return None  # Ou {} se preferir um dicionário vazio para entradas inválidas\n",
    "\n",
    "df_fomento['detalhes'] = df_fomento['detalhes'].astype(str).apply(convert_to_dict)\n",
    "\n",
    "# 3. Criar um grafo vazio\n",
    "G = nx.Graph()\n",
    "\n",
    "# 4. Obter todas as chaves únicas\n",
    "all_keys = set()\n",
    "for detalhes in df_fomento['detalhes']:\n",
    "    if detalhes:  # Verificar se detalhes é um dicionário válido\n",
    "        all_keys.update(detalhes.keys())\n",
    "\n",
    "# 5-8. Iterar, criar nós e adicionar propriedades\n",
    "for index, row in df_fomento.iterrows():\n",
    "    detalhes = row['detalhes']\n",
    "    G.add_node(index)\n",
    "    for key in all_keys:\n",
    "        G.nodes[index][key] = detalhes.get(key, None)\n",
    "\n",
    "# 9. Exibir informações sobre o grafo\n",
    "print(\"Número de nós:\", G.number_of_nodes())\n",
    "print(\"Número de arestas:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fomento.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[y for y in x.keys()] for x in df_fomento['detalhes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fomento.iloc[0]['financiadora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "cols_geninfo = ['financiadora','titulo','palavras-chave']\n",
    "cols_details = ['elegibilidade','descricao','valorfinanciado','datalimite']\n",
    "cols_moreinf = ['formasolicitacao']\n",
    "id=1\n",
    "w = 125\n",
    "\n",
    "for id,_ in enumerate(df_fomento.index):\n",
    "    print('-'*125)\n",
    "    print(f\"{cols_geninfo[-1].upper():>15}: {df_fomento.iloc[id][cols_geninfo[-1]].upper()} | {df_fomento.iloc[id][cols_geninfo[0]]}\")\n",
    "    for j in cols_details:\n",
    "        print(f\"{j.upper():>15}: {df_fomento['detalhes'][id][j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in df_fomento['detalhes'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fomento['detalhes'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Analisar dos dados de fomento - fase exploratória</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade semântica por similaridade de cossenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuml.cluster import KMeans, DBSCAN, HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import networkx as nx\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity_torch(a, b):\n",
    "  \"\"\"\n",
    "  Calcula a similaridade cosseno entre dois tensores a e b.\n",
    "\n",
    "  Args:\n",
    "      a: Primeiro tensor (torch.Tensor).\n",
    "      b: Segundo tensor (torch.Tensor).\n",
    "\n",
    "  Returns:\n",
    "      A similaridade cosseno entre os tensores a e b (um tensor com um único valor entre 0 e 1).\n",
    "  \"\"\"\n",
    "  return 1 - F.cosine_similarity(a, b, dim=0)\n",
    "\n",
    "def cosine_similarity_array(a, b):\n",
    "  \"\"\"\n",
    "  Calcula a similaridade cosseno entre dois vetores a e b.\n",
    "\n",
    "  Args:\n",
    "      a: Primeiro vetor (numpy array).\n",
    "      b: Segundo vetor (numpy array).\n",
    "\n",
    "  Returns:\n",
    "      A similaridade cosseno entre os vetores a e b (um valor entre 0 e 1).\n",
    "  \"\"\"\n",
    "  return 1 - cosine(a, b)\n",
    "\n",
    "# Criar nova coluna 'texto_para_embedding' combinando as informações desejadas\n",
    "def extrair_texto_para_embedding(row):\n",
    "    detalhes = row['detalhes']\n",
    "    texto = \"\"\n",
    "    if detalhes:\n",
    "        texto += detalhes.get('elegibilidade', '') + ' ' + detalhes.get('descricao', '')\n",
    "    texto += ' ' + row['palavras-chave']\n",
    "    return texto\n",
    "\n",
    "# Carregar o dataframe df_fomento (carregado e processado anteriormente)\n",
    "df_fomento = pdf\n",
    "\n",
    "# Preparar Dados\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2').to('cuda')\n",
    "\n",
    "# Combinar informações relevantes em um único texto\n",
    "df_fomento['texto_para_embedding'] = df_fomento.apply(extrair_texto_para_embedding, axis=1)\n",
    "\n",
    "# Gerar embeddings de texto na GPU\n",
    "embeddings = model.encode(df_fomento['texto_para_embedding'].tolist(), convert_to_tensor=True, device='cuda')\n",
    "embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "# Função para calcular a similaridade cosseno\n",
    "def cosine_similarity(a, b):\n",
    "    return 1 - cosine(a, b)\n",
    "\n",
    "# Benchmark e Agrupamento em Comunidades\n",
    "algorithms = {\n",
    "    'KMeans': KMeans(n_clusters=5, init='k-means++', random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'HDBSCAN': HDBSCAN(min_cluster_size=5, min_samples=2)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, algorithm in algorithms.items():\n",
    "    start_time = time.time()\n",
    "    clusters = algorithm.fit_predict(embeddings)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Criar Arestas no Grafo (dentro do loop para cada algoritmo)\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Obter todas as chaves únicas dos dicionários em 'detalhes'\n",
    "    all_keys = set()\n",
    "    for detalhes in df_fomento['detalhes']:\n",
    "        if detalhes:\n",
    "            all_keys.update(detalhes.keys())\n",
    "\n",
    "    similarity_threshold = 0.7\n",
    "\n",
    "    # Adicionar nós e arestas ao grafo\n",
    "    for i in range(len(embeddings)):\n",
    "        # Criar um nó com o índice da linha como ID e adicionar as propriedades do dicionário\n",
    "        G.add_node(i, **df_fomento.iloc[i]['detalhes'])\n",
    "\n",
    "        for j in range(i + 1, len(embeddings)):\n",
    "            if clusters[i] == clusters[j]:\n",
    "                similarity = cosine_similarity(embeddings[i], embeddings[j])\n",
    "                if similarity > similarity_threshold:\n",
    "                    G.add_edge(i, j, weight=similarity)\n",
    "\n",
    "    # Calcular métricas de avaliação\n",
    "    partition = {node: cluster for node, cluster in enumerate(clusters)}\n",
    "\n",
    "    # Converter clusters escalares em listas\n",
    "    communities = [[c] if isinstance(c, np.int32) else c for c in partition.values()] # type: ignore\n",
    "\n",
    "    # Remover clusters vazios\n",
    "    communities = [c for c in communities if c]\n",
    "\n",
    "    # Verificar se 'communities' é uma partição válida antes de calcular a modularidade\n",
    "    if nx.algorithms.community.is_partition(G, communities):\n",
    "        modularity = nx.algorithms.community.modularity(G, communities)\n",
    "    else:\n",
    "        print(f\"Aviso: {name} gerou uma partição inválida. Modularidade não será calculada.\")\n",
    "        modularity = None  # Ou outro valor padrão, como 0 ou -1\n",
    "\n",
    "    results[name] = {\n",
    "        'execution_time': execution_time,\n",
    "        'modularity': modularity,\n",
    "    }\n",
    "\n",
    "# Plotar Resultados\n",
    "fig = go.Figure()\n",
    "for name, result in results.items():\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=[name],\n",
    "        y=[result['execution_time']],\n",
    "        text=[f\"Tempo: {result['execution_time']:.2f}s<br>Modularidade: {result['modularity']:.3f}\"],\n",
    "        textposition='auto',\n",
    "        name=name\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Benchmark de Algoritmos de Agrupamento',\n",
    "    xaxis_title='Algoritmo',\n",
    "    yaxis_title='Tempo de Execução (segundos)'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir tempo para calcular Pagerank\n",
    "# We now have data as edge pairs\n",
    "# create a Graph using the source (src) and destination (dst) vertex pairs\n",
    "# G = cugraph.Graph()\n",
    "# G.from_cudf_edgelist(gdf, source='src', destination='dst')\n",
    "\n",
    "# # Let's now get the PageRank score of each vertex by calling cugraph.pagerank\n",
    "# df_page = cugraph.pagerank(G)\n",
    "\n",
    "# # Let's look at the top 10 PageRank Score\n",
    "# df_page.sort_values('pagerank', ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.08",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
