{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "video_url = \"https://developer.download.nvidia.com/video/HPC/HPC-SDK_KV_1145x220_v005.mp4\"\n",
    "# Criar a tag de vídeo HTML com o atributo loop\n",
    "video_tag = f'<video width=\"100%\" controls loop> <source src=\"{video_url}\" type=\"video/mp4\"> </video>'\n",
    "HTML(video_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Contexto e definição do fluxo da pesquisa</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filosofia e ciência da economia baseada em inovação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grandes filósofos durante toda história tem se oposto ao totalitarismo. Pelo menos a partir da visão Aristotélica as formas degeneradas de governar já são apontadas. Dentre as chamadas formas degeneradas, a tirania pode ser entendida como a pior forma de sistema político, sendo o resultado da maior degeneração do poder centralizado em uma única pessoa (como em uma monarquia absolutista). Na tirania o tirano governa sem consultar a ninguém e, geralmente, sua tomada de poder é ilegítima, e a lei não tem papel algum. Na oligarquia, onde o poder de decisão do governo é constituído por um pequeno número de privilegiados, há uma subclassificação quanto ao número dos donos do poder, como por exemplo, na Politirania onde os oligarcas governam hereditariamente e na riqueza, mas respeitando mais a lei. Ou nas oligarquias com uma maior percentagem de oligarcas, quando passa-se da hereditariedade à nomeação dos amigos do governo, independentemente da linhagem sanguínea destes, dentre outros tipos de oligarquia. \n",
    "\n",
    "Tomas de Aquino foi um filósofo de referência no papel da moral como orientação maior para a sociedade. Mais recentemente, Karl Popper foi um severo crítico ao totalitarismo e utopias demagógicas. São perceptíveis interseções e diálogos filosóficos entre as ideias centrais dessas filosofias, especialmente em epistemologia e ética, têm sido tema de discussão acadêmica. Por exemplo, na dissertação de David Gregory Broderick, intitulada \"Objetividade: Tomás de Aquino e Karl Popper\", explora-se a relação entre as noções de objetividade nas obras de Aquino e Popper. A dissertação sugere que, embora haja diferenças terminológicas, históricas e de interesses entre Aquino e Popper, também existem paralelos significativos em suas abordagens sobre objetividade e o crescimento do conhecimento. Além disso, na discussão sobre as raízes éticas da epistemologia de Popper, explorada pelo Grupo Ciencia, Razón y Fe (CRYF) da Universidad de Navarra, destaca-se a complementaridade das posições de Aquino e Popper. Esta análise sugere que a forte defesa de Popper do realismo, da verdade objetiva e da metodologia para o crescimento do conhecimento conjectural pode ser vista como complementar, ou até mesmo fundamentada, nos princípios éticos e metafísicos delineados por Aquino.\n",
    "\n",
    "Entendemos portanto, que embora Popper possa não ter citado explicitamente Aquino em suas principais obras, as bases filosóficas de seus pensamentos, especialmente em relação à objetividade, ética e crescimento do conhecimento, têm grandes paralelos e áreas de convergência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, tal como acontece com a sua defesa das eleições numa democracia, o argumento de Popper a favor da engenharia social fragmentada baseia-se principalmente na sua compatibilidade com o método de tentativa e erro das ciências naturais: uma teoria é proposta e testada, erros na teoria são detectados e eliminado, e uma teoria nova e melhorada emerge, reiniciando o ciclo. Através da engenharia gradual, o processo de progresso social é, portanto, paralelo ao progresso científico. Na verdade, Popper diz que a engenharia social fragmentada é a única abordagem à política pública que pode ser genuinamente científica: \"Isto - e nenhum planeamento utópico ou profecia histórica - significaria a introdução do método científico na política, uma vez que todo o segredo do método científico é uma disposição para aprender com os erros\" ( Open Society Vol 1., 163)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papel da Inovação Tecnológica nas economias modernas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O termo \"inovação tecnológica\", no sentido moderno, começou a ser mais amplamente discutido e compreendido no início do século XX. Joseph Schumpeter, um economista austríaco, foi uma das figuras-chave que destacou a inovação tecnológica como um motor para o crescimento econômico. Após a Primeira Guerra Mundial, pensadores como Thorstein Veblen e Herbert Hoover também enfatizaram a importância da inovação tecnológica para a segurança nacional e competitividade industrial. Esse conceito se expandiu particularmente após a Segunda Guerra Mundial, quando a inovação tecnológica começou a ser vista como crucial para a prosperidade industrial e a segurança militar, especialmente nos Estados Unidos. \n",
    "\n",
    "fonte: Encyclopedia.com sobre Inovação Tecnológica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abordagem falseável em Metodologia da Pesquisa Científica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma contribuição seminal para abordagem falseável da ciência é a descrita por Karl Popper em sua obra \"A Lógica da Pesquisa Científica\", publicada originalmente em 1934. É nesta obra que Popper argumenta que a ciência deve adotar uma metodologia baseada na falseabilidade. Segundo ele, nenhuma quantidade de experimentos pode provar uma teoria, mas um único experimento ou observação reproduzível pode refutá-la. Esta abordagem destaca a importância da capacidade de uma teoria ser testada e potencialmente falsificada, em vez de apenas verificada. Popper diferencia as teorias científicas das pseudociências e da metafísica, salientando que as teorias científicas devem ser testáveis e passíveis de refutação, enquanto pseudociências e metafísicas não permitem essa possibilidade. Popper enfatiza que a ciência deve estar em constante evolução, com as hipóteses sendo submetidas a testes contínuos para acompanhar o desenvolvimento da ciência e das tecnologias (POPPER, 1934)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos pré-treinados para similaridade semântica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi realizada uma análise completa e cientificamente robusta, para definir os modelos pré-treinados mais adequados para os dados específicos em análise, que envolviam textos em inglês e português sobre artigos, projetos de pesquisa e editais de fomento no âmbito da ICT em estudo.\n",
    "\n",
    "Para garantir a validade científica da análise, foram executadas:\n",
    "\n",
    "- Medidos e documentados os desempenhos dos modelos testados, registrando nomes e configurações de cada modelo avaliado.\n",
    "\n",
    "- A escolha dos modelos testados, foi realizada com base nas suas características, desempenho em benchmarks e adequação à tarefa de aproximação semântica em dados de texto.\n",
    "\n",
    "- Os resultados foram comparados de forma quantitativa, com métricas de avaliação de clusterização (índice de silhueta, índice de Davies-Bouldin) para comparar o desempenho dos modelos de forma objetiva.\n",
    "\n",
    "- Foi realizada ainda uma analise qualitativa, ao examinar os clusters gerados por cada modelo e avaliado se eles faziam sentido em relação aos seus dados e objetivos de delineamento de competências em pesquisa, desenvolvimento e inovação.\n",
    "\n",
    "- As principais limitações da análise foram discutidas, como o tamanho do conjunto de dados, a qualidade da tradução e a escolha dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No contexto de aprendizado de máquina e processamento de linguagem natural, \"embeddings\" se refere à representação matemática de um objeto (como uma palavra, frase ou imagem) em um espaço vetorial.\n",
    "\n",
    "Para analisar a qualidade dos clusters criados a partir do embeeding com modelos pré-treinados foi criada a classe EmbeddingsMulticriteriaAnalysis para oferecer uma análise completa e cientificamente válida para a escolha do melhor modelo de embedding. Foram utilizadas práticas de múltiplas rodadas e validação cruzada na classe nos métodos evaluate_clustering e calcular_pontuacao_multicriterio.\n",
    "\n",
    "O método evaluate_clustering executa o processo de clustering múltiplas vezes e calcula a média e o desvio padrão das métricas. A validação cruzada é incorporada dividindo os embeddings em conjuntos de treinamento e teste.\n",
    "\n",
    "O método calcular_pontuacao_multicriterio calcula a pontuação multicritério para cada algoritmo usando as médias das métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos Monolíngues (Inglês):\n",
    "\n",
    "- all-mpnet-base-v2: Este modelo é uma versão do MPNet que foi treinado em um conjunto de dados massivo de textos em inglês. Ele apresenta excelente desempenho em tarefas de similaridade semântica.\n",
    "\n",
    "- all-MiniLM-L6-v2: Este modelo é uma versão menor e mais rápida do MPNet, que ainda mantém um bom desempenho em tarefas de similaridade semântica. Servirá como opção para opção por modelo com menor consumo de recursos.\n",
    "\n",
    "- all-distilroberta-v1: Este modelo é baseado no RoBERTa, que é uma versão otimizada do BERT. Ele geralmente apresenta bom desempenho em tarefas de PLN em inglês e pode ser uma boa alternativa aos modelos baseados em MPNet.\n",
    "\n",
    "- sentence-transformers/gtr-t5-large: Este modelo é baseado na arquitetura T5 e foi treinado especificamente para tarefas de recuperação de texto (e.g., encontrar documentos relevantes para uma consulta). Uma opção para gerar embeddings que capturem bem a semântica dos documentos para fins de comparação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos Multilíngues com foco em Inglês:\n",
    "\n",
    "- sentence-transformers/xlm-r-100langs-bert-base-nli-mean-tokens: Este modelo é baseado no XLM-RoBERTa e foi treinado em 100 idiomas, com foco em inglês. Uma opção de um modelo que funciona bem em inglês e também pode generalizar para outros idiomas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilização dos modelos pré-treinados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pensando em como os embeddings serão usados nas diferentes etapas do projeto, tivemos dois momentos distintos:\n",
    "\n",
    "Avaliar modelos de embedding para clustering: para comparar diferentes modelos pré-treinados e medir qual gera os melhores embeddings para agrupar editais de fomento.\n",
    "\n",
    "Avaliar similaridade no grafo: após escolher o melhor modelo, usamos os embeddings para calcular a similaridade entre editais de fomento e artigos, entre artigos e competências, e assim sucessivamente de acordo com os objetivos de cada análise. \n",
    "\n",
    "Para cada momento, a forma ideal de gerar os embeddings foi a seguinte:\n",
    "\n",
    "1. Avaliar modelos para clustering:\n",
    "\n",
    "Removido o histórico de gradientes: para avaliar o clustering não é necessário o histórico de gradientes. O parâmetro manter_gradientes=False na sua função save_embeddings_dict economiza memória e tempo de processamento.\n",
    "\n",
    "2. Avaliar similaridade no grafo:\n",
    "\n",
    "Mantidos o histórico de gradientes: para usar os embeddings para calcular similaridade no grafo e integrar essa informação no processo de treinamento e de fine-tuning futuro, foi crucial manter o histórico de gradientes. O parâmetro manter_gradientes=True na função save_embeddings_dict permite que, no futuro se possa ralizar:\n",
    "\n",
    "Fine-tune os embeddings: ajustar os embeddings com base na similaridade no grafo, possivelmente melhorando a qualidade das representações e do clustering.\n",
    "\n",
    "Treinar o modelo: pode-se usar os embeddings como entrada para o modelo que aprendeu a prever relações entre entidades no grafo de conhecimento, como entre editais de fomento e artigos prévios de pesquisadores, dentre outras tarefas.\n",
    "\n",
    "Analisar a influência dos vizinhos: com o histórico de gradientes, pode-se analisar como a similaridade com os vizinhos no grafo influencia os embeddings e o desempenho do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequência de módulos que processam o texto de entrada para gerar o embedding final é exibida na representação em string de cada modelo. Cada módulo tem uma função específica exibido através de um número entre parênteses que indica os principais parâmetros e como se comporta o modelo. Essa estrutura modular permite que os modelos SentenceTransformer sejam flexíveis e adaptáveis a diferentes tarefas e datasets.\n",
    "\n",
    "Na representação em string de um objeto SentenceTransformer os números entre parênteses indicam os módulos que compõem esse modelo, na ordem em que são aplicados ao texto de entrada. Os vários módulos podem variar, geralmente, entre:\n",
    "\n",
    "- (0): Transformer(...): É o módulo principal, responsável por gerar as representações vetoriais (embeddings) das palavras no texto. Ele utiliza um modelo Transformer pré-treinado, como o T5EncoderModel, RobertaModel, MPNetModel ou BertModel, dependendo do modelo de base. Os parâmetros max_seq_length e do_lower_case controlam o tamanho máximo da sequência de entrada e se o texto deve ser convertido para minúsculas, respectivamente.\n",
    "\n",
    "- (1): Pooling(...): Esse módulo agrega as representações vetoriais das palavras em um único vetor que representa a sentença inteira. Isso indica que o modelo utiliza diferentes estratégias de pooling, como mean_tokens (média dos tokens), cls_token (usar o token especial de classificação), etc. Os parâmetros word_embedding_dimension e pooling_mode_* controlam a dimensão dos embeddings e o modo de pooling, respectivamente.\n",
    "\n",
    "- (2): Dense(...): Esse módulo opcional é uma camada densa (fully connected) que pode ser aplicada após o pooling para reduzir a dimensionalidade do vetor de embedding ou realizar outras transformações. Os parâmetros in_features, out_features, bias e activation_function controlam as características da camada densa.\n",
    "\n",
    "(3): Normalize(): Esse módulo opcional normaliza o vetor de embedding, geralmente para ter norma unitária."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada módulo mencionado acima (Transformer, Pooling, Dense, Normalize) pode ser considerado uma camada ou um conjunto de camadas em uma rede neural. Eles são os blocos de construção que, combinados em uma ordem específica, formam a arquitetura da rede neural do SentenceTransformer.\n",
    "\n",
    "Em uma analogia com processamento fabril, imaginemos uma fábrica onde cada módulo representa uma etapa no processo de produção, o comportamento da arquitetura é o seguinte:\n",
    "\n",
    "Transformer: Essa é a etapa principal, onde a matéria-prima (texto) é transformada em um produto intermediário (representações vetoriais das palavras). É como se fosse a etapa de \"montagem\" na fábrica.\n",
    "\n",
    "Pooling: Essa etapa pega os produtos intermediários (representações das palavras) e os combina para formar o produto final (representação da sentença). É como a etapa de \"embalagem\" na fábrica.\n",
    "\n",
    "Dense: (opcional) Essa etapa pode ser adicionada para refinar o produto final, como adicionar uma camada de proteção ou personalização. Seria como uma etapa de \"acabamento\" na fábrica.\n",
    "\n",
    "Normalize: (opcional) Essa etapa garante que o produto final esteja dentro dos padrões de qualidade, como garantir que tenha o tamanho e peso corretos. É como a etapa de \"controle de qualidade\" na fábrica.\n",
    "\n",
    "Cada módulo contém seus próprios parâmetros (pesos e biases) que são aprendidos durante o treinamento da rede neural. A ordem dos módulos define o fluxo de informações na rede neural, da entrada (texto) até a saída (embedding). A combinação de diferentes módulos e seus parâmetros permite criar redes neurais com diferentes capacidades e comportamentos.\n",
    "\n",
    "\n",
    "Os módulos são análogos a camadas em uma rede neural. Eles são os componentes básicos que, combinados de forma estratégica, permitem criar modelos complexos e poderosos para gerar embeddings de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceitos de pesos e vieses em Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em redes neurais, os conceitos de pesos e viés são são parâmetros essenciais que controlam o comportamento dos neurônios e da rede neural como um todo. Eles trabalham em conjunto para permitir que a rede aprenda com os dados e faça previsões precisas.\n",
    "\n",
    "- Pesos: Controlam a força da conexão entre dois neurônios. Um peso maior significa que o sinal de entrada terá um impacto maior na ativação do neurônio seguinte.\n",
    "\n",
    "- Viés: Ajusta o limiar de ativação do neurônio. Ele permite que o neurônio seja ativado mesmo com uma entrada fraca, ou que precise de uma entrada mais forte para ser ativado.\n",
    "\n",
    "Imagine um neurônio como um interruptor de luz com dimmer (potenciômetro de controle de intensidade).\n",
    "\n",
    "- Peso: Serve para controlar a intensidade da luz, aumentar um peso é como girar para reduzir a resistência elétrica do potenciômetro, ou seja, um deixar passar uma corrente elétrica maior, e assim, a luz acende com mais intensidade.\n",
    "\n",
    "- Viés: É como um regulador de sensibilidade do interruptor, ou seja a força que você tem que imprimir para ele girar. Se o viés for alto, a luz acende com um leve giro, um toque leve. Se o viés for baixo, você precisa pressionar com mais força para acender a luz.\n",
    "\n",
    "No contexto dos módulos da arquitetura de Sentece Transformers, cada módulo utiliza pesos e viés:\n",
    "\n",
    "Transformer: Os pesos controlam a importância de cada palavra e a relação entre elas na sequência de entrada. O viés ajusta a sensibilidade do modelo para diferentes padrões de linguagem.\n",
    "\n",
    "Pooling: Os pesos podem ser utilizados para dar diferentes importâncias às palavras durante a agregação (ex: dar mais peso ao verbo na sentença). O viés ajusta o limiar de ativação do mecanismo de pooling.\n",
    "\n",
    "Dense: Os pesos controlam a influência de cada dimensão do vetor de entrada na saída. O viés ajusta o limiar de ativação de cada neurônio na camada densa.\n",
    "\n",
    "Relação com o aprendizado:\n",
    "\n",
    "Durante o treinamento da rede neural, os pesos e vieses são ajustados para minimizar o erro do modelo. O objetivo é encontrar os valores que permitem à rede generalizar bem para novos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porque não modelos GPT ou LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os mais recentes modelos GPT são realmente poderosos e têm ganhado muita atenção. No entanto, para a necessidade específica de análise de similaridade e clusterização para construir o KGNN, os modelos BERT, especialmente aqueles pré-treinados com foco em semântica, como os da Sentence Transformers, tendem a ser mais adequados. Embora os modelos GPT sejam poderosos, para a necessidade específica de análise de similaridade semântica e clusterização para KGNN, os modelos BERT, especialmente os da Sentence Transformers, atualmente ainda são mais adequados e acessíveis, principalmente por oferecem em código aberto modelos para gerar embeddings de alta qualidade que representam o significado das frases de forma eficaz, o que é crucial para a tarefa de aproximação semântica.\n",
    "\n",
    "Comparando BERT e GPT:\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers):\n",
    "\n",
    "Treinado para entender o contexto de uma palavra considerando as palavras ao seu redor em ambas as direções (bidirecional).\n",
    "Excelente para capturar relações semânticas entre palavras e frases.\n",
    "Gera embeddings de alta qualidade que representam o significado das frases.\n",
    "Ideal para tarefas de similaridade, classificação e clusterização.\n",
    "GPT (Generative Pre-trained Transformer):\n",
    "\n",
    "Treinado para gerar texto, prevendo a próxima palavra em uma sequência (unidirecional).\n",
    "Muito bom em gerar textos coerentes e criativos.\n",
    "Pode ser usado para gerar embeddings, mas geralmente não são tão eficazes quanto os do BERT para tarefas de similaridade.\n",
    "Mais adequado para tarefas de geração de texto, tradução e resumo.\n",
    "Por que BERT é melhor para sua necessidade:\n",
    "\n",
    "Similaridade Semântica: BERT é especificamente projetado para capturar a semântica das frases, o que é crucial para a análise de similaridade e clusterização.\n",
    "Embeddings de Qualidade: Os modelos BERT, principalmente os da Sentence Transformers, são otimizados para gerar embeddings de alta qualidade que representam o significado das frases de forma eficaz.\n",
    "Eficiência: BERT é mais eficiente para gerar embeddings de frases do que GPT, que é mais focado em gerar sequências de texto longas.\n",
    "\n",
    "Acessibilidade:\n",
    "\n",
    "Sentence Transformers: Os modelos da Sentence Transformers são de código aberto e facilmente acessíveis através da biblioteca sentence_transformers. Eles são simples de usar e oferecem uma ampla variedade de modelos pré-treinados.\n",
    "\n",
    "GPT: Os modelos GPT, especialmente os maiores, como o GPT-3, geralmente exigem mais recursos computacionais e podem ter acesso restrito (e.g., através de APIs). No entanto, existem versões menores e de código aberto do GPT, como o GPT-2, que são mais acessíveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estruturar etapas da pesquisa para a tese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 01: Redigir uma boa questão de pesquisa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como chegar à uma boa questão de pesquisa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gênese de uma boa questão de pesquisa também é abordada por outros autores notáveis no campo da metodologia científica. Antonio Carlos Gil descreve que a elaboração de projetos de pesquisa deve ser guiada pela apresentação clara e acessível dos elementos necessários para a pesquisa, além da organização de conhecimentos dispersos. Gil enfatiza a natureza prática da pesquisa, abordando a importância de esclarecer os procedimentos para a elaboração de projetos em diversos tipos de pesquisa. (GIL, 2022).\n",
    "\n",
    "Alguns dos critérios essenciais para avaliar a qualidade de uma questão de pesquisa, e como integrar ou considerar cada um deles no processo de formulação:\n",
    "\n",
    "<b>Clareza e Especificidade</b>: Uma boa questão de pesquisa deve ser clara e específica, evitando ambiguidades. Isto pode ser parcialmente garantido pelo processamento de linguagem natural, mas também requer revisão humana para garantir que a questão seja compreensível e precisamente focada.\n",
    "\n",
    "<b>Relevância Acadêmica ou Científica</b>: A questão deve ser relevante para o campo de estudo e contribuir de alguma forma para o conhecimento existente. Isso geralmente requer uma compreensão do contexto acadêmico e das pesquisas atuais, o que pode ser além do escopo da automação completa.\n",
    "\n",
    "<b>Viabilidade</b>: A pergunta deve ser algo que pode ser realisticamente respondido através de métodos de pesquisa disponíveis. Este aspecto pode ser parcialmente verificado por meio de regras heurísticas programadas, mas frequentemente requer avaliação humana, especialmente para julgar a disponibilidade de dados ou recursos de pesquisa.\n",
    "\n",
    "<b>Originalidade</b>: Uma boa questão de pesquisa deve oferecer novas perspectivas ou abordar lacunas existentes na literatura. A originalidade pode ser difícil de avaliar automaticamente, mas técnicas avançadas de NLP, como análise semântica e comparação com bancos de dados de literatura existente, podem ajudar.\n",
    "\n",
    "<b>Importância Prática ou Teórica</b>: A questão deve ter alguma importância prática ou contribuir para a compreensão teórica de um tópico. Isso geralmente exige conhecimento especializado na área de estudo para avaliar.\n",
    "\n",
    "<b>Estruturação Adequada</b>: A pergunta deve ser estruturada de forma a facilitar uma abordagem de pesquisa clara. Isso inclui a utilização de uma formulação que se alinhe com métodos de pesquisa qualitativos ou quantitativos, conforme apropriado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como Integrar critérios de validação para responder a questão de pesquisa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerar uma boa questão de pesquisa depende de uma combinação de processamento automatizado, conhecimento especializado e revisão humana. A automação pode fornecer uma base útil, mas a supervisão e o julgamento humanos são cruciais para garantir a qualidade final da pergunta de pesquisa. As seguintes estratégias são utilizadas em nossa solução para chegar a uma boa questão de pesquisa:\n",
    "\n",
    "\n",
    "<b>Integração de várias Fontes de Dados de Literatura</b>: Integrar um banco de dados de literatura existente pode ajudar a avaliar a originalidade e a relevância da pergunta, comparando-a com pesquisas já publicadas.\n",
    "\n",
    "<b>Automatização com Revisão Humana</b>: Uma abordagem prática é usar a automação para gerar uma primeira versão da pergunta, que é então revisada e refinada por pesquisadores humanos (pesquisador principal, equipe de pesquisa e orientador). A automação garante que certos critérios básicos sejam atendidos (como clareza e estruturação), enquanto a revisão humana aborda aspectos mais sutis, como relevância, viabilidade e originalidade.\n",
    "\n",
    "<b>Instrução ao Usuário</b>: Fornecer orientações e exemplos de boas perguntas de pesquisa aos usuários pode ajudá-los a formular ideias iniciais mais eficazes, levando a melhores resultados na formulação automática.\n",
    "\n",
    "<b>Feedback Interativo</b>: Incorporar um sistema de feedback no processo, onde o usuário pode refinar suas ideias iniciais ou ajustar a pergunta gerada, pode ajudar a melhorar a qualidade da questão de pesquisa final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 02: Encontrar fontes de dados adequadas para realizar a pesquisa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editoras científicas com políticas de Open Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Várias editoras científicas proeminentes oferecem conteúdos em Open Access, proporcionando acesso livre a uma vasta gama de pesquisas acadêmicas. Algumas das principais editoras incluem:\n",
    "\n",
    "<b>Public Library of Science (PLOS)</b>: Fundada como uma organização sem fins lucrativos, a PLOS tem como objetivo catalisar o movimento de acesso aberto. Publica periódicos em diversas áreas, incluindo medicina e ciências da vida.\n",
    "\n",
    "<b>Springer Nature</b>: Com mais de 600 periódicos totalmente em acesso aberto e mais de 1000 livros em acesso aberto, a Springer Nature é uma das pioneiras no campo da pesquisa aberta. Oferece uma variedade de opções de publicação em acesso aberto, mantendo rigorosos processos de revisão por pares e editoriais.\n",
    "\n",
    "<b>Wiley</b>: A Wiley oferece mais de 150 periódicos revisados por pares em acesso aberto, abrangendo diversas disciplinas de pesquisa. Seus periódicos em acesso aberto estão disponíveis para leitura, download e compartilhamento gratuitamente através da Wiley Online Library e do PubMed Central.\n",
    "\n",
    "<b>Oxford University Press (OUP)</b>: A OUP publica mais de 120 periódicos totalmente em acesso aberto e mais de 250 livros em acesso aberto, abrangendo uma ampla gama de disciplinas. Muitos de seus periódicos são classificados como \"diamond OA\", o que significa que não há taxas de processamento de artigos para autores ou leitores.\n",
    "\n",
    "<b>Frontiers</b>: Reconhecida como uma editora líder em Acesso Aberto e plataforma de Ciência Aberta, a Frontiers é muito citada, com mais de um bilhão de visualizações e downloads e 1.6 milhão de citações em seus artigos acessíveis gratuitamente. Ela é ativa em áreas como neurociências, psiquiatria, fisiologia, medicina clínica, ciências naturais e engenharia.\n",
    "\n",
    "<b>MDPI AG</b>: Uma empresa suíça com presença global, a MDPI AG publica periódicos em ciência e engenharia, ciências sociais e filosofia.\n",
    "\n",
    "<b>Informa PLC</b>: Uma das maiores editoras nas humanidades e ciências sociais, a Informa publica em acesso aberto sob quatro selos: Taylor & Francis Open, Dove Medical Press, Cogent OA e Routledge Open.\n",
    "\n",
    "<b>Hindawi</b>: Inicialmente uma editora de periódicos por assinatura, a Hindawi fez a transição para um modelo de publicação em acesso aberto entre 2004 e 2007. Ela publica periódicos em ciência e engenharia, ciências sociais e filosofia.\n",
    "\n",
    "Estas editoras são conhecidas não apenas pela qualidade e diversidade de suas publicações, mas também por suas contribuições significativas para o movimento de acesso aberto na comunidade acadêmica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bases de dados digitais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao focar a pesquisa em fontes que oferecem conteúdos completos em Open Access, há bases de dados e repositórios acadêmicos importantes que se destacam em relevância e quantidade de conteúdos:\n",
    "\n",
    "<b>Directory of Open Access Journals (DOAJ)</b>: O DOAJ é um diretório online que indexa e fornece acesso a periódicos de alta qualidade, todos de acesso livre e revisados por pares. É uma excelente fonte para pesquisar artigos em uma ampla gama de disciplinas. O DOAJ oferece uma API para acessar seu índice de periódicos e artigos. A documentação e detalhes sobre a API estão disponíveis em DOAJ API.\n",
    "\n",
    "<b>PubMed Central</b>: Operado pela Biblioteca Nacional de Medicina dos EUA, o PubMed Central é um repositório gratuito de artigos de ciências biomédicas e ciências da vida. Embora seu foco seja mais na área da saúde, ele pode ter artigos relevantes sobre inovação tecnológica no contexto da saúde. A PubMed Central oferece uma API chamada Entrez Programming Utilities (E-utilities) para interagir com a base de dados. Mais informações podem ser encontradas em Entrez Programming Utilities Help.\n",
    "\n",
    "<b>arXiv</b>: O arXiv é um repositório de preprints em campos como física, matemática, ciência da computação, biologia quantitativa, finanças quantitativas e estatística. É uma boa fonte para literatura mais técnica e teórica sobre inovação tecnológica. O arXiv fornece uma API para acesso aos seus preprints. Informações detalhadas e documentação sobre a API estão disponíveis em arXiv API.\n",
    "\n",
    "<b>OpenAIRE</b>: Uma infraestrutura que promove a descoberta e o acesso a publicações científicas europeias de acesso livre. É especialmente útil para pesquisas que envolvem colaborações europeias ou focam em políticas e práticas de inovação na Europa. OpenAIRE oferece uma API para acessar seu repositório. Você pode encontrar mais informações sobre como usar esta API em OpenAIRE API.\n",
    "\n",
    "<b>Google Scholar</b>: Apesar de não ser exclusivamente dedicado ao Open Access, o Google Scholar pode ser utilizado para localizar artigos de acesso livre. Ele indexa uma variedade de fontes acadêmicas e muitas vezes inclui links para versões de acesso livre dos artigos. O Google Scholar não oferece uma API oficial para acesso programático.\n",
    "\n",
    "<b>CORE</b>: Agregador que permite o acesso a milhões de artigos de acesso livre. Ele reúne conteúdo de repositórios e periódicos de todo o mundo, sendo uma excelente ferramenta para uma pesquisa abrangente. CORE oferece uma API que permite acessar seu vasto repositório de artigos de acesso livre. A documentação da API pode ser encontrada em CORE API.\n",
    "\n",
    "<b>ScienceOpen</b>: Plataforma de pesquisa e publicação que oferece acesso a mais de 60 milhões de artigos e registros de pesquisa em todas as áreas. Não há informações disponíveis sobre uma API pública para o ScienceOpen.\n",
    "\n",
    "<b>SSRN (Social Science Research Network)</b>: Especializado em ciências sociais, o SSRN é um repositório de preprints que abrange uma ampla gama de áreas, incluindo economia, direito e gestão corporativa, onde você pode encontrar trabalhos relacionados à gestão da inovação. O SSRN não fornece uma API pública para acesso programático aos seus conteúdos.\n",
    "\n",
    "\n",
    "O tipo de busca mais comum é por palavras-chave, usadas para descobrir e para refinar a pesquisa e localizar artigos relevantes sobre os temas de interesse, no nosso caso, a gestão da inovação tecnológica em organizações públicas e privadas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 03: Modelar o problema de pesquisa em grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagrama estratégias para modelar o mundo real em grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "  mindmap\n",
    "  root((Modelagem do Mundo Real em Grafos para Aplicações em Saúde))\n",
    "    Grafos Simples\n",
    "      Aplicações\n",
    "        Interações Medicamento-Medicamento (Efeitos Adversos)\n",
    "        Co-ocorrência de Doenças (Comorbidades)\n",
    "      Limitações\n",
    "        Não modela a direção ou força das relações\n",
    "    Grafos Direcionados\n",
    "      Aplicações\n",
    "        Propagação de Doenças Infecciosas (Transmissão)\n",
    "        Vias Metabólicas (Reações Bioquímicas)\n",
    "      Limitações\n",
    "        Não modela múltiplas relações entre os mesmos elementos\n",
    "    Grafos Ponderados\n",
    "      Aplicações\n",
    "        Redes de Interação Proteína-Proteína (Afinidade de Ligação)\n",
    "        Redes de Coexpressão Gênica (Correlação)\n",
    "      Limitações\n",
    "        Não modela diferentes tipos de relações simultaneamente\n",
    "    Grafos Multicamadas (Multiplex)\n",
    "      Aplicações\n",
    "        Redes de Interação Fármaco-Alvo-Doença (Múltiplos Mecanismos)\n",
    "        Redes de Microbioma Humano (Diferentes Nichos Tecnológicos)\n",
    "      Limitações\n",
    "        Aumenta a complexidade da análise\n",
    "    Hipergrafos\n",
    "      Aplicações\n",
    "        Análise de Dados de Saúde Multimodais (Prontuários, Imagens, Genética)\n",
    "        Modelagem de Fatores de Risco Complexos para Doenças Multifatoriais\n",
    "      Limitações\n",
    "        Dificuldade de visualização e análise\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagrama desafio de promover inovação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "mindmap\n",
    "  root((Inovação, Competitividade e Novo Padrão Tecnológico da Indústria Brasileira))\n",
    "    (Conclusões)\n",
    "      Baixa intensidade de PDI como desafio para a competitividade\n",
    "      Necessidade de investimentos em PDI para aumentar a produtividade\n",
    "      Importância de políticas públicas de incentivo à inovação\n",
    "\n",
    "      (Implicações)\n",
    "          Foco em setores estratégicos com potencial de crescimento e inovação\n",
    "          Aumento do investimento em PDI\n",
    "          Políticas públicas que incentivem a inovação\n",
    "\n",
    "          (Recomendações)\n",
    "              Aumento do investimento público e privado em PDI\n",
    "              Incentivos fiscais para empresas que investem em inovação\n",
    "              Fortalecimento da cooperação entre universidades e empresas\n",
    "              Investimento em educação e formação de mão de obra qualificada\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "[![](https://mermaid.ink/img/pako:eNp9VM2O0zAQfpVRDqiVygvkxrJC6oFVBRxzmdqT7ojYk_VPBVrtuyA4IA6cEJe95sUYN_2L2iWKFHsy-f7s-LEyYqmqK8feOuwbDxBE0my29LLF4efwQxbwVlxPiRNv2aIlILiTrcAKbdD38ImMl274s2EjYBGW3g7PMQVGuAkYuSMOOJ8XbFAsb7och78UxwJAXeuHftYitPh6LfJ5fngDcIP8BYF9Ih9Hcr1Xr25VAx8VghGnzBSxZYEeAwKW2rnoE-QdGYonMPZbiokd-SQRyF2ij4C5dGBQ5D6IzZe4S9dLSMN3b9S4Aveaye_EBiP0w_O62412hEaRWAMcvp3RNH4CVdq1fhbTeL0TDVlVRkoSSAVr0JiGXyX8WEwrr4alGrpCZrTHjOYmpqagb_LYYWUSx7U0ph-urnl8yCeTioAvmCzXB1LF5O1Vq_9TtmcrpvrAW7QHsZcQy0PeEVqOBjmOK0qu13COegu4Quj9ktkx_pCwo0Oktuwz6Snst4oWA0H2ajyMW0xX6Mh0Tds0bbLZHKCgleD2E11Jt3_KWrU_ZOy41bQtNr5aVI60la3-yI-FpKnSPTlqqlqHllrMXWqqxj9pK-YkH796U9UpZFpUQfLmvqpb7KLOcm8x0S3jJqA7Vsmy7rb341GxOzGe_gFTjYIM?type=png)](https://mermaid.live/edit#pako:eNp9VM2O0zAQfpVRDqiVygvkxrJC6oFVBRxzmdqT7ojYk_VPBVrtuyA4IA6cEJe95sUYN_2L2iWKFHsy-f7s-LEyYqmqK8feOuwbDxBE0my29LLF4efwQxbwVlxPiRNv2aIlILiTrcAKbdD38ImMl274s2EjYBGW3g7PMQVGuAkYuSMOOJ8XbFAsb7och78UxwJAXeuHftYitPh6LfJ5fngDcIP8BYF9Ih9Hcr1Xr25VAx8VghGnzBSxZYEeAwKW2rnoE-QdGYonMPZbiokd-SQRyF2ij4C5dGBQ5D6IzZe4S9dLSMN3b9S4Aveaye_EBiP0w_O62412hEaRWAMcvp3RNH4CVdq1fhbTeL0TDVlVRkoSSAVr0JiGXyX8WEwrr4alGrpCZrTHjOYmpqagb_LYYWUSx7U0ph-urnl8yCeTioAvmCzXB1LF5O1Vq_9TtmcrpvrAW7QHsZcQy0PeEVqOBjmOK0qu13COegu4Quj9ktkx_pCwo0Oktuwz6Snst4oWA0H2ajyMW0xX6Mh0Tds0bbLZHKCgleD2E11Jt3_KWrU_ZOy41bQtNr5aVI60la3-yI-FpKnSPTlqqlqHllrMXWqqxj9pK-YkH796U9UpZFpUQfLmvqpb7KLOcm8x0S3jJqA7Vsmy7rb341GxOzGe_gFTjYIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagrama de blocos das fases da implementação do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "graph LR\n",
    "    subgraph Entrar e Pré-processar Dados\n",
    "        A[Currículos Pesquisadores\\nda ICT]\n",
    "        B[Entidades do\\nCEIS]\n",
    "        C[Produtos Estratégicos\\nCEIS]\n",
    "        D[Normalizar,\\nLimpar e\\nUnificar Termos]\n",
    "    end\n",
    "\n",
    "    subgraph Gerar Embeddings em GPU\n",
    "        H[Embedding Multilingue\\n Sentence Transformers]\n",
    "        I[Codificar\\nCompetências]\n",
    "        J[Codificar\\nÁreas de Pesquisa]\n",
    "    end\n",
    "\n",
    "    subgraph Construir e Analisar Grafos em GPU/CPU\n",
    "        K[Adicionar\\nNós e Arestas]\n",
    "        L[Grafo Multiplex\\nMacroprocessos PDI]\n",
    "    end\n",
    "\n",
    "    subgraph Modelo de Aprendizagem\\nNão-Supervisionada em Grafos    \n",
    "        M[Detectar\\nComunidades]\n",
    "        N[Analisar\\nSimilaridade]\n",
    "        O[Identificar Lacunas\\nao maximizar \\nModularidade]\n",
    "    end\n",
    "\n",
    "    subgraph Recomendar e Visualizar\n",
    "        P[Recomendar\\nAlinhamento\\nCompetências/Produtos]\n",
    "        Q[Visualizar\\nResultados]\n",
    "    end\n",
    "\n",
    "    A --> D\n",
    "    B --> D\n",
    "    C --> D\n",
    "    D --> H\n",
    "    H --> I\n",
    "    H --> J\n",
    "    I --> K\n",
    "    J --> K\n",
    "    N <--> M\n",
    "    L --> M\n",
    "    L --> N    \n",
    "    M --> O\n",
    "    N --> O\n",
    "    K --> L\n",
    "    O --> P\n",
    "    P --> Q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagrama de componentes do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "flowchart LR\n",
    "    id1((Circle))\n",
    "    id2([Stadium])\n",
    "    id3[(Database)]\n",
    "    id4(Box with round corner)\n",
    "    id5{{Hex}}\n",
    "    id6[\\Parallelogram\\]\n",
    "    id7[\\Trapezoid/]\n",
    "\n",
    "    id1-- 1st line ---id2\n",
    "    id1--> |2nd line| id3\n",
    "    id1--- |3rd line| id4\n",
    "    id2-.-|4th line| id5\n",
    "    id3 == 5th line ==> id6\n",
    "    id4 <--> id7 --> id6\n",
    "\n",
    "    style id1 fill:green,stroke:black\n",
    "    style id2 fill:white,stroke:#f66,stroke-dasharray: 5, 5,color:black\n",
    "    style id3 fill:#66f,stroke:#f6f,stroke-width:4px\n",
    "    style id4 fill:red,stroke:yellow\n",
    "    style id5 fill:orange,stroke:white,color:black\n",
    "    style id6 fill:yellow,stroke:blue,color:black\n",
    "    style id7 fill:brown,stroke:blue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quadrant diagram with dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "quadrantChart\n",
    "    x-axis x1 --> x2\n",
    "    y-axis y1 --> y2\n",
    "    quadrant-1 Comenta\n",
    "    quadrant-2 Gosta\n",
    "    quadrant-3 Inscreve\n",
    "    quadrant-4 Compartilha\n",
    "    p1: [0.3, 0.7]\n",
    "    p2: [0.6, 0.2]\n",
    "    p3: [0.8, 0.9]\n",
    "    p4: [0.2, 0.4]\n",
    "    p5: [0.6, 0.7]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIMPAR CONDA PARA LIBERAR ESPAÇO EM DISCO, APAGAR AMBIENTES INATIVOS, USAR COM CUIDADO!\n",
    "# import subprocess\n",
    "\n",
    "# # Executa o comando e captura a saída\n",
    "# process = subprocess.Popen(['conda', 'clean', '--all'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# # Lê a saída e verifica se há uma linha indicando arquivos a serem removidos\n",
    "# for line in process.stdout:\n",
    "#     print(line, end='')  # Imprime a saída na célula do notebook\n",
    "#     if 'tarball(s)' in line:  # Procura pela linha com a informação dos arquivos\n",
    "#         # Solicita a confirmação do usuário\n",
    "#         confirmacao = input(\"Deseja prosseguir com a limpeza? (s/n): \")\n",
    "#         if confirmacao.lower() == 's':\n",
    "#             process.stdin.write('y\\n')  # Envia 'y' para confirmar\n",
    "#         else:\n",
    "#             process.stdin.write('n\\n')  # Envia 'n' para cancelar\n",
    "#         process.stdin.flush()\n",
    "\n",
    "# # Aguarda o término do processo e imprime o resultado\n",
    "# stdout, stderr = process.communicate()\n",
    "# print(stdout)\n",
    "# print(stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principais operações utilizadas no modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operações de Matrizes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiplicação de Matrizes:** A operação mais comum em modelos de transformadores (como os utilizados pelo Sentence Transformers) é a multiplicação de matrizes, usada para calcular as atenções e as transformações lineares nas camadas do modelo.\n",
    "\n",
    "**Soma de Matrizes:** Soma de matrizes é utilizada para combinar os resultados de diferentes camadas ou para adicionar informações contextuais aos embeddings.\n",
    "\n",
    "**Normalização:** A normalização de vetores (por exemplo, Layer Normalization) é utilizada para estabilizar o treinamento e melhorar o desempenho do modelo.\n",
    "Outras Operações: Outras operações de matrizes, como transposição, concatenação e divisão por elemento, também podem ser utilizadas em diferentes etapas da conversão de embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de Ativação:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU, GELU, etc.:** Funções de ativação não lineares são aplicadas aos resultados das operações de matrizes para introduzir não linearidade no modelo e permitir que ele aprenda padrões mais complexos nos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operações Específicas do Modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax:** Utilizado para calcular as probabilidades de atenção em modelos de transformadores.\n",
    "\n",
    "**Embedding Lookup:** Utilizado para converter tokens de texto em embeddings de palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Princípios para otimizar cálculo em GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerações sobre o Hardware CPU x GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU:** A maioria das operações mencionadas acima é altamente paralelizável e pode ser acelerada significativamente por GPUs, que são projetadas para realizar cálculos de matrizes de forma eficiente.\n",
    "\n",
    "**CPU:** As CPUs podem ser usadas para realizar essas operações, mas geralmente são mais lentas do que as GPUs, especialmente para grandes volumes de dados ou modelos complexos.\n",
    "\n",
    "Portanto, a conversão de embeddings é um processo computacionalmente intensivo, onde a GPU pode acelerar significativamente esse processo, especialmente para grandes modelos e volumes de dados devido principalmente a necessidade de **operações de matrizes** e **funções de ativação**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arquiteturas das Placas Nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tesla**:        Não possuem Tensor Cores. O número de núcleos CUDA por SM varia entre as diferentes gerações da arquitetura Tesla.\n",
    "\n",
    "**Pascal**:       Não possuem Tensor Cores. Cada SM possui 128 núcleos CUDA.\n",
    "\n",
    "**Volta**:        Introduziu os Tensor Cores. Cada SM possui 64 núcleos CUDA e 8 Tensor Cores.\n",
    "\n",
    "**Turing**:       Cada SM possui 64 núcleos CUDA e 8 Tensor Cores. Introduziu os RT Cores para acelerar o ray tracing.\n",
    "\n",
    "**Ampere**:       Cada SM possui 128 núcleos CUDA e 4 Tensor Cores de terceira geração. Os Tensor Cores de terceira geração são duas vezes mais rápidos que os da geração anterior.\n",
    "\n",
    "**Ada Lovelace**: Cada SM possui 128 núcleos CUDA e 4 Tensor Cores de quarta geração. Os Tensor Cores de quarta geração são ainda mais rápidos e eficientes que os da geração anterior.\n",
    "\n",
    "**Hopper**:       Cada SM possui 128 núcleos CUDA e 8 Tensor Cores de quarta geração. Introduziu o Transformer Engine, um novo tipo de núcleo especializado em acelerar modelos de linguagem baseados em Transformer.\n",
    "\n",
    "**Blackwell**:    (arquitetura futura): Ainda não há informações oficiais sobre a arquitetura Blackwell, mas espera-se que ela continue a tendência de aumentar o número de núcleos CUDA e Tensor Cores por SM, além de introduzir novas tecnologias para acelerar ainda mais as cargas de trabalho de IA e HPC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sobre a utilização de Programação paralela em GPUs NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compreender como cada componente da GPU contribui para o desempenho em GML permite que você escolha a GPU certa, otimize seu código e obtenha o máximo desempenho em suas aplicações de aprendizado de máquina em grafos. Temos três tipos básicos de componentes processadores (CUDA cores e SMs) e memória, sendo eles:\n",
    "\n",
    "- Núcleos CUDA: Unidades básicas de processamento individuais na GPU.\n",
    "- SMs (Streaming Multiprocessor): Blocos maiores de processamento que agrupam núcleos CUDA e outros recursos.\n",
    "- Memória (VRAM): Onde os dados são armazenados para serem acessados pelos núcleos CUDA.\n",
    "\n",
    "Cada tipo de dispositivo na GPU tem sua respectivas frequências de clock, fornecendo informações sobre o desempenho atual da GPU em diferentes áreas de processamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados de clock e memória a cada instante são visualizáveis pelo nvidia-smi com os seguintes nomes:\n",
    "\n",
    "- Graphics (CUDA Cores): São as unidades de processamento mais básicas da GPU. Cada núcleo CUDA é capaz de executar uma única instrução de shader por vez. Eles são responsáveis pela execução em paralelo das operações matemáticas e lógicas necessárias para renderizar gráficos e realizar cálculos gerais. Os shaders são programas que processam gráficos e cálculos em paralelo. A frequência de clock \"Graphics\" indica a velocidade com que esses núcleos estão operando no momento. São os núcleos CUDA que acessam dados da memória da GPU (VRAM) para realizar seus cálculos. Quanto mais núcleos CUDA uma GPU tiver, maior será sua capacidade de processamento paralelo, porém, a velocidade com que eles podem acessar esses dados é influenciada pela largura de banda da memória.\n",
    "\n",
    "- SM: Significa \"Streaming Multiprocessor\". Cada SM é um conjunto de núcleos de processamento, caches e outros recursos dentro da GPU. A frequência de clock \"SM\" indica a velocidade com que os SMs estão operando.\n",
    "\n",
    "- Memory: Refere-se à memória de vídeo (VRAM) da GPU, onde são armazenados dados como texturas, modelos 3D e outros elementos gráficos, além de dados intermediários durante os cálculos. A frequência de clock \"Memory\" indica a velocidade com que a memória está operando.\n",
    "\n",
    "- Video: Refere-se ao mecanismo de codificação e decodificação de vídeo da GPU. É um componente de hardware dedicado ao processamento de vídeo que funciona como um motor de vídeo que pode acessar a Memory (VRAM) da GPU para ler e gravar dados de vídeo durante a codificação ou decodificação. Bem como pode usar os núcleos CUDA (Graphics) para realizar algumas etapas do processamento de vídeo, especialmente em codecs modernos que utilizam aceleração por hardware. A frequência de clock \"Video\" indica a velocidade com que esse mecanismo está operando. Os SMs gerenciam a execução dos threads nos núcleos CUDA, incluindo aqueles usados pelo motor de vídeo.\n",
    "\n",
    "Em síntese, os núcleos CUDA (Graphics) executam os shaders (programas que processam gráficos e cálculos em paralelo), que acessam dados na memória VRAM (Memory) e realizam cálculos. Os Streaming Multiprocessors (SMs) agrupam vários núcleos Graphics e outros recursos, permitindo que a GPU execute muitos shaders em paralelo. O motor de codificação e decodificação de vídeo (Video) é responsável por processar vídeos, codificando-os ou decodificando-os, e também pode acessar dados na Memory.\n",
    "\n",
    "Nem todas as GPUs possuem todos esses componentes. Algumas GPUs mais antigas podem não ter um mecanismo de vídeo dedicado, por exemplo. As frequências de clock podem variar dependendo da carga de trabalho da GPU e das configurações de energia. A seção \"Max Clocks\" no nvidia-smi mostra as frequências máximas que cada componente pode atingir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indo além do nvidi-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O nvidia-smi não fornece informações diretas sobre a utilização dos Tensor Cores, uma das evoluções mais recentes na fabricação de GPUs. Essa é uma limitação da ferramenta, que se concentra principalmente em métricas gerais de utilização da GPU, como a porcentagem de tempo em que os núcleos CUDA estão ativos e o uso da memória. Isso acontece devido ao nível de abstração e complexidade mais superficial do nvidia-smi.\n",
    "\n",
    "Abstração de Hardware: O nvidia-smi é projetado para fornecer uma visão geral do estado da GPU, sem se aprofundar em detalhes específicos da arquitetura, como a utilização de unidades de hardware especializadas como os Tensor Cores.\n",
    "\n",
    "Complexidade de Monitoramento: O monitoramento preciso da utilização dos Tensor Cores exigiria um nível mais profundo de acesso ao hardware e à execução dos kernels CUDA, o que poderia impactar o desempenho geral da GPU.\n",
    "Alternativas para Monitorar a Utilização dos Tensor Cores\n",
    "\n",
    "Para monitorar a utilização dos Tensor Cores, é necessário usar ferramentas de profiling mais avançadas, como o NVidia Profiler (Nsight Compute) ou recursos de profiling fornecidos por bibliotecas de Deep Learning.\n",
    "\n",
    "A escolha da ferramenta depende do seu nível de conhecimento em CUDA e das necessidades específicas da sua aplicação.\n",
    "Mesmo sem monitorar diretamente a utilização dos Tensor Cores, você ainda pode otimizar seu código para aproveitá-los ao máximo, utilizando operações e formatos de dados que sejam compatíveis com eles, como operações de multiplicação de matrizes em precisão mista (FP16 ou BF16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os Tensor Cores são unidades de hardware especializadas localizadas dentro dos SMs da GPU. Eles aceleram operações de multiplicação de matrizes em baixa precisão, melhorando o desempenho e a eficiência energética em tarefas como Deep Learning. Embora o nvidia-smi não mostre a utilização dos Tensor Cores diretamente, um alto uso da GPU e um menor consumo de energia podem indicar que eles estão sendo utilizados.\n",
    "\n",
    "Os Tensor Cores, embora não sejam explicitamente mostrados pelo nvidia-smi, estão intimamente relacionados aos Streaming Multiprocessors (SMs). Cada SM em uma GPU NVIDIA compatível com Tensor Cores contém um certo número desses núcleos especializados. Os Tensor Cores são projetados para acelerar operações específicas de multiplicação de matrizes em formatos de baixa precisão, como FP16 (meia precisão) e INT8 (inteiros de 8 bits).\n",
    "\n",
    "Os Tensor Cores funcionam em conjunto com os SMs em execução paralela aumentando o desempenho e eficiência energética:\n",
    "\n",
    "Execução Paralela: Os Tensor Cores operam em paralelo com os núcleos CUDA dentro de cada SM. Enquanto os núcleos CUDA executam instruções gerais de shader, os Tensor Cores se concentram em acelerar as operações de multiplicação de matrizes específicas para as quais foram projetados.\n",
    "\n",
    "Aumento de Desempenho: Ao descarregar essas operações de multiplicação de matrizes para os Tensor Cores, a GPU pode alcançar um desempenho significativamente maior em tarefas que se beneficiam desses cálculos, como o treinamento e a inferência de redes neurais profundas.\n",
    "\n",
    "Eficiência Energética: Os Tensor Cores são projetados para serem mais eficientes em termos de energia do que os núcleos CUDA para realizar essas operações específicas, o que pode levar a um menor consumo de energia da GPU.\n",
    "\n",
    "Monitoramento indireto de uso de Tensor cores pelo impacto nos dados do nvidia-smi:\n",
    "\n",
    "Utilização da GPU: Embora o nvidia-smi não mostre a utilização dos Tensor Cores diretamente, um alto uso da GPU (indicado pela métrica \"Gpu\" na seção \"Utilization\") pode sugerir que os Tensor Cores estão sendo utilizados, especialmente se a carga de trabalho envolve operações de multiplicação de matrizes em baixa precisão.\n",
    "\n",
    "Consumo de Energia: Se sua aplicação estiver utilizando os Tensor Cores de forma eficiente, você poderá observar um menor consumo de energia da GPU em comparação com uma aplicação semelhante que não utiliza os Tensor Cores.\n",
    "\n",
    "Desempenho: Em geral, aplicações que aproveitam os Tensor Cores tendem a ter um desempenho significativamente melhor em GPUs que os possuem, especialmente em tarefas de Deep Learning e outras que envolvem cálculos intensivos de matrizes.\n",
    "\n",
    "Para aproveitar ao máximo os Tensor Cores, é importante usar corretamente as bibliotecas e frameworks de Deep Learning que suportem operações em precisão mista e que sejam otimizados para GPUs NVIDIA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados principais disponíveis no nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seção: <b>PCIe Generation</b>\n",
    "\n",
    "    Max: A geração máxima do PCIe que a sua GPU suporta (neste caso, PCIe 4.0).\n",
    "    Current: A geração do PCIe que a sua GPU está atualmente usando (também PCIe 4.0).\n",
    "    Device Current/Max: A geração do PCIe que o dispositivo (GPU) está usando/suporta.\n",
    "    Host Max: A geração máxima do PCIe que a placa-mãe (host) suporta.\n",
    "    Seção: Link Width\n",
    "    Max: A largura de banda máxima do link PCIe que a sua GPU suporta (16x neste caso).\n",
    "    Current: A largura de banda do link PCIe que está sendo usada atualmente (também 16x).\n",
    "\n",
    "Seção: <b>Bridge Chip</b>\n",
    "\n",
    "    Replays Since Reset: O número de vezes que o barramento PCIe teve que retransmitir dados desde a última reinicialização. Idealmente, esse número deve ser baixo, indicando uma conexão estável.\n",
    "    Replay Number Rollovers: O número de vezes que o contador de replays foi reiniciado (estourou) desde a última reinicialização. Também deve ser baixo.\n",
    "    Tx Throughput: A taxa de transferência de dados transmitidos pela GPU através do barramento PCIe.\n",
    "    Rx Throughput: A taxa de transferência de dados recebidos pela GPU através do barramento PCIe.\n",
    "    Fan Speed: A velocidade atual da ventoinha da GPU em porcentagem da velocidade máxima.\n",
    "    Performance State: O estado de desempenho atual da GPU. P8 é o estado de desempenho mais alto, indicando que a GPU está operando em sua frequência máxima.\n",
    "\n",
    "Seção: <b>Clocks Event Reasons</b>\n",
    "\n",
    "    Idle: Indica se a GPU está ociosa (\"Active\") ou se está sendo usada por algum processo.\n",
    "\n",
    "Seção: <b>FB Memory Usage</b>\n",
    "\n",
    "    Total: A quantidade total de memória de vídeo (framebuffer) disponível na GPU.\n",
    "    Reserved: A quantidade de memória reservada pelo sistema operacional ou drivers.\n",
    "    Used: A quantidade de memória de vídeo atualmente em uso.\n",
    "    Free: A quantidade de memória de vídeo disponível para uso.\n",
    "\n",
    "Seção: <b>BAR1 Memory Usage (Base Address Register 1)</b>\n",
    "\n",
    "    Total: Tamanho total da BAR1 (Base Address Register 1), região de memória acessível pela CPU para comunicar com a GPU.\n",
    "    Used: A quantidade de memória BAR1 atualmente em uso.\n",
    "    Free: A quantidade de memória BAR1 disponível para uso.\n",
    "    \n",
    "Seção: <b>Conf Compute Protected Memory Usage</b>\n",
    "\n",
    "    Compute Mode: O modo de computação atual da GPU. \"Default\" significa que a GPU está operando no modo padrão, sem restrições especiais de acesso à memória.\n",
    "\n",
    "Seção: <b>Utilization</b>\n",
    "\n",
    "    Gpu: A porcentagem de utilização dos núcleos da GPU.\n",
    "    Memory: A porcentagem de utilização da memória de vídeo.\n",
    "    Encoder/Decoder/JPEG/OFA: Porcentagens de utilização de diferentes unidades de processamento da GPU, se aplicável.\n",
    "\n",
    "Seção: <b>Encoder Stats & FBC Stats</b>\n",
    "\n",
    "    Active Sessions/Average FPS/Average Latency: Informações sobre sessões de codificação e decodificação de vídeo ativas, se houver.\n",
    "\n",
    "Seção: <b>Temperature</b>\n",
    "\n",
    "    GPU Current Temp: A temperatura atual da GPU.\n",
    "    GPU Shutdown Temp: A temperatura na qual a GPU será desligada para evitar danos.\n",
    "    GPU Slowdown Temp: A temperatura na qual a GPU reduzirá sua frequência para evitar o superaquecimento.\n",
    "    GPU Max Operating Temp: A temperatura máxima de operação segura da GPU.\n",
    "    GPU Target Temperature: A temperatura alvo que a GPU tentará manter através do controle da ventoinha.\n",
    "\n",
    "Seção: <b>GPU Power Readings</b>\n",
    "\n",
    "    Power Draw: A potência atual sendo consumida pela GPU.\n",
    "    Current/Requested/Default/Min/Max Power Limit: Limites de potência configurados para a GPU.\n",
    "\n",
    "Seção: <b>Clocks</b>\n",
    "\n",
    "    Graphics/SM/Memory/Video: As frequências de clock atuais para diferentes componentes da GPU.\n",
    "\n",
    "Seção: <b>Max Clocks</b>\n",
    "\n",
    "    Graphics/SM/Memory/Video: As frequências de clock máximas que cada componente da GPU pode atingir.\n",
    "\n",
    "Seção: <b>Voltage</b>\n",
    "\n",
    "    Graphics: A tensão atual aplicada aos núcleos da GPU.\n",
    "\n",
    "Seção: <b>Health</b>\n",
    "\n",
    "    Processes: Lista os processos que estão usando a GPU atualmente. \"None\" significa que nenhum processo está usando a GPU no momento.\n",
    "\n",
    "Seção: <b>Capabilities</b>\n",
    "\n",
    "    EGM: Indica se o recurso de Gerenciamento de Memória de Erro (Error Memory Management) está habilitado ou desabilitado na GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profilling completo também a nível de Tensor Cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NVidia Profiler (Nsight Compute)*:\n",
    "Esta é a ferramenta oficial da NVIDIA para profiling e análise de desempenho de aplicações CUDA. Ela permite coletar métricas detalhadas sobre a execução de kernels CUDA, incluindo a utilização dos Tensor Cores. No entanto, o Nsight Compute requer conhecimento de CUDA e pode ter uma curva de aprendizado mais acentuada.\n",
    "\n",
    "*Bibliotecas de Deep Learning*:\n",
    "Algumas bibliotecas de Deep Learning, como o PyTorch e o TensorFlow, oferecem ferramentas de profiling que podem fornecer informações sobre a utilização dos Tensor Cores durante o treinamento e a inferência de modelos. É necessário\n",
    "consultar a documentação atualizada da biblioteca de Deep Learning em utilização para ver quando ela oferece recursos de profiling e como usá-los para monitorar os Tensor Cores.\n",
    "\n",
    "*Outras Ferramentas de Profiling*:\n",
    "Existem outras ferramentas de profiling de terceiros que podem fornecer informações sobre a utilização dos Tensor Cores, como o NSight Systems e o TAU (Tuning and Analysis Utilities).\n",
    "\n",
    "Apesar de muito completas e eficientes, essas ferramentas podem ser mais complexas de configurar e usar, e nem sempre oferecem suporte completo para todas as GPUs e versões do CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal uso em Graph Machine Learning dos compontentes das GPUs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Streaming Multiprocessor (SMs): Organizar e gerenciar a execução paralela.\n",
    "- Graphics (núcleos CUDA): Processar paralelamente as operações do GML.\n",
    "- Memory (VRAM): Armazenar temporáriamente os dados do grafo e do modelo.\n",
    "- Video: Motor de codificação e decodificação de vídeo tem o papel de visualizar os grafos e aplicações multimodais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguagem CUDA para criar kernels otimizados para GML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O controle e a interação dos componentes da GPU (CUDA Cores, SMs, Memória) com outros elementos de hardware e software em CUDA para otimizar kernels de Graph Machine Learning (GML) envolvem vários aspectos:\n",
    "\n",
    "1. Gerenciamento de Threads e Blocos:\n",
    "\n",
    "- Threads e Blocos: Em CUDA, o código é executado por threads organizados em blocos. Cada thread dentro de um bloco tem um ID único, permitindo que você acesse e processe diferentes partes do grafo em paralelo.\n",
    "Hierarquia de Memória: CUDA oferece diferentes níveis de memória com velocidades e capacidades variadas:\n",
    "- Registradores: Memória mais rápida, privada para cada thread.\n",
    "- Memória Compartilhada: Memória mais rápida, compartilhada entre threads de um mesmo bloco. Ideal para armazenar dados acessados frequentemente dentro de um bloco.\n",
    "- Memória Global: Memória principal da GPU, acessível por todos os threads. Usada para armazenar a maior parte dos dados do grafo e do modelo.\n",
    "- Sincronização: Para garantir a correção dos resultados, é crucial sincronizar os threads dentro de um bloco (__syncthreads()) ou entre blocos (cudaDeviceSynchronize()), especialmente quando há dependências de dados entre eles.\n",
    "\n",
    "2. Otimização de Acesso à Memória:\n",
    "\n",
    "- Coalescência de Memória: Acessar dados da memória global de forma sequencial e alinhada maximiza a largura de banda da memória. Evite acessos aleatórios e desalinhados, que podem causar grandes penalidades de desempenho.\n",
    "- Caches: Utilize a memória compartilhada para armazenar dados acessados com frequência dentro de um bloco, reduzindo o número de acessos à memória global mais lenta.\n",
    "- Transferência de Dados: Minimize a transferência de dados entre a CPU e a GPU, pois isso pode ser um gargalo de desempenho. Utilize cudaMemcpyAsync para transferências assíncronas e pinned memory para otimizar a cópia de dados.\n",
    "\n",
    "3. Otimização para GML:\n",
    "\n",
    "- Particionamento de Grafos: Divida o grafo em partições menores que caibam na memória da GPU, processando cada partição em paralelo.\n",
    "- Amostragem de Vizinhança: Em grafos grandes, utilize técnicas de amostragem para selecionar apenas um subconjunto dos vizinhos de cada nó durante o treinamento, reduzindo a quantidade de dados a serem processados.\n",
    "- Compressão de Grafos: Utilize formatos de representação de grafos mais compactos, como CSR (Compressed Sparse Row) ou COO (Coordinate Format), para reduzir o consumo de memória e melhorar a eficiência do acesso aos dados.\n",
    "- Modelos Eficientes: Utilize modelos de GML projetados para serem eficientes em GPUs, como GNNs com camadas convolucionais esparsas ou modelos que exploram a estrutura do grafo para reduzir o número de operações."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Exemplo de Kernel CUDA para Agregação de Mensagens em GNNs:\n",
    "\n",
    "C++\n",
    "__global__ void aggregate_messages(float* features, int* edge_index, int num_nodes, int num_edges) {\n",
    "    int node_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (node_id < num_nodes) {\n",
    "        float aggregated_message = 0.0;\n",
    "        for (int i = 0; i < num_edges; i++) {\n",
    "            if (edge_index[2 * i + 1] == node_id) {  // Se o nó é o destino da aresta\n",
    "                int source_node = edge_index[2 * i];\n",
    "                aggregated_message += features[source_node];  // Agrega a mensagem do nó de origem\n",
    "            }\n",
    "        }\n",
    "        features[node_id] = aggregated_message;  // Atualiza as features do nó\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mesma função para Agregar Mensagens em GNNs pode ser criada em python por meio do PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def aggregate_messages(features, edge_index):\n",
    "    \"\"\"\n",
    "    Agrega mensagens em um grafo usando PyTorch.\n",
    "\n",
    "    Args:\n",
    "        features: Tensor com as features dos nós (shape: [num_nodes, num_features]).\n",
    "        edge_index: Tensor com os índices das arestas (shape: [2, num_edges]).\n",
    "\n",
    "    Returns:\n",
    "        Tensor com as features agregadas dos nós (shape: [num_nodes, num_features]).\n",
    "    \"\"\"\n",
    "\n",
    "    # Criar um tensor esparso para representar o grafo\n",
    "    sparse_adj = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.shape[1]), \n",
    "                                         size=(features.shape[0], features.shape[0]))\n",
    "\n",
    "    # Realizar a agregação de mensagens usando multiplicação de matrizes esparsas\n",
    "    aggregated_messages = torch.sparse.mm(sparse_adj, features)\n",
    "\n",
    "    return aggregated_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frameworks e APIs de alto nível para otimizar execução em GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Geometric e DGL facilitam a criação de kernels otimizados para Graph Machine Learning (GML) de várias maneiras, abstraindo a complexidade da programação CUDA de baixo nível e fornecendo implementações eficientes de operações comuns em grafos.\n",
    "\n",
    "1. Abstração de CUDA:\n",
    "\n",
    "Foco no modelo, não na implementação: Ambas as bibliotecas permitem que você se concentre na definição do seu modelo de GML (GNNs, etc.) em Python, sem precisar escrever código CUDA diretamente. Elas cuidam da conversão do seu modelo em kernels CUDA otimizados nos bastidores.\n",
    "Flexibilidade: Você pode usar construções de alto nível do PyTorch (ou TensorFlow, no caso do DGL) para definir seu modelo, aproveitando a flexibilidade e expressividade dessas bibliotecas.\n",
    "\n",
    "Portabilidade: O código do seu modelo se torna mais portátil, pois não está diretamente vinculado a detalhes específicos da arquitetura da GPU ou da CUDA.\n",
    "\n",
    "2. Implementações Otimizadas:\n",
    "\n",
    "Operações comuns em grafos: Ambas as bibliotecas fornecem implementações otimizadas em CUDA para operações comuns em GML, como:\n",
    "\n",
    "- Convolução de grafos: Diferentes tipos de convolução, como GCNConv, GATConv, SAGEConv, etc.\n",
    "- Pooling de grafos: Para reduzir o tamanho do grafo, como TopKPooling, SAGPooling, etc.\n",
    "- Normalização de grafos: Para normalizar as features dos nós, como BatchNorm, LayerNorm, etc.\n",
    "- Outras operações: Funções de ativação, camadas lineares, etc.\n",
    "\n",
    "Aproveitamento de recursos da GPU: Essas implementações são projetadas para aproveitar ao máximo os recursos da GPU, como paralelismo, memória compartilhada e caches, para obter o melhor desempenho possível.\n",
    "\n",
    "Atualizações e otimizações: As bibliotecas são mantidas ativamente e recebem atualizações frequentes com novas otimizações e melhorias de desempenho.\n",
    "\n",
    "3. Abstração de detalhes de baixo nível:\n",
    "\n",
    "Gerenciamento de memória: As bibliotecas cuidam do gerenciamento de memória na GPU, alocando e liberando memória conforme necessário, para que você não precise se preocupar com esses detalhes.\n",
    "\n",
    "Sincronização de threads: A sincronização de threads é tratada automaticamente pelas bibliotecas, garantindo a correção dos resultados sem a necessidade de chamadas explícitas a __syncthreads().\n",
    "\n",
    "Transferência de dados: As bibliotecas otimizam a transferência de dados entre a CPU e a GPU, usando técnicas como cudaMemcpyAsync e pinned memory.\n",
    "\n",
    "4. Facilidade de uso:\n",
    "\n",
    "API intuitiva: Ambas as bibliotecas oferecem uma API Python de alto nível que é fácil de aprender e usar, mesmo para quem não tem experiência em CUDA.\n",
    "\n",
    "Documentação e exemplos: As bibliotecas possuem documentação abrangente e muitos exemplos de código que demonstram como usar as diferentes funcionalidades e construir modelos de GML.\n",
    "\n",
    "Comunidade ativa: Ambas as bibliotecas têm comunidades ativas e fóruns de suporte onde você pode encontrar ajuda e trocar ideias com outros usuários.\n",
    "\n",
    "Em resumo, PyTorch Geometric e DGL facilitam a criação de kernels otimizados para GML, permitindo que você se concentre na definição do seu modelo e abstraindo os detalhes de baixo nível da programação CUDA. Elas fornecem implementações eficientes de operações comuns em grafos, otimizam o uso dos recursos da GPU e oferecem uma API intuitiva e fácil de usar. Com essas bibliotecas, você pode desenvolver e implementar modelos de GML de forma mais rápida e eficiente, sem precisar ser um especialista em CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizando uso da NVIDIA GeForce RTX 4080 SUPER para GML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na utilização da NVIDIA GeForce RTX 4080 SUPER em aplicações de Graph Machine Learning, no contexto de pesquisas com restrições orçamentárias, Ao otimizar o uso de cada componente da GPU e aplicar as melhores práticas de GML, um desempenho razoável nas tarefas de aprendizado de máquina em grafos pode ser alcançado. Esta GPU oferece um grau de paralelismo, uma capacidade de memória e recursos avançados de vídeo com boa relação custo/benefício tanto em aquisição como em consumo de energia elétrica dentre os modelos não-profissionais. \n",
    "\n",
    "Cada componente dessa GPU pode ser utilizada de forma otimizada:\n",
    "\n",
    "1. Núcleos CUDA (Graphics)\n",
    "\n",
    "- Paralelismo Massivo: A RTX 4080 SUPER possui 76 núcleos CUDA, permitindo um alto grau de paralelismo. Esses núcleos podem ser bem utilizados para processar um grande número de nós e arestas simultaneamente, acelerando operações como agregação de mensagens, propagação de features e cálculos de atenção em GNNs.\n",
    "\n",
    "- Uso em Grafos Densos: A arquitetura Ada Lovelace da RTX 4080 SUPER é particularmente eficiente em operações com matrizes densas, que podem ser usadas para representar grafos densos em GML.\n",
    "\n",
    "2. Streaming Multiprocessors (SMs)\n",
    "\n",
    "- Gerenciamento Eficiente de Threads: Os 76 SMs da RTX 4080 SUPER gerenciam a execução dos threads nos núcleos CUDA, garantindo uma utilização eficiente dos recursos da GPU.\n",
    "\n",
    "- Modelos Complexos: Utilize os SMs para lidar com modelos de GML complexos com um grande número de parâmetros e operações, permitindo um treinamento e inferência mais rápidos.\n",
    "\n",
    "- Técnicas Avançadas: Implemente técnicas avançadas de GML, como Graph Attention Networks (GATs) ou Graph Transformers, que exigem um alto grau de paralelismo e gerenciamento eficiente de threads.\n",
    "\n",
    "3. Memória de Vídeo (VRAM)\n",
    "\n",
    "- Capacidade de 16 GB: A RTX 4080 SUPER possui 16 GB de VRAM, o que permite armazenar grafos de tamanho moderado diretamente na memória da GPU.\n",
    "\n",
    "- Grafos de Tamanho Médio: Utilize a VRAM para armazenar grafos com até alguns milhões de nós e arestas, evitando a necessidade de transferir dados entre a CPU e a GPU durante o processamento, o que pode ser um gargalo de desempenho.\n",
    "\n",
    "- Técnicas de Otimização de Memória: Para grafos maiores que não cabem inteiramente na VRAM, utilize técnicas de otimização de memória, como particionamento de grafos, amostragem de vizinhos ou carregamento de dados em lotes.\n",
    "\n",
    "4. Codificador/Decodificador de Vídeo\n",
    "\n",
    "- Visualização Interativa: A RTX 4080 SUPER possui um poderoso mecanismo de vídeo que pode ser usado para visualizar grafos e seus embeddings em tempo real, permitindo uma análise interativa e exploratória dos seus dados e resultados.\n",
    "\n",
    "- Aplicações Multimodais: Se sua aplicação de GML envolve dados multimodais, como imagens ou vídeos associados aos nós do grafo, o codificador/decodificador de vídeo pode ser usado para processar e incorporar esses dados, enriquecendo a representação do grafo e melhorando o desempenho do modelo.\n",
    "\n",
    "Recomendações Gerais:\n",
    "\n",
    "Bibliotecas: Utilize bibliotecas de GML otimizadas para GPUs NVIDIA, como o PyTorch Geometric e o DGL, que aproveitam ao máximo os recursos da RTX 4080 SUPER.\n",
    "\n",
    "Precisão Mista (Mixed Precision): Utilize treinamento em precisão mista (FP16 ou BF16) para acelerar o treinamento e reduzir o consumo de memória, especialmente em modelos grandes e complexos.\n",
    "\n",
    "Paralelismo de Dados: Se possível, paralelize o treinamento do seu modelo em várias GPUs para acelerar ainda mais o processo.\n",
    "\n",
    "Monitoramento: Utilize ferramentas como o nvidia-smi para monitorar o uso da GPU durante o treinamento e a inferência, identificando gargalos de desempenho e oportunidades de otimização.\n",
    "\n",
    "Conclusão: A GeForce RTX 4080 SUPER é uma GPU apresente o melhor custo/benefício em relação processamento/preço, dispõe de recursos que podem ser bem aproveitados para algoritmos de Graph Machine Learning (GML) apresentando desempenho razoável, dentre os melhores na categoria de placas GPU de uso não-profissional. \n",
    "\n",
    "Com relação ao modelo mais potente da arquitetura Ada Lovelace a RTX 4090 (24 GB GDDR6X, 450 Watt), a RTX 4080 Super (16 GB GDDR6X, 320 Watt) atinge desempenho de 88% do desempenho da RTX 4090 por apenas 60% do custo de aquisição nos Estados Unidos. Para aquisição no Brasil a distância pode ser ainda maior, pode-se encontrar 4080 Super po aproximadamente 31% do preço de aquisição da 4090 (Preços na Amazon: RTX 4090 R$34.970,00 versus RTX 4080 Super R$11.001 no Brasil em 2024). \n",
    "\n",
    "A maior limitação realmente é o tamanho da memória em 16Gb, contra os 24Gb da 4090 RTX.\n",
    "\n",
    "A 4080 Super ainda apresenta eficiência energética bem superior (45,62%) com relação à 4090 (28,81%). \n",
    "\n",
    "https://technical.city/pt/video/GeForce-RTX-4090-vs-GeForce-RTX-4080-SUPER\n",
    "\n",
    "https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Um Modelo Grafo Multicamada para PDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade Semântica em Grafos com Classificação Dinâmica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para alcançar o objetivo de criar um motor de análise de similaridade semântica em grafos com classificação dinâmica de nós, a combinação de Graph Machine Learning (GML) com técnicas de otimização de modularidade demanda implementar estratégias para representar as entidades do domínio, na forma de nós no modelo grafo, bem como representar as interações no mundo real entre essas entidades, através da representação dos relacionamentos em arestas do modelo grafo. Para tanto seguem as principais atividades a serem realizadas para implementar o modelo de análise em grafos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Representar entidades de mundo real como Nós e relacionamentos como Arestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incorporação de Nós (Node Embeddings)**: Modelos de GML como Graph Neural Networks (GNNs) foram utilizados para aprender representações vetoriais (embeddings) dos nós [1,2], capturando assim, a similaridade semântica com base na estrutura do grafo e nas propriedades dos nós.\n",
    "\n",
    "**Incorporação de Arestas (Edge Embeddings)**: De acordo com o contexto real do domínio em análise, se além dos nós em si, as relações entre os nós também carregam informações semânticas importantes, é necessário aprender representações vetoriais também para as arestas.\n",
    "\n",
    "Referências:\n",
    "\n",
    "    [1] Graph Convolutional Networks for Text Classification (https://arxiv.org/abs/1810.08403)\n",
    "    [2] Inductive Representation Learning on Large Graphs (https://arxiv.org/abs/1706.02216)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cálcular Similaridade Semântica no modelo grafo (entidades e relacionamentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Métricas de Distância**: Utilizadas métricas de distância como cosseno, ou distância euclidiana, dependendo da questão específica tratada, para calcular a similaridade entre os embeddings dos nós."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Realizar a classificação dinâmica de nós e arestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algoritmos de Clustering**: Para permitir uma classificação dinâmica de nós e relacionamentos, aqui utilizamos algoritmos de clustering, que permitiram classificar os nós com base na similaridade semântica de suas propriedades, passadas como parâmetros de classificação, e na maximização da modularidade do grafo gerado pela interação entre essas entidades. \n",
    "\n",
    "Foram experimentados para essa atividade os algoritmos:\n",
    "\n",
    "    Louvain: Algoritmo eficiente e amplamente utilizado para otimização de modularidade.\n",
    "\n",
    "    Leiden: Variação do Louvain com maior precisão na detecção de comunidades.\n",
    "\n",
    "    Infomap: Algoritmo baseado em fluxo de informação para identificar comunidades.\n",
    "\n",
    "**Algoritmos de Clustering Hierárquico**: Permitem visualizar a estrutura de comunidades em diferentes níveis de granularidade.\n",
    "\n",
    "    Maximização da Modularidade: A modularidade mede a qualidade da divisão do grafo em comunidades. Utilize a modularidade como função objetivo durante o processo de clustering para garantir que os nós sejam agrupados em comunidades coesas e significativas. \n",
    "\n",
    "Referências:\n",
    "\n",
    "    [3] Finding community structure in very large networks (https://arxiv.org/abs/cond-mat/0408187)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Modelagem do Schema do Grafo e Questões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema do Grafo: Defina o schema do grafo de forma a representar adequadamente os dados textuais e as relações entre eles. Utilize propriedades dos nós para armazenar informações relevantes para a análise de similaridade semântica.\n",
    "Questões: Formule as questões que deseja responder com a análise de similaridade semântica. Isso guiará a escolha das propriedades dos nós, o cálculo de similaridade e a interpretação dos resultados do clustering.\n",
    "Considerações Adicionais\n",
    "\n",
    "Pré-processamento de Texto: Aplique técnicas de pré-processamento de texto (tokenização, remoção de stop words, stemming/lemmatization) para melhorar a qualidade das representações vetoriais e a análise de similaridade semântica.\n",
    "Escolha do Modelo de GML: A escolha do modelo de GML (tipo de GNN, número de camadas, etc.) depende da complexidade do grafo e das características dos dados. Experimente diferentes modelos e avalie seu desempenho.\n",
    "Avaliação dos Resultados: Utilize métricas de avaliação de clustering para medir a qualidade das comunidades encontradas e a efetividade da classificação dinâmica dos nós.\n",
    "Escalabilidade: Para grafos muito grandes, considere técnicas de amostragem ou algoritmos de clustering distribuídos para lidar com a escalabilidade.\n",
    "\n",
    "Outas literaturas relevantes incluem os artigos:\n",
    "\n",
    "    [4] A Comprehensive Survey on Graph Neural Networks (https://arxiv.org/abs/1901.00596)\n",
    "    [5] Community Detection in Graphs\n",
    "\n",
    "Bibliotecas Python\n",
    "\n",
    "NetworkX: Para criação, manipulação e visualização de grafos.\n",
    "\n",
    "PyTorch Geometric: Para implementação de modelos de GML.\n",
    "\n",
    "Scikit-learn: Para algoritmos de clustering e métricas de avaliação.\n",
    "\n",
    "As características, tanto dos dados sendo processados, como das questões a responder são fundamentais para melhor abordagem de treinamento para combinar as técnicas e criar o motor de análise de similaridade semântica eficaz e dinâmico em grafos. Os testes de benchmarking para essa escolha são mostrados a seguir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simular interação de forças para visualizar análises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de forças no sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyVis para simula a busca por equilíbrio entre forças físicas nas entidades que interagem no grafo para determinar o posicionamento dos nós no grafo e assim criar o layout. As principais forças envolvidas são:\n",
    "\n",
    "#### 1. Força de Atração:\n",
    "\n",
    "Arestas (Links): As arestas entre os nós agem como molas, puxando os nós conectados um em direção ao outro. A intensidade dessa força é determinada pelo parâmetro springLength (comprimento ideal da mola) e springConstant (rigidez da mola).\n",
    "\n",
    "Gravidade Central: Uma força de atração em direção ao centro do grafo, controlada pelo parâmetro centralGravity. Essa força ajuda a evitar que os nós se dispersem muito e mantém o grafo mais compacto.\n",
    "\n",
    "#### 2. Força de Repulsão:\n",
    "\n",
    "Repulsão entre Nós: Os nós se repelem uns aos outros, como partículas carregadas com a mesma carga. A intensidade dessa força é determinada pelo parâmetro gravitationalConstant (constante gravitacional). Um valor negativo aumenta a repulsão, enquanto um valor positivo a diminui.\n",
    "\n",
    "Evitar Sobreposição: O parâmetro avoidOverlap controla se os nós devem evitar a sobreposição. Se ativado, uma força adicional é aplicada para afastar os nós que estão muito próximos.\n",
    "\n",
    "#### 3. Força de Amortecimento:\n",
    "\n",
    "Damping: O parâmetro damping controla o amortecimento do movimento dos nós. Um valor maior de amortecimento torna o movimento mais lento e suave, enquanto um valor menor permite movimentos mais rápidos e oscilatórios.\n",
    "\n",
    "#### 4. Forças Adicionais (Opcionais):\n",
    "\n",
    "Outras Forças: O PyVis permite adicionar outras forças personalizadas ao layout, como forças de atração/repulsão entre grupos de nós, ou forças que direcionam os nós para posições específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como as Forças Interagem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo ForceAtlas2Based iterativamente calcula as forças resultantes sobre cada nó e ajusta suas posições de acordo. O processo continua até que o layout se estabilize ou um número máximo de iterações seja atingido.\n",
    "\n",
    "O algoritmo Barnes-Hut otimiza o cálculo das forças de longo alcance, aproximando as forças entre grupos de nós distantes. Isso melhora o desempenho do algoritmo, especialmente em grafos grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equilíbrio das Forças:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "O objetivo do algoritmo é encontrar um equilíbrio entre as forças de atração e repulsão, de modo que os nós conectados fiquem próximos, mas sem se sobreporem, e o grafo tenha uma aparência geral agradável e informativa. O ajuste dos parâmetros do ForceAtlas2Based e do Barnes-Hut permite controlar esse equilíbrio e personalizar o layout do grafo de acordo com suas necessidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos ForceAtlas2Based e Barnes-Hut:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ForceAtlas2Based é um algoritmo de layout que simula um sistema físico onde os nós se atraem e se repelem com base em certas forças. No entanto, calcular essas forças para todos os pares de nós em um grafo grande pode ser computacionalmente caro.\n",
    "\n",
    "Para otimizar o cálculo das forças, o ForceAtlas2Based utiliza o algoritmo Barnes-Hut. Esse algoritmo agrupa nós distantes em clusters e aproxima suas forças de atração/repulsão, reduzindo significativamente o número de cálculos necessários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theta e a Distância entre Nós:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O parâmetro theta (θ) do Barnes-Hut define o limite entre as forças de curto e longo alcance. Quando a distância entre dois nós é menor que theta multiplicado pelo tamanho do cluster, as forças são calculadas individualmente (curto alcance). Caso contrário, as forças são aproximadas usando o centro de massa do cluster (longo alcance).\n",
    "\n",
    "Quando os nós estão muito próximos, a distância entre eles é menor que o limite definido por theta. Nesse caso, o algoritmo Barnes-Hut calcula as forças individualmente para cada par de nós, levando em consideração suas posições exatas. Isso permite que o ForceAtlas2Based posicione os nós próximos de forma mais precisa, evitando sobreposições e garantindo um layout visualmente agradável.\n",
    "\n",
    "Em resumo, o ForceAtlas2Based define as regras gerais para as forças de atração e repulsão entre os nós. No entanto, quando os nós estão próximos, o algoritmo Barnes-Hut assume o controle e calcula as forças de forma mais precisa, levando em consideração a distância exata entre os nós.\n",
    "\n",
    "O parâmetro theta do Barnes-Hut é fundamental para determinar o comportamento do layout em nós próximos, pois define o limite entre as forças de curto e longo alcance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O que é o Theta (θ)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O theta (θ) é um parâmetro interno do ForceAtlas2Based que controla o limite entre as forças de longo alcance (que afetam todos os nós) e as forças de curto alcance (que afetam apenas os nós próximos).\n",
    "\n",
    "Valores mais altos de theta: Aceleram o cálculo das forças, mas podem gerar mais erros e imprecisões no layout.\n",
    "Valores mais baixos de theta: Tornam o cálculo mais lento, mas produzem um layout mais preciso e com menos erros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como o Theta é Usado no ForceAtlas2Based?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ForceAtlas2Based utiliza uma técnica chamada Barnes-Hut para aproximar as forças de longo alcance, tornando o cálculo mais eficiente. O theta é usado para determinar quais nós estão \"próximos o suficiente\" para que suas forças sejam calculadas individualmente (curto alcance), e quais nós estão \"longe o suficiente\" para que suas forças sejam aproximadas usando a técnica Barnes-Hut (longo alcance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parâmetros do ForceAtlas2Based:\n",
    "\n",
    "gravitationalConstant: Define a força de atração global entre os nós. Valores negativos atraem os nós para o centro do grafo, enquanto valores positivos os repelem. Um valor mais negativo resultará em um grafo mais compacto, enquanto um valor mais positivo resultará em um grafo mais espalhado.\n",
    "\n",
    "centralGravity: Define a força de atração em direção ao centro do grafo. Valores maiores puxam os nós mais para o centro.\n",
    "\n",
    "springLength: Define o comprimento ideal das arestas (ligações entre os nós). Valores maiores resultam em arestas mais longas e um grafo mais espalhado.\n",
    "\n",
    "springConstant: Define a rigidez das arestas. Valores maiores tornam as arestas mais rígidas e o grafo menos flexível.\n",
    "\n",
    "damping: Controla a velocidade com que os nós se movem. Valores maiores amortecem o movimento, resultando em um layout mais estável, mas que pode levar mais tempo para convergir.\n",
    "\n",
    "avoidOverlap: Determina se os nós devem evitar a sobreposição. Um valor de 1 (verdadeiro) faz com que os nós se afastem uns dos outros para evitar sobreposição, enquanto um valor de 0 (falso) permite a sobreposição."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking de partes da implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delinear formas de melhorar a eficiência dos processos de Gestão em PDI, com base na escolha bem fundamentada em dados sobre evidências do mundo real, é uma oportunidade para melhorar tanto a efetividade como a eficiência dos processos de gestão em Pesquisa Desenvolvimento e Inovação (PDI). No âmbito das Instituições de Ciência e Tecnologia brasileiras (ICT), para propor novas tecnologias e melhorar as existentes, a gestão em PDI precisa implementar processsos computacionais de extração, tratamento, análise, visualização e apresentação de resultados para tratar a massa de dados disponível e permitir uma melhor tomada de decisão aos níveis gerenciais e executivos das ICTs. \n",
    "\n",
    "Tais necessidades e oportunidades não deve ser negligenciadas, principalmente nas economias em desenvolvimento, por representar uma forma viável para reduzir custos, e com suporte ao desenvolvimento de tecnologias nacionais, reduzir os déficits da balança comercial do País. Para aproveitar essa oportunidade, aplicar uma abordagem de benchmarking não supervisionado para modelos de aprendizagem de máquina, destinados à suportar os processos em PDI, é uma contribuição valiosa para suportar à criação de processos e aplicações mais inteligentes na tomada de decisão em PDI. \n",
    "\n",
    "Neste artigo demonstramos como comparar e selecionar modelos pré-treinados mais eficientes para gerar vetores de incorporação de textos (embeedings), de forma objetiva e sistemática, mesmo na ausência de rótulos de dados. Foram exploradas diferentes métricas de avaliação, algoritmos de clustering e técnicas de visualização para aprimorar ainda mais o processo de benchmarking e seleção de modelos, baseado em dados da realidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking de Leituras de dataframes Paralelizável"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura de dataframes com Pandas ou cuDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which python\n",
    "# !echo $PATH\n",
    "# !conda list git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install GitPython\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import cudf\n",
    "import cugraph\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from git import Repo\n",
    "\n",
    "def convert_to_dict(text):\n",
    "    try:\n",
    "        return ast.literal_eval(text)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "folder_utils = os.path.join(str(root_folder), 'utils') # type: ignore\n",
    "folder_domain = os.path.join(str(root_folder), 'source', 'domain') # type: ignore\n",
    "folder_data_input = os.path.join(str(root_folder), '_data', 'in_csv') # type: ignore\n",
    "folder_data_output = os.path.join(str(root_folder), '_data', 'out_json') # type: ignore\n",
    "filename = 'revistas_capes.csv'\n",
    "pathfilename = os.path.join(folder_data_input, filename)\n",
    "\n",
    "# read data into a Pandas DataFrame using read_csv\n",
    "start_time_cpu = time.time()\n",
    "pdf = pd.read_csv(pathfilename, header=0, delimiter=';')\n",
    "end_time_cpu = time.time()\n",
    "time_cpu = end_time_cpu - start_time_cpu\n",
    "print(f\"Tempos para gerar dataframes:\") \n",
    "print(f\"Na CPU com  Pandas: {time_cpu:>.2f} segundos\")\n",
    "\n",
    "# Apply the function to the 'detalhes' column\n",
    "# pdf['detalhes'] = pdf['detalhes'].apply(convert_to_dict)\n",
    "\n",
    "# Print the first 5 rows to verify\n",
    "# print(pdf.head()) \n",
    "\n",
    "# read data into a cuDF DataFrame using read_csv\n",
    "start_time_gpu = time.time()\n",
    "gdf = cudf.read_csv(pathfilename, header=0, delimiter=';')  # type: ignore\n",
    "qlin = len(gdf.index)\n",
    "end_time_gpu = time.time()\n",
    "time_gpu = end_time_gpu - start_time_gpu\n",
    "print(f\"Na GPU com cuGraph: {time_gpu:>.2f} segundos\")\n",
    "diff = time_cpu-time_gpu\n",
    "percent = np.round(diff/time_cpu*100,2)\n",
    "if time_gpu < time_cpu:\n",
    "    print(f\"cuGraph reduziu {percent:.2f}% do tempo do cálculo em CPU\")\n",
    "    print(f\"A leitura foi {time_cpu/time_gpu:.1f} vezes mais rápida com cuGraph\")\n",
    "else:\n",
    "    percent = np.round(-diff/time_cpu*100,2)\n",
    "    print(f\"cuGraph aumentou em {-diff:.2f} segundos o tempo de leitura do dataframe com {qlin} linhas\")\n",
    "    print(f\"A leitura foi {time_gpu/time_cpu:.1f} vezes mais lenta com cuGraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas lê arquivos CSV mais rápido que o cuDF em algumas situações, para arquivos menores ou em casos onde a sobrecarga de transferência de dados para a GPU supera o ganho de desempenho do processamento paralelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking do Cálculo de Centralidade Paralelizável"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medir tempo de cálculos de centralidade com e sem GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import cugraph as cnx\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"mensagem de aviso específica\")\n",
    "# help(cugraph.experimental)\n",
    "# help(cugraph.experimental.compat)\n",
    "# print(dir(cugraph.experimental))\n",
    "\n",
    "# Criar um grafo aleatório com 10mil nós e probabilidade de 1% de haver arestas entre cada par de nós\n",
    "print(f\"Criando grafo com 10000 nós e 10% de probabilidade de aresta entre nós...\")\n",
    "G = nx.gnp_random_graph(10000, 0.01) \n",
    "\n",
    "# Medir tempo para calcular centralidade de intermediação somente com uso de CPU\n",
    "print(f\"Iniciando cálculo de centralidade de intermediação com CPU...\")\n",
    "start_time = time.time()\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "end_time = time.time()\n",
    "time_cpu = end_time - start_time\n",
    "print(f\"  Tempos para calcular betweenness_centrality:\") \n",
    "print(f\"  Na CPU sem cuGraph: {time_cpu:>.2f} segundos\")\n",
    "\n",
    "# Medir tempo para calcular centralidade de intermediação com aceleração na GPU\n",
    "print(f\"Iniciando cálculo de centralidade de intermediação com GPU...\")\n",
    "start_time_gpu = time.time()\n",
    "betweenness_centrality_cugraph = cnx.betweenness_centrality(G)\n",
    "end_time_gpu = time.time()\n",
    "time_gpu = end_time_gpu - start_time_gpu\n",
    "\n",
    "# Mostrar os resultados comparativos\n",
    "print(f\"  Na GPU com cuGraph: {time_gpu:>.2f} segundos\")\n",
    "diff = time_cpu-time_gpu\n",
    "percent = np.round(diff/time_cpu*100,2)\n",
    "if time_gpu < time_cpu:\n",
    "    print(f\"\\ncuGraph reduziu {percent:.2f}% do tempo do cálculo em CPU\")\n",
    "    print(f\"O cálculo foi {time_cpu/time_gpu:.1f} vezes mais rápido com cuGraph\")\n",
    "else:\n",
    "    percent = np.round(-diff/time_cpu*100,2)\n",
    "    print(f\"cuGraph aumentou em {-diff:.2f} segundos o tempo de cálculo da centralidade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking da Geração de Embeedings Paralelizável"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar métricas adequadas para avaliar os resultados de clustering é crucial para determinar qual algoritmo e configuração funcionam melhor para os dados no contexto em análise. As seguintes métricas foram utilizadas para avaliar a qualidade dos clusters com os embeedings de cada modelo:\n",
    "\n",
    "Silhouette Score: avalia a qualidade dos clusters, combinando medidas de coesão (quão próximos os pontos dentro de um cluster estão uns dos outros) e separação (quão distantes os clusters estão entre si). Os score varia de -1 a 1, onde valores próximos a 1 indicam que os pontos estão bem agrupados em seus clusters e bem separados dos outros clusters, e valores próximos a 0 indicam que os pontos estão perto da fronteira entre clusters, ou que o número de clusters pode não ser ideal. Já valores próximos a -1 sugerem que os pontos podem estar atribuídos aos clusters errados.\n",
    "\n",
    "Calinski-Harabasz Index (ou Variance Ratio Criterion): mede a razão entre a dispersão entre clusters e a dispersão dentro dos clusters. Quanto maior o valor, melhor a qualidade do clustering. Ou seja, um valor alto indica que os clusters estão bem separados e densos.\n",
    "\n",
    "Davies-Bouldin Index: mede a similaridade média entre cada cluster e seu cluster mais similar. Quanto menor o valor, melhor a qualidade do clustering. Ou seja, um valor baixo indica que os clusters estão mais separados e menos dispersos.\n",
    "\n",
    "O teste de benchmarking destaca:\n",
    "\n",
    "    o maior Silhouette Score dentre os resultados para indicar uma melhor qualidade de clustering, com clusters mais coesos e separados.\n",
    "\n",
    "    os maiores valores do Calinski-Harabasz Index para cada algoritmo e configuração, pois isso sugere uma melhor separação entre os clusters.\n",
    "\n",
    "    o algoritmo e configuração com o menor Davies-Bouldin Index, pois isso indica clusters mais distintos e compactos."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cuml.metrics.cluster.silhouette_score.cython_silhouette_score(X, labels, metric='euclidean', chunksize=None, convert_dtype=True, handle=None)\n",
    "\n",
    "[fonte]\n",
    "Calcula o coeficiente de silhueta médio para os dados fornecidos.\n",
    "\n",
    "Dado um conjunto de rótulos de cluster para cada amostra nos dados fornecidos, calcula a distância média intracluster (a) e a distância média do cluster mais próximo (b) para cada amostra. O coeficiente de silhueta para uma amostra é então (b - a) / max(a, b).\n",
    "\n",
    "Parâmetros\n",
    "X\n",
    "array-like, shape = (n_samples, n_features)\n",
    "Os vetores de recursos para todas as amostras.\n",
    "\n",
    "labels\n",
    "array-like, shape = (n_samples,)\n",
    "Os rótulos de cluster atribuídos para cada amostra.\n",
    "\n",
    "metric\n",
    "string\n",
    "Uma representação de string da métrica de distância a ser usada para avaliar a pontuação da silhueta. As opções disponíveis são \"cityblock\", \"cosine\", \"euclidean\", \"l1\", \"l2\", \"manhattan\" e \"sqeuclidean\".\n",
    "\n",
    "chunksize\n",
    "inteiro (padrão = Nenhum)\n",
    "Um inteiro, 1 <= chunksize <= n_samples para agrupar os cálculos da matriz de distância em pares, de modo a reduzir o uso de memória quadrática de ter toda a matriz de distância em pares na memória da GPU. Se Nenhum, o chunksize será automaticamente definido como 40000, o que por meio de experimentos provou ser um número seguro para o cálculo ser executado em uma GPU com 16 GB de VRAM.\n",
    "\n",
    "handle\n",
    "cuml.Handle\n",
    "Especifica o cuml.handle que contém o estado CUDA interno para cálculos neste modelo. Mais importante, isso especifica o fluxo CUDA que será usado para os cálculos do modelo, para que os usuários possam executar diferentes modelos simultaneamente em diferentes fluxos, criando identificadores em vários fluxos. Se for Nenhum, um novo é criado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install -c rapidsai -c conda-forge -c nvidia nx-cugraph\n",
    "\n",
    "# %pip install sentence_transformers\n",
    "# %pip install seaborn\n",
    "# %pip install nltk\n",
    "# %pip install contextualSpellCheck\n",
    "# %pip install langdetect\n",
    "# %pip install ipywidgets\n",
    "# %pip install huggingface_hub\n",
    "# %pip install --upgrade httpcore\n",
    "# %pip install --upgrade httpx\n",
    "\n",
    "## Importações não mais usadas na classe de análise de embeedings\n",
    "# from transformers.tokenization_utils_base import TruncationStrategy\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# from transformers import pipeline, TranslationPipeline\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# from langdetect import detect\n",
    "# import contextualSpellCheck\n",
    "\n",
    "## GoogleTranslate apresentou muitos conflitos e não funcionou, optei por langdetect\n",
    "## Versão httpx==0.13.3 Para compatibilizar com GoogleTranslate, porém atrapalha openai 1.44.0 e jupyterlab 4.2.4\n",
    "# ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "# openai 1.44.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
    "# jupyterlab 4.2.4 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>1a. Calcular melhor modelo de Embeddings</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testar importação das bilbiotecas instaladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Apr_17_19:19:55_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.40\n",
      "Build cuda_12.5.r12.5/compiler.34177558_0\n",
      "2.5.1+cu124\n",
      "0.20.1+cu124\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Definir a variável de ambiente TOKENIZERS_PARALLELISM:\n",
    "TOKENIZERS_PARALLELISM=true: Força o paralelismo, mesmo após o fork (pode causar deadlocks).\n",
    "TOKENIZERS_PARALLELISM=false: Desativa o paralelismo completamente.\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCHINDUCTOR_FREEZING'] = '1'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "!nvcc --version\n",
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torchvision; print(torchvision.__version__)\"\n",
    "\n",
    "# !pip uninstall torchvision\n",
    "# !pip install torchvision\n",
    "# !pip uninstall retrying -y\n",
    "# !pip install retry\n",
    "# !pip install GPUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerar embeedings com cada um dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:19:52,145 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 16:19:52,145 - INFO - Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large\n",
      "2024-11-12 16:19:56,057 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 16:19:56,058 - INFO - Load pretrained SentenceTransformer: distiluse-base-multilingual-cased-v2\n",
      "2024-11-12 16:19:59,371 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 16:19:59,372 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2024-11-12 16:20:02,893 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 16:20:02,893 - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "2024-11-12 16:20:04,937 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 16:20:04,937 - INFO - Load pretrained SentenceTransformer: all-distilroberta-v1\n",
      "2024-11-12 16:20:07,407 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 16:20:07,407 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/gtr-t5-large\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--gtr-t5-large\n",
      "Espaço memória necessária: 646.96 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
      "  (3): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/distiluse-base-multilingual-cased-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--distiluse-base-multilingual-cased-v2\n",
      "Espaço memória necessária: 519.81 MB\n",
      "Número de características: 512\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2\n",
      "Espaço memória necessária: 457.51 MB\n",
      "Número de características: 384\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-mpnet-base-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-mpnet-base-v2\n",
      "Espaço memória necessária: 418.36 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-distilroberta-v1\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-distilroberta-v1\n",
      "Espaço memória necessária: 315.77 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-MiniLM-L6-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-MiniLM-L6-v2\n",
      "Espaço memória necessária: 87.34 MB\n",
      "Número de características: 384\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Gerando embeddings em GPU com modelo sentence-transformers/gtr-t5-large.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2af980d0ac4aee83f7b4e13e1154fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando sentenças:   0%|          | 0/1 [00:00<?, ?batch (batch_size 512)/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2a2c9648ce4cfc9fafbbad48a8c8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta local para o modelo: models--sentence-transformers--gtr-t5-large\n",
      "Espaço memória necessária: 646.96 MB\n",
      "Tamanho: 0.00 MB e tempo de execução: 00:14:55\n",
      "\n",
      "Gerando embeddings em GPU com modelo distiluse-base-multilingual-cased-v2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55638697ebe64f098940ab60bb99bade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando sentenças:   0%|          | 0/1 [00:00<?, ?batch (batch_size 512)/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec374b68f50d4310901dd49135de20f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta local para o modelo: models--sentence-transformers--distiluse-base-multilingual-cased-v2\n",
      "Espaço memória necessária: 519.81 MB\n",
      "Tamanho: 0.00 MB e tempo de execução: 00:14:51\n",
      "\n",
      "Gerando embeddings em GPU com modelo paraphrase-multilingual-MiniLM-L12-v2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36dbf1328f43482eaab9929d3f90b1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando sentenças:   0%|          | 0/1 [00:00<?, ?batch (batch_size 512)/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ca70bbc596493182a3831fd6656492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta local para o modelo: models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2\n",
      "Espaço memória necessária: 457.51 MB\n",
      "Tamanho: 0.00 MB e tempo de execução: 00:14:51\n",
      "\n",
      "Gerando embeddings em GPU com modelo all-mpnet-base-v2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84321dece7d46eda639a631a501dca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando sentenças:   0%|          | 0/1 [00:00<?, ?batch (batch_size 512)/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7c8a2f89cf47bea304ae8ee5a24daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta local para o modelo: models--sentence-transformers--all-mpnet-base-v2\n",
      "Espaço memória necessária: 418.36 MB\n",
      "Tamanho: 0.00 MB e tempo de execução: 00:14:52\n",
      "\n",
      "Gerando embeddings em GPU com modelo all-distilroberta-v1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18a744940ce4d589b7d491b40af20a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando sentenças:   0%|          | 0/1 [00:00<?, ?batch (batch_size 512)/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9961660e6454480f9ea6124812b8cc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta local para o modelo: models--sentence-transformers--all-distilroberta-v1\n",
      "Espaço memória necessária: 315.77 MB\n",
      "Tamanho: 0.00 MB e tempo de execução: 00:14:52\n",
      "\n",
      "Gerando embeddings em GPU com modelo all-MiniLM-L6-v2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e976f17eeddf46a9810890f51f2acf96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando sentenças:   0%|          | 0/1 [00:00<?, ?batch (batch_size 512)/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63ed2ae7d6c4da6a94b953ae0635f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta local para o modelo: models--sentence-transformers--all-MiniLM-L6-v2\n",
      "Espaço memória necessária: 87.34 MB\n",
      "Tamanho: 0.00 MB e tempo de execução: 00:14:50\n",
      "Arquivo de embeddings salvo: embeddings_clustering_funding.pt\n",
      "Número de modelos salvos: 6\n",
      "Conteúdo de self.embeddings: {'sentence-transformers/gtr-t5-large': tensor([[-0.0642,  0.0130, -0.0136,  ..., -0.0095,  0.0457,  0.0098],\n",
      "        [-0.0515, -0.0078, -0.0003,  ...,  0.0098,  0.0172,  0.0604],\n",
      "        [-0.0242, -0.0164, -0.0086,  ..., -0.0338,  0.0040,  0.0792],\n",
      "        ...,\n",
      "        [-0.0640, -0.0172,  0.0062,  ..., -0.0014, -0.0049,  0.0457],\n",
      "        [-0.0262, -0.0127, -0.0137,  ..., -0.0467,  0.0213,  0.0340],\n",
      "        [-0.0526, -0.0210,  0.0236,  ..., -0.0105,  0.0059,  0.0651]],\n",
      "       device='cuda:0'), 'distiluse-base-multilingual-cased-v2': tensor([[-0.0638,  0.0355, -0.0265,  ...,  0.0119, -0.0183, -0.0339],\n",
      "        [-0.0071,  0.0292, -0.0135,  ..., -0.0321,  0.0007, -0.0444],\n",
      "        [ 0.0213, -0.0044, -0.0068,  ..., -0.0043,  0.0189, -0.0535],\n",
      "        ...,\n",
      "        [-0.0498,  0.0433,  0.0320,  ...,  0.0068, -0.0372, -0.0558],\n",
      "        [-0.0378, -0.0248, -0.0208,  ...,  0.0088,  0.0154,  0.0137],\n",
      "        [-0.0342,  0.0242, -0.0018,  ...,  0.0186, -0.0568, -0.0467]],\n",
      "       device='cuda:0'), 'paraphrase-multilingual-MiniLM-L12-v2': tensor([[ 0.1381,  0.3902, -0.1663,  ...,  0.2135, -0.1676,  0.2476],\n",
      "        [ 0.4063, -0.1436, -0.2529,  ..., -0.4340, -0.1000,  0.2441],\n",
      "        [ 0.2416, -0.1265, -0.1752,  ..., -0.3452, -0.1451,  0.2369],\n",
      "        ...,\n",
      "        [ 0.4159, -0.0294,  0.1225,  ..., -0.4548, -0.1006,  0.1278],\n",
      "        [-0.1094,  0.1616, -0.1217,  ..., -0.1883,  0.1642,  0.1800],\n",
      "        [ 0.2024,  0.1470, -0.0109,  ..., -0.0204, -0.0747,  0.1581]],\n",
      "       device='cuda:0'), 'all-mpnet-base-v2': tensor([[ 2.6657e-02,  3.2566e-02, -3.8802e-02,  ..., -2.5337e-02,\n",
      "          2.0354e-02, -1.2952e-02],\n",
      "        [ 1.3426e-05,  5.5859e-02,  1.3467e-02,  ..., -2.6028e-02,\n",
      "         -6.3164e-03, -4.4706e-02],\n",
      "        [-2.2190e-02,  7.9800e-02,  3.9947e-02,  ...,  1.4922e-02,\n",
      "          1.2007e-02,  2.0198e-02],\n",
      "        ...,\n",
      "        [-8.8093e-03,  4.0078e-02, -2.7828e-02,  ..., -3.2549e-02,\n",
      "          1.3364e-03, -2.1434e-02],\n",
      "        [ 2.4140e-03,  5.2175e-02,  3.8283e-02,  ..., -1.5028e-02,\n",
      "         -4.1331e-02, -1.0438e-02],\n",
      "        [ 1.3424e-03,  4.0159e-02, -1.5637e-02,  ..., -2.5068e-02,\n",
      "         -4.0386e-03,  1.6808e-02]], device='cuda:0'), 'all-distilroberta-v1': tensor([[-0.0047, -0.0168, -0.0392,  ...,  0.0073, -0.0413, -0.0253],\n",
      "        [ 0.0037, -0.0015, -0.0302,  ..., -0.0657,  0.0846,  0.0124],\n",
      "        [-0.0055,  0.0112, -0.0220,  ..., -0.0156,  0.0253,  0.0189],\n",
      "        ...,\n",
      "        [ 0.0079,  0.0299, -0.0392,  ..., -0.0180,  0.0083,  0.0005],\n",
      "        [ 0.0200, -0.0247, -0.0461,  ..., -0.0277, -0.0130,  0.0121],\n",
      "        [ 0.0148,  0.0176, -0.0103,  ...,  0.0606,  0.0377,  0.0216]],\n",
      "       device='cuda:0'), 'all-MiniLM-L6-v2': tensor([[-0.0398,  0.0708, -0.0160,  ...,  0.0639, -0.0199,  0.0636],\n",
      "        [-0.0243, -0.0972, -0.0709,  ..., -0.0026, -0.0039,  0.0182],\n",
      "        [ 0.0219, -0.0010, -0.0619,  ...,  0.0100,  0.0635, -0.0243],\n",
      "        ...,\n",
      "        [-0.0282, -0.0622, -0.0208,  ..., -0.0585,  0.0381, -0.0099],\n",
      "        [-0.1382,  0.1097,  0.0172,  ..., -0.0148,  0.0868, -0.0660],\n",
      "        [-0.0689,  0.0765, -0.0531,  ...,  0.0369,  0.0369, -0.0194]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "## Instanciar a classe para manipular modelos da biblioteca SenteceTransformer\n",
    "from gml_embeddings_analyser import EmbeddingsMulticriteriaAnalysis\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Definir os nomes de modelo do SentenceTransformer a serem comparados\n",
    "model_names = [\n",
    "    'sentence-transformers/gtr-t5-large',\n",
    "    'distiluse-base-multilingual-cased-v2',\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'all-mpnet-base-v2',\n",
    "    'all-distilroberta-v1',\n",
    "    'all-MiniLM-L6-v2',\n",
    "]\n",
    "\n",
    "# Criar uma instância da classe EmbeddingsMulticriteriaAnalysis\n",
    "analisador = EmbeddingsMulticriteriaAnalysis(\n",
    "    model_names=model_names,\n",
    "    models= [SentenceTransformer(model_name) for model_name in model_names]\n",
    ")\n",
    "\n",
    "## Gerar e salvar embeedings com cada modelo (15min/modelo em média)\n",
    "embeddings_dict = analisador.generate_embeddings_batch()\n",
    "analisador.save_embeddings_dict(\"embeddings_clustering_funding.pt\", manter_gradientes=False)\n",
    "analisador.exportar_estatisticas_modelos()  # Salva as estatísticas em \"estatisticas_modelos.json\"\n",
    "\n",
    "## Usos futuros no pipeline de treinamento e inferências\n",
    "# # 1. Avaliar modelos para clustering: Salvar os embeddings com manter_gradientes=False\n",
    "# embeddings_modelo1 = modelo1.encode(textos_editais, convert_to_tensor=True)\n",
    "# analise.save_embeddings_dict(filename=\"embeddings_modelo1_clustering.pt\", manter_gradientes=False)\n",
    "\n",
    "# # 2. Avaliação de similaridade no grafo (após escolher o melhor modelo):\n",
    "# embeddings_modelo_escolhido = modelo_escolhido.encode(textos_editais, convert_to_tensor=True)\n",
    "# analise.save_embeddings_dict(filename=\"embeddings_modelo_escolhido_grafo.pt\", manter_gradientes=True)\n",
    "\n",
    "print(\"Conteúdo de self.embeddings:\", analisador.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar embeedings e avaliar \n",
    "\n",
    "Obs.: Há erro intermitente (cálculo com cuml) que torna necessário tentar várias vezes a avaliação até rodar com sucesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 21:48:53,730 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:48:53,730 - INFO - Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large\n",
      "2024-11-12 21:48:56,409 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:48:56,410 - INFO - Load pretrained SentenceTransformer: distiluse-base-multilingual-cased-v2\n",
      "2024-11-12 21:48:58,540 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:48:58,541 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2024-11-12 21:49:00,981 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:49:00,981 - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "2024-11-12 21:49:03,043 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:49:03,043 - INFO - Load pretrained SentenceTransformer: all-distilroberta-v1\n",
      "2024-11-12 21:49:04,830 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:49:04,831 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/gtr-t5-large\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--gtr-t5-large\n",
      "Espaço memória necessária: 646.96 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
      "  (3): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/distiluse-base-multilingual-cased-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--distiluse-base-multilingual-cased-v2\n",
      "Espaço memória necessária: 519.81 MB\n",
      "Número de características: 512\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2\n",
      "Espaço memória necessária: 457.51 MB\n",
      "Número de características: 384\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-mpnet-base-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-mpnet-base-v2\n",
      "Espaço memória necessária: 418.36 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-distilroberta-v1\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-distilroberta-v1\n",
      "Espaço memória necessária: 315.77 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-MiniLM-L6-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-MiniLM-L6-v2\n",
      "Espaço memória necessária: 87.34 MB\n",
      "Número de características: 384\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Arquivo de embeddings carregado: embeddings_clustering_funding.pt\n",
      "Número de modelos carregados: 6\n",
      "\n",
      "Iniciando avaliação de clustering com scikit-learn (CPU)...\n",
      "Avaliando modelo: sentence-transformers/gtr-t5-large\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 768)\n",
      "    Tamanho de X_test: (70, 768)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 768)\n",
      "    Tamanho de X_test: (70, 768)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 768)\n",
      "    Tamanho de X_test: (70, 768)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "Avaliando modelo: distiluse-base-multilingual-cased-v2\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 512)\n",
      "    Tamanho de X_test: (70, 512)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 512)\n",
      "    Tamanho de X_test: (69, 512)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 512)\n",
      "    Tamanho de X_test: (69, 512)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 512)\n",
      "    Tamanho de X_test: (69, 512)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 512)\n",
      "    Tamanho de X_test: (69, 512)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 512)\n",
      "    Tamanho de X_test: (70, 512)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 512)\n",
      "    Tamanho de X_test: (69, 512)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 512)\n",
      "    Tamanho de X_test: (69, 512)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 512)\n",
      "    Tamanho de X_test: (69, 512)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 512)\n",
      "    Tamanho de X_test: (69, 512)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 512)\n",
      "    Tamanho de X_test: (70, 512)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 512)\n",
      "    Tamanho de X_test: (69, 512)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 512)\n",
      "    Tamanho de X_test: (69, 512)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 512)\n",
      "    Tamanho de X_test: (69, 512)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 512)\n",
      "    Tamanho de X_test: (69, 512)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "Avaliando modelo: paraphrase-multilingual-MiniLM-L12-v2\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 384)\n",
      "    Tamanho de X_test: (70, 384)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 384)\n",
      "    Tamanho de X_test: (70, 384)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 384)\n",
      "    Tamanho de X_test: (70, 384)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "Avaliando modelo: all-mpnet-base-v2\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 768)\n",
      "    Tamanho de X_test: (70, 768)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 768)\n",
      "    Tamanho de X_test: (70, 768)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 768)\n",
      "    Tamanho de X_test: (70, 768)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "Avaliando modelo: all-distilroberta-v1\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 768)\n",
      "    Tamanho de X_test: (70, 768)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 768)\n",
      "    Tamanho de X_test: (70, 768)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 768)\n",
      "    Tamanho de X_test: (70, 768)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 768)\n",
      "    Tamanho de X_test: (69, 768)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "Avaliando modelo: all-MiniLM-L6-v2\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 384)\n",
      "    Tamanho de X_test: (70, 384)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 384)\n",
      "    Tamanho de X_test: (70, 384)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 1:\n",
      "    Tamanho de X_train: (276, 384)\n",
      "    Tamanho de X_test: (70, 384)\n",
      "    Tamanho de cluster_labels: (70,)\n",
      "  Split 2:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 3:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 4:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "  Split 5:\n",
      "    Tamanho de X_train: (277, 384)\n",
      "    Tamanho de X_test: (69, 384)\n",
      "    Tamanho de cluster_labels: (69,)\n",
      "Arquivo de resultados salvo: resultados_clustering.json\n",
      "Número de modelos avaliados: 6\n"
     ]
    }
   ],
   "source": [
    "## Instanciar a classe para manipular modelos da biblioteca SenteceTransformer\n",
    "from gml_embeddings_analyser import EmbeddingsMulticriteriaAnalysis\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Definir os nomes de modelo do SentenceTransformer a serem comparados\n",
    "model_names = [\n",
    "    'sentence-transformers/gtr-t5-large',\n",
    "    'distiluse-base-multilingual-cased-v2',\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'all-mpnet-base-v2',\n",
    "    'all-distilroberta-v1',\n",
    "    'all-MiniLM-L6-v2',\n",
    "]\n",
    "\n",
    "# Criar uma instância da classe EmbeddingsMulticriteriaAnalysis\n",
    "analisador = EmbeddingsMulticriteriaAnalysis(\n",
    "    model_names=model_names,\n",
    "    models= [SentenceTransformer(model_name) for model_name in model_names]\n",
    ")\n",
    "\n",
    "## Carregar embeddings previamente gerados (evita retrabalho em caso de erro na avaliação a seguir)\n",
    "embeddings_dict = analisador.load_embeddings_dict(\"embeddings_clustering_funding.pt\")\n",
    "\n",
    "## Realizar a análise de clusterização\n",
    "resultados = analisador.evaluate_clustering_cpu(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence-transformers/gtr-t5-large': {'KMeans': {'resultados': [{'silhouette': 0.533621,\n",
       "     'calinski_harabasz': 13.533913562844297,\n",
       "     'davies_bouldin': 1.6859954917891926},\n",
       "    {'silhouette': 0.50408417,\n",
       "     'calinski_harabasz': 10.938225625745329,\n",
       "     'davies_bouldin': 2.1041267155568475},\n",
       "    {'silhouette': 0.36231467,\n",
       "     'calinski_harabasz': 7.736712730033795,\n",
       "     'davies_bouldin': 2.1261601998630146},\n",
       "    {'silhouette': 0.4041367,\n",
       "     'calinski_harabasz': 7.7968643008140655,\n",
       "     'davies_bouldin': 1.8554691158112506},\n",
       "    {'silhouette': 0.55873007,\n",
       "     'calinski_harabasz': 12.844139666539363,\n",
       "     'davies_bouldin': 1.6838126642349527}],\n",
       "   'tempo': 0.24861111111111112},\n",
       "  'DBSCAN': {'resultados': [{'silhouette': 0.5177514,\n",
       "     'calinski_harabasz': 22.83191413132084,\n",
       "     'davies_bouldin': 0.9677100846079031},\n",
       "    {'silhouette': 0.44365636,\n",
       "     'calinski_harabasz': 15.385932477698203,\n",
       "     'davies_bouldin': 0.9832909136961151},\n",
       "    {'silhouette': 0.345959,\n",
       "     'calinski_harabasz': 23.75460919414886,\n",
       "     'davies_bouldin': 0.9889528322946813},\n",
       "    {'silhouette': 0.39519107,\n",
       "     'calinski_harabasz': 17.285966112539914,\n",
       "     'davies_bouldin': 0.9660458569534828},\n",
       "    {'silhouette': 0.30780447,\n",
       "     'calinski_harabasz': 13.340401342063881,\n",
       "     'davies_bouldin': 1.4630066236996466}],\n",
       "   'tempo': 0.24861111111111112},\n",
       "  'HDBSCAN': {'resultados': [{'silhouette': 0.34175384,\n",
       "     'calinski_harabasz': 13.91920263401491,\n",
       "     'davies_bouldin': 1.811058828333424},\n",
       "    {'silhouette': 0.2837999,\n",
       "     'calinski_harabasz': 9.385060361792203,\n",
       "     'davies_bouldin': 2.3822341614326676},\n",
       "    {'silhouette': 0.37378737,\n",
       "     'calinski_harabasz': 13.392711757494434,\n",
       "     'davies_bouldin': 1.6770750544576147},\n",
       "    {'silhouette': 0.48122185,\n",
       "     'calinski_harabasz': 22.577246412204932,\n",
       "     'davies_bouldin': 1.1447057975054926},\n",
       "    {'silhouette': 0.43215516,\n",
       "     'calinski_harabasz': 19.483830821901712,\n",
       "     'davies_bouldin': 1.8292377644701894}],\n",
       "   'tempo': 0.24861111111111112}},\n",
       " 'distiluse-base-multilingual-cased-v2': {'KMeans': {'resultados': [{'silhouette': 0.46700767,\n",
       "     'calinski_harabasz': 11.306691941062184,\n",
       "     'davies_bouldin': 1.7445869564774183},\n",
       "    {'silhouette': 0.41992927,\n",
       "     'calinski_harabasz': 11.110413015549266,\n",
       "     'davies_bouldin': 1.650222057824398},\n",
       "    {'silhouette': 0.47424698,\n",
       "     'calinski_harabasz': 12.57230847867934,\n",
       "     'davies_bouldin': 1.5020783793446406},\n",
       "    {'silhouette': 0.46071747,\n",
       "     'calinski_harabasz': 11.962913965746791,\n",
       "     'davies_bouldin': 1.4561381093801924},\n",
       "    {'silhouette': 0.6154766,\n",
       "     'calinski_harabasz': 18.338620729134053,\n",
       "     'davies_bouldin': 1.197226088208494}],\n",
       "   'tempo': 0.2475},\n",
       "  'DBSCAN': {'resultados': [{'silhouette': 0.34771144,\n",
       "     'calinski_harabasz': 13.665218884944586,\n",
       "     'davies_bouldin': 1.2790819199139234},\n",
       "    {'silhouette': 0.42145935,\n",
       "     'calinski_harabasz': 32.1458272602406,\n",
       "     'davies_bouldin': 0.9130503366309456},\n",
       "    {'silhouette': 0.38655084,\n",
       "     'calinski_harabasz': 15.263284496895077,\n",
       "     'davies_bouldin': 1.3243073752882304},\n",
       "    {'silhouette': 0.4470286,\n",
       "     'calinski_harabasz': 20.774504936610928,\n",
       "     'davies_bouldin': 0.8592828902498942},\n",
       "    {'silhouette': 0.5169716,\n",
       "     'calinski_harabasz': 25.798202367901105,\n",
       "     'davies_bouldin': 0.8660948676326186}],\n",
       "   'tempo': 0.2475},\n",
       "  'HDBSCAN': {'resultados': [{'silhouette': 0.4896008,\n",
       "     'calinski_harabasz': 18.522942955181644,\n",
       "     'davies_bouldin': 2.0355213435787567},\n",
       "    {'silhouette': 0.41130623,\n",
       "     'calinski_harabasz': 16.766125472306832,\n",
       "     'davies_bouldin': 2.279045520408988},\n",
       "    {'silhouette': 0.37279803,\n",
       "     'calinski_harabasz': 17.197018491225574,\n",
       "     'davies_bouldin': 1.5871302630241801},\n",
       "    {'silhouette': 0.40847376,\n",
       "     'calinski_harabasz': 12.509198946200843,\n",
       "     'davies_bouldin': 1.6814710256631955},\n",
       "    {'silhouette': 0.43511906,\n",
       "     'calinski_harabasz': 18.22012130563875,\n",
       "     'davies_bouldin': 1.9341305201893866}],\n",
       "   'tempo': 0.2475}},\n",
       " 'paraphrase-multilingual-MiniLM-L12-v2': {'KMeans': {'resultados': [{'silhouette': 0.30588594,\n",
       "     'calinski_harabasz': 9.396349311227569,\n",
       "     'davies_bouldin': 1.2379365827074778},\n",
       "    {'silhouette': 0.4582422,\n",
       "     'calinski_harabasz': 12.767452016146255,\n",
       "     'davies_bouldin': 1.241757793617256},\n",
       "    {'silhouette': 0.6088492,\n",
       "     'calinski_harabasz': 18.74401421844823,\n",
       "     'davies_bouldin': 1.0939185768278534},\n",
       "    {'silhouette': 0.40251267,\n",
       "     'calinski_harabasz': 10.149664326741545,\n",
       "     'davies_bouldin': 1.6337436204444784},\n",
       "    {'silhouette': 0.46064377,\n",
       "     'calinski_harabasz': 12.725940376713941,\n",
       "     'davies_bouldin': 1.097621311228375}],\n",
       "   'tempo': 0.2475},\n",
       "  'DBSCAN': {'resultados': [{'silhouette': 0.4007411,\n",
       "     'calinski_harabasz': 21.035172439605528,\n",
       "     'davies_bouldin': 0.882032912482578},\n",
       "    {'silhouette': 0.34629813,\n",
       "     'calinski_harabasz': 25.18825078949968,\n",
       "     'davies_bouldin': 0.9173678352440836},\n",
       "    {'silhouette': 0.36643678,\n",
       "     'calinski_harabasz': 17.59693044558077,\n",
       "     'davies_bouldin': 0.9457800070099678},\n",
       "    {'silhouette': 0.37951356,\n",
       "     'calinski_harabasz': 16.511657744301132,\n",
       "     'davies_bouldin': 1.0159540568216452},\n",
       "    {'silhouette': 0.33783627,\n",
       "     'calinski_harabasz': 14.28668570573464,\n",
       "     'davies_bouldin': 1.4093993832837317}],\n",
       "   'tempo': 0.2475},\n",
       "  'HDBSCAN': {'resultados': [{'silhouette': 0.44460258,\n",
       "     'calinski_harabasz': 15.610697592716926,\n",
       "     'davies_bouldin': 2.2180312208269592},\n",
       "    {'silhouette': 0.2775088,\n",
       "     'calinski_harabasz': 13.705123817647408,\n",
       "     'davies_bouldin': 2.6145373427186094},\n",
       "    {'silhouette': 0.46420082,\n",
       "     'calinski_harabasz': 20.678992939331284,\n",
       "     'davies_bouldin': 2.0929449293802325},\n",
       "    {'silhouette': 0.37924665,\n",
       "     'calinski_harabasz': 16.10796832894951,\n",
       "     'davies_bouldin': 2.7798388226126316},\n",
       "    {'silhouette': 0.28050366,\n",
       "     'calinski_harabasz': 9.67062744943103,\n",
       "     'davies_bouldin': 2.3188610520192667}],\n",
       "   'tempo': 0.2475}},\n",
       " 'all-mpnet-base-v2': {'KMeans': {'resultados': [{'silhouette': 0.45056885,\n",
       "     'calinski_harabasz': 11.719219847041556,\n",
       "     'davies_bouldin': 1.8191609371154878},\n",
       "    {'silhouette': 0.57813305,\n",
       "     'calinski_harabasz': 16.560814144245086,\n",
       "     'davies_bouldin': 1.6508625094717388},\n",
       "    {'silhouette': 0.49752614,\n",
       "     'calinski_harabasz': 13.280570936117737,\n",
       "     'davies_bouldin': 1.7411504145893986},\n",
       "    {'silhouette': 0.48807824,\n",
       "     'calinski_harabasz': 10.865389030047748,\n",
       "     'davies_bouldin': 1.719561765361796},\n",
       "    {'silhouette': 0.40185457,\n",
       "     'calinski_harabasz': 8.78028969511986,\n",
       "     'davies_bouldin': 1.9720157461025816}],\n",
       "   'tempo': 0.2477777777777778},\n",
       "  'DBSCAN': {'resultados': [{'silhouette': 0.50334567,\n",
       "     'calinski_harabasz': 19.2452073456353,\n",
       "     'davies_bouldin': 0.9787873959449066},\n",
       "    {'silhouette': 0.37544203,\n",
       "     'calinski_harabasz': 16.279088664324103,\n",
       "     'davies_bouldin': 0.9866497645011701},\n",
       "    {'silhouette': 0.45902833,\n",
       "     'calinski_harabasz': 21.43091408417923,\n",
       "     'davies_bouldin': 0.9463388450276873},\n",
       "    {'silhouette': 0.33061615,\n",
       "     'calinski_harabasz': 14.491418853195338,\n",
       "     'davies_bouldin': 0.9603356475611126},\n",
       "    {'silhouette': 0.43563035,\n",
       "     'calinski_harabasz': 19.125391838673075,\n",
       "     'davies_bouldin': 0.9708827562393066}],\n",
       "   'tempo': 0.2477777777777778},\n",
       "  'HDBSCAN': {'resultados': [{'silhouette': 0.47545695,\n",
       "     'calinski_harabasz': 18.963512116914988,\n",
       "     'davies_bouldin': 1.6247567832828291},\n",
       "    {'silhouette': 0.4025955,\n",
       "     'calinski_harabasz': 15.693758076821895,\n",
       "     'davies_bouldin': 1.719413630703471},\n",
       "    {'silhouette': 0.37905458,\n",
       "     'calinski_harabasz': 11.345888859382239,\n",
       "     'davies_bouldin': 1.7578962624046957},\n",
       "    {'silhouette': 0.4010775,\n",
       "     'calinski_harabasz': 15.073911564771814,\n",
       "     'davies_bouldin': 2.0805883456964187},\n",
       "    {'silhouette': 0.444328,\n",
       "     'calinski_harabasz': 15.801343702876368,\n",
       "     'davies_bouldin': 2.1672753747718287}],\n",
       "   'tempo': 0.2477777777777778}},\n",
       " 'all-distilroberta-v1': {'KMeans': {'resultados': [{'silhouette': 0.5111838,\n",
       "     'calinski_harabasz': 14.054771204538934,\n",
       "     'davies_bouldin': 1.591648857426688},\n",
       "    {'silhouette': 0.43396145,\n",
       "     'calinski_harabasz': 10.186483615824079,\n",
       "     'davies_bouldin': 1.760529683237531},\n",
       "    {'silhouette': 0.3799258,\n",
       "     'calinski_harabasz': 8.146048795853014,\n",
       "     'davies_bouldin': 1.6578824238744425},\n",
       "    {'silhouette': 0.56192666,\n",
       "     'calinski_harabasz': 14.294456767726189,\n",
       "     'davies_bouldin': 1.450927604935528},\n",
       "    {'silhouette': 0.44580516,\n",
       "     'calinski_harabasz': 10.222461147014512,\n",
       "     'davies_bouldin': 1.8094053816910396}],\n",
       "   'tempo': 0.2477777777777778},\n",
       "  'DBSCAN': {'resultados': [{'silhouette': 0.3888044,\n",
       "     'calinski_harabasz': 17.514471502972828,\n",
       "     'davies_bouldin': 0.9998903305541447},\n",
       "    {'silhouette': 0.51089865,\n",
       "     'calinski_harabasz': 16.276784946385018,\n",
       "     'davies_bouldin': 1.0008884756043253},\n",
       "    {'silhouette': 0.42572147,\n",
       "     'calinski_harabasz': 31.41438196699287,\n",
       "     'davies_bouldin': 0.9266798940620147},\n",
       "    {'silhouette': 0.30253902,\n",
       "     'calinski_harabasz': 20.10572498990318,\n",
       "     'davies_bouldin': 1.0297334648107057},\n",
       "    {'silhouette': 0.32465312,\n",
       "     'calinski_harabasz': 21.972161507272002,\n",
       "     'davies_bouldin': 1.0074624753233505}],\n",
       "   'tempo': 0.2477777777777778},\n",
       "  'HDBSCAN': {'resultados': [{'silhouette': 0.37095967,\n",
       "     'calinski_harabasz': 14.476122467596907,\n",
       "     'davies_bouldin': 2.131911413508786},\n",
       "    {'silhouette': 0.39405826,\n",
       "     'calinski_harabasz': 13.439853094558195,\n",
       "     'davies_bouldin': 1.844119223189492},\n",
       "    {'silhouette': 0.2856433,\n",
       "     'calinski_harabasz': 10.0600766552309,\n",
       "     'davies_bouldin': 2.154455544105766},\n",
       "    {'silhouette': 0.3385535,\n",
       "     'calinski_harabasz': 14.448823871696282,\n",
       "     'davies_bouldin': 1.708809812948864},\n",
       "    {'silhouette': 0.34785566,\n",
       "     'calinski_harabasz': 14.20257602668761,\n",
       "     'davies_bouldin': 1.841475600897784}],\n",
       "   'tempo': 0.2477777777777778}},\n",
       " 'all-MiniLM-L6-v2': {'KMeans': {'resultados': [{'silhouette': 0.40742844,\n",
       "     'calinski_harabasz': 10.083541708768285,\n",
       "     'davies_bouldin': 1.8962450510320514},\n",
       "    {'silhouette': 0.3900285,\n",
       "     'calinski_harabasz': 7.9153934236691565,\n",
       "     'davies_bouldin': 2.0941442567649},\n",
       "    {'silhouette': 0.39892504,\n",
       "     'calinski_harabasz': 9.88571408508449,\n",
       "     'davies_bouldin': 1.7120384469324381},\n",
       "    {'silhouette': 0.5342093,\n",
       "     'calinski_harabasz': 13.146917533712026,\n",
       "     'davies_bouldin': 1.6289943604784123},\n",
       "    {'silhouette': 0.5624474,\n",
       "     'calinski_harabasz': 14.560852036566493,\n",
       "     'davies_bouldin': 1.6001611376125635}],\n",
       "   'tempo': 0.24722222222222223},\n",
       "  'DBSCAN': {'resultados': [{'silhouette': 0.42321426,\n",
       "     'calinski_harabasz': 18.739972846650765,\n",
       "     'davies_bouldin': 1.0228032258637227},\n",
       "    {'silhouette': 0.42129928,\n",
       "     'calinski_harabasz': 14.762061622930094,\n",
       "     'davies_bouldin': 0.9930454890427873},\n",
       "    {'silhouette': 0.31147668,\n",
       "     'calinski_harabasz': 21.005850396556298,\n",
       "     'davies_bouldin': 1.0056783703880854},\n",
       "    {'silhouette': 0.35474193,\n",
       "     'calinski_harabasz': 15.542260586038614,\n",
       "     'davies_bouldin': 1.0142648790919597},\n",
       "    {'silhouette': 0.39080298,\n",
       "     'calinski_harabasz': 26.574226096006964,\n",
       "     'davies_bouldin': 0.9915191930478545}],\n",
       "   'tempo': 0.24722222222222223},\n",
       "  'HDBSCAN': {'resultados': [{'silhouette': 0.4342466,\n",
       "     'calinski_harabasz': 17.60729166291403,\n",
       "     'davies_bouldin': 1.7007143369360418},\n",
       "    {'silhouette': 0.36533073,\n",
       "     'calinski_harabasz': 12.569676406882461,\n",
       "     'davies_bouldin': 2.2929243171147244},\n",
       "    {'silhouette': 0.45128357,\n",
       "     'calinski_harabasz': 13.191011376063166,\n",
       "     'davies_bouldin': 1.3725828959393955},\n",
       "    {'silhouette': 0.29719132,\n",
       "     'calinski_harabasz': 12.371329549787596,\n",
       "     'davies_bouldin': 1.6315374362826154},\n",
       "    {'silhouette': 0.4132501,\n",
       "     'calinski_harabasz': 15.641843470738424,\n",
       "     'davies_bouldin': 1.7013967778928876}],\n",
       "   'tempo': 0.24722222222222223}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretação das Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Silhouette score: quanto maior melhor clustering, por coesão e separação [-1 a 1]\n",
    "    Calinski_Harabasz: quanto maior melhor clustering, sugere uma melhor separação\n",
    "       Davies-Bouldin: quanto menor melhor clustering, clusters mais distintos e compactos\n",
    "\n",
    "Portanto, a lógica de interpretação correta para cada métrica é:\n",
    "\n",
    "   - Silhouette-------------> Quanto maior, melhor\n",
    "   - Calinski-Harabasz--> Quanto maior, melhor\n",
    "   - Davies-Bouldin------> Quanto menor, melhor\n",
    "   - Tempo------------------> Quanto menor, melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 21:49:39,769 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:49:39,769 - INFO - Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large\n",
      "2024-11-12 21:49:42,267 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:49:42,268 - INFO - Load pretrained SentenceTransformer: distiluse-base-multilingual-cased-v2\n",
      "2024-11-12 21:49:44,310 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:49:44,310 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2024-11-12 21:49:46,757 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:49:46,757 - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "2024-11-12 21:49:48,465 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:49:48,465 - INFO - Load pretrained SentenceTransformer: all-distilroberta-v1\n",
      "2024-11-12 21:49:50,235 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:49:50,235 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/gtr-t5-large\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--gtr-t5-large\n",
      "Espaço memória necessária: 646.96 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
      "  (3): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/distiluse-base-multilingual-cased-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--distiluse-base-multilingual-cased-v2\n",
      "Espaço memória necessária: 519.81 MB\n",
      "Número de características: 512\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2\n",
      "Espaço memória necessária: 457.51 MB\n",
      "Número de características: 384\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-mpnet-base-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-mpnet-base-v2\n",
      "Espaço memória necessária: 418.36 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-distilroberta-v1\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-distilroberta-v1\n",
      "Espaço memória necessária: 315.77 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-MiniLM-L6-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-MiniLM-L6-v2\n",
      "Espaço memória necessária: 87.34 MB\n",
      "Número de características: 384\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "sentence-transformers/gtr-t5-large\n",
      "KMeans\n",
      "  resultados: [{'silhouette': 0.5336210131645203, 'calinski_harabasz': 13.533913562844297, 'davies_bouldin': 1.6859954917891926}, {'silhouette': 0.5040841698646545, 'calinski_harabasz': 10.938225625745329, 'davies_bouldin': 2.1041267155568475}, {'silhouette': 0.3623146712779999, 'calinski_harabasz': 7.736712730033795, 'davies_bouldin': 2.1261601998630146}, {'silhouette': 0.40413668751716614, 'calinski_harabasz': 7.7968643008140655, 'davies_bouldin': 1.8554691158112506}, {'silhouette': 0.5587300658226013, 'calinski_harabasz': 12.844139666539363, 'davies_bouldin': 1.6838126642349527}]\n",
      "  tempo: 0.24861111111111112\n",
      "DBSCAN\n",
      "  resultados: [{'silhouette': 0.5177513957023621, 'calinski_harabasz': 22.83191413132084, 'davies_bouldin': 0.9677100846079031}, {'silhouette': 0.4436563551425934, 'calinski_harabasz': 15.385932477698203, 'davies_bouldin': 0.9832909136961151}, {'silhouette': 0.34595900774002075, 'calinski_harabasz': 23.75460919414886, 'davies_bouldin': 0.9889528322946813}, {'silhouette': 0.3951910734176636, 'calinski_harabasz': 17.285966112539914, 'davies_bouldin': 0.9660458569534828}, {'silhouette': 0.3078044652938843, 'calinski_harabasz': 13.340401342063881, 'davies_bouldin': 1.4630066236996466}]\n",
      "  tempo: 0.24861111111111112\n",
      "HDBSCAN\n",
      "  resultados: [{'silhouette': 0.34175384044647217, 'calinski_harabasz': 13.91920263401491, 'davies_bouldin': 1.811058828333424}, {'silhouette': 0.2837998867034912, 'calinski_harabasz': 9.385060361792203, 'davies_bouldin': 2.3822341614326676}, {'silhouette': 0.37378737330436707, 'calinski_harabasz': 13.392711757494434, 'davies_bouldin': 1.6770750544576147}, {'silhouette': 0.48122185468673706, 'calinski_harabasz': 22.577246412204932, 'davies_bouldin': 1.1447057975054926}, {'silhouette': 0.43215516209602356, 'calinski_harabasz': 19.483830821901712, 'davies_bouldin': 1.8292377644701894}]\n",
      "  tempo: 0.24861111111111112\n",
      "distiluse-base-multilingual-cased-v2\n",
      "KMeans\n",
      "  resultados: [{'silhouette': 0.46700766682624817, 'calinski_harabasz': 11.306691941062184, 'davies_bouldin': 1.7445869564774183}, {'silhouette': 0.41992926597595215, 'calinski_harabasz': 11.110413015549266, 'davies_bouldin': 1.650222057824398}, {'silhouette': 0.4742469787597656, 'calinski_harabasz': 12.57230847867934, 'davies_bouldin': 1.5020783793446406}, {'silhouette': 0.46071746945381165, 'calinski_harabasz': 11.962913965746791, 'davies_bouldin': 1.4561381093801924}, {'silhouette': 0.6154766082763672, 'calinski_harabasz': 18.338620729134053, 'davies_bouldin': 1.197226088208494}]\n",
      "  tempo: 0.2475\n",
      "DBSCAN\n",
      "  resultados: [{'silhouette': 0.347711443901062, 'calinski_harabasz': 13.665218884944586, 'davies_bouldin': 1.2790819199139234}, {'silhouette': 0.4214593470096588, 'calinski_harabasz': 32.1458272602406, 'davies_bouldin': 0.9130503366309456}, {'silhouette': 0.3865508437156677, 'calinski_harabasz': 15.263284496895077, 'davies_bouldin': 1.3243073752882304}, {'silhouette': 0.44702860713005066, 'calinski_harabasz': 20.774504936610928, 'davies_bouldin': 0.8592828902498942}, {'silhouette': 0.5169715881347656, 'calinski_harabasz': 25.798202367901105, 'davies_bouldin': 0.8660948676326186}]\n",
      "  tempo: 0.2475\n",
      "HDBSCAN\n",
      "  resultados: [{'silhouette': 0.48960080742836, 'calinski_harabasz': 18.522942955181644, 'davies_bouldin': 2.0355213435787567}, {'silhouette': 0.411306232213974, 'calinski_harabasz': 16.766125472306832, 'davies_bouldin': 2.279045520408988}, {'silhouette': 0.37279802560806274, 'calinski_harabasz': 17.197018491225574, 'davies_bouldin': 1.5871302630241801}, {'silhouette': 0.40847375988960266, 'calinski_harabasz': 12.509198946200843, 'davies_bouldin': 1.6814710256631955}, {'silhouette': 0.43511906266212463, 'calinski_harabasz': 18.22012130563875, 'davies_bouldin': 1.9341305201893866}]\n",
      "  tempo: 0.2475\n",
      "paraphrase-multilingual-MiniLM-L12-v2\n",
      "KMeans\n",
      "  resultados: [{'silhouette': 0.3058859407901764, 'calinski_harabasz': 9.396349311227569, 'davies_bouldin': 1.2379365827074778}, {'silhouette': 0.4582422077655792, 'calinski_harabasz': 12.767452016146255, 'davies_bouldin': 1.241757793617256}, {'silhouette': 0.6088492274284363, 'calinski_harabasz': 18.74401421844823, 'davies_bouldin': 1.0939185768278534}, {'silhouette': 0.40251266956329346, 'calinski_harabasz': 10.149664326741545, 'davies_bouldin': 1.6337436204444784}, {'silhouette': 0.4606437683105469, 'calinski_harabasz': 12.725940376713941, 'davies_bouldin': 1.097621311228375}]\n",
      "  tempo: 0.2475\n",
      "DBSCAN\n",
      "  resultados: [{'silhouette': 0.4007411003112793, 'calinski_harabasz': 21.035172439605528, 'davies_bouldin': 0.882032912482578}, {'silhouette': 0.34629812836647034, 'calinski_harabasz': 25.18825078949968, 'davies_bouldin': 0.9173678352440836}, {'silhouette': 0.36643677949905396, 'calinski_harabasz': 17.59693044558077, 'davies_bouldin': 0.9457800070099678}, {'silhouette': 0.37951356172561646, 'calinski_harabasz': 16.511657744301132, 'davies_bouldin': 1.0159540568216452}, {'silhouette': 0.33783626556396484, 'calinski_harabasz': 14.28668570573464, 'davies_bouldin': 1.4093993832837317}]\n",
      "  tempo: 0.2475\n",
      "HDBSCAN\n",
      "  resultados: [{'silhouette': 0.4446025788784027, 'calinski_harabasz': 15.610697592716926, 'davies_bouldin': 2.2180312208269592}, {'silhouette': 0.27750879526138306, 'calinski_harabasz': 13.705123817647408, 'davies_bouldin': 2.6145373427186094}, {'silhouette': 0.46420082449913025, 'calinski_harabasz': 20.678992939331284, 'davies_bouldin': 2.0929449293802325}, {'silhouette': 0.37924665212631226, 'calinski_harabasz': 16.10796832894951, 'davies_bouldin': 2.7798388226126316}, {'silhouette': 0.28050366044044495, 'calinski_harabasz': 9.67062744943103, 'davies_bouldin': 2.3188610520192667}]\n",
      "  tempo: 0.2475\n",
      "all-mpnet-base-v2\n",
      "KMeans\n",
      "  resultados: [{'silhouette': 0.4505688548088074, 'calinski_harabasz': 11.719219847041556, 'davies_bouldin': 1.8191609371154878}, {'silhouette': 0.5781330466270447, 'calinski_harabasz': 16.560814144245086, 'davies_bouldin': 1.6508625094717388}, {'silhouette': 0.4975261390209198, 'calinski_harabasz': 13.280570936117737, 'davies_bouldin': 1.7411504145893986}, {'silhouette': 0.488078236579895, 'calinski_harabasz': 10.865389030047748, 'davies_bouldin': 1.719561765361796}, {'silhouette': 0.40185457468032837, 'calinski_harabasz': 8.78028969511986, 'davies_bouldin': 1.9720157461025816}]\n",
      "  tempo: 0.2477777777777778\n",
      "DBSCAN\n",
      "  resultados: [{'silhouette': 0.5033456683158875, 'calinski_harabasz': 19.2452073456353, 'davies_bouldin': 0.9787873959449066}, {'silhouette': 0.3754420280456543, 'calinski_harabasz': 16.279088664324103, 'davies_bouldin': 0.9866497645011701}, {'silhouette': 0.45902833342552185, 'calinski_harabasz': 21.43091408417923, 'davies_bouldin': 0.9463388450276873}, {'silhouette': 0.33061614632606506, 'calinski_harabasz': 14.491418853195338, 'davies_bouldin': 0.9603356475611126}, {'silhouette': 0.43563035130500793, 'calinski_harabasz': 19.125391838673075, 'davies_bouldin': 0.9708827562393066}]\n",
      "  tempo: 0.2477777777777778\n",
      "HDBSCAN\n",
      "  resultados: [{'silhouette': 0.47545695304870605, 'calinski_harabasz': 18.963512116914988, 'davies_bouldin': 1.6247567832828291}, {'silhouette': 0.40259549021720886, 'calinski_harabasz': 15.693758076821895, 'davies_bouldin': 1.719413630703471}, {'silhouette': 0.37905457615852356, 'calinski_harabasz': 11.345888859382239, 'davies_bouldin': 1.7578962624046957}, {'silhouette': 0.4010775089263916, 'calinski_harabasz': 15.073911564771814, 'davies_bouldin': 2.0805883456964187}, {'silhouette': 0.4443280100822449, 'calinski_harabasz': 15.801343702876368, 'davies_bouldin': 2.1672753747718287}]\n",
      "  tempo: 0.2477777777777778\n",
      "all-distilroberta-v1\n",
      "KMeans\n",
      "  resultados: [{'silhouette': 0.5111837983131409, 'calinski_harabasz': 14.054771204538934, 'davies_bouldin': 1.591648857426688}, {'silhouette': 0.4339614510536194, 'calinski_harabasz': 10.186483615824079, 'davies_bouldin': 1.760529683237531}, {'silhouette': 0.37992578744888306, 'calinski_harabasz': 8.146048795853014, 'davies_bouldin': 1.6578824238744425}, {'silhouette': 0.5619266629219055, 'calinski_harabasz': 14.294456767726189, 'davies_bouldin': 1.450927604935528}, {'silhouette': 0.445805162191391, 'calinski_harabasz': 10.222461147014512, 'davies_bouldin': 1.8094053816910396}]\n",
      "  tempo: 0.2477777777777778\n",
      "DBSCAN\n",
      "  resultados: [{'silhouette': 0.3888044059276581, 'calinski_harabasz': 17.514471502972828, 'davies_bouldin': 0.9998903305541447}, {'silhouette': 0.5108986496925354, 'calinski_harabasz': 16.276784946385018, 'davies_bouldin': 1.0008884756043253}, {'silhouette': 0.4257214665412903, 'calinski_harabasz': 31.41438196699287, 'davies_bouldin': 0.9266798940620147}, {'silhouette': 0.30253902077674866, 'calinski_harabasz': 20.10572498990318, 'davies_bouldin': 1.0297334648107057}, {'silhouette': 0.32465311884880066, 'calinski_harabasz': 21.972161507272002, 'davies_bouldin': 1.0074624753233505}]\n",
      "  tempo: 0.2477777777777778\n",
      "HDBSCAN\n",
      "  resultados: [{'silhouette': 0.37095966935157776, 'calinski_harabasz': 14.476122467596907, 'davies_bouldin': 2.131911413508786}, {'silhouette': 0.3940582573413849, 'calinski_harabasz': 13.439853094558195, 'davies_bouldin': 1.844119223189492}, {'silhouette': 0.2856433093547821, 'calinski_harabasz': 10.0600766552309, 'davies_bouldin': 2.154455544105766}, {'silhouette': 0.3385534882545471, 'calinski_harabasz': 14.448823871696282, 'davies_bouldin': 1.708809812948864}, {'silhouette': 0.34785565733909607, 'calinski_harabasz': 14.20257602668761, 'davies_bouldin': 1.841475600897784}]\n",
      "  tempo: 0.2477777777777778\n",
      "all-MiniLM-L6-v2\n",
      "KMeans\n",
      "  resultados: [{'silhouette': 0.40742844343185425, 'calinski_harabasz': 10.083541708768285, 'davies_bouldin': 1.8962450510320514}, {'silhouette': 0.3900285065174103, 'calinski_harabasz': 7.9153934236691565, 'davies_bouldin': 2.0941442567649}, {'silhouette': 0.3989250361919403, 'calinski_harabasz': 9.88571408508449, 'davies_bouldin': 1.7120384469324381}, {'silhouette': 0.5342093110084534, 'calinski_harabasz': 13.146917533712026, 'davies_bouldin': 1.6289943604784123}, {'silhouette': 0.5624474287033081, 'calinski_harabasz': 14.560852036566493, 'davies_bouldin': 1.6001611376125635}]\n",
      "  tempo: 0.24722222222222223\n",
      "DBSCAN\n",
      "  resultados: [{'silhouette': 0.42321425676345825, 'calinski_harabasz': 18.739972846650765, 'davies_bouldin': 1.0228032258637227}, {'silhouette': 0.4212992787361145, 'calinski_harabasz': 14.762061622930094, 'davies_bouldin': 0.9930454890427873}, {'silhouette': 0.3114766776561737, 'calinski_harabasz': 21.005850396556298, 'davies_bouldin': 1.0056783703880854}, {'silhouette': 0.3547419309616089, 'calinski_harabasz': 15.542260586038614, 'davies_bouldin': 1.0142648790919597}, {'silhouette': 0.3908029794692993, 'calinski_harabasz': 26.574226096006964, 'davies_bouldin': 0.9915191930478545}]\n",
      "  tempo: 0.24722222222222223\n",
      "HDBSCAN\n",
      "  resultados: [{'silhouette': 0.43424659967422485, 'calinski_harabasz': 17.60729166291403, 'davies_bouldin': 1.7007143369360418}, {'silhouette': 0.3653307259082794, 'calinski_harabasz': 12.569676406882461, 'davies_bouldin': 2.2929243171147244}, {'silhouette': 0.4512835741043091, 'calinski_harabasz': 13.191011376063166, 'davies_bouldin': 1.3725828959393955}, {'silhouette': 0.297191321849823, 'calinski_harabasz': 12.371329549787596, 'davies_bouldin': 1.6315374362826154}, {'silhouette': 0.4132500886917114, 'calinski_harabasz': 15.641843470738424, 'davies_bouldin': 1.7013967778928876}]\n",
      "  tempo: 0.24722222222222223\n"
     ]
    }
   ],
   "source": [
    "## Instanciar a classe para manipular modelos da biblioteca SenteceTransformer\n",
    "from gml_embeddings_analyser import EmbeddingsMulticriteriaAnalysis\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Definir os nomes de modelo do SentenceTransformer a serem comparados\n",
    "model_names = [\n",
    "    'sentence-transformers/gtr-t5-large',\n",
    "    'distiluse-base-multilingual-cased-v2',\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'all-mpnet-base-v2',\n",
    "    'all-distilroberta-v1',\n",
    "    'all-MiniLM-L6-v2',\n",
    "]\n",
    "\n",
    "# Criar uma instância da classe EmbeddingsMulticriteriaAnalysis\n",
    "analisador = EmbeddingsMulticriteriaAnalysis(\n",
    "    model_names=model_names,\n",
    "    models= [SentenceTransformer(model_name) for model_name in model_names]\n",
    ")\n",
    "\n",
    "## Carregar análise de clusterização realizada previamente\n",
    "resultados = analisador.load_results()\n",
    "\n",
    "for i,j in resultados.items():\n",
    "    print(i)\n",
    "    for k,l in j.items():\n",
    "        print(k)\n",
    "        for m,n in l.items():\n",
    "            print(f\"  {m}: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence-transformers/gtr-t5-large': {'KMeans': 90.0209,\n",
       "  'DBSCAN': 90.0103,\n",
       "  'HDBSCAN': 89.9625},\n",
       " 'distiluse-base-multilingual-cased-v2': {'KMeans': 89.6231,\n",
       "  'DBSCAN': 89.6136,\n",
       "  'HDBSCAN': 89.5454},\n",
       " 'paraphrase-multilingual-MiniLM-L12-v2': {'KMeans': 89.6171,\n",
       "  'DBSCAN': 89.6049,\n",
       "  'HDBSCAN': 89.5456},\n",
       " 'all-mpnet-base-v2': {'KMeans': 89.7225,\n",
       "  'DBSCAN': 89.7131,\n",
       "  'HDBSCAN': 89.6677},\n",
       " 'all-distilroberta-v1': {'KMeans': 89.72,\n",
       "  'DBSCAN': 89.7086,\n",
       "  'HDBSCAN': 89.5884},\n",
       " 'all-MiniLM-L6-v2': {'KMeans': 89.5188, 'DBSCAN': 89.507, 'HDBSCAN': 89.4305}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define os pesos para cada métrica (ajuste os valores conforme necessário)\n",
    "pesos = {\n",
    "    \"silhouette\": 0.3,\n",
    "    \"calinski_harabasz\": 0.3,\n",
    "    \"davies_bouldin\": 0.3,\n",
    "    \"tempo\": 0.1\n",
    "}\n",
    "\n",
    "pontuacoes = analisador.calcular_pontuacao_multicriterio(resultados, pesos)\n",
    "pontuacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pontuações médias ponderadas por cada modelo e algoritmo\n",
      "  sentence-transformers/gtr-t5-large: 89.9979\n",
      "  distiluse-base-multilingual-cased-v2: 89.5940\n",
      "  paraphrase-multilingual-MiniLM-L12-v2: 89.5892\n",
      "  all-mpnet-base-v2: 89.7011\n",
      "  all-distilroberta-v1: 89.6723\n",
      "  all-MiniLM-L6-v2: 89.4854\n",
      "\n",
      "O melhor modelo nas pontuações ponderadas médias foi: sentence-transformers/gtr-t5-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          0.4725773215293884
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          0.487475597858429
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          0.44722676277160645
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          0.48323217034339905
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          0.46656057238578796
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          0.4586077451705933
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.4020724594593048
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.423944365978241
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.36616516709327696
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.4208125054836273
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.3905233323574066
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.3803070247173309
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.3825436234474182
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.4234595775604248
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.36921250224113467
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.420502507686615
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.3474140763282776
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.39226046204566956
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          10.56997117719537
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          13.058189626034325
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          12.756684049855508
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          12.241256730514399
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          11.380844306191346
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          11.11848375756009
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          18.51976465155434
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          21.52940758931846
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          18.92373942494435
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          18.11440415720141
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          21.456704982705183
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          19.32487430963655
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          15.751610397481636
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          16.64308143411073
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          15.154682025615234
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          15.375682864153461
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          13.32549042315398
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          14.276230493277136
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          1.8911128374510515
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          1.5100503182470288
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          1.260995576965088
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          1.7805502745282005
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          1.6540787902330458
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          1.786316650564073
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.0738012622503657
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.0483634779431223
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.0341068389684012
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          0.9685988818548367
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          0.9929309280709082
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.005462231486882
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.7688623212398777
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.9034597345729014
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          2.40484267351154
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.8699860793718486
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.9361543189301382
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.7398311528331327
         ],
         "yaxis": "y3"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "silhouette",
          "x": 0.14444444444444446,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "calinski_harabasz",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "davies_bouldin",
          "x": 0.8555555555555556,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "arrowhead": 4,
          "ax": 0,
          "ay": 30,
          "showarrow": true,
          "text": "Maior Melhor",
          "x": 0.23,
          "xref": "paper",
          "y": 1,
          "yref": "paper"
         },
         {
          "arrowhead": 4,
          "ax": 0,
          "ay": 30,
          "showarrow": true,
          "text": "Maior Melhor",
          "x": 0.59,
          "xref": "paper",
          "y": 1,
          "yref": "paper"
         },
         {
          "arrowhead": 4,
          "ax": 0,
          "ay": -30,
          "showarrow": true,
          "text": "Menor Melhor",
          "x": 0.77,
          "xref": "paper",
          "y": 0.9,
          "yref": "paper"
         }
        ],
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Comparação da qualidade de clustering com embeedings gerados em cada um dos modelos"
        },
        "width": 1200,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.2888888888888889
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.35555555555555557,
          0.6444444444444445
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.7111111111111111,
          1
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          "0.473 ± 0.076"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          "0.487 ± 0.067"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          "0.447 ± 0.098"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          "0.483 ± 0.058"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          "0.467 ± 0.063"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          "0.459 ± 0.074"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.402 ± 0.074"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.424 ± 0.057"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.366 ± 0.023"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.421 ± 0.061"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.391 ± 0.075"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.380 ± 0.042"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.383 ± 0.069"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.423 ± 0.039"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.369 ± 0.079"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.421 ± 0.035"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.347 ± 0.036"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.392 ± 0.056"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          "10.570 ± 2.442"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          "13.058 ± 2.690"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          "12.757 ± 3.284"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          "12.241 ± 2.603"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          "11.381 ± 2.403"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          "11.118 ± 2.401"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "18.520 ± 4.103"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "21.529 ± 6.821"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "18.924 ± 3.815"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "18.114 ± 2.441"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "21.457 ± 5.359"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "19.325 ± 4.263"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "15.752 ± 4.690"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "16.643 ± 2.165"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "15.155 ± 3.572"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "15.376 ± 2.428"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "13.325 ± 1.675"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "14.276 ± 2.034"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          "1.891 ± 0.193"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          "1.510 ± 0.187"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          "1.261 ± 0.197"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          "1.781 ± 0.110"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          "1.654 ± 0.127"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          "1.786 ± 0.185"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.074 ± 0.195"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.048 ± 0.208"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.034 ± 0.193"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "0.969 ± 0.014"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "0.993 ± 0.035"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.005 ± 0.012"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.769 ± 0.395"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.903 ± 0.249"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "2.405 ± 0.255"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.870 ± 0.214"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.936 ± 0.176"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.740 ± 0.302"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Silhouette",
          "x": 0.06875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Calinski-Harabasz",
          "x": 0.35624999999999996,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Davies-Bouldin",
          "x": 0.6437499999999999,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Tempo (s)",
          "x": 0.93125,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Comparação da qualidade de clustering com embeddings gerados em cada um dos modelos"
        },
        "width": 1600,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.1375
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.2875,
          0.425
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.575,
          0.7124999999999999
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.8625,
          1
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Silhouette"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Calinski-Harabasz"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Davies-Bouldin"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Tempo (s)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Escolher o melhor modelo\n",
    "melhor_modelo = analisador.escolher_melhor_modelo(resultados)\n",
    "print(f\"\\nO melhor modelo nas pontuações ponderadas médias foi: {melhor_modelo}\")\n",
    "\n",
    "# Gerar o relatório de benchmarking (opcional)\n",
    "analisador.generate_report(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar relatório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 21:52:48,959 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:52:48,959 - INFO - Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large\n",
      "2024-11-12 21:52:51,456 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:52:51,457 - INFO - Load pretrained SentenceTransformer: distiluse-base-multilingual-cased-v2\n",
      "2024-11-12 21:52:53,521 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:52:53,522 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2024-11-12 21:52:56,193 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:52:56,193 - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "2024-11-12 21:52:57,938 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:52:57,939 - INFO - Load pretrained SentenceTransformer: all-distilroberta-v1\n",
      "2024-11-12 21:53:00,002 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:53:00,003 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/gtr-t5-large\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--gtr-t5-large\n",
      "Espaço memória necessária: 646.96 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
      "  (3): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/distiluse-base-multilingual-cased-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--distiluse-base-multilingual-cased-v2\n",
      "Espaço memória necessária: 519.81 MB\n",
      "Número de características: 512\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2\n",
      "Espaço memória necessária: 457.51 MB\n",
      "Número de características: 384\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-mpnet-base-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-mpnet-base-v2\n",
      "Espaço memória necessária: 418.36 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-distilroberta-v1\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-distilroberta-v1\n",
      "Espaço memória necessária: 315.77 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-MiniLM-L6-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-MiniLM-L6-v2\n",
      "Espaço memória necessária: 87.34 MB\n",
      "Número de características: 384\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Templates na pasta: ['recommendations_report.html', 'benchmark_report.html', 'template_fioce.html', 'template_fioce_cnpq.html']\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          0.4725773215293884
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          0.487475597858429
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          0.44722676277160645
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          0.48323217034339905
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          0.46656057238578796
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          0.4586077451705933
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.4020724594593048
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.423944365978241
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.36616516709327696
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.4208125054836273
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.3905233323574066
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.3803070247173309
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.3825436234474182
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.4234595775604248
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.36921250224113467
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.420502507686615
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.3474140763282776
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          0.39226046204566956
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          10.56997117719537
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          13.058189626034325
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          12.756684049855508
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          12.241256730514399
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          11.380844306191346
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          11.11848375756009
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          18.51976465155434
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          21.52940758931846
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          18.92373942494435
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          18.11440415720141
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          21.456704982705183
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          19.32487430963655
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          15.751610397481636
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          16.64308143411073
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          15.154682025615234
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          15.375682864153461
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          13.32549042315398
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          14.276230493277136
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          1.8911128374510515
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          1.5100503182470288
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          1.260995576965088
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          1.7805502745282005
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          1.6540787902330458
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          1.786316650564073
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.0738012622503657
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.0483634779431223
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.0341068389684012
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          0.9685988818548367
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          0.9929309280709082
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.005462231486882
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.7688623212398777
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.9034597345729014
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          2.40484267351154
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.8699860793718486
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.9361543189301382
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          1.7398311528331327
         ],
         "yaxis": "y3"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "silhouette",
          "x": 0.14444444444444446,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "calinski_harabasz",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "davies_bouldin",
          "x": 0.8555555555555556,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "arrowhead": 4,
          "ax": 0,
          "ay": 30,
          "showarrow": true,
          "text": "Maior Melhor",
          "x": 0.23,
          "xref": "paper",
          "y": 1,
          "yref": "paper"
         },
         {
          "arrowhead": 4,
          "ax": 0,
          "ay": 30,
          "showarrow": true,
          "text": "Maior Melhor",
          "x": 0.59,
          "xref": "paper",
          "y": 1,
          "yref": "paper"
         },
         {
          "arrowhead": 4,
          "ax": 0,
          "ay": -30,
          "showarrow": true,
          "text": "Menor Melhor",
          "x": 0.77,
          "xref": "paper",
          "y": 0.9,
          "yref": "paper"
         }
        ],
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Comparação da qualidade de clustering com embeedings gerados em cada um dos modelos"
        },
        "width": 1200,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.2888888888888889
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.35555555555555557,
          0.6444444444444445
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.7111111111111111,
          1
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          "0.473 ± 0.076"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          "0.487 ± 0.067"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          "0.447 ± 0.098"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          "0.483 ± 0.058"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          "0.467 ± 0.063"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": true,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x",
         "y": [
          "0.459 ± 0.074"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.402 ± 0.074"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.424 ± 0.057"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.366 ± 0.023"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.421 ± 0.061"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.391 ± 0.075"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.380 ± 0.042"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.383 ± 0.069"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.423 ± 0.039"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.369 ± 0.079"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.421 ± 0.035"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.347 ± 0.036"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x",
         "y": [
          "0.392 ± 0.056"
         ],
         "yaxis": "y"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          "10.570 ± 2.442"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          "13.058 ± 2.690"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          "12.757 ± 3.284"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          "12.241 ± 2.603"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          "11.381 ± 2.403"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x2",
         "y": [
          "11.118 ± 2.401"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "18.520 ± 4.103"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "21.529 ± 6.821"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "18.924 ± 3.815"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "18.114 ± 2.441"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "21.457 ± 5.359"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "19.325 ± 4.263"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "15.752 ± 4.690"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "16.643 ± 2.165"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "15.155 ± 3.572"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "15.376 ± 2.428"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "13.325 ± 1.675"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x2",
         "y": [
          "14.276 ± 2.034"
         ],
         "yaxis": "y2"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          "1.891 ± 0.193"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          "1.510 ± 0.187"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          "1.261 ± 0.197"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          "1.781 ± 0.110"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          "1.654 ± 0.127"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x3",
         "y": [
          "1.786 ± 0.185"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.074 ± 0.195"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.048 ± 0.208"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.034 ± 0.193"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "0.969 ± 0.014"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "0.993 ± 0.035"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.005 ± 0.012"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.769 ± 0.395"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.903 ± 0.249"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "2.405 ± 0.255"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.870 ± 0.214"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.936 ± 0.176"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x3",
         "y": [
          "1.740 ± 0.302"
         ],
         "yaxis": "y3"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "KMeans"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "DBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "sentence-transformers/gtr-t5-large",
         "marker": {
          "color": "blue"
         },
         "name": "sentence-transformers/gtr-t5-large",
         "offsetgroup": "0",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "distiluse-base-multilingual-cased-v2",
         "marker": {
          "color": "green"
         },
         "name": "distiluse-base-multilingual-cased-v2",
         "offsetgroup": "1",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "paraphrase-multilingual-MiniLM-L12-v2",
         "marker": {
          "color": "yellow"
         },
         "name": "paraphrase-multilingual-MiniLM-L12-v2",
         "offsetgroup": "2",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-mpnet-base-v2",
         "marker": {
          "color": "cyan"
         },
         "name": "all-mpnet-base-v2",
         "offsetgroup": "3",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-distilroberta-v1",
         "marker": {
          "color": "orange"
         },
         "name": "all-distilroberta-v1",
         "offsetgroup": "4",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        },
        {
         "legendgroup": "all-MiniLM-L6-v2",
         "marker": {
          "color": "purple"
         },
         "name": "all-MiniLM-L6-v2",
         "offsetgroup": "5",
         "showlegend": false,
         "type": "bar",
         "x": [
          "HDBSCAN"
         ],
         "xaxis": "x4",
         "y": [
          "0.25"
         ],
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Silhouette",
          "x": 0.06875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Calinski-Harabasz",
          "x": 0.35624999999999996,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Davies-Bouldin",
          "x": 0.6437499999999999,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Tempo (s)",
          "x": 0.93125,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Comparação da qualidade de clustering com embeddings gerados em cada um dos modelos"
        },
        "width": 1600,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.1375
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.2875,
          0.425
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.575,
          0.7124999999999999
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.8625,
          1
         ],
         "title": {
          "text": "Algoritmo"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Silhouette"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Calinski-Harabasz"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Davies-Bouldin"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Tempo (s)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Instanciar a classe para manipular modelos da biblioteca SenteceTransformer\n",
    "import os\n",
    "from git import Repo\n",
    "from gml_embeddings_analyser import EmbeddingsMulticriteriaAnalysis\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Definir os nomes de modelo do SentenceTransformer a serem comparados\n",
    "model_names = [\n",
    "    'sentence-transformers/gtr-t5-large',\n",
    "    'distiluse-base-multilingual-cased-v2',\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'all-mpnet-base-v2',\n",
    "    'all-distilroberta-v1',\n",
    "    'all-MiniLM-L6-v2',\n",
    "]\n",
    "\n",
    "# Criar uma instância da classe EmbeddingsMulticriteriaAnalysis\n",
    "analisador = EmbeddingsMulticriteriaAnalysis(\n",
    "    model_names=model_names,\n",
    "    models= [SentenceTransformer(model_name) for model_name in model_names]\n",
    ")\n",
    "\n",
    "try:\n",
    "    repo = Repo(search_parent_directories=True)\n",
    "    root_folder = repo.working_tree_dir\n",
    "    template_folder = os.path.join(str(root_folder),\"source/template/\")\n",
    "    print(f\"Templates na pasta: {os.listdir(template_folder)}\")\n",
    "\n",
    "    # Gerar o relatório HTML\n",
    "    analisador.generate_report(resultados)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar relatório: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from benchmarking_reports import BenchmarkReportGenerator\n",
    "# try:\n",
    "#     # Criar uma instância do BenchmarkReportGenerator\n",
    "#     report_generator = BenchmarkReportGenerator(\"benchmark_report.html\") \n",
    "\n",
    "#     # Gerar o relatório HTML\n",
    "#     report_generator.generate_beckmarking_clustering_report(resultados, melhor_modelo)\n",
    "# except Exception as e:\n",
    "#     print(f\"Erro ao gerar relatório: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>1b. Carregar resultados salvos e gerar relatórios</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciar a classe e modelos de SenteceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 21:53:24,678 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:53:24,678 - INFO - Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large\n",
      "2024-11-12 21:53:27,143 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:53:27,144 - INFO - Load pretrained SentenceTransformer: distiluse-base-multilingual-cased-v2\n",
      "2024-11-12 21:53:29,164 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:53:29,164 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2024-11-12 21:53:31,885 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:53:31,885 - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "2024-11-12 21:53:33,637 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:53:33,637 - INFO - Load pretrained SentenceTransformer: all-distilroberta-v1\n",
      "2024-11-12 21:53:35,583 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 21:53:35,584 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/gtr-t5-large\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--gtr-t5-large\n",
      "Espaço memória necessária: 646.96 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
      "  (3): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/distiluse-base-multilingual-cased-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--distiluse-base-multilingual-cased-v2\n",
      "Espaço memória necessária: 519.81 MB\n",
      "Número de características: 512\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2\n",
      "Espaço memória necessária: 457.51 MB\n",
      "Número de características: 384\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-mpnet-base-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-mpnet-base-v2\n",
      "Espaço memória necessária: 418.36 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-distilroberta-v1\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-distilroberta-v1\n",
      "Espaço memória necessária: 315.77 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-MiniLM-L6-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-MiniLM-L6-v2\n",
      "Espaço memória necessária: 87.34 MB\n",
      "Número de características: 384\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from gml_embeddings_analyser import EmbeddingsMulticriteriaAnalysis\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Definir os nomes de modelo do SentenceTransformer a serem comparados\n",
    "model_names = [\n",
    "    'sentence-transformers/gtr-t5-large',\n",
    "    'distiluse-base-multilingual-cased-v2',\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'all-mpnet-base-v2',\n",
    "    'all-distilroberta-v1',\n",
    "    'all-MiniLM-L6-v2',\n",
    "]\n",
    "\n",
    "# Criar uma instância da classe EmbeddingsMulticriteriaAnalysis\n",
    "analisador = EmbeddingsMulticriteriaAnalysis(\n",
    "    model_names=model_names,\n",
    "    models= [SentenceTransformer(model_name) for model_name in model_names]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo ponderado para escolher modelo de embeeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pontuações médias ponderadas por cada modelo e algoritmo\n",
      "  sentence-transformers/gtr-t5-large: 89.9979\n",
      "  distiluse-base-multilingual-cased-v2: 89.5940\n",
      "  paraphrase-multilingual-MiniLM-L12-v2: 89.5892\n",
      "  all-mpnet-base-v2: 89.7011\n",
      "  all-distilroberta-v1: 89.6723\n",
      "  all-MiniLM-L6-v2: 89.4854\n",
      "\n",
      "O melhor modelo nas pontuações ponderadas médias foi: sentence-transformers/gtr-t5-large\n"
     ]
    }
   ],
   "source": [
    "## Carregar análise de clusterização realizada previamente\n",
    "resultados = analisador.load_results()\n",
    "\n",
    "# Escolher o melhor modelo\n",
    "melhor_modelo = analisador.escolher_melhor_modelo(resultados)\n",
    "print(f\"\\nO melhor modelo nas pontuações ponderadas médias foi: {melhor_modelo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar embeedings de editais gerados previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo de embeddings carregado: embeddings_clustering_funding.pt\n",
      "Número de modelos carregados: 6\n",
      "Dimensões do tensor: torch.Size([346, 768])\n"
     ]
    }
   ],
   "source": [
    "## Carregar os embeddings previamente gerados (em caso de erro na avaliação a seguir)\n",
    "embeddings_dict = analisador.load_embeddings_dict(\"embeddings_clustering_funding.pt\")\n",
    "\n",
    "# Acessar o tensor de embeddings para o modelo avaliado como melhor previamente\n",
    "best_embeddings_tensor = embeddings_dict.get(melhor_modelo) # type: ignore\n",
    "\n",
    "# Exibir as dimensões do tensor\n",
    "if best_embeddings_tensor is not None:\n",
    "    print(\"Dimensões do tensor:\", best_embeddings_tensor.shape)\n",
    "else:\n",
    "    print(\"Modelo não encontrado no dicionário de embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar dados dos currículos extraídos previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de currículos carregados: 40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from git import Repo\n",
    "\n",
    "# Acessar o tensor de embeddings para o modelo 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "editais_embeddings = embeddings_dict.get(melhor_modelo) # type: ignore\n",
    "\n",
    "# Carregar os currículos\n",
    "filename = \"input_curriculos.json\"\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "pathfilename = os.path.join(str(root_folder), \"_data\", \"out_json\",filename)\n",
    "with open(pathfilename, \"r\", encoding='utf-8') as f:\n",
    "    curriculos = json.load(f)\n",
    "\n",
    "\n",
    "# Exibir as dimensões do tensor\n",
    "if curriculos is not None:\n",
    "    print(\"Quantidade de currículos carregados:\", len(curriculos))\n",
    "else:\n",
    "    print(\"Arquivo de currículos não encontrado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traduzir títulos para inglês para embeedings de artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando 40 currículos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteração atual:   0%|          | 0/40 [01:11<?, ?it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 2/2 [00:00<00:00,  6.34it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 9/9 [00:02<00:00,  3.52it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 2/2 [00:00<00:00, 455.04it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 25/25 [00:04<00:00,  5.18it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 10/10 [00:01<00:00,  9.47it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 2/2 [00:00<00:00,  6.41it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 106/106 [00:12<00:00,  8.21it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 49/49 [00:01<00:00, 35.95it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 7/7 [00:00<00:00, 483.99it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 45/45 [00:00<00:00, 65.93it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 10/10 [00:00<00:00, 10.24it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 45/45 [00:00<00:00, 448.04it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 6/6 [00:00<00:00, 455.04it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 64/64 [00:01<00:00, 39.61it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 23/23 [00:00<00:00, 479.76it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 69/69 [00:19<00:00,  3.47it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 11/11 [00:02<00:00,  3.79it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 13/13 [00:00<00:00, 19.46it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 26/26 [00:00<00:00, 378.34it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 74/74 [00:17<00:00,  4.15it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 110/110 [00:05<00:00, 21.56it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 31/31 [00:00<00:00, 74.09it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 45/45 [00:04<00:00,  9.31it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 89/89 [00:23<00:00,  3.81it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 21/21 [00:00<00:00, 385.72it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 28/28 [00:00<00:00, 526.72it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 151/151 [00:23<00:00,  6.54it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 18/18 [00:01<00:00, 11.36it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 7/7 [00:00<00:00, 10.29it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 45/45 [00:09<00:00,  4.57it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 4/4 [00:00<00:00,  6.23it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 42/42 [00:02<00:00, 20.61it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 66/66 [00:01<00:00, 61.32it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 97/97 [00:17<00:00,  5.50it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 55/55 [00:15<00:00,  3.66it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 35/35 [00:10<00:00,  3.34it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 6/6 [00:00<00:00, 16.51it/s]\n",
      "  Traduzindo títulos: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460 artigos com títulos traduzidos para inglês\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install py2neo\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from json import JSONDecodeError\n",
    "from git import Repo\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "\n",
    "# Instanciar o tradutor\n",
    "tradutor = GoogleTranslator(source='pt', target='en')\n",
    "\n",
    "## Carregar os currículos\n",
    "filename = \"input_curriculos.json\"\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "pathfilename = os.path.join(str(root_folder),\"_data\",\"out_json\",filename)\n",
    "with open(pathfilename, \"r\", encoding='utf-8') as f:\n",
    "    curriculos = json.load(f)\n",
    "\n",
    "# Extrair os títulos dos artigos publicados, traduzir para inglês (quando necessário) \n",
    "# e gerar embeddings em lotes, incluindo tratamento de erros e títulos vazios\n",
    "artigos_titulos = []\n",
    "\n",
    "# Criar uma barra de progresso manual para o loop externo\n",
    "print(f\"Processando {len(curriculos)} currículos...\")\n",
    "pbar_curriculos = tqdm(total=len(curriculos), desc=f\"Iteração atual\")\n",
    "\n",
    "titulos_en = []\n",
    "for curriculo in curriculos:\n",
    "    artigos = curriculo.get('Produções', {}).get('Artigos completos publicados em periódicos', [])\n",
    "    # print(f\"{' '*48}{len(artigos):02} artigos a processar\")\n",
    "    for artigo in tqdm(artigos, desc=\"  Traduzindo títulos\", leave=True):\n",
    "        titulo = artigo.get('titulo')\n",
    "        if titulo:\n",
    "            try:\n",
    "                idioma = detect(titulo)\n",
    "                if idioma != 'en':\n",
    "                    # print(f\"Detectado idioma: {idioma} | Título: {titulo}\")\n",
    "                    titulo_en = tradutor.translate(titulo)\n",
    "                    # print(f\"              Título traduzido: {titulo_en}\")\n",
    "            except (JSONDecodeError, Exception) as e:  # Capturar todas as exceções\n",
    "                print(f\"Erro ao processar título: {e}\")\n",
    "                titulo_en = titulo\n",
    "                # Opções para tratar o erro:\n",
    "                # 1. Usar o título original: titulo = titulo\n",
    "                # 2. Usar um título vazio: titulo = \"\"\n",
    "                # 3. Ignorar o título: continue\n",
    "                continue  # Ignorar o título com erro\n",
    "        else:\n",
    "            titulo_en = \"\"  # Usar um título vazio se o título original for None\n",
    "\n",
    "        titulos_en.append(titulo_en)  # Adicionar o título (traduzido ou original) à lista\n",
    "        pbar_curriculos.update(1)  # Atualizar a barra de progresso externa\n",
    "\n",
    "print(f\"{len(titulos_en)} artigos com títulos traduzidos para inglês\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerar os embeedings de artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 22:00:49,143 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 22:00:49,144 - INFO - Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large\n",
      "Gerando embeddings: 100%|██████████| 1460/1460 [00:00<00:00, 5419189.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 currículos processados com total de 1460 embeddings de artigos gerados.\n",
      "Dimensões do primeiro embedding: torch.Size([768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instanciar o modelo de linguagem (use o mesmo modelo usado para os editais)\n",
    "modelo = SentenceTransformer(melhor_modelo)\n",
    "\n",
    "# Gerar embeddings para os títulos em inglês em lote\n",
    "artigos_embeddings = []  # Lista para armazenar os embeddings de cada artigo\n",
    "embedding = modelo.encode(titulo, convert_to_tensor=True, show_progress_bar=False)\n",
    "if titulos_en:\n",
    "    for titulo in tqdm(titulos_en, desc=\"Gerando embeddings\"):  # Adiciona tqdm aqui\n",
    "        try:\n",
    "            artigos_embeddings.append(embedding)  # Adiciona o embedding do artigo à lista\n",
    "        except Exception as e:\n",
    "            print(f\"  Erro ao gerar embeddings: {e}\")\n",
    "            print(f\"  Título com problema: {titulo}\")\n",
    "\n",
    "# Imprimir informações sobre os embeddings gerados\n",
    "if artigos_embeddings:\n",
    "    print(f\"{len(curriculos)} currículos processados com total de {len(artigos_embeddings)} embeddings de artigos gerados.\")\n",
    "    # Imprimir as dimensões do primeiro embedding como exemplo\n",
    "    print(\"Dimensões do primeiro embedding:\", artigos_embeddings[0].shape)  \n",
    "else:\n",
    "    print(\"Nenhum embedding de artigo foi gerado.\")\n",
    "\n",
    "# ## Caso se deseje concatenar os embeddings em um único tensor\n",
    "# if artigos_embeddings:\n",
    "#     artigos_embeddings = torch.cat(artigos_embeddings, dim=0)\n",
    "#     print(f\"{len(curriculos)} processados com total de {len(artigos_embeddings)} embeedings de artigos gerados.\")\n",
    "#     print(\"Dimensões do tensor:\", artigos_embeddings.shape)\n",
    "# else:\n",
    "#     print(\"Nenhum embedding de artigo foi gerado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>2. Montar Camada Pesquisa (Artigos - Fomento)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar a análise de similaridades e identificação de comunidades como uma primeira camada no seu KGNN, os seguintes passos foram executados:\n",
    "\n",
    "1. Calculadas as similaridades entre editais e artigos:\n",
    "\n",
    "Foram utilizadas medidas de similaridade por cosseno para calcular a similaridade entre os embeddings de cada edital e cada artigo. As distâncias de similaridade foram armazenadas em uma matriz, onde as linhas representam os editais e as colunas representam os artigos.\n",
    "\n",
    "2. Foram identificadas as comunidades de editais e artigos:\n",
    "\n",
    "Foi aplicado algoritmo de agrupamento (clustering), como k-means ou DBSCAN, na matriz de similaridades para identificar grupos (comunidades) de editais e artigos que são similares entre si.\n",
    "\n",
    "Foram armazenadas as informações sobre as comunidades, como os IDs dos editais e artigos que pertencem a cada comunidade e a similaridade média dentro de cada comunidade.\n",
    "\n",
    "3. Foi criada a camada de embedding de comunidade:\n",
    "\n",
    "Criado um novo embedding para cada comunidade, que representa as características da comunidade como um todo, utilizando a média dos embeddings dos editais e artigos que pertencem à mesma comunidade como embedding da comunidade.\n",
    "\n",
    "4. Foram conectadas as entidades às comunidades no grafo:\n",
    "\n",
    "Adicionadas comunidades como novos nós no seu grafo de conhecimento.\n",
    "Criadas arestas entre os editais e artigos e suas respectivas comunidades.\n",
    "Utilizados peso nas arestas que representam a similaridade entre a entidade e a comunidade (similaridade do cosseno entre o embedding da entidade e o embedding da comunidade).\n",
    "\n",
    "5. Foi integrada a camada de comunidade no KGNN:\n",
    "\n",
    "Foram utilizados os embeddings das comunidades como entrada para a próxima camada do KGNN. O KGNN usa as informações sobre as comunidades para aprender representações mais ricas e informativas das entidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistir em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 22:01:07,802 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 22:01:07,802 - INFO - Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large\n",
      "2024-11-12 22:01:10,340 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 22:01:10,341 - INFO - Load pretrained SentenceTransformer: distiluse-base-multilingual-cased-v2\n",
      "2024-11-12 22:01:12,429 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 22:01:12,430 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2024-11-12 22:01:14,981 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 22:01:14,982 - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "2024-11-12 22:01:16,720 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 22:01:16,720 - INFO - Load pretrained SentenceTransformer: all-distilroberta-v1\n",
      "2024-11-12 22:01:18,468 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 22:01:18,468 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2024-11-12 22:01:26,825 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 22:01:26,825 - INFO - Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/gtr-t5-large\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--gtr-t5-large\n",
      "Espaço memória necessária: 646.96 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
      "  (3): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/distiluse-base-multilingual-cased-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--distiluse-base-multilingual-cased-v2\n",
      "Espaço memória necessária: 519.81 MB\n",
      "Número de características: 512\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2\n",
      "Espaço memória necessária: 457.51 MB\n",
      "Número de características: 384\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-mpnet-base-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-mpnet-base-v2\n",
      "Espaço memória necessária: 418.36 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-distilroberta-v1\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-distilroberta-v1\n",
      "Espaço memória necessária: 315.77 MB\n",
      "Número de características: 768\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Nome modelo base original: sentence-transformers/all-MiniLM-L6-v2\n",
      "Treinado nas tarefas para: semantic textual similarity, semantic search, paraphrase mining, text classification, clustering\n",
      "Pasta local para o modelo: models--sentence-transformers--all-MiniLM-L6-v2\n",
      "Espaço memória necessária: 87.34 MB\n",
      "Número de características: 384\n",
      "Detalhe módulos do modelo: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Pontuações médias ponderadas por cada modelo e algoritmo\n",
      "  sentence-transformers/gtr-t5-large: 89.9979\n",
      "  distiluse-base-multilingual-cased-v2: 89.5940\n",
      "  paraphrase-multilingual-MiniLM-L12-v2: 89.5892\n",
      "  all-mpnet-base-v2: 89.7011\n",
      "  all-distilroberta-v1: 89.6723\n",
      "  all-MiniLM-L6-v2: 89.4854\n",
      "Modelo escolhido para gerar embeedings: sentence-transformers/gtr-t5-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 22:01:29,394 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-12 22:01:29,394 - INFO - Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Statement.SyntaxError] Failed to parse string literal. The query must contain an even number of non-escaped quotes. (line 2, column 50 (offset: 50))\n",
      "\"                MERGE (n:Publicacao { descricao: \"Publicação: 2011 | 005.16 | Structure And Function Of A \\\" })\"\n",
      "                                                  ^\n"
     ]
    }
   ],
   "source": [
    "# !pip install py2neo\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from googletrans import Translator\n",
    "from json import JSONDecodeError\n",
    "from git import Repo\n",
    "\n",
    "## Instanciar a classe para manipular modelos da biblioteca SenteceTransformer\n",
    "from gml_embeddings_analyser import EmbeddingsMulticriteriaAnalysis\n",
    "from kgnn import KGNN\n",
    "\n",
    "## Definir os nomes de modelo do SentenceTransformer a serem comparados\n",
    "model_names = [\n",
    "    'sentence-transformers/gtr-t5-large',\n",
    "    'distiluse-base-multilingual-cased-v2',\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'all-mpnet-base-v2',\n",
    "    'all-distilroberta-v1',\n",
    "    'all-MiniLM-L6-v2',\n",
    "]\n",
    "\n",
    "## Criar uma instância da classe EmbeddingsMulticriteriaAnalysis com melhor modelo\n",
    "analisador = EmbeddingsMulticriteriaAnalysis(\n",
    "    model_names=model_names,\n",
    "    models= [SentenceTransformer(model_name) for model_name in model_names]\n",
    ")\n",
    "\n",
    "## Carregar análise de clusterização realizada previamente\n",
    "resultados = analisador.load_results()\n",
    "\n",
    "## Escolher o melhor modelo\n",
    "melhor_modelo = analisador.escolher_melhor_modelo(resultados)\n",
    "print(f\"Modelo escolhido para gerar embeedings: {melhor_modelo}\")\n",
    "\n",
    "\n",
    "## Carregar os currículos\n",
    "filename = \"input_curriculos.json\"\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "pathfilename = os.path.join(str(root_folder),\"_data\",\"out_json\",filename)\n",
    "\n",
    "with open(pathfilename, \"r\", encoding='utf-8') as f:\n",
    "    curriculos = json.load(f)\n",
    "\n",
    "## Criar uma instância do KGNN\n",
    "kgnn = KGNN(\n",
    "    embedding_model_name=melhor_modelo,\n",
    "    neo4j_uri=\"bolt://localhost:7687\",\n",
    "    neo4j_user=\"neo4j\",\n",
    "    neo4j_password=\"password\",\n",
    "    curriculae_path=pathfilename\n",
    ")\n",
    "\n",
    "## Criar subgrafos para cada um dos currículos\n",
    "for curriculo in curriculos:\n",
    "    try:\n",
    "        subgrafo = kgnn.criar_subgrafo_curriculo(curriculo)\n",
    "        kgnn.ingerir_subgrafo(subgrafo)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Alinhar competências às necessidades do CEIS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executar tarefas do modelo em Suporte ao PDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "# %conda install pytorch::faiss-gpu\n",
    "# !pip install py-cpuinfo\n",
    "# !pip install xformers\n",
    "# !pip install unidecode\n",
    "\n",
    "# Installing relevant libraries\n",
    "# !pip install sentence_transformers\n",
    "# !pip install datasets\n",
    "# !pip install faiss-gpu\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisar dos dados de fomento - fase exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, cudf\n",
    "from funding_analyser import FundingEmbeddingGenerator\n",
    "\n",
    "df_fomento = pd.DataFrame()\n",
    "\n",
    "# Criar uma instância do EmbeddingGenerator\n",
    "embedding_generator = FundingEmbeddingGenerator()\n",
    "\n",
    "# Criar a coluna 'texto_para_embedding' no dataframe df_fomento usando cuDF (se disponível)\n",
    "try:\n",
    "    df_fomento = embedding_generator.create_embedding_column(use_cudf=True)\n",
    "except:\n",
    "    # Ou, criar a coluna 'texto_para_embedding' sem usar cuDF, usando apenas Pandas\n",
    "    df_fomento = embedding_generator.create_embedding_column(use_cudf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fomento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fomento['detalhes'][0] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Inicializar o modelo de embeddings de texto\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2').to('cuda')\n",
    "\n",
    "# Gerar embeddings para os detalhes dos editais\n",
    "edital_embeddings = model.encode(df_fomento['detalhes'].to_arrow(), convert_to_tensor=True).cpu().numpy() # type: ignore\n",
    "\n",
    "# Vector dimensionality\n",
    "vec_dim = edital_embeddings.shape[1]\n",
    "vec_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um índice Faiss para busca eficiente\n",
    "index = faiss.IndexFlatL2(edital_embeddings.shape[1])\n",
    "index.add(edital_embeddings) # type: ignore\n",
    "\n",
    "def recomendar_fomento(competencias_pesquisador, k=10):\n",
    "    \"\"\"\n",
    "    Recomenda editais de fomento com base nas competências do pesquisador, \n",
    "    utilizando embeddings de texto e busca de similaridade com Faiss.\n",
    "\n",
    "    Args:\n",
    "        competencias_pesquisador (list): Uma lista de strings representando as competências do pesquisador.\n",
    "        k (int): O número de editais mais similares a serem recomendados.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Uma tupla contendo:\n",
    "            - pandas.DataFrame: Os editais de fomento recomendados.\n",
    "            - numpy.ndarray: As distâncias entre as competências do pesquisador e os editais recomendados.\n",
    "            - numpy.ndarray: Os índices dos editais recomendados no índice Faiss.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Gerar embeddings para as competências do pesquisador\n",
    "    competencias_embedding = model.encode(competencias_pesquisador, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # Realizar a busca de similaridade no índice Faiss\n",
    "    D, I = index.search()  # type: ignore # Buscar os k editais mais similares\n",
    "\n",
    "    # Retornar os editais recomendados\n",
    "    return df_fomento.iloc[I[0]].to_pandas(), D, I # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar classes do pacote\n",
    "from research_process_automation import QuestionFormulation, InteractiveFeedback, QuestionCriteriaEvaluator\n",
    "\n",
    "# Criar instância da classe QuestionFormulation\n",
    "question_formulator = QuestionFormulation()\n",
    "\n",
    "# Solicitar informações do usuário\n",
    "question_formulator.input_ideas()\n",
    "\n",
    "# Gerar a pergunta de pesquisa\n",
    "research_question = question_formulator.generate_question()\n",
    "print(\"PASSO 01: Questão de pesquisa\")\n",
    "print(research_question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolução de entidades de pesquisa com Ontologias específicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# class BioPortalAPI:\n",
    "#     def __init__(self, api_key):\n",
    "#         self.api_key = '6e1ec84f-53df-45a3-9115-25d245caefb1'\n",
    "#         self.base_url = \"https://data.bioontology.org/api\"\n",
    "\n",
    "#     def search_entities(self, query, ontologies=None, max_results=10):\n",
    "#         \"\"\"\n",
    "#         Realiza uma busca por entidades no BioPortal.\n",
    "\n",
    "#         Args:\n",
    "#             query: Termo de busca.\n",
    "#             ontologies: Lista de acrônimos de ontologias (opcional).\n",
    "#             max_results: Número máximo de resultados (opcional).\n",
    "\n",
    "#         Returns:\n",
    "#             Lista de dicionários com informações sobre as entidades encontradas.\n",
    "#         \"\"\"\n",
    "#         url = f\"{self.base_url}/search?q={query}&apikey={self.api_key}\"\n",
    "#         if ontologies:\n",
    "#             url += f\"&ontologies={','.join(ontologies)}\"\n",
    "#         if max_results:\n",
    "#             url += f\"&pagesize={max_results}\"\n",
    "\n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code == 200:\n",
    "#             return response.json()[\"collection\"]\n",
    "#         else:\n",
    "#             raise Exception(f\"Erro na requisição: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Listar dados de cada currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import warnings\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "pathfilename = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not os.path.exists(pathfilename):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {pathfilename}\")\n",
    "\n",
    "try:\n",
    "    with open(pathfilename, \"r\") as f:\n",
    "        curricula_data = json.load(f)\n",
    "except json.JSONDecodeError:\n",
    "    raise ValueError(f\"Erro ao decodificar o arquivo JSON: {pathfilename}\")\n",
    "\n",
    "from competence_extraction import CompetenceExtractor\n",
    "competence_extractor = CompetenceExtractor(curriculae_path=pathfilename)\n",
    "\n",
    "# Extrair e imprimir as competências de cada pesquisador\n",
    "for researcher_index, researcher_data in enumerate(curricula_data):\n",
    "    competences = competence_extractor.extract_competences(researcher_data)\n",
    "    processed_competences = competence_extractor.preprocess_competences(competences)\n",
    "    print(f\"\\nCompetências do pesquisador {researcher_index + 1}:\")\n",
    "    for competence in processed_competences:\n",
    "        print(f\"- {competence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Visualizar áreas de pesquisa de cada currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "import os\n",
    "import warnings\n",
    "from git import Repo\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "from tqdm.notebook import tqdm # Importando tqdm do notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def load_curriculae():\n",
    "    with open(curriculae_file, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extrair_areas(areas_dict):\n",
    "    lista_grdareas = []\n",
    "    lista_areas = []\n",
    "    lista_subareas = []\n",
    "    # Expressão regular corrigida para extrair as áreas\n",
    "    pattern = r'Grande área:\\s*(.*?)\\s*/\\s*Área:\\s*(.*?)\\s*(?:/ Subárea:\\s*(.*?)\\s*)?\\.'\n",
    "\n",
    "    for _, valor in areas_dict.items():\n",
    "        match = re.search(pattern, valor)\n",
    "        if match:\n",
    "            areas = {\n",
    "                'Grande Área': match.group(1).strip() if match.group(1) else None , \n",
    "                'Área': match.group(2).strip() if match.group(2) else None ,\n",
    "                'Subárea': match.group(3).strip() if match.group(3) else None  \n",
    "            }\n",
    "            lista_grdareas.append(areas.get('Grande Área'))\n",
    "            lista_areas.append(areas.get('Área'))\n",
    "            lista_subareas.append(areas.get('Subárea'))\n",
    "\n",
    "    return {'Grande Áreas': lista_grdareas, 'Áreas': lista_areas, 'Subáreas': lista_subareas}\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curriculae_file = os.path.join(str(root_folder), filename)\n",
    "\n",
    "curricula_data = load_curriculae()\n",
    "\n",
    "for researcher_data in curricula_data:\n",
    "    areas =  researcher_data.get('Áreas')\n",
    "    print(extrair_areas(areas).get('Áreas'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extrair competências do currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm # Importando tqdm do notebook\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1' ## Descomentar em caso de erros na GPU\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" ## Descomentar em caso de erros na GPU\n",
    "\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "\n",
    "# Função para extrair pares de competências\n",
    "def extract_competence_pairs(curricula_data):\n",
    "    similar_pairs = []\n",
    "    dissimilar_pairs = []\n",
    "\n",
    "    # Exemplo: extrair competências da mesma área (similar)\n",
    "    for i in range(len(curricula_data)):\n",
    "        for j in range(i + 1, len(curricula_data)):\n",
    "            researcher1 = curricula_data[i]\n",
    "            researcher2 = curricula_data[j]\n",
    "            if researcher1.get('Áreas') == researcher2.get('Áreas'):  # Verifica se a área é a mesma\n",
    "                competences1 = competence_extractor.extract_competences(researcher1)\n",
    "                competences2 = competence_extractor.extract_competences(researcher2)\n",
    "                for comp1 in competences1:\n",
    "                    for comp2 in competences2:\n",
    "                        similar_pairs.append((comp1, comp2))\n",
    "\n",
    "    # Exemplo: extrair competências de áreas diferentes (dissimilar)\n",
    "    for i in range(len(curricula_data)):\n",
    "        for j in range(i + 1, len(curricula_data)):\n",
    "            researcher1 = curricula_data[i]\n",
    "            researcher2 = curricula_data[j]\n",
    "            if researcher1.get('Áreas') != researcher2.get('Áreas'):  # Verifica se a área é diferente\n",
    "                competences1 = competence_extractor.extract_competences(researcher1)\n",
    "                competences2 = competence_extractor.extract_competences(researcher2)\n",
    "                for comp1 in competences1:\n",
    "                    for comp2 in competences2:\n",
    "                        dissimilar_pairs.append((comp1, comp2))\n",
    "\n",
    "    return {\n",
    "        'similar': similar_pairs,\n",
    "        'dissimilar': dissimilar_pairs\n",
    "    }\n",
    "\n",
    "# Lista de modelos a serem avaliados\n",
    "model_names = [\n",
    "    'sentence-transformers/gtr-t5-large',\n",
    "    'distiluse-base-multilingual-cased-v2',\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'all-mpnet-base-v2',\n",
    "    'all-distilroberta-v1',\n",
    "    'all-MiniLM-L6-v2',\n",
    "]\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curriculae_file = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# Criar o avaliador e comparar os modelos\n",
    "evaluator = EmbeddingModelEvaluator(curriculae_file, model_names)\n",
    "\n",
    "# Extrair os pares de competências dos currículos\n",
    "validation_data = extract_competence_pairs(evaluator.curricula_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_data.keys())\n",
    "print(len(validation_data.get('similar')))\n",
    "print(len(validation_data.get('dissimilar')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.get('dissimilar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar por algoritmos tradicionais\n",
    "    - SVM, Logistic Regression, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA não está disponível. Verifique a instalação e configuração da CUDA.\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device) ## Debug\n",
    "\n",
    "# Limpar o cache da GPU\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Sincronizar as operações CUDA\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Verificar se o arquivo de currículos existe\n",
    "if not os.path.exists(curriculae_file):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {curriculae_file}\")\n",
    "\n",
    "use_cross_validation = True  # ou False\n",
    "\n",
    "# ## Para usar validação cruzada com Regressão Logística default\n",
    "# results = evaluator.evaluate_models(validation_data, use_cross_validation=True)\n",
    "\n",
    "# ## Ou escolher tipo de algoritmo para avaliação\n",
    "# classifier_name = \"LogisticRegression\"  # LogisticRegression ou \"MultinomialNB\", \"SVC\", \"RandomForestClassifier\"\n",
    "\n",
    "# Para usar avaliação de modelos por divisão em treinamento e teste com SVM\n",
    "results = evaluator.evaluate_models(validation_data, use_cross_validation=False, classifier_name=\"SVC\")\n",
    "\n",
    "comparator = ModelComparator(results)\n",
    "best_model, best_score = comparator.get_best_model()\n",
    "\n",
    "if best_model is not None:\n",
    "    print(f\"\\nO melhor modelo é: {best_model} com pontuação de {best_score:.4f}\")\n",
    "elif not results:\n",
    "    print(\"\\nNenhum modelo foi avaliado com sucesso.\")\n",
    "else:\n",
    "    print(\"\\nNão foi possível determinar o melhor modelo.\")\n",
    "\n",
    "# Visualizar os resultados\n",
    "visualizer = PlotlyResultVisualizer(results)\n",
    "visualizer.plot_similarity_distributions()\n",
    "\n",
    "if not use_cross_validation: # Plotar acurácia apenas se não for utilizada a validação cruzada\n",
    "    visualizer.plot_accuracy_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar por clustering gerado com os embeedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import silhouette_score\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "\n",
    "def evaluate_embeddings(X, y, metric=cosine_similarity, n_splits=5):\n",
    "    scores = defaultdict(list)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "        y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "        for i, area in enumerate(y_test):\n",
    "            area_idx = [j for j, a in enumerate(y_train) if a == area]\n",
    "            competence_embeddings = [X_train[j] for j in area_idx]\n",
    "            similarities = metric([X_test[i]], competence_embeddings)\n",
    "            scores['intra_class'].append(np.mean(similarities))\n",
    "            dissimilar_idx = [j for j, a in enumerate(y_train) if a != area]\n",
    "            dissimilar_embeddings = [X_train[j] for j in dissimilar_idx]\n",
    "            similarities = metric([X_test[i]], dissimilar_embeddings)\n",
    "            scores['inter_class'].append(np.mean(similarities))\n",
    "\n",
    "        scores['silhouette'].append(silhouette_score(X_train, y_train, metric=metric)) # type: ignore\n",
    "\n",
    "    results = {k: np.mean(v) for k, v in scores.items()}\n",
    "    results['std_dev_intra_class'] = np.std(scores['intra_class'])\n",
    "    results['std_dev_inter_class'] = np.std(scores['inter_class'])\n",
    "    results['std_dev_silhouette'] = np.std(scores['silhouette'])\n",
    "    return results\n",
    "\n",
    "model_names = [\n",
    "    'sentence-transformers/gtr-t5-large',\n",
    "    'distiluse-base-multilingual-cased-v2',\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'all-mpnet-base-v2',\n",
    "    'all-distilroberta-v1',\n",
    "    'all-MiniLM-L6-v2',\n",
    "]\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curriculae_file = os.path.join(str(root_folder), filename)\n",
    "\n",
    "classifier = EmbeddingModelEvaluator(curriculae_file, model_names)\n",
    "\n",
    "# Defina o dispositivo (GPU se disponível, caso contrário CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Especifique a GPU correta (e.g., \"cuda:0\") se tiver várias\n",
    "    print(\"Usando GPU para processamento.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA não disponível, usando CPU.\")\n",
    "\n",
    "# Loop para avaliar os modelos\n",
    "for model_name in model_names:\n",
    "    # Limpeza de cache da GPU\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Redução do tamanho do lote\n",
    "    embeddings = model.encode(processed_competences, convert_to_tensor=True, batch_size=32)  # Tente um valor menor\n",
    "\n",
    "    # Carregue o modelo com o dispositivo especificado\n",
    "    model = SentenceTransformer(model_name, device=device) # type: ignore\n",
    "    X, y = classifier.prepare_data_for_classification(model) # type: ignore\n",
    "    score = evaluate_embeddings(X, y)\n",
    "    print(f\"Modelo: {model_name}, Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Escolher melhor vetorização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação individual do modelo de embeedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import spacy\n",
    "from git import Repo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from competence_extraction import CompetenceExtractor\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "pathfilename = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not os.path.exists(pathfilename):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {pathfilename}\")\n",
    "\n",
    "try:\n",
    "    with open(pathfilename, \"r\") as f:\n",
    "        curricula_data = json.load(f)\n",
    "except json.JSONDecodeError:\n",
    "    raise ValueError(f\"Erro ao decodificar o arquivo JSON: {pathfilename}\")\n",
    "\n",
    "competence_extractor = CompetenceExtractor(curriculae_path=pathfilename)\n",
    "\n",
    "# Extrair e vetorizar as competências de todos os pesquisadores\n",
    "all_competences = []\n",
    "for researcher_index, researcher_data in enumerate(curricula_data):  # Itera sobre a lista\n",
    "    competences = competence_extractor.extract_competences(researcher_data)\n",
    "    processed_competences = competence_extractor.preprocess_competences(competences)\n",
    "    all_competences.extend(processed_competences)\n",
    "\n",
    "# Vetorizar as competências em lote\n",
    "competence_vectors = competence_extractor.vectorize_competences(all_competences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(competence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Suponha que você tenha uma lista de pares de competências semelhantes e não semelhantes\n",
    "similar_pairs    = [(\"machine learning\", \"deep learning\"), (\"biologia molecular\", \"genética\")]\n",
    "dissimilar_pairs = [(\"machine learning\", \"medicina\"), (\"biologia molecular\", \"finanças\")]\n",
    "\n",
    "# Calcule a similaridade de cosseno entre os embeddings dos pares\n",
    "for pair in similar_pairs + dissimilar_pairs:\n",
    "    embedding1 = competence_extractor.vectorize_competences([pair[0]])\n",
    "    embedding2 = competence_extractor.vectorize_competences([pair[1]])\n",
    "    similarity = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "    print(f\"Similaridade entre '{pair[0]}' e '{pair[1]}': {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação comparativa entre os modelos de embeedings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas de Avaliação: As métricas utilizadas (similaridade média e acurácia) outras métricas de acordo com necessidade.\n",
    "\n",
    "Dados de Validação: Os dados de validação fornecidos são apenas exemplos. \n",
    "\n",
    "Os dados precisam ser reais e representativos para avaliar os modelos de forma adequada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lista de modelos a serem avaliados (atualizada)\n",
    "# model_names = [\"sentence-transformers/distiluse-base-multilingual-cased-v2\", \n",
    "#                \"bert-base-multilingual-cased\", \n",
    "#                \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"]\n",
    "\n",
    "# model_names = [\n",
    "#     \"sentence-transformers/distiluse-base-multilingual-cased-v2\", \n",
    "#     \"bert-base-uncased\",  # Modelo compatível com xFormers\n",
    "#     \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "# ]\n",
    "\n",
    "# Lista de modelos a serem avaliados\n",
    "# model_names = [\"distiluse-base-multilingual-cased-v2\",\n",
    "#                 \"bert-base-multilingual-cased\", \n",
    "#                 \"paraphrase-multilingual-mpnet-base-v2\"] ## Nome antigo não mais disponível\n",
    "\n",
    "# model_names = [\n",
    "#     'sentence-transformers/gtr-t5-large',\n",
    "#     'distiluse-base-multilingual-cased-v2',\n",
    "#     'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "#     'all-mpnet-base-v2',\n",
    "#     'all-distilroberta-v1',\n",
    "#     'all-MiniLM-L6-v2',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gpustat\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import gpustat\n",
    "\n",
    "gpu_stats = gpustat.GPUStatCollection.new_query()\n",
    "print(gpu_stats)\n",
    "print(torch.cuda.memory_summary())\n",
    "\n",
    "# Desabilitar funcionalidades da GPU em caso de erros recorrentes\n",
    "# export CUDA_LAUNCH_BLOCKING=1  # No Linux/macOS\n",
    "# !set CUDA_LAUNCH_BLOCKING=1  # No Windows\n",
    "\n",
    "# Limpeza de memória após a avaliação de cada modelo\n",
    "# del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from git import Repo\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from competence_extraction import EmbeddingModelEvaluator, ModelComparator, PlotlyResultVisualizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Lista de modelos a serem avaliados (atualizada)\n",
    "model_names = [\n",
    "    'sentence-transformers/gtr-t5-large',\n",
    "    'distiluse-base-multilingual-cased-v2',\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'all-mpnet-base-v2',\n",
    "    'all-distilroberta-v1',\n",
    "    'all-MiniLM-L6-v2',\n",
    "]\n",
    "\n",
    "\n",
    "# Caminho para o arquivo de currículos\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "curriculae_file = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# Verificar se o arquivo de currículos existe\n",
    "if not os.path.exists(curriculae_file):\n",
    "    raise FileNotFoundError(f\"Arquivo JSON não encontrado: {curriculae_file}\")\n",
    "\n",
    "# Dados de validação com base em seleção humana (exemplo)\n",
    "# validation_data = {\n",
    "#     'similar': [\n",
    "#         (\"machine learning\", \"deep learning\"),\n",
    "#         (\"biologia molecular\", \"genética\")\n",
    "#     ],\n",
    "#     'dissimilar': [\n",
    "#         (\"machine learning\", \"medicina\"),\n",
    "#         (\"biologia molecular\", \"finanças\")\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# Escolha do tipo de avaliação\n",
    "use_cross_validation = True  # ou False\n",
    "classifier_name = \"LogisticRegression\"  # ou \"MultinomialNB\", \"SVC\", \"RandomForestClassifier\"\n",
    "\n",
    "# Criar o avaliador e comparar os modelos\n",
    "evaluator = EmbeddingModelEvaluator(curriculae_file, model_names)\n",
    "\n",
    "# Usar validação cruzada com Regressão Logística default\n",
    "# results = evaluator.evaluate_models(validation_data, use_cross_validation=True)\n",
    "\n",
    "# Usar divisão em treinamento e teste com SVM\n",
    "# results = evaluator.evaluate_models(validation_data, use_cross_validation=False, classifier_name=\"SVC\")\n",
    "results = evaluator.evaluate_models(validation_data, use_cross_validation=False, classifier_name=\"MultinomialNB\")\n",
    "\n",
    "comparator = ModelComparator(results)\n",
    "best_model, best_score = comparator.get_best_model()\n",
    "\n",
    "if best_model is not None:\n",
    "    print(f\"\\nO melhor modelo é: {best_model} com pontuação de {best_score:.4f}\")\n",
    "elif not results:\n",
    "    print(\"\\nNenhum modelo foi avaliado com sucesso.\")\n",
    "else:\n",
    "    print(\"\\nNão foi possível determinar o melhor modelo.\")\n",
    "\n",
    "# Visualizar os resultados\n",
    "visualizer = PlotlyResultVisualizer(results)\n",
    "visualizer.plot_similarity_distributions()\n",
    "if not use_cross_validation: # Plota a acurácia apenas se não for validação cruzada\n",
    "    visualizer.plot_accuracy_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking em Gerar Embeedings de Competências em PDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lime\n",
    "\n",
    "# # Lista de modelos a serem avaliados\n",
    "# model_names = [\n",
    "#     'sentence-transformers/gtr-t5-large',\n",
    "#     'distiluse-base-multilingual-cased-v2',\n",
    "#     'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "#     'all-mpnet-base-v2',\n",
    "#     'all-distilroberta-v1',\n",
    "#     'all-MiniLM-L6-v2',\n",
    "# ]\n",
    "\n",
    "# # Caminho para o arquivo de currículos\n",
    "# repo = Repo(search_parent_directories=True)\n",
    "# root_folder = repo.working_tree_dir\n",
    "# filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "# curricula_file = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# # Dispositivo de processamento\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Instanciando as classes de avaliação\n",
    "# hardware_evaluator = HardwareEvaluator()\n",
    "# estimator = ProcessingCapacityEstimator(hardware_evaluator)\n",
    "# classifier = EmbeddingModelEvaluator(curricula_file, model_names)\n",
    "\n",
    "# # Tamanhos de lote para testar\n",
    "# batch_sizes = [8, 16, 32, 64, 128]\n",
    "\n",
    "# # Dicionário para armazenar os resultados da avaliação\n",
    "# results = {}\n",
    "\n",
    "# # Realizar benchmarks e recomendações\n",
    "# for model_name in model_names:\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "#         torch.cuda.synchronize()\n",
    "#         print('='*125)\n",
    "#         print(f'MODELO PRÉ-TREINADO: {model_name}')\n",
    "#         try:\n",
    "#             # Limpar a memória da GPU antes de carregar o modelo\n",
    "#             torch.cuda.empty_cache() \n",
    "\n",
    "#             # Monitorar o uso de memória antes e depois de carregar o modelo\n",
    "#             print(f\"Memória ocupada na GPU  antes de carregar o modelo: {np.round(torch.cuda.memory_allocated() / 1024**2,2):>8} MB\")\n",
    "#             model = SentenceTransformer(model_name, device=device) #type: ignore\n",
    "#             print(f\"Memória ocupada na GPU depois de carregar o modelo: {np.round(torch.cuda.memory_allocated() / 1024**2,2):>8} MB\")\n",
    "#             print('-'*125)\n",
    "\n",
    "#             # Preparar os dados para classificação (passe o objeto 'model')\n",
    "#             X, y = classifier.prepare_data_for_classification(model) # type: ignore\n",
    "\n",
    "#             # Converter X e y para arrays NumPy, movendo os tensores para a CPU primeiro\n",
    "#             if isinstance(X, list):\n",
    "#                 X = np.array([tensor.cpu().numpy() for tensor in X]) \n",
    "#             if isinstance(y, list):\n",
    "#                 y = np.array(y) \n",
    "\n",
    "#             # Dividir os dados em conjuntos de treinamento e teste\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    " \n",
    "#             # Escolha do classificador (adaptado para lidar com dados desbalanceados ou uma classe)\n",
    "#             unique_classes = np.unique(y_train)\n",
    "#             if len(unique_classes) == 1:\n",
    "#                 # Se houver apenas uma classe, use um algoritmo de detecção de anomalias\n",
    "#                 clf = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)  # Ou IsolationForest, LocalOutlierFactor\n",
    "#                 clf.fit(X_train)\n",
    "#                 y_pred = clf.predict(X_test)\n",
    "#                 # Converter as previsões para 0 (normal) ou 1 (anomalia)\n",
    "#                 y_pred = [0 if pred == 1 else 1 for pred in y_pred]\n",
    "#                 # Como estamos em um cenário de detecção de anomalias, \n",
    "#                 # assumimos que a classe minoritária (ou única) é a anomalia (classe 1)\n",
    "#                 y_test = [1 if label == unique_classes[0] else 0 for label in y_test] \n",
    "#             else:\n",
    "#                 # Se houver mais de uma classe, use um classificador robusto a dados desbalanceados\n",
    "#                 clf = LogisticRegression(class_weight='balanced', max_iter=1000)  # Ou outro classificador adequado\n",
    "#                 clf.fit(X_train, y_train)\n",
    "#                 y_pred = clf.predict(X_test)\n",
    "\n",
    "#             # Avaliar o modelo\n",
    "#             report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "#             # Explicabilidade com LIME\n",
    "#             explainer = lime.lime_text.LimeTextExplainer(class_names=clf.classes_)\n",
    "#             idx = 0  # Escolha um exemplo para explicar\n",
    "#             exp = explainer.explain_instance(X_test[idx], clf.predict_proba, num_features=10)\n",
    "#             print(f\"Explicação para a instância {idx}:\")\n",
    "#             print(exp.as_list())\n",
    "\n",
    "#             # Armazenar os resultados\n",
    "#             results[model_name] = {\n",
    "#                 'report': report,\n",
    "#                 'explainer': explainer  # Armazene o objeto explainer para uso posterior\n",
    "#             }\n",
    "#         except RuntimeError as e:\n",
    "#             if \"CUDA out of memory\" in str(e):\n",
    "#                 print(f\"Erro de memória da GPU para o modelo {model_name}. Tente reduzir o tamanho do lote ou usar um modelo menor.\")\n",
    "#             else:\n",
    "#                 print(f\"\\n  Erro ao processar o modelo {model_name}\")\n",
    "#                 print(f\"  {e}\")\n",
    "#     else:\n",
    "#         print('GPU não configurada corretamente, CUDA indisponível')\n",
    "\n",
    "# if results:\n",
    "#     # Comparar os modelos e escolher o melhor\n",
    "#     best_model = max(results, key=lambda k: results[k]['report']['macro avg']['f1-score'])\n",
    "#     print(f\"\\nO melhor modelo é: {best_model}\")\n",
    "#     print(results[best_model]['report'])\n",
    "# else:\n",
    "#     print('Não foi possível ler os resultados para realizar as comparações de desempenho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gml_benchmark\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import time\n",
    "\n",
    "# # Carregar os dados do currículo usando sua classe CompetenceExtractor\n",
    "# repo = Repo(search_parent_directories=True)\n",
    "# root_folder = repo.working_tree_dir\n",
    "# filename = os.path.join(\"_data\", \"in_csv\", \"docents_dict_list.json\")\n",
    "# pathfilename = os.path.join(str(root_folder), filename)\n",
    "\n",
    "# # Verificar se o arquivo existe\n",
    "# if not os.path.exists(pathfilename):\n",
    "#     raise FileNotFoundError(f\"Arquivo JSON não encontrado: {pathfilename}\")\n",
    "\n",
    "# try:\n",
    "#     with open(pathfilename, \"r\") as f:\n",
    "#         curricula_data = json.load(f)\n",
    "# except json.JSONDecodeError:\n",
    "#     raise ValueError(f\"Erro ao decodificar o arquivo JSON: {pathfilename}\")\n",
    "\n",
    "# competence_extractor = CompetenceExtractor(curricula_file=pathfilename)\n",
    "# curricula_data = competence_extractor.load_curricula()\n",
    "# competences = []\n",
    "# for researcher_data in curricula_data:\n",
    "#     competences.extend(competence_extractor.extract_competences(researcher_data))\n",
    "# processed_competences = competence_extractor.preprocess_competences(competences)\n",
    "\n",
    "# # Defina o número de clusters desejado para a classificação\n",
    "# num_clusters = 5  # Ou ajuste conforme necessário\n",
    "\n",
    "# # Defina a operação de benchmarking (geração de embeddings e classificação)\n",
    "# def embedding_generation_and_classification(graph, model, data, num_clusters):\n",
    "#     embeddings = gml_benchmark.generate_embeddings(model, data)\n",
    "\n",
    "#     if isinstance(graph, torch_geometric.data.Data):\n",
    "#         graph.x = embeddings\n",
    "#     elif isinstance(graph, dgl.DGLGraph):\n",
    "#         graph.ndata['feat'] = embeddings\n",
    "\n",
    "#     node_similarity = gml_benchmark.calculate_node_similarity(embeddings)\n",
    "#     edge_similarity = gml_benchmark.calculate_edge_similarity(graph)\n",
    "\n",
    "#     node_labels = gml_benchmark.classify_nodes(embeddings, num_clusters)\n",
    "#     edge_labels = gml_benchmark.classify_edges(edge_similarity, num_clusters)\n",
    "\n",
    "# # Lista de modelos a serem avaliados (mesma do código anterior)\n",
    "# model_names = [\n",
    "#     \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "#     \"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "#     \"sentence-transformers/LaBSE\"\n",
    "# ]\n",
    "\n",
    "# # Dispositivo de processamento\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Benchmarking\n",
    "# results = {}\n",
    "# for model_name in model_names:\n",
    "#     print('-' * 125)\n",
    "#     print(f'MODELO PRÉ-TREINADO: {model_name}')\n",
    "\n",
    "#     try:\n",
    "#         # Carregar o modelo pré-treinado\n",
    "#         model = SentenceTransformer(model_name, device=device) # type: ignore\n",
    "\n",
    "#         # Criar os grafos (implemente as funções de criação de grafo)\n",
    "#         pyg_graph = gml_benchmark.create_pytorch_geometric_graph([], [])  # Substitua [] pelos seus dados de aresta\n",
    "#         dgl_graph = gml_benchmark.create_dgl_graph([], [])            # Substitua [] pelos seus dados de aresta\n",
    "\n",
    "#         # Realizar o benchmarking\n",
    "#         pyg_time = gml_benchmark.benchmark_operation(\n",
    "#             pyg_graph, \"PyTorch Geometric\",\n",
    "#             lambda graph: embedding_generation_and_classification(graph, model, processed_competences, num_clusters)\n",
    "#         )\n",
    "#         dgl_time = gml_benchmark.benchmark_operation(\n",
    "#             dgl_graph, \"DGL\",\n",
    "#             lambda graph: embedding_generation_and_classification(graph, model, processed_competences, num_clusters)\n",
    "#         )\n",
    "\n",
    "#         # Armazenar os resultados\n",
    "#         results[model_name] = {\"PyTorch Geometric\": pyg_time, \"DGL\": dgl_time}\n",
    "\n",
    "#     except RuntimeError as e:\n",
    "#         if \"CUDA out of memory\" in str(e):\n",
    "#             print(f\"Erro de memória da GPU para o modelo {model_name}. Tente reduzir o tamanho do lote ou usar um modelo menor.\")\n",
    "#         else:\n",
    "#             print(f\"Erro ao processar o modelo {model_name}: {e}\")\n",
    "\n",
    "# # Plotar os resultados\n",
    "# gml_benchmark.plot_benchmark_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gerar o modelo grafo inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx\n",
    "# !pip install matplotlib\n",
    "# !pip3 install selenium\n",
    "# !pip3 install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('../../templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from IPython.display import IFrame\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# from pathlib import Path\n",
    "# def find_repo_root(path='.', depth=10):\n",
    "#         ''' \n",
    "#         Busca o arquivo .git e retorna string com a pasta raiz do repositório.\n",
    "#         '''\n",
    "#         # Prevenir recursão infinita limitando a profundidade\n",
    "#         if depth < 0:\n",
    "#             return None\n",
    "#         path = Path(path).absolute()\n",
    "#         if (path / '.git').is_dir():\n",
    "#             return path\n",
    "#         # Corrigido para usar LattesScraper.find_repo_root para chamada recursiva\n",
    "#         return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "import os\n",
    "from git import Repo\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "# data_folder = os.path.join(str(root_folder),\"_data\",\"in_pdf\")\n",
    "\n",
    "# Criar o grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar nós (dominios, processos e entidades)\n",
    "dominios = [\"Pesquisar\", \"Desenvolver\", \"Inovar\"]\n",
    "processos = [\"P001\", \"P002\", \"P003\", \"P004\", \"P005\", \"P006\", \"P007\", \"P008\", \"P009\"]\n",
    "entidades = {\n",
    "    \"P001\": [\"Dores\", \"Desejos\", \"Desafios\"],\n",
    "    \"P002\": [\"Temas\", \"Tópicos\", \"Assuntos\"],\n",
    "    \"P003\": [\"Atitudes\", \"Experiências\", \"Habilidades\"],\n",
    "    \"P004\": [\"Papeis\", \"Tempo\", \"Orçamentos\"],\n",
    "    \"P005\": [\"Projetos\", \"Processos\", \"Programas\"],\n",
    "    \"P006\": [\"Ensaios\", \"Equipamentos\", \"Ambientes\"],\n",
    "    \"P007\": [\"Aplicação\", \"Solução\", \"Produto-Serviço\"],\n",
    "    \"P008\": [\"Modelos\", \"Protótipos\", \"Empreendimentos\"],\n",
    "    \"P009\": [\"Indicadores\", \"Evidências\", \"Mensuração\"]\n",
    "}\n",
    "\n",
    "# Criar visualização dos nós de acordo com a estrutura de dados\n",
    "for macroprocesso in dominios:\n",
    "    G.add_node(macroprocesso, type=\"macroprocesso\")\n",
    "\n",
    "for processo in processos:\n",
    "    G.add_node(processo, type=\"processo\")\n",
    "\n",
    "for processo, entidades_list in entidades.items():\n",
    "    for entidade in entidades_list:\n",
    "        G.add_node(entidade, type=\"entidade\")\n",
    "\n",
    "# Adicionar arestas (relacionamentos)\n",
    "for macroprocesso in dominios:\n",
    "    for i in range(1, 4):\n",
    "        G.add_edge(macroprocesso, f\"P00{i + 3*(dominios.index(macroprocesso))}\")\n",
    "\n",
    "for processo, entidades_list in entidades.items():\n",
    "    for entidade in entidades_list:\n",
    "        G.add_edge(processo, entidade)\n",
    "\n",
    "# (Opcional) Adicionar relacionamentos entre entidades, para formar Demanda, Faturamento, Lucro, Reinvestimento... etc\n",
    "# G.add_edge(\"Dores\", \"Desejos\")\n",
    "\n",
    "# Calcular distâncias, definir cores, tamanhos e tamanhos de fonte\n",
    "node_distances = {}\n",
    "for macroprocesso in dominios:\n",
    "    for node, distance in nx.shortest_path_length(G, source=macroprocesso).items():\n",
    "        node_distances[node] = distance\n",
    "\n",
    "cores_base = {\"macroprocesso\": \"#007BFF\", \"processo\": \"#28A745\", \"entidade\": \"#FFC107\"}\n",
    "node_colors = {}\n",
    "node_sizes = {}\n",
    "node_font_sizes = {}  # Dicionário para armazenar os tamanhos de fonte\n",
    "for node in G.nodes():\n",
    "    node_type = G.nodes[node][\"type\"]\n",
    "    cor_base = cores_base[node_type]\n",
    "    alpha = max(0, 255 - 25 * node_distances[node])\n",
    "    node_colors[node] = f\"{cor_base}{alpha:02X}\"\n",
    "\n",
    "    # Definir tamanhos e tamanhos de fonte com base na distância\n",
    "    tamanho_base = {\"macroprocesso\": 50, \"processo\": 30, \"entidade\": 15}\n",
    "    font_size_base = {\"macroprocesso\": 42, \"processo\": 28, \"entidade\": 18}  # Tamanhos de fonte iniciais\n",
    "    node_sizes[node] = tamanho_base[node_type] - 5 * node_distances[node]\n",
    "    node_font_sizes[node] = font_size_base[node_type] - node_distances[node]  # Reduzir 1 pixel por passo\n",
    "\n",
    "# Configurar o PyVis (notebook=False para renderizar na célula)\n",
    "net = Network(notebook=False, width=\"100%\", height=\"1200px\", bgcolor=\"#ffffff\", font_color=\"black\") # type: ignore\n",
    "net.barnes_hut()\n",
    "\n",
    "# Adicionar nós e arestas ao PyVis\n",
    "for node in G.nodes():\n",
    "    net.add_node(node, label=node, color=node_colors[node], title=G.nodes[node][\"type\"], \n",
    "                 size=node_sizes[node], font={\"size\": node_font_sizes[node], \"color\": \"black\"})  # Adicionar tamanho da fonte\n",
    "\n",
    "for edge in G.edges():\n",
    "    weight = 1\n",
    "    net.add_edge(*edge, value=weight)\n",
    "\n",
    "# Configurar o ForceAtlas2\n",
    "net.options.physics.solver = \"forceAtlas2Based\"\n",
    "net.options.physics.forceAtlas2Based = { # type: ignore\n",
    "    \"gravitationalConstant\": -50,\n",
    "    \"centralGravity\": 0.01,\n",
    "    \"springLength\": 100,\n",
    "    \"springConstant\": 0.08,\n",
    "    \"damping\": 0.4,\n",
    "    \"avoidOverlap\": 1\n",
    "}\n",
    "\n",
    "driver_path = None\n",
    "try:\n",
    "    # Caminho para o chromedriver no sistema local\n",
    "    if platform.system() == \"Windows\":\n",
    "        driver_path=os.path.join(str(root_folder),'chromedriver','chromedriver.exe')\n",
    "    else:\n",
    "        driver_path=os.path.join(str(root_folder),'chromedriver','chromedriver')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"Não foi possível estabelecer uma conexão, verifique o chromedriver\")\n",
    "    print(e)\n",
    "\n",
    "# print(driver_path)\n",
    "# service = Service(driver_path)\n",
    "# driver = webdriver.Chrome(service=service)\n",
    "# driver.get(\"grafo_interativo.html\")\n",
    "\n",
    "# Adicionar controles interativos (opcional)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "# Adicionar estilo inline para fundo branco\n",
    "net.html = net.html.replace(\"<body>\", '<body style=\"background-color: white;\">')\n",
    "\n",
    "# Salvar o HTML na pasta templates\n",
    "template_dir = os.path.join(str(root_folder),'templates')\n",
    "pathfilename = os.path.join(template_dir,\"grafo_interativo.html\")\n",
    "net.save_graph(pathfilename)\n",
    "\n",
    "print(f\"Grafo interativo salvo em: {pathfilename}\")\n",
    "# Renderizar na célula do Jupyter Notebook\n",
    "# net.show(\"'../../templates'grafo_interativo.html\")  # Definir fundo branco ao salvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Renderizar grafo no Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "net = Network(notebook=True, \n",
    "              width=\"100%\", \n",
    "              height=\"1200px\", \n",
    "              bgcolor=\"#ffffff\", \n",
    "              font_color=\"black\") # type: ignore\n",
    "\n",
    "IFrame(src='http://127.0.0.1:5000/grafo_interativo.html', width='100%', height='800px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renderizar grafo em HTML na célula do Jupyter Notebook\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=\"grafo_interativo.html\", width=\"100%\", height=\"600px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Tarefas para monitorar políticas públicas</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obter dados de Protocolos e Diretrizes Clínicas vigentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Protocolos clínicos vigentes no site do Ministério da Saúde\n",
    "\n",
    "Fonte de dados para extração de protocolos: https://www.gov.br/saude/pt-br/assuntos/pcdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Protocolos clínicos vigentes no site do CONITEC\n",
    "Fonte de dados para extração de protocolos: https://www.gov.br/conitec/pt-br/assuntos/avaliacao-de-tecnologias-em-saude/protocolos-clinicos-e-diretrizes-terapeuticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baixar Protocolos e Diretrizes Clínicas do Site do MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_protocol import SaudeGovDataExtractor\n",
    "\n",
    "extractor = SaudeGovDataExtractor(\"https://www.gov.br/saude/pt-br/assuntos/pcdt\")\n",
    "df_docs, sucessos, erros = extractor.download_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs[:60] # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitorar fluxos e fontes de dados no CONITEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fluxo elaboração/atualização de Protocolos e Diretrizes\n",
    "https://www.gov.br/conitec/pt-br/assuntos/avaliacao-de-tecnologias-em-saude/pcdt-em-elaboracao-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fluxo de Incorporação de Tecnologia no SUS\n",
    "https://www.gov.br/conitec/pt-br/assuntos/fluxo-de-incorporacao-de-tecnologias-no-sus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitorar Tecnologias discutidas no CONITEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gov.br/conitec/pt-br/assuntos/avaliacao-de-tecnologias-em-saude/monitoramento-de-tecnologias-em-saude#MHT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acompanhar Publicações Diretrizes Clínicas CONITEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gov.br/conitec/pt-br/assuntos/noticias/2024/abril/atualizacao-anual-de-diretrizes-clinicas-pelo-ministerio-da-saude-segue-criterios-de-priorizacao-e-considera-encaminhamento-de-areas-tecnicas\n",
    "\n",
    "https://www.gov.br/conitec/pt-br/@@search?SearchableText=Diretrizes%20Cl%C3%ADnicas\n",
    "\n",
    "https://www.gov.br/conitec/pt-br/midias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(str(path.parent), depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Sempre construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(str(base_repo_dir), 'utils')\n",
    "folder_domain = os.path.join(str(base_repo_dir), 'source', 'domain')\n",
    "folder_data_input = os.path.join(str(base_repo_dir), '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(str(base_repo_dir), '_data', 'out_json')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "from environment_setup import EnvironmentSetup\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from neo4j_persister import Neo4jPersister\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DictToHDF5, attribute_to_be_non_empty\n",
    "from extract_protocol import SaudeGovDataExtractor\n",
    "# from scraper_pasteur import PasteurScraper\n",
    "# from scraper_sucupira import SucupiraScraper\n",
    "# from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "\n",
    "t00 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from extract_protocol import SaudeGovDataExtractor\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "extractor = SaudeGovDataExtractor(\"https://www.gov.br/saude/pt-br/assuntos/pcdt\")\n",
    "links = extractor.get_links()\n",
    "if links:\n",
    "    print(f'{len(links)} links extraídos')\n",
    "\n",
    "## Visualizar links para documentos de protocolos e diretrizes\n",
    "# for link in links:\n",
    "#     filename = os.path.join(str(root_folder),\"_data\",\"in_pdf\", link.split(\"/\")[-1])\n",
    "#     if '#' in filename:\n",
    "#         filename = filename.split('#')[0]\n",
    "#     print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Tarefas para obter dados de Inovação no Mundo</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desempenho em Inovação no mundo e no Brasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medição da inovação em nível mundial é um campo importante para entender como diferentes nações estão progredindo em termos de capacidade e sucesso em inovação. Uma das iniciativas mais conhecidas neste campo é o Global Innovation Index (GII).\n",
    "\n",
    "O GII é um ranking anual publicado pela World Intellectual Property Organization (WIPO) em parceria com a INSEAD e outras instituições, como a Cornell University. Iniciado em 2007, o índice é baseado em dados subjetivos e objetivos obtidos de várias fontes, incluindo a International Telecommunication Union, o World Bank e o World Economic Forum. O GII classifica os países com base em dois sub-índices: o Innovation Input Index e o Innovation Output Index, compostos por cinco e dois pilares, respectivamente, cada um descrevendo um atributo da inovação.\n",
    "\n",
    "Além do GII, há outras iniciativas semelhantes, como o International Innovation Index, que medem o nível de inovação de um país. Este índice é produzido em conjunto pelo Boston Consulting Group (BCG), pela National Association of Manufacturers (NAM) e pelo Manufacturing Institute (MI), o afiliado de pesquisa apartidária da NAM.\n",
    "\n",
    "Cada um desses índices oferece uma perspectiva única sobre a inovação em nível de país, ajudando governos, formuladores de políticas e acadêmicos a entender as tendências de inovação e a identificar áreas para melhoria e investimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "for i in range(0,20):\n",
    "    print(math.factorial(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(1, 100, 100)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "# Valores de entrada (tamanho do problema) - ajustados\n",
    "n = np.linspace(1, 100, 100)\n",
    "\n",
    "# Cálculo das complexidades\n",
    "O_1 = np.ones(n.shape)\n",
    "O_log_n = np.log2(n)\n",
    "O_n = n\n",
    "O_n_log_n = n * np.log2(n)\n",
    "O_n_squared = n**2\n",
    "O_2_n = 2**n-1\n",
    "max_factorial_visible = 120  # Valor máximo para o cálculo do fatorial 6!=120\n",
    "O_n_factorial = np.array([math.factorial(int(i)) if i <= max_factorial_visible else np.nan for i in n])\n",
    "\n",
    "# Exibir O_n_factorial em notação científica\n",
    "for valor in O_n_factorial[:max_factorial_visible + 1]:\n",
    "    print(f\"{valor}\")\n",
    "\n",
    "# Criar um DataFrame para os dados a serem exibidos\n",
    "df = pd.DataFrame({'n': n[:max_factorial_visible + 1], 'O(n!)': O_n_factorial[:max_factorial_visible + 1]})\n",
    "\n",
    "# Criar o gráfico de linha\n",
    "chart = alt.Chart(df).mark_line(point=True).encode(  # Adicionamos point=True para mostrar os pontos\n",
    "    x=alt.X('n:Q', axis=alt.Axis(labelAngle=-45, format='.2f')),  # Formato com 4 casas decimais\n",
    "    y=alt.Y('O(n!)', scale=alt.Scale(type='log')),\n",
    "    tooltip=['n', 'O(n!)']\n",
    ").properties(\n",
    "    title='Valores de O(n!) até n = 20',\n",
    "    height=400,\n",
    "    width=800,\n",
    ").interactive()\n",
    "\n",
    "# Exibir o gráfico\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Valores de entrada (tamanho do problema) - ajustados\n",
    "n = np.linspace(1, 100, 100)\n",
    "\n",
    "# Cálculo das complexidades\n",
    "O_1 = np.ones(n.shape)\n",
    "O_log_n = np.log2(n)\n",
    "O_n = n\n",
    "O_n_log_n = n * np.log2(n)\n",
    "O_n_squared = n**2\n",
    "O_2_n = 2**n - 1\n",
    "\n",
    "# Calcular O(n!) até um ponto visível\n",
    "max_factorial = 100  # Valor máximo de n para o qual O(n!) é visível no gráfico\n",
    "O_n_factorial = np.array([math.factorial(int(i)) if i <= max_factorial_visible else np.nan for i in n])\n",
    "\n",
    "# Paleta de cores similar à imagem de referência\n",
    "colors = ['darkgreen', 'green', 'orange', 'gold', 'red', 'darkblue', 'purple']\n",
    "\n",
    "# Criar o gráfico Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar as curvas ao gráfico, usando cores personalizadas\n",
    "fig.add_trace(go.Scatter(x=n, y=O_1, mode='lines', name='O(1)', line=dict(color=colors[0], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_log_n, mode='lines', name='O(log n)', line=dict(color=colors[1], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n, mode='lines', name='O(n)', line=dict(color=colors[2], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_log_n, mode='lines', name='O(n log n)', line=dict(color=colors[3], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_squared, mode='lines', name='O(n²)', line=dict(color=colors[4], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_2_n, mode='lines', name='O(2ⁿ)', line=dict(color=colors[5], width=2)))\n",
    "fig.add_trace(go.Scatter(x=n[:max_factorial + 1], y=O_n_factorial[:max_factorial + 1], mode='lines', name='O(n!)', line=dict(color=colors[6], width=2)))\n",
    "\n",
    "# Configuração do layout do gráfico\n",
    "fig.update_layout(\n",
    "    title='Comparação de Complexidades Algorítmicas (Big O)',\n",
    "    xaxis_title='Tamanho dos Dados de Entrada (n)',\n",
    "    yaxis_title='Tempo de Execução em escala logarítmica (em operações)',\n",
    "    yaxis_type='log',\n",
    "    yaxis=dict(range=[-0.04, 2]),  # Escala logarítmica com limite definido para y\n",
    "    xaxis=dict(range=[1, 20]),  # Escala linear com limite definido para x\n",
    "    showlegend=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(l=50, r=50, t=80, b=50),\n",
    ")\n",
    "\n",
    "# Função para calcular as posições das anotações\n",
    "def calculate_annotations(fig):\n",
    "    annotations = []\n",
    "    x_end = fig.layout.xaxis.range[1] * 0.95  \n",
    "    for i, trace in enumerate(fig.data):\n",
    "        # Encontrar o índice do valor de x mais próximo de x_end\n",
    "        idx = np.abs(trace.x - x_end).argmin()\n",
    "\n",
    "        # Se o valor de y for NaN, usar o último valor válido antes de x_end\n",
    "        if np.isnan(trace.y[idx]):\n",
    "            valid_indices = np.where(~np.isnan(trace.y) & (trace.x <= x_end))[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                idx = valid_indices[-1]\n",
    "\n",
    "        x = trace.x[idx]\n",
    "        y = trace.y[idx]\n",
    "\n",
    "        # Calcular a posição da anotação no eixo y, considerando a escala logarítmica, exceto para O(1)\n",
    "        if trace.name == 'O(n log n)':\n",
    "            y_annotation = 1.95\n",
    "        if trace.name == \"O(n)\":\n",
    "            y_annotation = 1.35\n",
    "            x_annotation = x\n",
    "        if trace.name == \"O(log n)\":\n",
    "            y_annotation = 0.7\n",
    "            x_annotation = x-0.4\n",
    "        if trace.name == \"O(1)\":\n",
    "            y_annotation = 0.05\n",
    "            x_annotation = x\n",
    "        if trace.name == \"O(n²)\":\n",
    "            y_annotation = 2\n",
    "            x_annotation = 10\n",
    "        if trace.name == \"O(2ⁿ)\":\n",
    "            y_annotation = 2\n",
    "            x_annotation = 6.7\n",
    "        if trace.name == \"O(n!)\":\n",
    "            y_annotation = 2\n",
    "            x_annotation = 5\n",
    "\n",
    "        annotations.append(\n",
    "            dict(\n",
    "                x=x_annotation,\n",
    "                y=y_annotation,\n",
    "                text=trace.name,\n",
    "                showarrow=False,\n",
    "                xanchor='left' if trace.name != 'O(n log n)' else 'center',  # Ajustar o alinhamento para O(n log n)\n",
    "                yanchor='middle',  # Centralizar o rótulo na vertical\n",
    "                font=dict(size=12, color=colors[i], family='Arial'),  # Usar a cor correspondente da paleta\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Calcular e adicionar as anotações\n",
    "fig.update_layout(annotations=calculate_annotations(fig))\n",
    "\n",
    "# Exibir o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Valores de entrada (tamanho do problema) - ajustados\n",
    "n = np.linspace(1, 100, 1000)\n",
    "\n",
    "# Cálculo das complexidades\n",
    "O_1 = np.ones(n.shape)\n",
    "O_log_n = np.log2(n)\n",
    "O_n = n\n",
    "O_n_log_n = n * np.log2(n)\n",
    "O_n_squared = n**2\n",
    "O_2_n = 2**n - 1\n",
    "\n",
    "# Cálculo da complexidade O(n!) para inteiros até max_factorial_visible\n",
    "max_factorial_visible = 100\n",
    "n_int = np.arange(1, max_factorial_visible + 1)  # Valores inteiros de 1 a max_factorial_visible\n",
    "O_n_factorial_int = np.array([math.factorial(i) for i in n_int])\n",
    "\n",
    "# Interpolação para valores fracionários\n",
    "O_n_factorial = np.array([math.gamma(i + 1) for i in n])\n",
    "\n",
    "# Paleta de cores similar à imagem de referência\n",
    "colors = ['darkgreen', 'green', 'gold', 'orange', 'red', 'darkblue', 'black']\n",
    "\n",
    "# Criar o gráfico Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar as curvas ao gráfico, usando cores personalizadas\n",
    "fig.add_trace(go.Scatter(x=n, y=O_1, mode='lines', name='O(1)', line=dict(color=colors[0], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_log_n, mode='lines', name='O(log n)', line=dict(color=colors[1], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n, mode='lines', name='O(n)', line=dict(color=colors[2], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_log_n, mode='lines', name='O(n log n)', line=dict(color=colors[3], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_n_squared, mode='lines', name='O(n²)', line=dict(color=colors[4], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n, y=O_2_n, mode='lines', name='O(2ⁿ)', line=dict(color=colors[5], width=4)))\n",
    "fig.add_trace(go.Scatter(x=n[:max_factorial_visible + 1], y=O_n_factorial[:max_factorial_visible + 1], \n",
    "    mode='lines', name='O(n!)', line=dict(color=colors[6], width=4)))\n",
    "\n",
    "# Configuração do layout do gráfico\n",
    "fig.update_layout(\n",
    "    title='Comparação de Complexidades Assintótica dos Algoritmos (Big O)',\n",
    "    xaxis_title='Tamanho dos Dados de Entrada (n)',\n",
    "    yaxis_title='Tempo de Execução em escala logarítmica (em operações)',\n",
    "    yaxis_type='log',\n",
    "    xaxis=dict(\n",
    "        range=[1, 20],          # Escala linear com limite definido para x\n",
    "        gridcolor='lightgray',  # Cor mais clara para a grade\n",
    "        gridwidth=0.25,         # Largura menor para a grade\n",
    "        showgrid=True,\n",
    "        griddash='dot',\n",
    "    ),  \n",
    "    yaxis=dict(\n",
    "        range=[-0.04, 2],       # Escala logarítmica com limite definido para y\n",
    "        gridcolor='lightgray',  \n",
    "        gridwidth=0.25,\n",
    "        showgrid=True,\n",
    "        griddash='dot',\n",
    "        # tickformat=\".0e\",     # Formatar os ticks em notação científica com expoente inteiro\n",
    "        # tickformat=\",d\"         # Formatar os ticks como números inteiros com separador de milhar\n",
    "        tickvals=[1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000],  # Valores dos ticks\n",
    "        ticktext=['1', '10', '100', '1,000', '10,000', '100,000', '1,000,000', '10,000,000', '100,000,000']  # Rótulos em números inteiros\n",
    "\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(l=25, r=50, t=80, b=50),\n",
    ")\n",
    "\n",
    "# Função para calcular as posições das anotações\n",
    "def calculate_annotations(fig):\n",
    "    annotations = []\n",
    "    x_end = fig.layout.xaxis.range[1] * 0.95  \n",
    "    for i, trace in enumerate(fig.data):\n",
    "        # Encontrar o índice do valor de x mais próximo de x_end\n",
    "        idx = np.abs(trace.x - x_end).argmin()\n",
    "\n",
    "        # Se o valor de y for NaN, usar o último valor válido antes de x_end\n",
    "        if np.isnan(trace.y[idx]):\n",
    "            valid_indices = np.where(~np.isnan(trace.y) & (trace.x <= x_end))[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                idx = valid_indices[-1]\n",
    "\n",
    "        x = trace.x[idx]\n",
    "        y = trace.y[idx]\n",
    "\n",
    "        # Calcular a posição da anotação no eixo y, considerando a escala logarítmica, exceto para O(1)\n",
    "        if trace.name == 'O(n log n)':\n",
    "            y_annotation = 1.95\n",
    "        if trace.name == \"O(n)\":\n",
    "            y_annotation = 1.4\n",
    "            x_annotation = x+0.75\n",
    "        if trace.name == \"O(log n)\":\n",
    "            y_annotation = 0.7\n",
    "            x_annotation = x+0.75\n",
    "        if trace.name == \"O(1)\":\n",
    "            y_annotation = 0.075\n",
    "            x_annotation = x+0.75\n",
    "        if trace.name == \"O(n²)\":\n",
    "            y_annotation = 1.95\n",
    "            x_annotation = 11.4\n",
    "        if trace.name == \"O(2ⁿ)\":\n",
    "            y_annotation = 1.95\n",
    "            x_annotation = 8\n",
    "        if trace.name == \"O(n!)\":\n",
    "            y_annotation = 1.95\n",
    "            x_annotation = 4.75         \n",
    "\n",
    "        annotations.append(\n",
    "            dict(\n",
    "                x=x_annotation,\n",
    "                y=y_annotation,\n",
    "                text=trace.name,\n",
    "                showarrow=False,\n",
    "                xanchor='right',\n",
    "                yanchor='middle',\n",
    "                font=dict(size=20, color='black', family='Arial'),  # Usar a cor correspondente da paleta\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Calcular e adicionar as anotações\n",
    "fig.update_layout(annotations=calculate_annotations(fig),\n",
    "    shapes=[\n",
    "        # Região verde (diagonal até um pouco acima de O(log n))\n",
    "        dict(\n",
    "            type=\"path\",\n",
    "            path=f\"M 0, {10**fig.layout.yaxis.range[0]} L 20, {O_log_n[-1]} L 20, {10**fig.layout.yaxis.range[0]} Z\", # type: ignore\n",
    "            fillcolor=\"lightgreen\",\n",
    "            opacity=0.3,\n",
    "            line_width=0,\n",
    "            layer='below' # para que as curvas fiquem por cima\n",
    "        ),\n",
    "        # Região amarela (diagonal até um pouco acima de O(n log n))\n",
    "        dict(\n",
    "            type=\"path\",\n",
    "            path=f\"M 0, {10**fig.layout.yaxis.range[0]} L 20, {O_n_log_n[-1]-5} L 20, {O_log_n[-1]} Z\", # type: ignore\n",
    "            fillcolor=\"yellow\",\n",
    "            opacity=0.3,\n",
    "            line_width=0,\n",
    "            layer='below' # para que as curvas fiquem por cima\n",
    "        ),\n",
    "        # Região vermelha (diagonal até o topo do gráfico)\n",
    "        dict(\n",
    "            type=\"path\",\n",
    "            # path=f\"M 0, {10**fig.layout.yaxis.range[0]} L 20, {10**fig.layout.yaxis.range[1]} L 0, {10**fig.layout.yaxis.range[1]} Z\",\n",
    "            path=f\"M 20, {O_n_log_n[-1] - 5} L 0, {10**(int(np.log10(O_n_log_n[-1])) + 1)} L 0, {10**fig.layout.yaxis.range[0]} Z\", # type: ignore\n",
    "            fillcolor=\"red\",\n",
    "            opacity=0.3,\n",
    "            line_width=0,\n",
    "            layer='below'  # para que as curvas fiquem por cima\n",
    "        ),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "# Exibir o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histórico no Global Innovation Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pathfilename = os.path.join(folder_data_input, 'gii_history_data.csv')\n",
    "df_gii = pd.read_csv(pathfilename)\n",
    "\n",
    "def calculate_percentages(df_gii):\n",
    "    df_avaliacao = pd.DataFrame(columns=['Ano', 'Participantes', 'Above Brazil', 'Below Brazil', 'Above Brazil (%)', 'Below Brazil (%)'])\n",
    "    \n",
    "    for year in df_gii['Ano'].unique():\n",
    "        df_year = df_gii[df_gii['Ano'] == year]\n",
    "        total_countries = df_year['Países Participantes'].values[0]\n",
    "        brazil_position = df_year['Colocação do Brasil'].values[0]\n",
    "        above_brazil = brazil_position - 1\n",
    "        below_brazil = total_countries - brazil_position\n",
    "        \n",
    "        above_percent = (above_brazil / total_countries) * 100\n",
    "        below_percent = (below_brazil / total_countries) * 100\n",
    "        \n",
    "        df_avaliacao = pd.concat([df_avaliacao, pd.DataFrame({'Ano': [year], 'Participantes': [total_countries], 'Above Brazil': [above_brazil], 'Below Brazil': [below_brazil], 'Above Brazil (%)': [above_percent], 'Below Brazil (%)': [below_percent]})], ignore_index=True)\n",
    "    \n",
    "    return df_avaliacao\n",
    "\n",
    "# Call the function to create df_avaliacao\n",
    "df_avaliacao = calculate_percentages(df_gii)\n",
    "df_avaliacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a figure with secondary y-axis for the line plots\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add stacked bars for 'Below Brazil (%)' and 'Above Brazil (%)'\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_avaliacao['Ano'], \n",
    "           y=df_avaliacao['Below Brazil (%)'], \n",
    "           name='Below Brazil (%)'),\n",
    "           secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_avaliacao['Ano'], \n",
    "           y=df_avaliacao['Above Brazil (%)'], \n",
    "           name='Above Brazil (%)'),\n",
    "           secondary_y=False,\n",
    ")\n",
    "\n",
    "# Correct the data label positions for each segment of the stacked bars\n",
    "for index, row in df_avaliacao.iterrows():\n",
    "    # Position for label of 'Above Brazil (%)'\n",
    "    position_above = row['Below Brazil (%)'] + row['Above Brazil (%)'] / 2\n",
    "    fig.add_annotation(\n",
    "        x=row['Ano'], y=position_above,\n",
    "        text=f\"{row['Above Brazil (%)']:.1f}%\",\n",
    "        showarrow=False, font=dict(color='white')\n",
    "    )\n",
    "\n",
    "    # Position for label of 'Below Brazil (%)'\n",
    "    position_below = row['Below Brazil (%)'] / 2\n",
    "    fig.add_annotation(\n",
    "        x=row['Ano'], y=position_below,\n",
    "        text=f\"{row['Below Brazil (%)']:.1f}%\",\n",
    "        showarrow=False, font=dict(color='white')\n",
    "    )\n",
    "\n",
    "# Update layout for stacked bars\n",
    "fig.update_layout(barmode='stack')\n",
    "\n",
    "# Add line for total number of participants\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_avaliacao['Ano'], \n",
    "               y=df_avaliacao['Participantes'], \n",
    "               name='Total Participants', \n",
    "               mode='lines+markers+text', \n",
    "               text=df_avaliacao['Participantes'], \n",
    "               textposition=\"top center\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add line for Brazil's performance, but on the primary y-axis\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_avaliacao['Ano'], \n",
    "               y=df_avaliacao['Below Brazil (%)'], \n",
    "               name='Brazil Performance', \n",
    "               mode='lines+markers', \n",
    "               line=dict(color='yellow', \n",
    "                         dash='dot')),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "# Update the bar colors\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Above Brazil (%)'),\n",
    "    marker=dict(color='orange')\n",
    ")\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Below Brazil (%)'),\n",
    "    marker=dict(color='blue')\n",
    ")\n",
    "\n",
    "# Update the line trace for total number of participants to have a thickness of 4\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Total Participants'),\n",
    "    line=dict(width=2)\n",
    ")\n",
    "\n",
    "# Update the line trace for Brazil's performance to have a thickness of 4\n",
    "fig.update_traces(\n",
    "    selector=dict(name='Brazil Performance'),\n",
    "    line=dict(width=6)\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "vr_max = max(df_avaliacao['Participantes']) * 1.1\n",
    "fig.update_layout(\n",
    "    title='Performance Comparison: Brazil vs. Other Countries',\n",
    "    height=600,\n",
    "    yaxis=dict(title='Percentage', range=[0, vr_max]),  # Extending primary y-axis range\n",
    "    yaxis2=dict(title='Total Participants', overlaying='y', side='right', range=[0, vr_max])\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickvals=df_avaliacao['Ano'])\n",
    "\n",
    "# Re-render the chart\n",
    "fig.show(renderer=\"notebook\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Gerar análises no domínio PDI para o CEIS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Analisar similaridade tópicos - questões pesquisa</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(str(path.parent), depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(str(base_repo_dir), 'utils')\n",
    "folder_domain = os.path.join(str(base_repo_dir), 'source', 'domain')\n",
    "folder_data_input = os.path.join(str(base_repo_dir), '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(str(base_repo_dir), '_data', 'out_json')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "# from scraper_pasteur import PasteurScraper\n",
    "# from scraper_sucupira import SucupiraScraper\n",
    "# from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "from environment_setup import EnvironmentSetup\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from neo4j_persister import Neo4jPersister\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DictToHDF5, attribute_to_be_non_empty\n",
    "\n",
    "t00 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(folder_data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Carregar dados dos produtos prioritários, equipamentos e questões de pesquisa\n",
    "curr_pathfilename = os.path.join(folder_data_input,'docents_dict_list.json')\n",
    "with open(curr_pathfilename, 'r', encoding='utf-8') as f:\n",
    "    curriculos = json.load(f)\n",
    "\n",
    "prod_pathfilename = os.path.join(folder_data_output,'matriz_ceis.json')\n",
    "with open(prod_pathfilename, 'r', encoding='utf-8') as f:\n",
    "    matriz_produtos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(curriculos)} currículos carregados')\n",
    "# [x.get('Áreas') for x in curriculos]\n",
    "produtos = [produto.get('nome') for bloco in matriz_produtos.get('blocos', []) for produto in bloco.get('produtos', [])]\n",
    "print(f'{len(produtos)} produtos carregados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vega\n",
    "# !jupyter nbextension install --sys-prefix --py vega\n",
    "# !jupyter nbextension enable --sys-prefix --py vega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_validator import *\n",
    "\n",
    "# Exemplo de uso (substituir pelos dados reais)\n",
    "y_true = [[\"A\", \"B\"], [\"B\"], [\"A\", \"C\"]]\n",
    "y_pred = [[\"A\"], [\"B\", \"C\"], [\"A\", \"B\"]]\n",
    "classes = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "# Probabilidades aqui\n",
    "y_proba = [[0.8, 0.7, 0.3], [0.2, 0.9, 0.5], [0.6, 0.8, 0.4]]\n",
    "\n",
    "\n",
    "validar_modelo(y_true, y_pred, y_proba, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Criar grafo de conhecimento</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict = []\n",
    "equipamentos = [...]\n",
    "questoes_pesquisa = [...]\n",
    "produtos_prioritarios = []\n",
    "\n",
    "# Criar o grafo heterogêneo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar nós e arestas para pesquisadores\n",
    "for pesquisador in curriculos:\n",
    "    G.add_node(pesquisador['Identificação']['ID Lattes'], type=\"pesquisador\", **pesquisador)  # Adicionar atributos do currículo\n",
    "    \n",
    "    # Conectar pesquisador às suas áreas de atuação\n",
    "    for area in pesquisador['Áreas'].values():\n",
    "        G.add_node(area, type=\"area\")\n",
    "        G.add_edge(pesquisador['Identificação']['ID Lattes'], area)\n",
    "\n",
    "    # Conectar pesquisador às suas publicações\n",
    "    for publicacao in pesquisador['Produções']['Artigos completos publicados em periódicos']:\n",
    "        G.add_node(publicacao['titulo'], type=\"publicacao\")\n",
    "        G.add_edge(pesquisador['Identificação']['ID Lattes'], publicacao['titulo'])\n",
    "\n",
    "    # Conectar pesquisador a projetos\n",
    "    for pesquisador in curriculos:\n",
    "        for tipo_projeto in [\"ProjetosPesquisa\", \"ProjetosExtensão\", \"ProjetosDesenvolvimento\", \"ProjetosOutros\"]:\n",
    "            if tipo_projeto in pesquisador:\n",
    "                for projeto in pesquisador[tipo_projeto]:\n",
    "                    G.add_node(projeto['titulo_projeto'], type=\"projeto\")\n",
    "                    G.add_edge(pesquisador['Identificação']['ID Lattes'], projeto['titulo_projeto'])\n",
    "\n",
    "    # Conectar pesquisador a equipamentos\n",
    "    # (Assumindo que você tem uma lista de equipamentos mencionados nos currículos)\n",
    "    equipamentos_citados = []  # Preencha com os nomes dos equipamentos mencionados nos currículos\n",
    "    for pesquisador in curriculos:\n",
    "        for equipamento in equipamentos_citados:\n",
    "            if equipamento in pesquisador['Atuação Profissional'][0]['Descrição']:  # Exemplo: busca na descrição da atuação profissional\n",
    "                G.add_edge(pesquisador['Identificação']['ID Lattes'], equipamento)\n",
    "\n",
    "    # Conectar pesquisador a questões de pesquisa\n",
    "    # (Assumindo que você tem uma lista de questões de pesquisa e um método para associá-las aos pesquisadores)\n",
    "    for pesquisador in curriculos:\n",
    "        for questao in questoes_pesquisa:\n",
    "            if pesquisador_tem_interesse_na_questao(pesquisador, questao):  # inferência do interesse\n",
    "                G.add_edge(pesquisador['Identificação']['ID Lattes'], questao['descricao']) # type: ignore\n",
    "\n",
    "    # TO-DO\n",
    "    def calculate_similarity(pesquisador, list_dict):\n",
    "        \n",
    "        similarity=0\n",
    "        return similarity\n",
    "    \n",
    "    # TO-DO\n",
    "    def extract_topicos(questao, topico_pesquisa):\n",
    "        \n",
    "        lista_topicos=[]\n",
    "        return lista_topicos    \n",
    "\n",
    "    # Função para inferir o interesse do pesquisador em uma questão\n",
    "    def pesquisador_tem_interesse_na_questao(pesquisador, questao):\n",
    "        # Analise o currículo do pesquisador (áreas de atuação, publicações, projetos, etc.)\n",
    "        # e compare com a descrição da questão de pesquisa para determinar o interesse\n",
    "        # Retorna True se houver interesse, False caso contrário\n",
    "        topicos_pesquisador = extract_topicos(pesquisador, list_dict)\n",
    "        interesses_pesquisador = []\n",
    "        flag_interesse = False\n",
    "        threshold = 0.8\n",
    "        for i in topicos_pesquisador:\n",
    "            similarity = calculate_similarity(questao, i)\n",
    "            if similarity >= threshold:\n",
    "                interesses_pesquisador.append(questao)\n",
    "                flag_interesse = True\n",
    "\n",
    "        return flag_interesse\n",
    "\n",
    "\n",
    "# Adicionar nós e arestas para produtos prioritários, equipamentos e questões de pesquisa\n",
    "for produto in produtos_prioritarios:\n",
    "    G.add_node(produto['nome'], type=\"produto\", **produto)  # Adicionar atributos do produto (nome, descrição, área, etc.)\n",
    "\n",
    "    # Conectar produto às suas áreas (assumindo que o produto tem uma lista de áreas)\n",
    "    for area in produto.get('areas', []):  # Usar get() para evitar KeyError se 'areas' não existir\n",
    "        G.add_edge(produto['nome'], area)\n",
    "\n",
    "# Adicionar nós e arestas para equipamentos\n",
    "for equipamento in equipamentos:\n",
    "    G.add_node(equipamento['nome'], type=\"equipamento\", **equipamento)  # type: ignore # Adicionar atributos do equipamento\n",
    "\n",
    "    # Conectar equipamento às suas áreas (assumindo que o equipamento tem uma lista de áreas)\n",
    "    for area in equipamento.get('areas', []): # type: ignore\n",
    "        G.add_edge(equipamento['nome'], area) # type: ignore\n",
    "\n",
    "# Adicionar nós e arestas para questões de pesquisa\n",
    "for questao in questoes_pesquisa:\n",
    "    G.add_node(questao['descricao'], type=\"questao_pesquisa\", **questao)  # type: ignore # Adicionar atributos da questão\n",
    "\n",
    "    # Conectar questão de pesquisa às suas áreas (assumindo que a questão tem uma lista de áreas)\n",
    "    for area in questao.get('areas', []): # type: ignore\n",
    "        G.add_edge(questao['descricao'], area) # type: ignore\n",
    "\n",
    "# Adicionar nós e arestas para equipamentos\n",
    "for equipamento in equipamentos:\n",
    "    G.add_node(equipamento['nome'], type=\"equipamento\", **equipamento)  # type: ignore # Adicionar atributos do equipamento\n",
    "\n",
    "    # Conectar equipamento às suas áreas (assumindo que o equipamento tem uma lista de áreas)\n",
    "    if 'areas' in str(equipamento):  # Verificar se o equipamento possui áreas associadas\n",
    "        for area in equipamento['areas']: # type: ignore\n",
    "            if area in G.nodes:  # Verificar se a área já existe no grafo\n",
    "                G.add_edge(equipamento['nome'], area) # type: ignore\n",
    "            else:\n",
    "                # Se a área não existir, você pode decidir se quer adicioná-la como um novo nó\n",
    "                # G.add_node(area, type=\"area\")  \n",
    "                # G.add_edge(equipamento['nome'], area)\n",
    "                print(f\"Área '{area}' não encontrada para o equipamento '{equipamento['nome']}'.\") # type: ignore\n",
    "    else:\n",
    "        print(f\"Equipamento '{equipamento['nome']}' não possui áreas associadas.\") # type: ignore\n",
    "\n",
    "# Adicionar nós e arestas para questões de pesquisa\n",
    "for questao in questoes_pesquisa:\n",
    "    G.add_node(questao['descricao'], type=\"questao_pesquisa\", **questao)  # type: ignore # Adicionar atributos da questão\n",
    "\n",
    "    # Conectar questão de pesquisa às suas áreas\n",
    "    if 'areas' in str(questao):  # Verificar se a questão possui áreas associadas\n",
    "        for area in questao['areas']: # type: ignore\n",
    "            if area in G.nodes:  # Verificar se a área já existe no grafo\n",
    "                G.add_edge(questao['descricao'], area) # type: ignore\n",
    "            else:\n",
    "                # Se a área não existir, você pode decidir se quer adicioná-la como um novo nó\n",
    "                # G.add_node(area, type=\"area\")  \n",
    "                # G.add_edge(questao['descricao'], area)\n",
    "                print(f\"Área '{area}' não encontrada para a questão de pesquisa '{questao['descricao']}'.\") # type: ignore\n",
    "    else:\n",
    "        print(f\"Questão de pesquisa '{questao['descricao']}' não possui áreas associadas.\") # type: ignore\n",
    "\n",
    "# Análise 1: Agrupamento de Pesquisadores\n",
    "# Extrair características textuais dos currículos (ex: usando TF-IDF)\n",
    "# Exemplo usando a descrição da formação acadêmica\n",
    "corpus = [pesquisador['Formação']['Acadêmica'][0]['Descrição'] for pesquisador in curriculos]  \n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Calcular similaridade entre pesquisadores (ex: usando cosseno)\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "\n",
    "# Aplicar algoritmo de agrupamento (ex: DBSCAN)\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=0.3, min_samples=2).fit(similarity_matrix)\n",
    "labels = clustering.labels_\n",
    "\n",
    "# Adicionar atributo 'cluster' aos nós dos pesquisadores\n",
    "for i, pesquisador in enumerate(curriculos):\n",
    "    G.nodes[pesquisador['Identificação']['ID Lattes']]['cluster'] = labels[i]\n",
    "\n",
    "\n",
    "# Análise 2: Agrupamento de Questões de Pesquisa\n",
    "# Extrair características textuais das questões de pesquisa (ex: usando TF-IDF)\n",
    "corpus_questoes = [questao['descricao'] for questao in questoes_pesquisa] # type: ignore\n",
    "vectorizer_questoes = TfidfVectorizer()\n",
    "X_questoes = vectorizer_questoes.fit_transform(corpus_questoes)\n",
    "\n",
    "# Calcular similaridade entre questões (ex: usando cosseno)\n",
    "similarity_matrix_questoes = cosine_similarity(X_questoes)\n",
    "\n",
    "# Aplicar algoritmo de agrupamento (ex: K-Means)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 5  # Defina o número de clusters desejado\n",
    "clustering_questoes = KMeans(n_clusters=n_clusters).fit(similarity_matrix_questoes)\n",
    "labels_questoes = clustering_questoes.labels_\n",
    "\n",
    "# Adicionar atributo 'cluster' aos nós das questões de pesquisa\n",
    "for i, questao in enumerate(questoes_pesquisa):\n",
    "    G.nodes[questao['descricao']]['cluster'] = labels_questoes[i] # type: ignore\n",
    "\n",
    "# Análise 3: Recomendação de Projetos (exemplo simplificado)\n",
    "def recomendar_projetos(pesquisador_id):\n",
    "    pesquisador_areas = list(G.neighbors(pesquisador_id))  # Obter áreas do pesquisador\n",
    "    projetos_recomendados = []\n",
    "    for projeto_id in G.nodes:\n",
    "        if G.nodes[projeto_id]['type'] == \"projeto\":\n",
    "            projeto_areas = list(G.neighbors(projeto_id))\n",
    "            if set(pesquisador_areas) & set(projeto_areas):  # Verificar se há áreas em comum\n",
    "                projetos_recomendados.append(projeto_id)\n",
    "    return projetos_recomendados\n",
    "\n",
    "# Análise 4: Detecção de Oportunidades\n",
    "def detectar_oportunidades(G, produtos_prioritarios, top_n=5):\n",
    "    areas_importantes = {}\n",
    "    for produto in produtos_prioritarios:\n",
    "        for area in G.neighbors(produto['nome']):\n",
    "            areas_importantes[area] = areas_importantes.get(area, 0) + 1\n",
    "\n",
    "    # Ponderar pela concentração de pesquisadores e questões de pesquisa\n",
    "    for area in areas_importantes:\n",
    "        pesquisadores_na_area = len([n for n in G.neighbors(area) if G.nodes[n]['type'] == \"pesquisador\"])\n",
    "        questoes_na_area = len([n for n in G.neighbors(area) if G.nodes[n]['type'] == \"questao_pesquisa\"])\n",
    "        areas_importantes[area] *= (pesquisadores_na_area + questoes_na_area)\n",
    "\n",
    "    # Ordenar áreas por importância\n",
    "    areas_importantes = dict(sorted(areas_importantes.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return list(areas_importantes.keys())[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "print(stopwords.words('portuguese'))\n",
    "print(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade httpx\n",
    "# !pip install --upgrade httpcore\n",
    "# !pip install --upgrade googletrans==4.0.0-rc1\n",
    "# !pip install deep-translator\n",
    "\n",
    "import nltk\n",
    "print(nltk.data.find(\"corpora/stopwords\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_path = nltk.data.find(\"corpora/stopwords\")\n",
    "stopwords_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from deep_translator import GoogleTranslator  # Import the Googletrans library\n",
    "\n",
    "def identify_researcher_topics(data, fields, translate=False, target_language='en'):\n",
    "    \"\"\"\n",
    "    Identifies the top 3 research topics of a researcher based on their Lattes CV data.\n",
    "\n",
    "    Args:\n",
    "        data: A dictionary containing the researcher's Lattes CV data.\n",
    "        fields: A list of field names to be used for topic identification.\n",
    "\n",
    "    Returns:\n",
    "        A list of the top 3 research topics.\n",
    "    \"\"\"\n",
    "\n",
    "    def translate_text(text, target_language):\n",
    "        \"\"\"\n",
    "        Translates the text to the target language using Google Translate.\n",
    "        \"\"\"\n",
    "        translated = GoogleTranslator(source='auto', target=target_language).translate(text)\n",
    "        return translated\n",
    "\n",
    "    def extract_text_from_fields(data, fields, corpus):\n",
    "        \"\"\"\n",
    "        Recursively extracts text from the specified fields in the data dictionary.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if key in fields:\n",
    "                    if isinstance(value, list):\n",
    "                        for item in value:\n",
    "                            corpus.append(item.get(\"Descricao\", \"\") + item.get(\"titulo\", \"\"))\n",
    "                    else:\n",
    "                        corpus.append(str(value))  # Convert non-string values to string\n",
    "                else:\n",
    "                    extract_text_from_fields(value, fields, corpus)\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                extract_text_from_fields(item, fields, corpus)\n",
    "\n",
    "    # def preprocess_text(text, target_language='en'):\n",
    "    #     \"\"\"\n",
    "    #     Preprocesses the text by tokenizing, removing stopwords and proper nouns, translating, and removing punctuation.\n",
    "    #     \"\"\"\n",
    "    #     # Tokenize the text\n",
    "    #     words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    #     # Remove stopwords in the original language (if available)\n",
    "    #     source_language = detect(text)\n",
    "    #     try:\n",
    "    #         words = [word for word in words if word not in stopwords.words(source_language) and not word.istitle()]\n",
    "    #     except OSError:\n",
    "    #         print(f\"Warning: Stopwords not found for language '{source_language}'. Skipping stopword removal.\")\n",
    "\n",
    "    #     # Translate the text (if it's not already in the target language)\n",
    "    #     if target_language != 'auto' and source_language != target_language:\n",
    "    #         translator = GoogleTranslator(source=source_language, target=target_language)\n",
    "    #         text = translator.translate(text)\n",
    "\n",
    "    #         # Tokenize the translated text\n",
    "    #         words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    #         # Remove stopwords in the target language\n",
    "    #         words = [word for word in words if word not in stopwords.words(target_language)]\n",
    "\n",
    "    #     # Remove punctuation and numbers\n",
    "    #     words = [re.sub(r'[^\\w\\s]', '', word) for word in words if not word.isdigit()]\n",
    "\n",
    "    #     return \" \".join(words)\n",
    "\n",
    "    def preprocess_text(text, target_language='en'):\n",
    "        \"\"\"\n",
    "        Preprocesses the text by tokenizing, removing stopwords and proper nouns, translating, and removing punctuation.\n",
    "        \"\"\"\n",
    "        # Tokenize the text\n",
    "        words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "        # Remove stopwords in the original language (if available)\n",
    "        source_language = detect(text)\n",
    "        stopwords_path = nltk.data.find(\"corpora/stopwords\")\n",
    "        # stopwords_path = os.path.join(os.getenv('APPDATA'), 'Roaming', 'nltk_data', 'corpora', 'stopwords')  # Get stopwords path\n",
    "        if os.path.exists(os.path.join(str(stopwords_path), source_language)):\n",
    "            with open(os.path.join(str(stopwords_path), source_language), 'r', encoding='utf-8') as f:\n",
    "                stop_words = set(f.read().splitlines())\n",
    "            words = [word for word in words if word not in stop_words and not word.istitle()]\n",
    "\n",
    "        # Translate the text (if it's not already in the target language)\n",
    "        if target_language != 'auto' and source_language != target_language:\n",
    "            translator = GoogleTranslator(source=source_language, target=target_language)\n",
    "            text = translator.translate(text)\n",
    "\n",
    "            # Tokenize the translated text\n",
    "            words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "            # Remove stopwords in the target language\n",
    "            with open(os.path.join(str(stopwords_path), target_language), 'r', encoding='utf-8') as f:\n",
    "                stop_words = set(f.read().splitlines())\n",
    "            words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Remove punctuation and numbers\n",
    "        words = [re.sub(r'[^\\w\\s]', '', word) for word in words if not word.isdigit()]\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "    # Concatenate text from specified fields\n",
    "    corpus = []\n",
    "    extract_text_from_fields(data, fields, corpus)\n",
    "\n",
    "    # Preprocess the corpus\n",
    "    corpus = [preprocess_text(text) for text in corpus]\n",
    "\n",
    "    # Vectorize the text using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Apply LDA for topic modeling\n",
    "    lda = LatentDirichletAllocation(n_components=7, random_state=0)\n",
    "    lda.fit(X)\n",
    "\n",
    "    # Get the top words for each topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words.append([feature_names[i] for i in topic.argsort()[:-4:-1]])\n",
    "\n",
    "    return top_words\n",
    "\n",
    "pathfilename = os.path.join(folder_data_input,'docents_dict_list.json')\n",
    "with open(pathfilename, 'r', encoding='utf-8') as file:\n",
    "    docents_data = json.load(file)\n",
    "    print(f'{len(docents_data)} currículos carregados')\n",
    "\n",
    "# fields_to_use = [\"Formação Acadêmica\", \"Atuação Profissional\", \"Produções\"]\n",
    "fields_to_use = [\"titulo\"]\n",
    "\n",
    "for researcher in docents_data:\n",
    "    topics = identify_researcher_topics(researcher, fields_to_use)\n",
    "    print(f\"Principais tópicos de interesse para {researcher['Identificação']['Nome']}:\")\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"  Tópico {i+1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "oportunidades = detectar_oportunidades(G, produtos_prioritarios)\n",
    "print(\"Áreas de oportunidade:\", oportunidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo\n",
    "import requests\n",
    "\n",
    "# Cabeçalhos para a requisição\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "import http.client\n",
    "import ssl\n",
    "\n",
    "# Cria um contexto SSL sem verificação de certificado\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "# conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\")\n",
    "\n",
    "conn.request(\"GET\", \"/cities/filters?language=en\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import pandas as pd\n",
    "import ssl\n",
    "\n",
    "# Cria um contexto SSL sem verificação de certificado (NÃO RECOMENDADO PARA PRODUÇÃO)\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "# Função para fazer a requisição com tratamento de erro RETORNA OBJETO BYTES\n",
    "# def fazer_requisicao(conn, endpoint, method=\"GET\", params=None, body=None):\n",
    "#     try:\n",
    "#         # Cabeçalhos para a requisição\n",
    "#         headers = {\n",
    "#             \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "#             \"Accept\": \"application/json\",\n",
    "#         }\n",
    "#         conn.request(method, endpoint, body=body, headers=headers)\n",
    "#         res = conn.getresponse()\n",
    "#         data = res.read()\n",
    "#         return json.loads(data.decode(\"utf-8\"))\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro na requisição: {e}\")\n",
    "#         return None\n",
    "\n",
    "# Função para fazer a requisição com tratamento de erro e decodificação de bytes\n",
    "def fazer_requisicao(conn, endpoint, method=\"GET\", params=None, body=None):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        }\n",
    "        conn.request(method, endpoint, body=body, headers=headers)\n",
    "        res = conn.getresponse()\n",
    "\n",
    "        # Lê os dados da resposta como bytes\n",
    "        data = res.read()  \n",
    "\n",
    "        # Decodifica os bytes para uma string UTF-8\n",
    "        data_str = data.decode(\"utf-8\")  \n",
    "\n",
    "        # Converte a string JSON para um dicionário Python\n",
    "        return json.loads(data_str)  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na requisição: {e}\")\n",
    "        return None\n",
    "\n",
    "# Conexão com a API\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "\n",
    "# Endpoint para obter os anos disponíveis\n",
    "anos_url = \"/cities/dates/years\"\n",
    "anos_response = fazer_requisicao(conn, anos_url)\n",
    "\n",
    "# Inicializa as variáveis com valores padrão\n",
    "ano_inicial = 1997  # Ano inicial padrão\n",
    "ano_final = 2024   # Ano final padrão (ou o ano atual)\n",
    "\n",
    "if anos_response:\n",
    "    if anos_response.get('success', False):  # Verifica se a requisição foi bem-sucedida\n",
    "        if 'data' in anos_response and 'min' in anos_response['data'] and 'max' in anos_response['data']:\n",
    "            ano_inicial = int(anos_response['data']['min'])\n",
    "            ano_final = int(anos_response['data']['max'])\n",
    "        else:\n",
    "            print(\"Erro: As chaves 'data', 'min' e/ou 'max' não foram encontradas na resposta da API.\")\n",
    "    else:\n",
    "        print(f\"Erro na requisição para obter os anos: {anos_response}\")\n",
    "else:\n",
    "    print(\"Erro na requisição para obter os anos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anos_response['data']['min']) # type: ignore\n",
    "print(anos_response['data']['max']) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.request(\"GET\", \"/cities/filters?language=en\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint para obter os valores do filtros disponíveis na API\n",
    "endpoint = \"/cities/filters\"\n",
    "filters_params = {\"language\": \"pt\"}\n",
    "filters_response = fazer_requisicao(conn, endpoint, params=filters_params)\n",
    "filters_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtros = [x['filter'] for x in filters_response.get('data').get('list')] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_options_filters(api_filter):\n",
    "    # Cria um contexto SSL sem verificação de certificado\n",
    "    context = ssl._create_unverified_context()\n",
    "    conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "    conn.request(\"GET\", f\"/general/filters/{api_filter}?language=pt\")\n",
    "\n",
    "    res = conn.getresponse()\n",
    "    # print(f'objeto       res: {type(res)}')\n",
    "    data = res.read()\n",
    "    # print(f'objeto      data: {type(data)}')\n",
    "    data_str = data.decode(\"utf-8\")\n",
    "    # print(f'objeto  data_str: {type(data_str)}')\n",
    "    data_json = json.loads(data_str)\n",
    "    # print(f'objeto data_json: {type(data_json)}')\n",
    "    try:\n",
    "        results = [x.get('text') for x in data_json.get('data')[0]]\n",
    "        print(f'{len(results):>4} resultados para filtro {api_filter}')\n",
    "    except Exception as e:\n",
    "        print(f'     Erro ao buscar dados para filtro {api_filter}: {e}')\n",
    "    return [results][0]\n",
    "\n",
    "todos_campos_filtro={}\n",
    "for n,i in enumerate(filtros):\n",
    "    try:\n",
    "        todos_campos_filtro[i] = get_options_filters(i)\n",
    "    except Exception as e:\n",
    "        print(f'     Filtro {i} não disponível na API. Erro: {e}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicos=[]\n",
    "[unicos.append(x) for x in todos_campos_filtro['economicBlock'] if x not in unicos]\n",
    "unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicos=[]\n",
    "[unicos.append(x) for x in todos_campos_filtro['state'] if x not in unicos]\n",
    "unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_interesse = [\n",
    "    'VI - Produtos das indústrias químicas ou indústrias conexas', \n",
    "    'XVIII - Instrumentos e aparelhos de ótica, fotografia ou cinematografia, medida, controle ou de precisão; Instrumentos e aparelhos médico-cirúrgicos; Relógios e aparelhos semelhantes; Instrumentos musicais; Suas partes e acessórios',\n",
    "    'XX - Mercadorias e produtos diversos',\n",
    "    'XXII - Transações especiais'\n",
    "    ]\n",
    "\n",
    "chapter_interesse = [\n",
    "    '28 - Produtos químicos inorgânicos; compostos inorgânicos ou orgânicos de metais preciosos, de elementos radioativos, de metais das terras raras ou de isótopos',\n",
    "    '29 - Produtos químicos orgânicos',\n",
    "    '30 - Produtos farmacêuticos',\n",
    "    '35 - Matérias albuminóides; produtos à base de amidos ou de féculas modificados; colas; enzimas',\n",
    "    '38 - Produtos diversos das indústrias químicas',\n",
    "    ]\n",
    "\n",
    "heading_interesse = [\n",
    "    '2801 - Flúor, cloro, bromo e iodo',\n",
    "    '2802 - Enxofre sublimado ou precipitado; enxofre coloidal',\n",
    "    '2803 - Carbono (negros-de-carbono e outras formas não compreendidas em outras posições)',\n",
    "    '2804 - Hidrogénio, gases raros e outros elementos não metálicos',\n",
    "    '2805 - Metais alcalinos ou alcalino-terrosos; metais de terras raras, escândio e ítrio, mesmo misturados ou ligados entre si; mercúrio',\n",
    "    '2806 - Cloreto de hidrogénio (ácido clorídrico); ácido clorossulfúrico',\n",
    "    '2807 - Ácido sulfúrico e ácido sulfúrico fumante (oleum)',\n",
    "    '2808 - Ácido nítrico; ácidos sulfonítricos',\n",
    "    '2809 - Pentóxido de difosfóro; ácido fosfórico; ácidos polifosfóricos, de constituição química definida ou não',\n",
    "    '2810 - Óxidos de boro; ácidos bóricos',\n",
    "    '2811 - Outros ácidos inorgânicos e outros compostos oxigenados inorgânicos dos elementos não metálicos',\n",
    "    '2812 - Halogenetos e oxialogenetos dos elementos não metálicos',\n",
    "    '2813 - Sulfuretos dos elementos não metálicos; trissulfureto de fósforo comercial',\n",
    "    '2814 - Amoníaco anidro ou em solução aquosa (amónia)',\n",
    "    '2815 - Hidróxido de sódio (soda cáustica); hidróxido de potássio (potassa cáustica); peróxidos de sódio ou de potássio',\n",
    "    '2816 - Hidróxido e peróxido de magnésio; óxidos, hidróxidos e peróxidos, de estrôncio ou de bário',\n",
    "    '2817 - Óxido de zinco; peróxido de zinco',\n",
    "    '2818 - Corindo artificial, quimicamente definido ou não; óxido de alumínio; hidróxido de alumínio',\n",
    "    '2819 - Óxidos e hidróxidos de crómio',\n",
    "    '2820 - Óxidos de manganés',\n",
    "    '2821 - Óxidos e hidróxidos de ferro; terras corantes contendo, em peso, 70\\xa0% ou mais de ferro combinado, expresso em Fe2O3',\n",
    "    '2822 - Óxidos e hidróxidos de cobalto, inclusive os comerciais',\n",
    "    '2823 - Óxidos de titânio',\n",
    "    '2824 - Óxidos de chumbo; mínio (zarcão) e mínio-laranja (mine-orange)',\n",
    "    '2825 - Hidrazina e hidroxilamina, e seus sais inorgânicos; outras bases inorgânicas; outros óxidos, hidróxidos e peróxidos, de metais',\n",
    "    '2826 - Fluoretos; fluorossilicatos, fluoroaluminatos e outros sais complexos de flúor',\n",
    "    '2827 - Cloretos, oxicloretos e hidroxicloretos; brometos e oxibrometos; iodetos e oxiiodetos',\n",
    "    '2828 - Hipocloritos; hipoclorito de cálcio comercial; cloritos; hipobromitos',\n",
    "    '2829 - Cloratos e percloratos; bromatos e perbromatos; iodatos e periodatos',\n",
    "    '2830 - Sulfuretos; polissulfuretos, de constituição química definida ou não',\n",
    "    '2831 - Ditionites e sulfoxilatos',\n",
    "    '2832 - Sulfitos; tiosulfatos',\n",
    "    '2833 - Sulfatos; alúmenes; peroxosulfatos (persulfatos)',\n",
    "    '2834 - Nitritos; nitratos',\n",
    "    '2835 - Fosfinatos (hipofosfitos), fosfonatos (fosfitos) e fosfatos; polifosfatos, de constituição química definida ou não:',\n",
    "    '2836 - Carbonatos; peroxocarbonatos (percarbonatos); carbonato de amónio comercial contendo carbamato de amónio',\n",
    "    '2837 - Cianetos, oxicianetos e cianetos complexos',\n",
    "    '2838 - Fulminatos, cianatos e tiocianatos',\n",
    "    '2839 - Silicatos; silicatos dos metais alcalinos comerciais',\n",
    "    '2840 - Boratos; peroxoboratos (perboratos)',\n",
    "    '2841 - Sais dos ácidos oxometálicos ou peroxometálicos',\n",
    "    '2842 - Outros sais dos ácidos ou peroxoácidos inorgânicos (incluindo aluminossilicatos de constituição química definida ou não), exceto azidas',\n",
    "    '2843 - Metais preciosos no estado coloidal; compostos inorgânicos ou orgânicos de metais preciosos, de constituição química definida ou não; amálgamas de metais preciosos',\n",
    "    '2844 - Elementos químicos radioactivos e isótopos radioactivos (incluídos os elementos químicos e isótopos cindíveis ou férteis), e seus compostos; misturas e resíduos contendo esses produtos',\n",
    "    '2845 - Isótopos não incluídos na posição\\xa02844; seus compostos inorgânicos ou orgânicos, de constituição química definida ou não',\n",
    "    '2846 - Compostos, inorgânicos ou orgânicos, dos metais das terras raras, de ítrio ou de escândio ou das misturas destes metais',\n",
    "    '2847 - Peróxido de hidrogênio (água oxigenada), mesmo solidificado com ureia',\n",
    "    '2848 - Fosfetos, exceto ferrofósforos, quimicamente definidos ou não',\n",
    "    '2849 - Carbonetos de constituição química definida ou não',\n",
    "    '2850 - Hidretos, nitretos, azidas, silicietos e boretos, quimicamente definidos ou não',\n",
    "    '2851 - Compostos inorgânicos nesoi: liq ar: amálgamas nesoi',\n",
    "    '2852 - Compostos, inorgânicos ou orgânicos, de mercúrio, de constituição química definida ou não, exceto as amálgamas',\n",
    "    '2853 - Outros compostos inorgânicos (incluídas as águas destiladas, de condutibilidade ou de igual grau de pureza); ar líquido (incluído o ar líquido cujos gases raros foram eliminados); ar comprimido; amálgamas, exceto de metais preciosos.',\n",
    "    '2901 - Hidrocarbonetos acíclicos',\n",
    "    '2902 - Hidrocarbonetos cíclicos',\n",
    "    '2903 - Derivados halogenados dos hidrocarbonetos',\n",
    "    '2904 - Derivados sulfonados, nitrados ou nitrosados dos hidrocarbonetos, mesmo halogenados',\n",
    "    '2905 - Álcoois acíclicos e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2906 - Álcoois cíclicos e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2907 - Fenóis; fenóis-álcoois',\n",
    "    '2908 - Derivados halogenados, sulfonados, nitrados ou nitrosados dos fenóis ou dos fenóis-álcoois',\n",
    "    '2909 - Éteres, éteres-álcoois, éteres-fenóis, éteres-álcoois-fenóis, peróxidos de álcoois, peróxidos de éteres, peróxidos de cetonas (de constituição química definida ou não), e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2910 - Epóxidos, epoxi-álcoois, epoxi-fenóis e epoxi-éteres, com três átomos no ciclo, e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2911 - Acetais, semi-acetais, mesmo contendo outras funções oxigenadas, e seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2912 - Aldeídos, mesmo contendo outras funções oxigenadas; polímeros cíclicos dos aldeídos; paraformaldeído',\n",
    "    '2913 - Derivados halogenados, sulfonados, nitrados ou nitrosados dos produtos da posição 2912',\n",
    "    '2914 - Cetonas e quinonas, mesmo contendo outras funções oxigenadas, e seus derivados halogenados, sulfonados, nitratos ou nitrosados',\n",
    "    '2915 - Ácidos monocarboxílicos acíclicos saturados e seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2916 - Ácidos monocarboxílicos acíclicos não saturados e ácidos monocarboxílicos cíclicos, seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2917 - Ácidos policarboxílicos, seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2918 - Ácidos carboxílicos contendo funções oxigenadas suplementares e seus anidridos, halogenetos, peróxidos e peroxiácidos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2919 - Ésteres fosfóricos e seus sais, incluindo os lactofosfatos; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2920 - Ésteres de outros ácidos inorgânicos de não-metais (exceto os ésteres de halogenetos de hidrogénio) e seus sais; seus derivados halogenados, sulfonados, nitrados ou nitrosados',\n",
    "    '2921 - Compostos de função amina',\n",
    "    '2922 - Compostos aminados de funções oxigenadas',\n",
    "    '2923 - Sais e hidróxidos de amónio quaternários; lecitinas e outros fosfoaminolípidos, de constitução química definida ou não',\n",
    "    '2924 - Compostos de função carboxiamida; compostos de função amida do ácido carbónico',\n",
    "    '2925 - Compostos de função carboxiimida (incluindo a sacarina e seus sais) ou de função imina',\n",
    "    '2926 - Compostos de função nitrilo',\n",
    "    '2927 - Compostos diazóicos, azóicos e azóxicos',\n",
    "    '2928 - Derivados orgânicos da hidrazina e hidroxilamina',\n",
    "    '2929 - Compostos de outras funções azotadas (nitrogenadas)',\n",
    "    '2930 - Tiocompostos orgânicos',\n",
    "    '2931 - Outros compostos organo-inorgânicos',\n",
    "    '2932 - Compostos heterocíclicos exclusivamente de hetero-átomo(s) de oxigénio',\n",
    "    '2933 - Compostos heterocíclicos, exclusivamente de hetero-átomo(s) de azoto (nitrogénio)',\n",
    "    '2934 - Ácidos nucleicos e seus sais, de constituição química definida ou não; outros compostos heterocíclicos',\n",
    "    '2935 - Sulfonamidas',\n",
    "    '2936 - Provitaminas e vitaminas, naturais ou sintéticas (incluídos os concentrados naturais), bem como os seus derivados utilizados principalmente como vitaminas, misturados ou não entre si, mesmo em quaisquer soluções',\n",
    "    '2937 - Hormonas, prostaglandinas, tromboxanos e leucotrienos, naturais ou reproduzidos por síntese; seus derivados e análogos estruturais, incluindo os polipéptidos de cadeia modificada, utilizados principalmente como hormonas',\n",
    "    '2938 - Heterósidos, naturais ou sintéticos, seus sais, éteres, ésteres e outros derivados',\n",
    "    '2939 - Alcalóides vegetais, naturais ou sintéticos, seus sais, éteres, ésteres e outros derivados',\n",
    "    '2940 - Açúcares quimicamente puros, exceto sacarose, lactose, maltose, glicose e frutose; seus éteres e ésteres e seus sais',\n",
    "    '2941 - Antibióticos',\n",
    "    '2942 - Outros compostos orgânicos',\n",
    "    '3001 - Glândulas e outros órgãos para usos opoterápicos, dessecados, mesmo em pó; extractos de glândulas ou de outros órgãos ou das suas secreções, para usos opoterápicos; heparina e seus sais; outras substâncias humanas ou animais preparadas para fins terapêuti',\n",
    "    '3002 - Sangue humano; sangue animal preparado para usos terapêuticos, profilácticos ou de diagnóstico; anti-soros, outras fracções do sangue, produtos imunológicos modificados, mesmo obtidos por via biotecnológica; vacinas, toxinas, culturas de microrganismos (e',\n",
    "    '3003 - Medicamentos (exceto os produtos das posições\\xa03002, 3005\\xa0ou\\xa03006) constituídos por produtos misturados entre si, preparados para fins terapêuticos ou profilácticos, mas não apresentados em doses nem acondicionados para venda a retalho',\n",
    "    '3004 - Medicamentos (exceto os produtos das posições\\xa03002, 3005\\xa0ou\\xa03006) constituídos por produtos misturados ou não misturados, preparados para fins terapêuticos ou profilácticos, apresentados em doses (incluindo os destinados a serem administrados por via sub',\n",
    "    '3005 - Pastas (ouates), gazes, ataduras e artigos análogos (por exemplo: pensos, esparadrapos, sinapismos), impregnados ou recobertos de substâncias farmacêuticas ou acondicionados para venda a retalho para usos medicinais, cirúrgicos, dentários ou veterinários',\n",
    "    '3006 - Preparações e artigos farmacêuticos indicados na Nota\\xa04\\xa0do presente capítulo',\n",
    "    '3501 - Caseínas, caseinatos e outros derivados das caseínas; colas de caseína',\n",
    "    '3502 - Albuminas (incluídos os concentrados de várias proteínas de soro de leite, contendo, em peso calculado sobre matéria seca, mais de\\xa080\\xa0% de proteínas do soro de leite), albuminatos e outros derivados das albuminas',\n",
    "    '3503 - Gelatinas e seus derivados; ictiocola e outras colas de origem animal, exceto cola de caseína',\n",
    "    '3504 - Peptonas e seus derivados; outras matérias protéicas e seus derivados; pó de peles',\n",
    "    '3505 - Dextrina e outros amidos e féculas modificados (por exemplo: amidos e féculas pré-gelatinizados ou esterificados); colas à base de amidos ou de féculas, de dextrina ou de outros amidos ou féculas modificados',\n",
    "    '3506 - Colas e outros adesivos preparados, não especificados nem compreendidos em outras posições; produtos de qualquer espécie utilizados como colas ou adesivos, acondicionados para venda a retalho como colas ou adesivos, com peso líquido não superior a\\xa01\\xa0kg',\n",
    "    '3507 - Enzimas; enzimas preparadas não especificadas nem compreendidas em outras posições',\n",
    "    '3821 - Meios de cultura preparados para o desenvolvimento e a manutenção de microrganismos (incluindo os vírus e os organismos similares) ou de células vegetais, humanas ou animais',\n",
    "    '3822 - Reagentes de diagnóstico ou de laboratório, em qualquer suporte ou preparados, exceto os das posições 3002 ou 3006; materiais de referência certificados',\n",
    "    '3823 - Ácidos gordos monocarboxílicos industriais; óleos ácidos de refinação; alcoóis gordos industriais',\n",
    "    '3824 - Aglutinantes preparados para moldes ou para núcleos de fundição; produtos químicos e preparações das indústrias químicas ou das indústrias conexas (incluídos os constituídos por misturas de produtos naturais), não especificados nem compreendidos noutras p',\n",
    "    '3825 - Produtos residuais das indústrias químicas ou das indústrias conexas, não especificados nem compreendidos em outras posições; resíduos municipais; lamas de depuração; outros resíduos mencionados na Nota\\xa06 do presente capítulo',\n",
    "    '3901 - Polímeros de etileno, em formas primárias',\n",
    "    '3902 - Polímeros de propileno ou de outras olefinas, em formas primárias',\n",
    "    '3903 - Polímeros de estireno, em formas primárias',\n",
    "    '3904 - Polímeros de cloreto de vinilo ou de outras olefinas halogenadas, em formas primárias',\n",
    "    '3905 - Polímeros de acetato de vinilo ou de outros ésteres de vinilo, em formas primárias; outros polímeros de vinilo, em formas primárias',\n",
    "    '3906 - Polímeros acrílicos, em formas primárias',\n",
    "    '3907 - Poliacetais, outros poliéteres e resinas epóxidas, em formas primárias; policarbonatos, resinas alquídicas, poliésteres alílicos e outros poliésteres, em formas primárias',\n",
    "    '3908 - Poliamidas em formas primárias',\n",
    "    '3909 - Resinas amínicas, resinas fenólicas e poliuretanos, em formas primárias',\n",
    "    '3910 - Silicones, em formas primárias',\n",
    "    '8417 - Fornos industriais ou de laboratório, incluídos os incineradores, não elétricos',\n",
    "    '8418 - Refrigeradores, congeladores (freezers) e outro material, máquinas e aparelhos para a produção de frio, com equipamento eléctrico ou outro; bombas de calor, excluídas as máquinas e aparelhos de ar condicionado da posição 8415',\n",
    "    '8419 - Aparelhos e dispositivos, mesmo aquecidos electricamente (exceto fornos e outros aparelhos da posição 8514), para tratamento de matérias por meio de operações que impliquem mudança de temperatura, tais como o aquecimento, cozimento, torrefacção, destilaç',\n",
    "    '8420 - Calandras e laminadores, exceto os destinados ao tratamento de metais ou vidro, e seus cilindros',\n",
    "    '8421 - Centrifugadores, incluídos os secadores centrífugos, aparelhos para filtrar ou depurar líquidos ou gases',\n",
    "    '8423 - Aparelhos e instrumentos de pesagem, incluídas as básculas e balanças para verificar peças fabricadas, excluídas as balanças sensíveis a pesos não superiores a 5 cg; pesos para quaisquer balanças',\n",
    "    '8471 - Máquinas automáticas para processamento de dados e suas unidades; leitores magnéticos ou ópticos, máquinas para registar dados em suporte sob forma codificada, e máquinas para processamento desses dados, não especificadas nem compreendidas em outras posiç',\n",
    "    '8472 - Outras máquinas e aparelhos de escritório [por exemplo: duplicadores hectográficos ou a stencil, máquinas para imprimir endereços, distribuidores automáticos de papel-moeda, máquinas para seleccionar, contar ou empacotar moedas, afiadores (apontadores) me',\n",
    "    '9011 - Microscópios ópticos, incluídos os microscópios para fotomicrografia, cinefotomicrografia ou microprojecção',\n",
    "    '9012 - Microscópios, exceto ópticos; difractógrafos',\n",
    "    '9013 - Dispositivos de cristais líquidos que não constituam artigos compreendidos mais especificamente em outras posições; lasers, exceto díodos laser; outros aparelhos e instrumentos de óptica, não especificados nem compreendidos em outras posições do presente',\n",
    "    '9016 - Balanças sensíveis a pesos >= 5 cg, com ou sem pesos',\n",
    "    '9021 - Artigos e aparelhos ortopédicos, incluídas as cintas e fundas médico-cirúrgicas e as muletas; talas, goteiras e outros artigos e aparelhos para fracturas; artigos e aparelhos de prótese; aparelhos para facilitar a audição dos surdos e outros aparelhos par',\n",
    "    '9022 - Aparelhos de raios X e aparelhos que utilizem as radiações alfa, beta ou gama, mesmo para usos médicos, cirúrgicos, odontológicos ou veterinários, incluídos os aparelhos de radiofotografia ou de radioterapia, os tubos de raios X e outros dispositivos gera',\n",
    "    '9023 - Instrumentos, aparelhos e modelos, concebidos para demonstração (por exemplo, no ensino e nas exposições), não suscetíveis de outros usos',\n",
    "    '9024 - Máquinas e aparelhos para ensaios de dureza, tracção, compressão, elasticidade e de outras propriedades mecânicas de materiais (por exemplo: metais, madeira, têxteis, papel, plásticos)',\n",
    "    '9025 - Densímetros, areómetros, pesa-líquidos e instrumentos flutuantes semelhantes, termómetros, pirómetros, barómetros, higrómetros e psicrómetros, registadores ou não, mesmo combinados entre si',\n",
    "    '9026 - Instrumentos e aparelhos para medida ou controlo do caudal (vazão), do nível, da pressão ou de outras características variáveis dos líquidos ou gases (por exemplo: medidores de caudal, indicadores de nível, manómetros, contadores de calor), exceto os ins',\n",
    "    '9027 - Instrumentos e aparelhos para análises físicas ou químicas (por exemplo: polarímetros, refractómetros, espectrómetros, analisadores de gases ou de fumos); instrumentos e aparelhos para ensaios de viscosidade, porosidade, dilatação, tensão superficial ou s',\n",
    "    '9028 - Contadores de gases, de líquidos ou de electricidade, incluídos os aparelhos para a sua aferição',\n",
    "    '9029 - Outros contadores (por exemplo: contadores de voltas, contadores de produção, taxímetros, totalizadores de caminho percorrido, podómetros); indicadores de velocidade e tacómetros, exceto os das posições 9014 ou 9015; estroboscópios',\n",
    "    '9030 - Osciloscópios, analisadores de espectro e outros instrumentos e aparelhos para medida ou controlo de grandezas elétricas; instrumentos e aparelhos para medida ou detecção de radiações alfa, beta, gama, X, cósmicas ou outras radiações ionizantes',\n",
    "    '9031 - Instrumentos, aparelhos e máquinas de medida ou controlo, não especificados nem compreendidos em outras posições do presente capítulo; projectores de perfis',\n",
    "    '9032 - Instrumentos e aparelhos para regulação ou controlo, automáticos',\n",
    "    '9033 - Partes e acessórios não especificados nem compreendidos noutras posições do presente Capítulo, para máquinas, aparelhos, instrumentos ou artigos do Capítulo 90',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "conn.request(\"GET\", \"/tables/ncm/02042200\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "conn.request(\"GET\", \"/tables/ncm?language=pt\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesse = []\n",
    "# f = filtros[0]\n",
    "# print(f)\n",
    "# for n,i in enumerate(todos_campos_filtro.get(f)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Endpoint para obter os valores por filtro disponíveis na API\n",
    "# # Lista os valores disponíveis para um filtro específico. Exemplos de como acessar os valores possíveis por diferentes filtros:\n",
    "# # Países: /general/filters/country?language=pt\n",
    "# # Blocos Econômicos: /general/filters/economicBlock?language=pt\n",
    "# # Seções (do Sistema Harmonizado - SH): /general/filters/section?language=pt\n",
    "# # NCM (Nomenclatura Comum do Mercosul): /general/filters/ncm?language=pt\n",
    "\n",
    "# import http.client\n",
    "\n",
    "# # Cria um contexto SSL sem verificação de certificado\n",
    "# context = ssl._create_unverified_context()\n",
    "# conn = http.client.HTTPSConnection(\"api-comexstat.mdic.gov.br\", context=context)\n",
    "# conn.request(\"GET\", \"/general/filters/heading?language=pt\")\n",
    "\n",
    "# res = conn.getresponse()\n",
    "# print(f'objeto       res: {type(res)}')\n",
    "# data = res.read()\n",
    "# print(f'objeto      data: {type(data)}')\n",
    "# data_str = data.decode(\"utf-8\")\n",
    "# print(f'objeto  data_str: {type(data_str)}')\n",
    "# data_json = json.loads(data_str)\n",
    "# print(f'objeto data_json: {type(data_json)}')\n",
    "\n",
    "# Lista de campos no filtro heading\n",
    "# heading_list = [y.get('text') for y in data_json.get('data')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de campos no filtro chapter\n",
    "campos = ['country', 'economicBlock', 'state', 'city', 'heading', 'chapter', 'section']\n",
    "filter='economicBlock'\n",
    "api_filters = ['']\n",
    "list_country = get_options_filters('country')\n",
    "list_economicBlock = get_options_filters('economicBlock')\n",
    "list_state = get_options_filters('state')\n",
    "list_heading = get_options_filters('heading')\n",
    "list_chapter = get_options_filters('chapter')\n",
    "list_section = get_options_filters('section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Inicializa a lista de NCMs de saúde com valores padrão\n",
    "# ncms_saude = []  # Lista vazia caso a requisição falhe\n",
    "\n",
    "# if ncm_response:\n",
    "#     ncms = ncm_response.json()\n",
    "#     # Filtrar os NCMs relacionados à saúde (exemplo)\n",
    "#     ncms_saude = [ncm['id'] for ncm in ncms if ncm['desc'].startswith(\"Medicamentos\")]\n",
    "\n",
    "# # Endpoint para consulta dos dados\n",
    "# consulta_url = \"/cities\"\n",
    "\n",
    "# # Parâmetros da consulta (exemplo)\n",
    "# params = {\n",
    "#     \"flow\": [\"export\", \"import\"],\n",
    "#     \"monthDetail\": False,\n",
    "#     \"period\": {\"from\": f\"{ano_inicial}-01\", \"to\": f\"{ano_final}-12\"},\n",
    "#     \"filters\": [{\"filter\": \"ncm\", \"values\": ncms_saude}],\n",
    "#     \"details\": [\"ncm\"],\n",
    "#     \"metrics\": [\"metricFOB\"],\n",
    "# }\n",
    "\n",
    "# # Realiza a consulta\n",
    "# response = fazer_requisicao(conn, consulta_url, params=params)\n",
    "\n",
    "# if response:\n",
    "#     data = response\n",
    "#     df = pd.DataFrame(data['data'])\n",
    "\n",
    "#     # Calcula o déficit por ano e NCM\n",
    "#     df_pivot = df.pivot_table(\n",
    "#         index=[\"year\", \"ncm\"], columns=\"flow\", values=\"metricFOB\", aggfunc=\"sum\"\n",
    "#     )\n",
    "#     df_pivot[\"deficit\"] = df_pivot[\"import\"] - df_pivot[\"export\"]\n",
    "\n",
    "#     # Salva os resultados em um arquivo CSV\n",
    "#     df_pivot.to_csv(\"deficit_balanca_comercial_saude.csv\")\n",
    "\n",
    "#     print(\"Dados salvos com sucesso!\")\n",
    "\n",
    "# # Fecha a conexão\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Inicializa a lista de NCMs de saúde com valores padrão\n",
    "# # com lista vazia caso a requisição falhe\n",
    "# ncms_saude = []  \n",
    "# ncm_response = []\n",
    "\n",
    "# if ncm_response:\n",
    "#     ncms = ncm_response.json()\n",
    "#     # Filtrar os NCMs relacionados à saúde (exemplo)\n",
    "#     ncms_saude = [ncm['id'] for ncm in ncms if ncm['desc'].startswith(\"Medicamentos\")]\n",
    "#     print(ncms_saude)\n",
    "\n",
    "# # Endpoint para consulta dos dados\n",
    "# consulta_url = \"https://api-comexstat.mdic.gov.br/cities\"\n",
    "\n",
    "# # Parâmetros da consulta (exemplo)\n",
    "# params = {\n",
    "#     \"flow\": [\"export\", \"import\"],\n",
    "#     \"monthDetail\": False,\n",
    "#     \"period\": {\"from\": f\"{ano_inicial}-01\", \"to\": f\"{ano_final}-12\"},\n",
    "#     \"filters\": [{\"filter\": \"ncm\", \"values\": ncms_saude}],\n",
    "#     \"details\": [\"ncm\"],\n",
    "#     \"metrics\": [\"metricFOB\"],\n",
    "# }\n",
    "\n",
    "# # Realiza a consulta\n",
    "# response = fazer_requisicao(consulta_url, json=params)\n",
    "\n",
    "# if response:\n",
    "#     data = response.json()\n",
    "#     df = pd.DataFrame(data)\n",
    "\n",
    "#     # # Calcula o déficit por ano e NCM\n",
    "#     # df_pivot = df.pivot_table(\n",
    "#     #     index=[\"coAno\", \"ncm\"], columns=\"flow\", values=\"metricFOB\", aggfunc=\"sum\"\n",
    "#     # )\n",
    "#     # df_pivot[\"deficit\"] = df_pivot[\"import\"] - df_pivot[\"export\"]\n",
    "\n",
    "#     # # Salva os resultados em um arquivo CSV\n",
    "#     # df_pivot.to_csv(\"deficit_balanca_comercial_saude.csv\")\n",
    "\n",
    "#     # print(\"Dados salvos com sucesso!\")\n",
    "# for i in df['data'].items():\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncm_response = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Gerar ilustrações para modelos pré-treinados</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir parâmetros adequado para trabalhar com tensores na GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O parâmetro return_tensors=\"pt\" em funções como o tokenizer do Hugging Face Transformers indica que você deseja que o tokenizador retorne os resultados da tokenização como tensores PyTorch.\n",
    "\n",
    "PyTorch é uma biblioteca popular de aprendizado de máquina que usa tensores como sua estrutura de dados principal.\n",
    "Por que usar return_tensors=\"pt\"?\n",
    "\n",
    "Tensores são estruturas de dados multidimensionais semelhantes a arrays NumPy, mas com a capacidade de serem processados ​​em GPUs para acelerar cálculos numéricos.\n",
    "\n",
    "A maioria dos modelos do Hugging Face Transformers são implementados em PyTorch. Ao usar return_tensors=\"pt\", você garante que os resultados da tokenização estejam no formato correto para serem alimentados diretamente nesses modelos.\n",
    "\n",
    "Se houver uma GPU dsiponivel, os tensores PyTorch podem ser movidos para a GPU para aproveitar sua capacidade de processamento paralelo e acelerar os cálculos.\n",
    "\n",
    "Alternativas\n",
    "\n",
    "return_tensors=\"tf\": Retorna os resultados como tensores do TensorFlow, outra biblioteca popular de aprendizado de máquina.\n",
    "\n",
    "return_tensors=\"np\": Retorna os resultados como arrays NumPy, que são úteis para algumas operações de pré-processamento ou análise, mas geralmente não são tão eficientes em GPUs quanto os tensores PyTorch.\n",
    "\n",
    "return_tensors=None (padrão): Retorna os resultados como listas de Python, que são mais fáceis de entender e manipular, mas podem ser menos eficientes para alimentar modelos de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"This is a test sentence.\"\n",
    "\n",
    "# Tokenização com tensores PyTorch\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence1 = \"This is a test sentence.\"\n",
    "sentence2 = \"How are you?\"\n",
    "\n",
    "# Tokenização com duas sentenças\n",
    "inputs = tokenizer([sentence1, sentence2], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "for i,j in inputs.items():\n",
    "    print(f\"{i:>15}: {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_pt\", model=\"unicamp-dl/translation-en-pt-t5\")\n",
    "print(translator.tokenizer.model_max_length) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificar pilelines do Spacy para PLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Carregar o modelo en_core_web_trf\n",
    "nlp_en = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Imprimir os nomes dos pipes disponíveis no modelo\n",
    "print(nlp_en.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificar informações básicas para modelos do Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Input [Input]\n",
    "    end\n",
    "\n",
    "    subgraph Embeddings\n",
    "        word_embeddings([\"word_embeddings\"])\n",
    "        position_embeddings([\"position_embeddings\"])\n",
    "        token_type_embeddings([\"token_type_embeddings\"])\n",
    "        LayerNorm([\"LayerNorm\"])\n",
    "        Dropout([\"Dropout\"])\n",
    "    end\n",
    "    \n",
    "    Input --> Embeddings\n",
    "\n",
    "    subgraph Encoder [Encoder]\n",
    "        BertLayers[12 x BertLayer]\n",
    "    end\n",
    "\n",
    "    Embeddings --> Encoder\n",
    "\n",
    "    subgraph Pooler [Pooler]\n",
    "        dense(( Linear ))\n",
    "        activation(( Tanh ))\n",
    "    end\n",
    "\n",
    "    Encoder --> Pooler\n",
    "    \n",
    "    subgraph BertLayer\n",
    "        BertAttention\n",
    "        BertIntermediate\n",
    "        BertOutput\n",
    "        LayerNorm_output([\"LayerNorm\"])\n",
    "        Dropout_output([\"Dropout\"])\n",
    "    end\n",
    "\n",
    "    subgraph BertAttention\n",
    "        SelfAttention\n",
    "        output\n",
    "    end\n",
    "\n",
    "    BertLayer --> BertAttention\n",
    "    BertLayer --> BertIntermediate\n",
    "    \n",
    "    subgraph BertSelfAttention\n",
    "        query(( Linear ))\n",
    "        key(( Linear ))\n",
    "        value(( Linear ))\n",
    "        Dropout([\"Dropout\"])\n",
    "    end\n",
    "\n",
    "    BertAttention --> BertSelfAttention\n",
    "\n",
    "    subgraph BertSelfOutput\n",
    "        dense(( Linear ))\n",
    "        LayerNorm([\"LayerNorm\"])\n",
    "        Dropout([\"Dropout\"])\n",
    "    end\n",
    "\n",
    "    BertSelfAttention --> BertSelfOutput\n",
    "\n",
    "    subgraph BertIntermediate\n",
    "        dense(( Linear ))\n",
    "        intermediate_act_fn(( GELUActivation ))\n",
    "    end\n",
    "\n",
    "    BertSelfOutput --> BertIntermediate\n",
    "\n",
    "    subgraph BertOutput\n",
    "        dense(( Linear ))\n",
    "        LayerNorm([\"LayerNorm\"])\n",
    "        Dropout([\"Dropout\"])\n",
    "    end\n",
    "\n",
    "    %% BertLayers --> BertLayer\n",
    "    \n",
    "    BertAttention -.-> BertSelfOutput  \n",
    "    BertSelfOutput --> LayerNorm_output\n",
    "    LayerNorm_output --> Dropout_output\n",
    "    Dropout_output --> BertLayer \n",
    "    BertLayer -.-> BertOutput \n",
    "    \n",
    "    BertIntermediate --> GELUActivation\n",
    "    BertIntermediate --> BertOutput\n",
    "    BertSelfAttention --> query\n",
    "    BertSelfAttention --> key\n",
    "    BertSelfAttention --> value\n",
    "    BertSelfAttention --> Dropout\n",
    "    \n",
    "    Pooler --> Tanh\n",
    "    Tanh --> pooler_output(( pooler_output ))\n",
    "\n",
    "    BertOutput --> dense\n",
    "    BertOutput --> LayerNorm\n",
    "    BertOutput --> Dropout\n",
    "    BertOutput --> last_hidden_state(( last_hidden_state ))\n",
    "    BertOutput --> hidden_states(( hidden_states ))\n",
    "    BertAttention --> attentions(( attentions ))\n",
    "\n",
    "    %% Embeddings - vertical arrangement\n",
    "    word_embeddings --> position_embeddings\n",
    "    position_embeddings --> token_type_embeddings\n",
    "    token_type_embeddings --> LayerNorm\n",
    "    LayerNorm --> Dropout\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir o nome de cada camada\n",
    "for name, module in model.named_modules():\n",
    "    print(name)\n",
    "\n",
    "# Acessar uma camada específica\n",
    "first_layer = model.get_encoder().layer[0]\n",
    "print(first_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mermaid_diagram_ok(model):\n",
    "    \"\"\"\n",
    "    Generates a Mermaid diagram representing the structure of a Hugging Face model, \n",
    "    collapsing repetitive layers into shapes with layer count and using rounded rectangles for internal layers.\n",
    "\n",
    "    Args:\n",
    "        model: The Hugging Face model to be analyzed.\n",
    "    \"\"\"\n",
    "\n",
    "    diagram = \"graph LR\\n\"\n",
    "    layer_counts = {}\n",
    "\n",
    "    root_nome_name = f\"{model.__class__.__name__}\"\n",
    "\n",
    "    # Adicionar o nó raiz que representa o modelo\n",
    "    diagram += f\"    {root_nome_name}{{ {root_nome_name} }}\\n\"\n",
    "\n",
    "    # Função recursiva para percorrer a estrutura do modelo \n",
    "    def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "        nonlocal diagram, layer_counts\n",
    "        for name, child_module in module.named_children():\n",
    "            current_layer_type = child_module.__class__.__name__\n",
    "            node_name = parent_name + \".\" + name if parent_name else name\n",
    "\n",
    "            ## Elementos filhos repetitivos dentro de um pai, mas precisa detectar dinamicamente o nome do elemento\n",
    "            layer_counts[current_layer_type] = layer_counts.get(current_layer_type, 0) + 1\n",
    "\n",
    "            if layer_counts[current_layer_type] == 1:\n",
    "                # Use rounded rectangle for single occurrences or non-repetitive layers\n",
    "                diagram += f\"    {node_name}{ (current_layer_type) }\\n\"\n",
    "                diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "            add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "    add_nodes_and_connections(model)\n",
    "\n",
    "    # Adicionar formas para camadas repetitivas com contagem e retângulos para camadas internas\n",
    "    named_children = list(model.named_children())\n",
    "    for layer_type, count in layer_counts.items():\n",
    "        if count > 1:\n",
    "            diagram += f\"    subgraph {layer_type}s\\n\"\n",
    "            diagram += f\"        {layer_type}x{count}[\\\"{count} x {layer_type}\\\"]\\n\"  \n",
    "            diagram += \"    end\\n\"\n",
    "            diagram += f\"    {model.__class__.__name__} --> {layer_type}s\\n\"\n",
    "\n",
    "            # Conectar a forma ao componente subsequente se não for o último filho\n",
    "            if count < len(named_children): \n",
    "                diagram += f\"    {layer_type}s --> {named_children[count][0]}\\n\"\n",
    "        else:\n",
    "            try:\n",
    "                parent_name = root_nome_name\n",
    "                first_child_name = layer_type\n",
    "                # add_nodes_and_connections(model)\n",
    "            except:\n",
    "                first_child_name = None\n",
    "            print(parent_name, first_child_name)\n",
    "\n",
    "    return diagram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "def generate_mermaid_diagram(model):\n",
    "    \"\"\"\n",
    "    Generates a Mermaid diagram representing the structure of a Hugging Face model, \n",
    "    collapsing repetitive layers into shapes with layer count and using rounded rectangles for internal layers.\n",
    "\n",
    "    Args:\n",
    "        model: The Hugging Face model to be analyzed.\n",
    "    \"\"\"\n",
    "\n",
    "    diagram = \"graph LR\\n\"\n",
    "    layer_counts = {}\n",
    "    parent_shapes = {}\n",
    "\n",
    "    # Add the root node representing the model\n",
    "    diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "    # Recursive function to traverse the model structure \n",
    "    def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "        nonlocal diagram, layer_counts, parent_shapes\n",
    "        for name, child_module in module.named_children():\n",
    "            current_layer_type = child_module.__class__.__name__\n",
    "\n",
    "            # Sanitize the node name more aggressively to be Mermaid-compatible\n",
    "            node_name = re.sub(r'[^a-zA-Z0-9_-]', '_', parent_name + \".\" + name if parent_name else name).strip()\n",
    "            # Replace consecutive underscores with a single underscore\n",
    "            node_name = re.sub(r'_+', '_', node_name)\n",
    "\n",
    "            layer_counts[node_name] = layer_counts.get(node_name, 0) + 1\n",
    "\n",
    "            # Always add nodes to the diagram\n",
    "            diagram += f\"    {node_name}(( {current_layer_type} ))\\n\"\n",
    "\n",
    "            # If this is the first child of its parent, create a subgraph for the parent\n",
    "            if layer_counts[node_name] == 1 and parent_name:\n",
    "                if parent_name not in parent_shapes:\n",
    "                    # Sanitize the parent name as well\n",
    "                    parent_name = re.sub(r'[^a-zA-Z0-9_-]', '_', parent_name).strip()\n",
    "                    parent_name = re.sub(r'_+', '_', parent_name)\n",
    "                    diagram += f\"    subgraph {parent_name}\\n\"\n",
    "                    parent_shapes[parent_name] = True\n",
    "\n",
    "            diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "            add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "            # If this is the last child of its parent, close the subgraph if it was opened\n",
    "            if layer_counts[node_name] == len(list(module.named_children())) and parent_name in parent_shapes:\n",
    "                diagram += \"    end\\n\"\n",
    "\n",
    "    add_nodes_and_connections(model)\n",
    "\n",
    "    # Add shapes for repetitive layers with count and rounded rectangles for internal layers\n",
    "    named_children = list(model.named_children())\n",
    "    for layer_type, count in layer_counts.items():\n",
    "        if count > 1:\n",
    "            parent_name = \".\".join(layer_type.split(\".\")[:-1])\n",
    "            # Sanitize the parent name\n",
    "            parent_name = re.sub(r'[^a-zA-Z0-9_-]', '_', parent_name).strip()\n",
    "            parent_name = re.sub(r'_+', '_', parent_name)\n",
    "\n",
    "            diagram = diagram.replace(f\"    subgraph {parent_name}\\n\", \n",
    "                                      f\"    subgraph {parent_name} [{count} x {layer_type.split('.')[-1]}]\\n\")\n",
    "\n",
    "            # Connect the grandparent to the shape\n",
    "            grandparent_name = \".\".join(parent_name.split(\".\")[:-1]) \n",
    "            if grandparent_name:\n",
    "                diagram += f\"    {grandparent_name} --> {parent_name}\\n\"\n",
    "\n",
    "            # Connect the shape to the subsequent component if it's not the last child\n",
    "            if any(name.startswith(parent_name + \".\") for name in layer_counts):\n",
    "                next_component = next(name for name in layer_counts if name.startswith(parent_name + \".\"))\n",
    "                diagram += f\"    {parent_name} --> {next_component}\\n\"\n",
    "\n",
    "    return diagram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar o código Mermaid\n",
    "mermaid_code = generate_mermaid_diagram_ok(model)\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar o código Mermaid\n",
    "mermaid_code = generate_mermaid_diagram(model)\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir o diagrama usando Markdown\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(f\"```mermaid\\n{mermaid_code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "graph TD\n",
    "    BertModel{{BertModel}} --> embeddings{{BertEmbeddings}} --> embeddings --> embeddings(BertEmbeddings) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "graph TD\n",
    "    BertModel{{BertModel}} --> embeddings{{BertEmbeddings}} --> embeddings --> embeddings((BertEmbeddings)) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "print(model)\n",
    "print()\n",
    "print(model.config)\n",
    "\n",
    "# Imprimir o nome de cada camada\n",
    "for name, module in model.named_modules():\n",
    "    print(name)\n",
    "\n",
    "# Acessar uma camada específica\n",
    "print(f\"\\nBusca por camada específica no modelo HF:\")\n",
    "camada = \"get_encoder\"\n",
    "try:\n",
    "    first_layer = model.camada().layer[0]\n",
    "    print(first_layer)\n",
    "except:\n",
    "    print(f\"  Modelo {model_name} não possui camada de {camada}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entradas de referência:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entradas de citações para referências:\n",
    "\n",
    "@book{gil2022comoelaborarprojetosdepesquisa,\n",
    "  title={Como Elaborar Projetos de Pesquisa},\n",
    "  author={Gil, Antonio Carlos},\n",
    "  edition={7},\n",
    "  publisher={Atlas},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "@book{popper1934logic,\n",
    "  title={A Lógica da Pesquisa Científica},\n",
    "  author={Popper, Karl},\n",
    "  year={1934}\n",
    "}\n",
    "\n",
    "@misc{IEP2023Popper,\n",
    "  author = {Internet Encyclopedia of Philosophy},\n",
    "  title = {Karl Popper: Political Philosophy},\n",
    "  year = {2023},\n",
    "  howpublished = {\\url{https://iep.utm.edu/popp-pol/}},\n",
    "  note = {Acesso em: 01/01/2024}\n",
    "}\n",
    "\n",
    "@phdthesis{Broderick1984,\n",
    "  author = {David Gregory Broderick},\n",
    "  title = {Objectivity: Thomas Aquinas and Karl Popper},\n",
    "  school = {Boston College},\n",
    "  year = {1984}\n",
    "}\n",
    "\n",
    "@article{CRYFUNavara2023,\n",
    "  author = {Grupo Ciencia, Razón y Fe (CRYF)},\n",
    "  title = {The Ethical Roots of Karl Popper's Epistemology},\n",
    "  journal = {Universidad de Navarra},\n",
    "  year = {2023},\n",
    "  url = {https://www.unav.edu/web/ciencia-razon-y-fe/poppers-epistemology}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bases de Dados:\n",
    "\n",
    "@misc{arxiv,\n",
    "  title = {{arXiv}},\n",
    "  url = {https://arxiv.org}\n",
    "}\n",
    "\n",
    "@misc{core,\n",
    "  title = {{CORE}},\n",
    "  url = {https://core.ac.uk}\n",
    "}\n",
    "\n",
    "@misc{doaj,\n",
    "  title = {{Directory of Open Access Journals (DOAJ)}},\n",
    "  url = {https://doaj.org}\n",
    "}\n",
    "\n",
    "@misc{googlescholar,\n",
    "  title = {{Google Scholar}},\n",
    "  url = {https://scholar.google.com}\n",
    "}\n",
    "\n",
    "@misc{openaire,\n",
    "  title = {{OpenAIRE}},\n",
    "  url = {https://www.openaire.eu}\n",
    "}\n",
    "\n",
    "@misc{pubmedcentral,\n",
    "  title = {{PubMed Central}},\n",
    "  url = {https://www.ncbi.nlm.nih.gov/pmc/}\n",
    "}\n",
    "\n",
    "@misc{ssrn,\n",
    "  title = {{Social Science Research Network (SSRN)}},\n",
    "  url = {https://www.ssrn.com}\n",
    "}\n",
    "\n",
    "@misc{scienceopen,\n",
    "  title = {{ScienceOpen}},\n",
    "  url = {https://www.scienceopen.com}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editoras com políticas de OA:\n",
    "\n",
    "@misc{springernature,\n",
    "  title = {{Springer Nature}},\n",
    "  url = {https://www.springernature.com}\n",
    "}\n",
    "\n",
    "@misc{oup,\n",
    "  title = {{Oxford University Press (OUP)}},\n",
    "  url = {https://academic.oup.com}\n",
    "}\n",
    "\n",
    "@misc{frontiers,\n",
    "  title = {{Frontiers}},\n",
    "  url = {https://www.frontiersin.org}\n",
    "}\n",
    "\n",
    "@misc{wiley,\n",
    "  title = {{Wiley}},\n",
    "  url = {https://www.wiley.com}\n",
    "}\n",
    "\n",
    "@misc{plos,\n",
    "  title = {{Public Library of Science (PLOS)}},\n",
    "  url = {https://www.plos.org}\n",
    "}\n",
    "\n",
    "@misc{hindawi,\n",
    "  title = {{Hindawi}},\n",
    "  url = {https://www.hindawi.com}\n",
    "}\n",
    "\n",
    "@misc{mdpi,\n",
    "  title = {{MDPI AG}},\n",
    "  url = {https://www.mdpi.com}\n",
    "}\n",
    "\n",
    "@misc{informa,\n",
    "  title = {{Informa PLC}},\n",
    "  url = {https://www.informa.com}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GII\n",
    "@misc{GII-WIPO,\n",
    "      title = {Global Innovation Index - WIPO Series},\n",
    "      abstract = {The Global Innovation Index (GII) ranks the innovation performance of some 131 countries and economies around the world, based on 80+ indicators. Co-published by WIPO, Cornell University and INSEAD, the report provides an annual ranking of the innovation capabilities and performance of economies around the world.},\n",
    "      {url = https://www.wipo.int/publications/en/series/index.jsp?id=129}\n",
    "}\n",
    "\n",
    "@misc{GII-2011,\n",
    "  title = {Global Innovation Index 2011 - Accelerating Growth and Development},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=274&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2012,\n",
    "  title = {Global Innovation Index 2012 - Stronger Innovation Linkages for Global Growth},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=247&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2013,\n",
    "  title = {Global Innovation Index 2013 - The Local Dynamics of Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=368&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2014,\n",
    "  title = {Global Innovation Index 2014 - The Human Factor in Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=3254&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2015,\n",
    "  title = {Global Innovation Index 2015 - Effective Innovation Policies for Development},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=3978&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2016,\n",
    "  title = {Global Innovation Index 2016 - Winning with Global Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4064&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2017,\n",
    "  title = {Global Innovation Index 2017 - Innovation Feeding the World},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4193&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2018,\n",
    "  title = {Global Innovation Index 2018 - Energizing the World with Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4330&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2019,\n",
    "  title = {Global Innovation Index 2019 - Creating Healthy Lives — The Future of Medical Innovation},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4434&plang=EN}\n",
    "}\n",
    "\n",
    "@article{40579,\n",
    "      author = {Cornell University.},\n",
    "      url = {http://tind.wipo.int/record/40579},\n",
    "      title = {Global Innovation Index 2019 - Executive version.},\n",
    "      abstract = {The Global Innovation Index 2019 provides detailed metrics  about the innovation performance of 129 countries and  economies around the world. Its 80 indicators explore a  broad vision of innovation, including political  environment, education, infrastructure and business  sophistication. The GII 2019 analyzes the medical  innovation landscape of the next decade, looking at how  technological and non-technological medical innovation will  transform the delivery of healthcare worldwide. It also  explores the role and dynamics of medical innovation as it  shapes the future of healthcare, and the potential  influence this may have on economic growth. Chapters of the  report provide more details on this year’s theme from  academic, business, and particular country perspectives  from leading experts and decision makers.},\n",
    "      doi = {https://doi.org/10.34667/tind.40579},\n",
    "      recid = {40579},\n",
    "      pages = {214 pages ;},\n",
    "}\n",
    "\n",
    "@article{35279,\n",
    "      url = {http://tind.wipo.int/record/35279},\n",
    "      title = {Índice Global de inovação de 2019 - PRINCIPAIS  RESULTADOS.},\n",
    "      abstract = {Criar Vidas Sadias - O Futuro da Inovação Médica.},\n",
    "      doi = {https://doi.org/10.34667/tind.35279},\n",
    "      recid = {35279},\n",
    "      pages = {20 pages ;},\n",
    "}\n",
    "\n",
    "@misc{GII-2020,\n",
    "  title = {Global Innovation Index 2020 - Who Will Finance Innovation?},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4514&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2021,\n",
    "  title = {Global Innovation Index 2021 - Tracking Innovation through the COVID-19 Crisis},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4560&plang=EN}\n",
    "}\n",
    "\n",
    "@article{46620,\n",
    "      url = {http://tind.wipo.int/record/46620},\n",
    "      title = {Índice Global de Inovação 2022 : Resumo executivo.},\n",
    "      abstract = {O Índice Global da Inovação 2022 (IGI) analisa as  tendências globais no campo da inovação em um cenário  marcado pela pandemia de COVID-19 em curso, por um  crescimento desacelerado da produtividade e pelo surgimento  de novos desafios. O IGI revela as economias mais  inovadoras do mundo, classificando o desempenho em  inovação de 132 economias, destacando seus pontos fortes  e fracos em matéria de inovação e identificando lacunas  em suas métricas de inovação. Esta edição de 2022 tem  como foco o efeito da inovação sobre a produtividade e o  bem-estar da sociedade ao longo das próximas décadas.},\n",
    "      doi = {https://doi.org/10.34667/tind.46620},\n",
    "      recid = {46620},\n",
    "      pages = {28 pages :},\n",
    "}\n",
    "\n",
    "@misc{GII-2022,\n",
    "  title = {Global Innovation Index 2022 - What is the future of innovation driven growth?},\n",
    "  url = {https://www.wipo.int/publications/en/details.jsp?id=4622&plang=EN}\n",
    "}\n",
    "\n",
    "@misc{GII-2023,\n",
    "  title = {Global Innovation Index 2023},\n",
    "  url = {https://www.globalinnovationindex.org/gii-2023}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar ou apontar para local atual do Spacy para PLN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar instalação desnecessária quando o Spacy já está instalado no Windows onde o WSL está rodando, é possível apontar para o diretório do spaCy instalado no Windows a partir do WSL para evitar uma nova instalação e economizar espaço em disco. \n",
    "\n",
    "No WSL, o sistema de arquivos do Windows é montado em /mnt/c. Portanto, para navegar até o diretório c:\\.spacy no WSL, você pode mudar para o diretório com o seguinte comando no Terminal do WSL:\n",
    "\n",
    "Para que o comando import spacy funcione corretamente no WSL sem precisar reinstalar o spaCy, você deve colocar o link simbólico dentro do diretório onde o Python do WSL procura por pacotes instalados. Geralmente, esse diretório é:\n",
    "\n",
    "    /home/<seu_nome_de_usuario>/.local/lib/python<versão>/site-packages/\n",
    "\n",
    "Para criar um link simbólico no WSL no diretório adequado para apontar para o spaCy no Windows executamos o seguinte comando:\n",
    "\n",
    "Bash\n",
    "\n",
    "    ln -s /mnt/c/Users/<seu_nome_de_usuario>/.spacy /path/to/spacy/in/wsl\n",
    "    \n",
    "No meu caso aqui, para criar dentro do diretório do ambiente virtual que desejo usar o Spacy, por exemplo, ficou:\n",
    "\n",
    "    ln -s /mnt/c/.spacy /home/mak/miniconda3/envs/rapids-24.08/lib/python3.11/site-packages/spacy\n",
    "\n",
    "Para remover o link usar:\n",
    "\n",
    "    unlink /home/mak/miniconda3/envs/rapids-24.08/lib/python3.11/site-packages/spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import translate\n",
    "# def translate_pt_to_en(text):\n",
    "#     translate_client = translate.TranslationServiceClient()\n",
    "#     parent = \"projects/seu-projeto-id/locations/global\" \n",
    "#     response = translate_client.translate_text(\n",
    "#         request={\n",
    "#             \"parent\": parent,\n",
    "#             \"contents\": [text],\n",
    "#             \"mime_type\": \"text/plain\",\n",
    "#             \"source_language_code\": \"pt\",\n",
    "#             \"target_language_code\": \"en\",\n",
    "#         }\n",
    "#     )\n",
    "#     return response.translations[0].translated_text\n",
    "#\n",
    "# def translate_to_pt(text):\n",
    "#     translate_client = translate.TranslationServiceClient()\n",
    "#     parent = \"projects/seu-projeto-id/locations/global\"  # Substitua 'seu-projeto-id' pelo ID real do seu projeto\n",
    "#     response = translate_client.translate_text(\n",
    "#         request={\n",
    "#             \"parent\": parent,\n",
    "#             \"contents\": [text],\n",
    "#             \"mime_type\": \"text/plain\",\n",
    "#             \"target_language_code\": \"pt\",  # Traduzir para português\n",
    "#         }\n",
    "#     )\n",
    "#     return response.translations[0].translated_text\n",
    "\n",
    "    # # 0. Distribuição do número de palavras-chave por edital (neste caso é inútil pois montei com apenas uma palavra-chave)\n",
    "    # # Verificar valores únicos na coluna 'palavras-chave'\n",
    "    # print(df_fomento['palavras-chave'].unique().to_pandas()) \n",
    "\n",
    "    # # Normalizar caracteres Unicode e substituir valores especiais por NaN (on CPU)\n",
    "    # pd_series = df_fomento['palavras-chave'].to_pandas()\n",
    "    # pd_series = pd_series.astype(str).map(\n",
    "    #     lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # )\n",
    "    # pd_series = pd_series.str.replace(r'^\\s*$', '', regex=True) \n",
    "    # pd_series = pd_series.fillna('')\n",
    "\n",
    "    # # Aplicar str.split(',') e str.len() no Pandas\n",
    "    # pd_series_split = pd_series.str.split(',')\n",
    "    # pd_series_len = pd_series_split.str.len()\n",
    "\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # sns.histplot(pd_series_len, kde=True)  # Passar a Series do Pandas para o Seaborn\n",
    "    # plt.title('Distribuição do Número de Palavras-chave por Edital')\n",
    "    # plt.xlabel('Número de Palavras-chave')\n",
    "    # plt.ylabel('Frequência')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import cudf\n",
    "# import nltk\n",
    "# import torch\n",
    "# import spacy\n",
    "# import string\n",
    "# import logging\n",
    "# import unicodedata\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import contextualSpellCheck\n",
    "\n",
    "# from transformers.tokenization_utils_base import TruncationStrategy\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# from transformers import pipeline, TranslationPipeline\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from spacy.tokens import Doc, Token\n",
    "# from spacy.language import Language\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# from nltk.corpus import stopwords\n",
    "# from collections import Counter\n",
    "# from googletrans import Translator\n",
    "# from wordcloud import WordCloud\n",
    "# from langdetect import detect\n",
    "# from git import Repo\n",
    "# from tqdm.notebook import tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "# def detect_language(text):\n",
    "#     try:\n",
    "#         return detect(text)\n",
    "#     except langdetect.lang_detect_exception.LangDetectException:\n",
    "#         return 'unknown'\n",
    "\n",
    "# def translate_to_pt(texts):\n",
    "#     try:\n",
    "#         # Tradução usando o modelo Hugging Face\n",
    "#         inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "#         outputs = model.generate(**inputs)\n",
    "#         translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "#         return translations\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro na tradução: {e}\")\n",
    "#         return texts\n",
    "\n",
    "# # Função de pré-processamento otimizada para GPU\n",
    "# def gpu_preprocess_text(text):\n",
    "#     # Carregar as stopwords em inglês\n",
    "#     stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "#     # Adicionar as stopwords personalizadas em inglês\n",
    "#     stop_words_en.update([\"must\", \"due\", \"track\", \"may\", \"non\", \"year\", \"apply\", \"prepare\", \"era\", \"eligibility\",\n",
    "#                           \"funded value\", \"deadline\", \"application form\", \"description\", \"homepage\", \"Name\",\n",
    "#                           \"address\", \"phone\", \"Fax\", \"e-mail\", \"email\", \"contact\", \"home page\", \"home\", \"page\"])\n",
    "\n",
    "#     # Traduzir o texto para português (se necessário) em lote\n",
    "#     try:\n",
    "#         # logging.info(\"Traduzindo texto para o português (se necessário)...\")\n",
    "#         text_translated = translate_to_pt([text])[0] if detect_language(text) != 'pt' else text\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Erro na tradução: {e}\")\n",
    "#         return []\n",
    "\n",
    "#     # Converter para minúsculas e remover pontuação\n",
    "#     # logging.info(\"Limpando e normalizando o texto...\")\n",
    "#     text_translated = text_translated.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#     # Truncar o texto traduzido se for muito longo\n",
    "#     max_length = 512  # Ajuste conforme necessário\n",
    "#     text_translated = text_translated[:max_length]\n",
    "\n",
    "#     # Aplicar o corretor ortográfico e lematizar em inglês em lote (usando pipe do spaCy)\n",
    "#     # logging.info(\"Processando o texto com spaCy...\")\n",
    "#     with nlp_en.disable_pipes('ner'):  # Desabilitar NER para economizar memória da GPU\n",
    "#         docs = nlp_en.pipe([text_translated], batch_size=64) \n",
    "\n",
    "#     for doc in docs:\n",
    "#         words_en = [token.lemma_.lower() if token.text.lower() not in [\"institute\", \"institution\", \"institutional\"] else \"institution\"\n",
    "#                     for token in doc \n",
    "#                     if token.is_alpha and not token.is_stop and token.lemma_.lower() not in stop_words_en]\n",
    "\n",
    "#     return words_en  \n",
    "\n",
    "# # Configurar o logging (opcional)\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Load the transformer-based Portuguese and English models \n",
    "# nlp_pt = spacy.load(\"pt_core_news_sm\")  \n",
    "# nlp_en = spacy.load(\"en_core_web_trf\") \n",
    "\n",
    "# # Add the contextual spell checker to the English pipeline \n",
    "# contextualSpellCheck.add_to_pipe(nlp_en)\n",
    "\n",
    "# # Load the translation pipeline\n",
    "# translator = pipeline(\"translation\", model=\"unicamp-dl/translation-pt-en-t5\") \n",
    "\n",
    "# # Certifique-se de que `detect_language(text)` está definida em algum lugar do seu código\n",
    "# # Medir tempo para pré-processar (remover sw, traduzir para português, lematizar) sem cuGrpah mas já usando GPU\n",
    "# start_time = time.time()\n",
    "# # Aplicar a função de pré-processamento à coluna 'texto_para_embedding' em lotes, com barra de progresso\n",
    "# all_words = df_fomento['texto_para_embedding'].to_pandas().progress_apply(gpu_preprocess_text) \n",
    "# end_time = time.time()\n",
    "# time_com = end_time - start_time\n",
    "# print(f\"Tempos para pré-processar usando mais a GPU:\") \n",
    "# print(f\"Carregando o modelo diretamente na GPU: {time_com:>.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definir a função de pré-processamento \n",
    "# def br_preprocess_text(text):\n",
    "#     # Carregar as stopwords em português\n",
    "#     stop_words_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "#     # Adicionar as stopwords personalizadas em português\n",
    "#     stop_words_pt.update([\"deve\", \"devido\", \"acompanhar\", \"pode\", \"não\", \"ano\", \"aplicar\", \"preparar\", \"era\", \"elegibilidade\",\n",
    "#                        \"valorfinanciado\", \"datalimite\", \"formuláriodesolicitacao\", \"descrição\", \"homepage\", \"nome\",\n",
    "#                        \"endereço\", \"telefone\", \"fax\", \"e-mail\", \"contato\", \"home page\", \"casa\", \"página\"])\n",
    "\n",
    "#     # Traduzir o texto para português (se necessário)\n",
    "#     try:\n",
    "#         if detect_language(text) != 'pt':\n",
    "#             # logging.info(\"Traduzindo texto para o português...\")\n",
    "#             text_translated = translator(text, src_lang = \"auto\", tgt_lang=\"pt\")[0]['translation_text']\n",
    "#         else: \n",
    "#             text_translated = text \n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Erro na tradução: {e}\")\n",
    "#         return [] \n",
    "\n",
    "#     # Converter para minúsculas e remover pontuação\n",
    "#     # logging.info(\"Limpando e normalizando o texto...\")\n",
    "#     text_translated = text_translated.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#     # Truncar o texto traduzido se for muito longo\n",
    "#     max_length = 512 \n",
    "#     text_translated = text_translated[:max_length]\n",
    "\n",
    "#     # Aplicar o corretor ortográfico (se disponível para português)\n",
    "#     # logging.info(\"Aplicando o corretor ortográfico...\")\n",
    "#     # doc_pt_spell_check = nlp_pt(text_translated)\n",
    "#     # text_corrected = doc_pt_spell_check._.outcome_spellCheck \n",
    "\n",
    "#     # Lematizar em português\n",
    "#     # logging.info(\"Lematizando o texto...\")\n",
    "#     doc_pt = nlp_pt(text_translated)\n",
    "#     words_pt = [token.lemma_.lower() \n",
    "#                 for token in doc_pt \n",
    "#                 if token.is_alpha and not token.is_stop and token.lemma_.lower() not in stop_words_pt \n",
    "#                 and not (token.pos_ == \"PROPN\" and token.text.lower() not in stop_words_pt)]\n",
    "\n",
    "#     return words_pt\n",
    "\n",
    "# # Configurar o logging (opcional)\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Carregar o modelo de tradução e o tokenizador do Hugging Face\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/translation-pt-en-t5\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/translation-pt-en-t5\")\n",
    "\n",
    "# # Move the Hugging Face model to the GPU\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# if device == 'cuda':\n",
    "#     print(f\"Caregando modelo para GPU...\")\n",
    "# else:\n",
    "#     print(f\"GPU indisponível, usando aoenas CPU...\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Load the transformer-based English model\n",
    "# nlp_en = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# # Add the contextual spell checker to the pipeline\n",
    "# contextualSpellCheck.add_to_pipe(nlp_en)\n",
    "\n",
    "# # Medir tempo para pré-processar (remocer sw, traduzir para português, lematizar) sem cuGrpah mas já usando GPU\n",
    "# start_time = time.time()\n",
    "# # Aplicar a função de pré-processamento à coluna 'texto_para_embedding' em lotes\n",
    "# all_words = df_fomento['texto_para_embedding'].to_pandas().apply(br_preprocess_text)\n",
    "# end_time = time.time()\n",
    "# time_sem = end_time - start_time\n",
    "# print(f\"Tempos para pré-processar usando GPU somente indiretamente:\") \n",
    "# print(f\"Sem carregar o modelo diretamente na GPU: {time_sem:>.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import unicodedata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import contextualSpellCheck\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from spacy.tokens import Doc, Token\n",
    "from spacy.language import Language\n",
    "# from googletrans import Translator\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from git import Repo\n",
    "import logging\n",
    "\n",
    "# Configurar o logging (opcional)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# def translate_to_pt(text):\n",
    "#     translator = Translator()\n",
    "#     try:\n",
    "#         translation = translator.translate(text, dest='pt')\n",
    "#         return translation.text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro na tradução: {e}\")\n",
    "#         return text \n",
    "\n",
    "# def translate_pt_to_en(text):\n",
    "#     translator = Translator()\n",
    "#     try:\n",
    "#         translation = translator.translate(text, src='pt', dest='en')\n",
    "#         return translation.text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro na tradução: {e}\")\n",
    "#         return text\n",
    "\n",
    "# # Carregar os modelos de língua portuguesa e inglesa do spaCy\n",
    "# # nlp_pt = spacy.load('pt_core_news_sm')\n",
    "# # nlp_en = spacy.load('en_core_web_lg')\n",
    "\n",
    "# # Load the transformer-based English model\n",
    "# nlp_en = spacy.load(\"en_core_web_trf\") \n",
    "\n",
    "# # Add the contextual spell checker to the pipeline\n",
    "# contextualSpellCheck.add_to_pipe(nlp_en)\n",
    "\n",
    "# # Definir a função de pré-processamento (recebendo stop_words como argumento)\n",
    "# def cpu_preprocess_text(text):\n",
    "#     # Carregar as stopwords em inglês\n",
    "#     stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "#     # Adicionar as stopwords personalizadas em inglês\n",
    "#     stop_words_en.update([\"must\", \"due\", \"track\", \"may\", \"non\", \"year\", \"apply\", \"prepare\", \"era\", \"eligibility\",\n",
    "#                        \"funded value\", \"deadline\", \"application form\", \"description\", \"homepage\", \"Name\",\n",
    "#                        \"address\", \"phone\", \"Fax\", \"e-mail\", \"email\", \"contact\", \"home page\", \"home\", \"page\"])\n",
    "\n",
    "#     # Traduzir o texto de português para inglês\n",
    "#     try:\n",
    "#         logging.info(\"Pre-processar termos (traduzir para o inglês, corrigir ortografia e lematizar)...\")\n",
    "#         text_translated = translate_pt_to_en(text)\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Erro na tradução: {e}\")\n",
    "#         return []\n",
    "\n",
    "#     # Converter para minúsculas e remover pontuação\n",
    "#     text_translated = text_translated.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "#     # Truncate the translated text if it's too long\n",
    "#     max_length = 512  \n",
    "#     text_translated = text_translated[:max_length]\n",
    "\n",
    "#     # Aplicar o corretor ortográfico contextual\n",
    "#     doc_en_spell_check = nlp_en(text_translated)\n",
    "#     text_corrected = doc_en_spell_check._.outcome_spellCheck # Usando a extensão correta do contextualSpellCheck\n",
    "\n",
    "#     # Lematizar em inglês\n",
    "#     doc_en = nlp_en(text_corrected)\n",
    "#     words_en = [token.lemma_.lower() if token.text.lower() not in [\"institute\", \"institution\", \"institutional\"] else \"institution\"\n",
    "#                 for token in doc_en \n",
    "#                 if token.is_alpha and not token.is_stop and token.lemma_.lower() not in stop_words_en]\n",
    "\n",
    "#     return words_en\n",
    "\n",
    "# def analisar_dados_fomento(all_words):\n",
    "#     \"\"\"\n",
    "#     Realiza análises exploratórias nos dados de oportunidades de fomento.\n",
    "\n",
    "#     Args:\n",
    "#         arquivo_csv: Caminho para o arquivo CSV contendo os dados de fomento.\n",
    "#     \"\"\"\n",
    "#     # Contar a frequência das palavras\n",
    "#     word_counts = Counter(word for words in all_words for word in words)\n",
    "\n",
    "#     # Obter as palavras mais frequentes\n",
    "#     top_words = word_counts.most_common(20)\n",
    "\n",
    "#     # Plotar um gráfico de barras com as palavras mais frequentes\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.bar(*zip(*top_words))\n",
    "#     plt.title('Palavras Mais Frequentes (sem Stopwords e com Lematização)')\n",
    "#     plt.xlabel('Palavra')\n",
    "#     plt.ylabel('Frequência')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Criar uma nuvem de palavras\n",
    "#     wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)\n",
    "\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "#     # 2. Visualização dos embeddings em 2D usando PCA\n",
    "#     pca = PCA(n_components=2)\n",
    "#     embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "#     plt.title('Visualização dos Embeddings (PCA)')\n",
    "#     plt.xlabel('Componente Principal 1')\n",
    "#     plt.ylabel('Componente Principal 2')\n",
    "#     plt.show()\n",
    "\n",
    "#     # 3. Visualização dos embeddings em 2D usando t-SNE\n",
    "#     tsne = TSNE(n_components=2, perplexity=30, learning_rate=200)\n",
    "#     embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "#     plt.title('Visualização dos Embeddings (t-SNE)')\n",
    "#     plt.xlabel('Dimensão 1')\n",
    "#     plt.ylabel('Dimensão 2')\n",
    "#     plt.show()\n",
    "\n",
    "# analisar_dados_fomento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, HTML\n",
    "\n",
    "# def generate_mermaid_diagram_v0(model):\n",
    "#     \"\"\"\n",
    "#     Gera um diagrama Mermaid representando a estrutura de um modelo Hugging Face.\n",
    "\n",
    "#     Args:\n",
    "#         model: O modelo Hugging Face a ser analisado.\n",
    "\n",
    "#     Returns:\n",
    "#         Uma string contendo o código Mermaid para o diagrama.\n",
    "#     \"\"\"\n",
    "\n",
    "#     diagram = \"graph LR\\n\"\n",
    "\n",
    "#     # Adicionar o nó raiz representando o modelo\n",
    "#     diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "#     # Função recursiva para percorrer a estrutura do modelo e adicionar nós e conexões ao diagrama\n",
    "#     def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "#         nonlocal diagram\n",
    "#         for name, child_module in module.named_children():\n",
    "#             node_name = parent_name + \".\" + name if parent_name else name\n",
    "#             diagram += f\"    {node_name}{{ {child_module.__class__.__name__} }}\\n\"\n",
    "#             diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "#             add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "#     # Chamar a função recursiva para construir o diagrama\n",
    "#     add_nodes_and_connections(model)\n",
    "\n",
    "#     return diagram\n",
    "\n",
    "# def generate_mermaid_diagram_v1(model):\n",
    "#     \"\"\"\n",
    "#     Generates a Mermaid diagram representing the structure of a Hugging Face model, \n",
    "#     collapsing repetitive layers.\n",
    "\n",
    "#     Args:\n",
    "#         model: The Hugging Face model to be analyzed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     diagram = \"graph LR\\n\"\n",
    "#     prev_layer_type = None\n",
    "#     layer_count = 0\n",
    "\n",
    "#     # Add the root node representing the model\n",
    "#     diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "#     # Recursive function to traverse the model structure and add nodes and connections to the diagram\n",
    "#     def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "#         nonlocal diagram, prev_layer_type, layer_count\n",
    "#         for name, child_module in module.named_children():\n",
    "#             current_layer_type = child_module.__class__.__name__\n",
    "#             node_name = parent_name + \".\" + name if parent_name else name\n",
    "\n",
    "#             if current_layer_type == prev_layer_type:\n",
    "#                 layer_count += 1\n",
    "#             else:\n",
    "#                 if layer_count > 1:\n",
    "#                     diagram += f\"    {parent_name}.{prev_layer_type}_...(...):::same\\n\"  # Add ellipses for repetitive layers\n",
    "#                 prev_layer_type = current_layer_type\n",
    "#                 layer_count = 1\n",
    "#                 diagram += f\"    {node_name}{{ {current_layer_type} }}\\n\"\n",
    "#                 diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "#             add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "#         # Handle remaining repetitive layers at the end of a module\n",
    "#         if layer_count > 1:\n",
    "#             diagram += f\"    {parent_name}.{prev_layer_type}_...(...):::same\\n\"\n",
    "\n",
    "#     # Call the recursive function to build the diagram\n",
    "#     add_nodes_and_connections(model)\n",
    "\n",
    "#     # # Encapsulate the Mermaid code in an HTML block for rendering in Jupyter Notebook\n",
    "#     # html_code = f\"\"\"\n",
    "#     # <div class=\"mermaid\">\n",
    "#     # {diagram}\n",
    "#     # </div>\n",
    "#     # \"\"\"\n",
    "\n",
    "#     # # Display the diagram in the Jupyter Notebook cell\n",
    "#     # display(HTML(html_code))\n",
    "\n",
    "#     return diagram\n",
    "\n",
    "# def generate_mermaid_diagram_v2(model):\n",
    "#     \"\"\"\n",
    "#     Generates a Mermaid diagram representing the structure of a Hugging Face model, \n",
    "#     collapsing repetitive layers into shapes.\n",
    "\n",
    "#     Args:\n",
    "#         model: The Hugging Face model to be analyzed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     diagram = \"graph LR\\n\"\n",
    "#     prev_layer_type = None\n",
    "#     layer_count = 0\n",
    "#     shape_id = 0  # To keep track of unique shape IDs\n",
    "\n",
    "#     # Add the root node representing the model\n",
    "#     diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "#     # Recursive function to traverse the model structure and add nodes and connections to the diagram\n",
    "#     def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "#         nonlocal diagram, prev_layer_type, layer_count, shape_id\n",
    "#         for name, child_module in module.named_children():\n",
    "#             current_layer_type = child_module.__class__.__name__\n",
    "#             node_name = parent_name + \".\" + name if parent_name else name\n",
    "\n",
    "#             if current_layer_type == prev_layer_type:\n",
    "#                 layer_count += 1\n",
    "#             else:\n",
    "#                 if layer_count > 1:\n",
    "#                     # Create a shape to group repetitive layers\n",
    "#                     shape_id += 1\n",
    "#                     diagram += f\"    subgraph shape{shape_id} [{prev_layer_type} x {layer_count}]\\n\"\n",
    "#                     diagram += f\"        {parent_name}.{prev_layer_type}_0[\\\"{prev_layer_type} 0\\\"]\\n\"\n",
    "#                     diagram += f\"        {parent_name}.{prev_layer_type}_...[\\\" ... \\\"]\\n\"\n",
    "#                     diagram += f\"        {parent_name}.{prev_layer_type}_{layer_count - 1}[\\\"{prev_layer_type} {layer_count - 1}\\\"]\\n\"\n",
    "#                     diagram += \"    end\\n\"\n",
    "#                     diagram += f\"    {parent_name} --> shape{shape_id}\\n\"\n",
    "#                 else:\n",
    "#                     diagram += f\"    {node_name}{{ {current_layer_type} }}\\n\"\n",
    "#                     diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "#                 prev_layer_type = current_layer_type\n",
    "#                 layer_count = 1\n",
    "\n",
    "#             add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "#         # Handle remaining repetitive layers at the end of a module\n",
    "#         if layer_count > 1:\n",
    "#             shape_id += 1\n",
    "#             diagram += f\"    subgraph shape{shape_id} [{prev_layer_type} x {layer_count}]\\n\"\n",
    "#             diagram += f\"        {parent_name}.{prev_layer_type}_0[\\\"{prev_layer_type} 0\\\"]\\n\"\n",
    "#             diagram += f\"        {parent_name}.{prev_layer_type}_...[\\\" ... \\\"]\\n\"\n",
    "#             diagram += f\"        {parent_name}.{prev_layer_type}_{layer_count - 1}[\\\"{prev_layer_type} {layer_count - 1}\\\"]\\n\"\n",
    "#             diagram += \"    end\\n\"\n",
    "#             diagram += f\"    {parent_name} --> shape{shape_id}\\n\"\n",
    "\n",
    "#     # Call the recursive function to build the diagram\n",
    "#     add_nodes_and_connections(model)\n",
    "\n",
    "#     # # Encapsulate the Mermaid code in an HTML block for rendering in Jupyter Notebook\n",
    "#     # html_code = f\"\"\"\n",
    "#     # <div class=\"mermaid\">\n",
    "#     # {diagram}\n",
    "#     # </div>\n",
    "#     # \"\"\"\n",
    "\n",
    "#     # # Display the diagram in the Jupyter Notebook cell\n",
    "#     # display(HTML(html_code))\n",
    "\n",
    "#     return diagram\n",
    "\n",
    "# def generate_mermaid_diagram_v3_4_5(model):\n",
    "#     \"\"\"\n",
    "#     Generates a Mermaid diagram representing the structure of a Hugging Face model, \n",
    "#     collapsing repetitive layers into shapes with numbered instances.\n",
    "\n",
    "#     Args:\n",
    "#         model: The Hugging Face model to be analyzed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     diagram = \"graph LR\\n\"\n",
    "#     layer_counts = {}  # To keep track of layer counts for each type\n",
    "\n",
    "#     # Add the root node representing the model\n",
    "#     diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "#     # Recursive function to traverse the model structure and add nodes and connections to the diagram\n",
    "#     def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "#         nonlocal diagram, layer_counts\n",
    "#         for name, child_module in module.named_children():\n",
    "#             current_layer_type = child_module.__class__.__name__\n",
    "#             node_name = parent_name + \".\" + name if parent_name else name\n",
    "\n",
    "#             # Count occurrences of each layer type\n",
    "#             layer_counts[current_layer_type] = layer_counts.get(current_layer_type, 0) + 1\n",
    "\n",
    "#             # If this is the first occurrence of this layer type, add it to the diagram\n",
    "#             if layer_counts[current_layer_type] == 1:\n",
    "#                 diagram += f\"    {node_name}{{ {current_layer_type} }}\\n\"\n",
    "#                 diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "#             add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "#     # Call the recursive function to build the diagram\n",
    "#     add_nodes_and_connections(model)\n",
    "\n",
    "#     # Add shapes for repetitive layers\n",
    "#     named_children = list(model.named_children())\n",
    "#     for layer_type, count in layer_counts.items():\n",
    "#         if count > 1:\n",
    "#             diagram += f\"    subgraph {layer_type}s [x{count}]\\n\"\n",
    "#             for i in range(count):\n",
    "#                 diagram += f\"        {layer_type}_{i}{{ {layer_type} {i} }}\\n\"\n",
    "#             diagram += \"    end\\n\"\n",
    "#             # Connect the parent to the first layer in the shape \n",
    "#             diagram += f\"    {model.__class__.__name__} --> {layer_type}_0\\n\"\n",
    "            \n",
    "#             # Connect the last layer to the subsequent component only if it exists\n",
    "#             if count < len(named_children):  # Check if there's a subsequent component\n",
    "#                 diagram += f\"    {layer_type}_{count - 1} --> {named_children[count][0]}\\n\"\n",
    "\n",
    "#     # # Encapsulate the Mermaid code in an HTML block for rendering in Jupyter Notebook\n",
    "#     # html_code = f\"\"\"\n",
    "#     # <div class=\"mermaid\">\n",
    "#     # {diagram}\n",
    "#     # </div>\n",
    "#     # \"\"\"\n",
    "\n",
    "#     # # Display the diagram in the Jupyter Notebook cell\n",
    "#     # display(HTML(html_code))\n",
    "\n",
    "#     return diagram\n",
    "\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# def generate_mermaid_diagram(model):\n",
    "#     \"\"\"\n",
    "#     Generates a Mermaid diagram representing the structure of a Hugging Face model, \n",
    "#     collapsing repetitive layers into shapes with layer count and using rounded rectangles for internal layers.\n",
    "\n",
    "#     Args:\n",
    "#         model: The Hugging Face model to be analyzed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     diagram = \"graph LR\\n\"\n",
    "#     layer_counts = {}\n",
    "\n",
    "#     # Add the root node representing the model\n",
    "#     diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "#     # Recursive function to traverse the model structure \n",
    "#     def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "#         nonlocal diagram, layer_counts\n",
    "#         for name, child_module in module.named_children():\n",
    "#             current_layer_type = child_module.__class__.__name__\n",
    "#             node_name = parent_name + \".\" + name if parent_name else name\n",
    "\n",
    "#             layer_counts[current_layer_type] = layer_counts.get(current_layer_type, 0) + 1\n",
    "\n",
    "#             if layer_counts[current_layer_type] == 1:\n",
    "#                 # Use rounded rectangle for single occurrences or non-repetitive layers\n",
    "#                 diagram += f\"    {node_name}(( {current_layer_type} ))\\n\"\n",
    "#                 diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "#             add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "#     add_nodes_and_connections(model)\n",
    "\n",
    "#     # Add shapes for repetitive layers with count and rounded rectangles for internal layers\n",
    "#     named_children = list(model.named_children())\n",
    "#     for layer_type, count in layer_counts.items():\n",
    "#         if count > 1:\n",
    "#             diagram += f\"    subgraph {layer_type}s\\n\"\n",
    "#             diagram += f\"        {layer_type}x{count}[\\\"{count} x {layer_type}\\\"]\\n\"  \n",
    "#             diagram += \"    end\\n\"\n",
    "#             diagram += f\"    {model.__class__.__name__} --> {layer_type}s\\n\"\n",
    "            \n",
    "#             if count < len(named_children): \n",
    "#                 diagram += f\"    {layer_type}s --> {named_children[count][0]}\\n\"\n",
    "\n",
    "#     # # Encapsulate the Mermaid code in an HTML block for rendering in Jupyter Notebook\n",
    "#     # html_code = f\"\"\"\n",
    "#     # <div class=\"mermaid\">\n",
    "#     # {diagram}\n",
    "#     # </div>\n",
    "#     # \"\"\"\n",
    "\n",
    "#     # # Display the diagram in the Jupyter Notebook cell\n",
    "#     # display(HTML(html_code))\n",
    "    \n",
    "#     return diagram\n",
    "\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# def generate_mermaid_diagram(model):\n",
    "#     \"\"\"\n",
    "#     Generates a Mermaid diagram representing the structure of a Hugging Face model,\n",
    "#     collapsing repetitive layers into shapes with layer count and using rounded\n",
    "#     rectangles for internal layers.\n",
    "\n",
    "#     Args:\n",
    "#         model: The Hugging Face model to be analyzed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     diagram = \"graph LR\\n\"\n",
    "#     layer_counts = {}\n",
    "\n",
    "#     # Add the root node representing the model\n",
    "#     diagram += f\"    {model.__class__.__name__}{{ {model.__class__.__name__} }}\\n\"\n",
    "\n",
    "#     # Recursive function to traverse the model structure\n",
    "#     def add_nodes_and_connections(module, parent_name=\"\"):\n",
    "#         nonlocal diagram, layer_counts\n",
    "#         for name, child_module in module.named_children():\n",
    "#             current_layer_type = child_module.__class__.__name__\n",
    "#             node_name = parent_name + \".\" + name if parent_name else name\n",
    "\n",
    "#             layer_counts[current_layer_type] = layer_counts.get(current_layer_type, 0) + 1\n",
    "\n",
    "#             if layer_counts[current_layer_type] == 1:\n",
    "#                 # Use rounded rectangle for single occurrences or non-repetitive layers\n",
    "#                 diagram += f\"    {node_name}{ (current_layer_type) }\\n\"\n",
    "#                 diagram += f\"    {parent_name} --> {node_name}\\n\"\n",
    "\n",
    "#             add_nodes_and_connections(child_module, node_name)\n",
    "\n",
    "#     add_nodes_and_connections(model)\n",
    "\n",
    "#     # Add shapes for repetitive layers with count and rounded rectangles for internal layers\n",
    "#     named_children = list(model.named_children())\n",
    "#     for layer_type, count in layer_counts.items():\n",
    "#         if count > 1:\n",
    "#             diagram += f\"    subgraph {layer_type}s\\n\"\n",
    "#             diagram += f\"        {layer_type}x{count}[\\\"{count} x {layer_type}\\\"]\\n\"  \n",
    "#             diagram += \"    end\\n\"\n",
    "#             diagram += f\"    {model.__class__.__name__} --> {layer_type}s\\n\"\n",
    "\n",
    "#             # Connect the shape to the subsequent component if it's not the last child\n",
    "#             if count < len(named_children): \n",
    "#                 diagram += f\"    {layer_type}s --> {named_children[count][0]}\\n\"\n",
    "\n",
    "#     # # Encapsulate the Mermaid code in an HTML block for rendering in Jupyter Notebook\n",
    "#     # html_code = f\"\"\"\n",
    "#     # <div class=\"mermaid\">\n",
    "#     # {diagram}\n",
    "#     # </div>\n",
    "#     # \"\"\"\n",
    "\n",
    "#     # # Display the diagram in the Jupyter Notebook cell\n",
    "#     # display(HTML(html_code))\n",
    "\n",
    "#     return diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versões antigas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from git import Repo\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from gml_unsupervised_learning_tools import DataPreprocessor\n",
    "# from gml_unsupervised_learning_tools import EmbeddingEvaluator\n",
    "# from funding_analyser import FundingEmbeddingGenerator\n",
    "\n",
    "# # Criar uma instância do FundingEmbeddingGenerator\n",
    "# embedding_generator = FundingEmbeddingGenerator()\n",
    "\n",
    "# try:\n",
    "#     # Criar a coluna 'texto_para_embedding' no dataframe df_fomento usando cuDF (se disponível)\n",
    "#     df_fomento = embedding_generator.create_embedding_column(use_cudf=True)\n",
    "# except:\n",
    "#     # Ou, criar a coluna 'texto_para_embedding' sem usar cuDF, usando apenas Pandas\n",
    "#     df_fomento = embedding_generator.create_embedding_column(use_cudf=False)\n",
    "\n",
    "# # Define the model names and the models you want to compare\n",
    "# model_names = [\n",
    "#     'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "#     'all-MiniLM-L6-v2'\n",
    "#     # Add more model names here if needed\n",
    "# ]\n",
    "\n",
    "# models = [\n",
    "#     SentenceTransformer(model_name)\n",
    "#     for model_name in model_names\n",
    "# ]\n",
    "\n",
    "# # Create an instance of EmbeddingEvaluator\n",
    "# benchmark = EmbeddingEvaluator(model_names, models, df_fomento) \n",
    "\n",
    "# # Gere o relatório de benchmarking\n",
    "# benchmark.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking de pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, cudf\n",
    "from funding_analyser import FundingEmbeddingGenerator\n",
    "\n",
    "df_fomento = pd.DataFrame()\n",
    "\n",
    "# Criar uma instância do EmbeddingGenerator\n",
    "embedding_generator = FundingEmbeddingGenerator()\n",
    "\n",
    "# Criar a coluna 'texto_para_embedding' no dataframe df_fomento usando cuDF (se disponível)\n",
    "try:\n",
    "    df_fomento = embedding_generator.create_embedding_column(use_cudf=True)\n",
    "except:\n",
    "    # Ou, criar a coluna 'texto_para_embedding' sem usar cuDF, usando apenas Pandas\n",
    "    df_fomento = embedding_generator.create_embedding_column(use_cudf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funding_analyser import ENPreprocessor\n",
    "\n",
    "# Criar instâncias do pré-processador\n",
    "en_preprocessor = ENPreprocessor()\n",
    "\n",
    "# Medir tempo pré-processar em lotes (remover sw, traduzir p/inglês, corrigir ortografia e lematizar)\n",
    "start_time = time.time()\n",
    "all_words_en = df_fomento['texto_para_embedding'].to_pandas().progress_apply(en_preprocessor.preprocess_text)  # type: ignore\n",
    "end_time = time.time()\n",
    "time_en = end_time - start_time\n",
    "print(f\"Tempo de execução da função en_preprocessor: {time_en:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funding_analyser import BRPreprocessor\n",
    "\n",
    "# Criar instância do pré-processador\n",
    "br_preprocessor = BRPreprocessor()\n",
    "\n",
    "# Medir tempo para pré-processar sem usar processamento em lotes (remover sw, traduzir p/português, lematizar)\n",
    "start_time = time.time()\n",
    "all_words_br = df_fomento['texto_para_embedding'].to_pandas().progress_apply(br_preprocessor.preprocess_text) # type: ignore\n",
    "end_time = time.time()\n",
    "time_br = end_time - start_time\n",
    "print(f\"Tempo de execução da função br_preprocess_text: {time_br:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in all_words_en[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in all_words_br[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking de geração de embeedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking - Geração de embeddings sem batches\n",
    "start_time = time.time()\n",
    "embeddings_sem_batch = embedding_generator.generate_embeddings(df_fomento)\n",
    "end_time = time.time()\n",
    "tempo_sem_batch = end_time - start_time\n",
    "print(f\"Tempo de execução sem batches: {tempo_sem_batch:.2f} segundos\")\n",
    "\n",
    "# Benchmarking - Geração de embeddings com batches\n",
    "start_time = time.time()\n",
    "embeddings_com_batch = embedding_generator.generate_embeddings_batch(df_fomento)\n",
    "end_time = time.time()\n",
    "tempo_com_batch = end_time - start_time\n",
    "print(f\"Tempo de execução com batches: {tempo_com_batch:.2f} segundos\")\n",
    "\n",
    "# Comparar os resultados (opcional)\n",
    "if np.allclose(embeddings_sem_batch, embeddings_com_batch):\n",
    "    print(\"Os embeddings gerados são iguais.\")\n",
    "else:\n",
    "    print(\"Os embeddings gerados são diferentes. Verifique a implementação.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir tempo de execução da função original\n",
    "start_time = time.time()\n",
    "all_words_original = df_fomento['texto_para_embedding'].to_pandas().apply(cpu_preprocess_text) # type: ignore\n",
    "original_time = time.time() - start_time\n",
    "print(f\"Tempo de execução da função original: {original_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir tempo de execução da função otimizada\n",
    "start_time = time.time()\n",
    "all_words_optimized = df_fomento['texto_para_embedding'].to_pandas().apply(gpu_preprocess_text) # type: ignore\n",
    "optimized_time = time.time() - start_time\n",
    "print(f\"Tempo de execução da função otimizada: {optimized_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# 1. Carregar o dataframe df_fomento (carregado e processado anteriormente)\n",
    "df_fomento = pdf\n",
    "\n",
    "# 2. Criar um grafo vazio\n",
    "G = nx.Graph()\n",
    "\n",
    "# 3. Obter todas as chaves únicas\n",
    "all_keys = set()\n",
    "for keys in df_fomento['detalhes'].map(lambda x: x.keys()):\n",
    "    all_keys.update(keys)\n",
    "\n",
    "# 4. Iterar sobre as linhas do dataframe\n",
    "for index, row in df_fomento.iterrows():\n",
    "    # 5. Criar um nó com o índice da linha como ID\n",
    "    G.add_node(index)\n",
    "\n",
    "    # 6. Adicionar as propriedades do dicionário ao nó\n",
    "    for key in all_keys:\n",
    "        G.nodes[index][key] = row['detalhes'].get(key, None)  # Usar None para chaves ausentes\n",
    "\n",
    "# 7. Exibir informações sobre o grafo\n",
    "print(\"Número de nós:\", G.number_of_nodes())\n",
    "print(\"Número de arestas:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fomento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Converter a coluna 'detalhes' para dicionários\n",
    "def convert_to_dict(text):\n",
    "    try:\n",
    "        return ast.literal_eval(text)\n",
    "    except ValueError:\n",
    "        return None  # Ou {} se preferir um dicionário vazio para entradas inválidas\n",
    "\n",
    "df_fomento['detalhes'] = df_fomento['detalhes'].astype(str).apply(convert_to_dict)\n",
    "\n",
    "# 3. Criar um grafo vazio\n",
    "G = nx.Graph()\n",
    "\n",
    "# 4. Obter todas as chaves únicas\n",
    "all_keys = set()\n",
    "for detalhes in df_fomento['detalhes']:\n",
    "    if detalhes:  # Verificar se detalhes é um dicionário válido\n",
    "        all_keys.update(detalhes.keys())\n",
    "\n",
    "# 5-8. Iterar, criar nós e adicionar propriedades\n",
    "for index, row in df_fomento.iterrows():\n",
    "    detalhes = row['detalhes']\n",
    "    G.add_node(index)\n",
    "    for key in all_keys:\n",
    "        G.nodes[index][key] = detalhes.get(key, None)\n",
    "\n",
    "# 9. Exibir informações sobre o grafo\n",
    "print(\"Número de nós:\", G.number_of_nodes())\n",
    "print(\"Número de arestas:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fomento.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[y for y in x.keys()] for x in df_fomento['detalhes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fomento.iloc[0]['financiadora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "cols_geninfo = ['financiadora','titulo','palavras-chave']\n",
    "cols_details = ['elegibilidade','descricao','valorfinanciado','datalimite']\n",
    "cols_moreinf = ['formasolicitacao']\n",
    "id=1\n",
    "w = 125\n",
    "\n",
    "for id,_ in enumerate(df_fomento.index):\n",
    "    print('-'*125)\n",
    "    print(f\"{cols_geninfo[-1].upper():>15}: {df_fomento.iloc[id][cols_geninfo[-1]].upper()} | {df_fomento.iloc[id][cols_geninfo[0]]}\")\n",
    "    for j in cols_details:\n",
    "        print(f\"{j.upper():>15}: {df_fomento['detalhes'][id][j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in df_fomento['detalhes'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fomento['detalhes'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Analisar dos dados de fomento - fase exploratória</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade semântica por similaridade de cossenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuml.cluster import KMeans, DBSCAN, HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import networkx as nx\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity_torch(a, b):\n",
    "  \"\"\"\n",
    "  Calcula a similaridade cosseno entre dois tensores a e b.\n",
    "\n",
    "  Args:\n",
    "      a: Primeiro tensor (torch.Tensor).\n",
    "      b: Segundo tensor (torch.Tensor).\n",
    "\n",
    "  Returns:\n",
    "      A similaridade cosseno entre os tensores a e b (um tensor com um único valor entre 0 e 1).\n",
    "  \"\"\"\n",
    "  return 1 - F.cosine_similarity(a, b, dim=0)\n",
    "\n",
    "def cosine_similarity_array(a, b):\n",
    "  \"\"\"\n",
    "  Calcula a similaridade cosseno entre dois vetores a e b.\n",
    "\n",
    "  Args:\n",
    "      a: Primeiro vetor (numpy array).\n",
    "      b: Segundo vetor (numpy array).\n",
    "\n",
    "  Returns:\n",
    "      A similaridade cosseno entre os vetores a e b (um valor entre 0 e 1).\n",
    "  \"\"\"\n",
    "  return 1 - cosine(a, b)\n",
    "\n",
    "# Criar nova coluna 'texto_para_embedding' combinando as informações desejadas\n",
    "def extrair_texto_para_embedding(row):\n",
    "    detalhes = row['detalhes']\n",
    "    texto = \"\"\n",
    "    if detalhes:\n",
    "        texto += detalhes.get('elegibilidade', '') + ' ' + detalhes.get('descricao', '')\n",
    "    texto += ' ' + row['palavras-chave']\n",
    "    return texto\n",
    "\n",
    "# Carregar o dataframe df_fomento (carregado e processado anteriormente)\n",
    "df_fomento = pdf\n",
    "\n",
    "# Preparar Dados\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2').to('cuda')\n",
    "\n",
    "# Combinar informações relevantes em um único texto\n",
    "df_fomento['texto_para_embedding'] = df_fomento.apply(extrair_texto_para_embedding, axis=1)\n",
    "\n",
    "# Gerar embeddings de texto na GPU\n",
    "embeddings = model.encode(df_fomento['texto_para_embedding'].tolist(), convert_to_tensor=True, device='cuda')\n",
    "embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "# Função para calcular a similaridade cosseno\n",
    "def cosine_similarity(a, b):\n",
    "    return 1 - cosine(a, b)\n",
    "\n",
    "# Benchmark e Agrupamento em Comunidades\n",
    "algorithms = {\n",
    "    'KMeans': KMeans(n_clusters=5, init='k-means++', random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'HDBSCAN': HDBSCAN(min_cluster_size=5, min_samples=2)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, algorithm in algorithms.items():\n",
    "    start_time = time.time()\n",
    "    clusters = algorithm.fit_predict(embeddings)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Criar Arestas no Grafo (dentro do loop para cada algoritmo)\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Obter todas as chaves únicas dos dicionários em 'detalhes'\n",
    "    all_keys = set()\n",
    "    for detalhes in df_fomento['detalhes']:\n",
    "        if detalhes:\n",
    "            all_keys.update(detalhes.keys())\n",
    "\n",
    "    similarity_threshold = 0.7\n",
    "\n",
    "    # Adicionar nós e arestas ao grafo\n",
    "    for i in range(len(embeddings)):\n",
    "        # Criar um nó com o índice da linha como ID e adicionar as propriedades do dicionário\n",
    "        G.add_node(i, **df_fomento.iloc[i]['detalhes'])\n",
    "\n",
    "        for j in range(i + 1, len(embeddings)):\n",
    "            if clusters[i] == clusters[j]:\n",
    "                similarity = cosine_similarity(embeddings[i], embeddings[j])\n",
    "                if similarity > similarity_threshold:\n",
    "                    G.add_edge(i, j, weight=similarity)\n",
    "\n",
    "    # Calcular métricas de avaliação\n",
    "    partition = {node: cluster for node, cluster in enumerate(clusters)}\n",
    "\n",
    "    # Converter clusters escalares em listas\n",
    "    communities = [[c] if isinstance(c, np.int32) else c for c in partition.values()] # type: ignore\n",
    "\n",
    "    # Remover clusters vazios\n",
    "    communities = [c for c in communities if c]\n",
    "\n",
    "    # Verificar se 'communities' é uma partição válida antes de calcular a modularidade\n",
    "    if nx.algorithms.community.is_partition(G, communities):\n",
    "        modularity = nx.algorithms.community.modularity(G, communities)\n",
    "    else:\n",
    "        print(f\"Aviso: {name} gerou uma partição inválida. Modularidade não será calculada.\")\n",
    "        modularity = None  # Ou outro valor padrão, como 0 ou -1\n",
    "\n",
    "    results[name] = {\n",
    "        'execution_time': execution_time,\n",
    "        'modularity': modularity,\n",
    "    }\n",
    "\n",
    "# Plotar Resultados\n",
    "fig = go.Figure()\n",
    "for name, result in results.items():\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=[name],\n",
    "        y=[result['execution_time']],\n",
    "        text=[f\"Tempo: {result['execution_time']:.2f}s<br>Modularidade: {result['modularity']:.3f}\"],\n",
    "        textposition='auto',\n",
    "        name=name\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Benchmark de Algoritmos de Agrupamento',\n",
    "    xaxis_title='Algoritmo',\n",
    "    yaxis_title='Tempo de Execução (segundos)'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir tempo para calcular Pagerank\n",
    "# We now have data as edge pairs\n",
    "# create a Graph using the source (src) and destination (dst) vertex pairs\n",
    "# G = cugraph.Graph()\n",
    "# G.from_cudf_edgelist(gdf, source='src', destination='dst')\n",
    "\n",
    "# # Let's now get the PageRank score of each vertex by calling cugraph.pagerank\n",
    "# df_page = cugraph.pagerank(G)\n",
    "\n",
    "# # Let's look at the top 10 PageRank Score\n",
    "# df_page.sort_values('pagerank', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "translator = GoogleTranslator(source='pt', target='en')\n",
    "resultado = translator.translate(\"Exemplo de tradução\")\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "translator = GoogleTranslator(source='de', target='en')\n",
    "resultado = translator.translate(\"Übersetzungsbeispiel\")\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "translator = GoogleTranslator(source='es', target='en')\n",
    "resultado = translator.translate(\"Ejemplo de traducción\")\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizar alguns dados dos currículos\n",
    "# for n,curriculo in enumerate(curriculos):\n",
    "#     print([x.get('Descrição') for x in curriculos[n].get('Linhas de Pesquisa')])\n",
    "\n",
    "# for n,curriculo in enumerate(curriculos):\n",
    "#     print([x.get('titulo') for x in curriculos[n].get('Produções').get('Artigos completos publicados em periódicos')])\n",
    "\n",
    "# curriculos[1].get('Áreas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --force-reinstall googletrans\n",
    "# !pip install --upgrade --force-reinstall googletrans==4.0.0-rc1 httpx==0.13.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes das traduções de frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funding_analyser import ENPreprocessor, BRPreprocessor\n",
    "\n",
    "# Criar uma instância do ENPreprocessor\n",
    "en_preprocessor = ENPreprocessor()\n",
    "\n",
    "# Frase em inglês para testar a tradução\n",
    "frase_portugues = \"Esta frase é em português.\"\n",
    "\n",
    "# Traduzir a frase para português\n",
    "traducao = en_preprocessor.translate_to_en([frase_portugues])[0]\n",
    "\n",
    "# Imprimir a tradução\n",
    "print(f\"Frase português: {frase_portugues}\")\n",
    "print(f\"Frase em inglês: {traducao}\")\n",
    "\n",
    "# Criar uma instância do BRPreprocessor\n",
    "br_preprocessor = BRPreprocessor()\n",
    "\n",
    "# Frase em inglês para testar a tradução\n",
    "frase_ingles = \"This is a test sentence.\"\n",
    "\n",
    "# Traduzir a frase para português\n",
    "traducao = br_preprocessor.translate_to_pt([frase_ingles])[0]\n",
    "\n",
    "# Imprimir a tradução\n",
    "print(f\"\\nFrase em inglês: {frase_ingles}\")\n",
    "print(f\"Frase português: {traducao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar embeedings para currículos (Abordagem abandonada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Instanciar o modelo de linguagem (use o mesmo modelo usado para os editais)\n",
    "modelo = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extrair o texto dos currículos\n",
    "curriculos_texto = []\n",
    "for curriculo in curriculos:\n",
    "    texto = \"\"\n",
    "    for key, value in curriculo.items():\n",
    "        if isinstance(value, str):\n",
    "            texto += value + \" \"\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, str):\n",
    "                    texto += item + \" \"\n",
    "                elif isinstance(item, dict):\n",
    "                    for subkey, subvalue in item.items():\n",
    "                        texto += str(subvalue) + \" \"\n",
    "    curriculos_texto.append(texto)\n",
    "\n",
    "# Gerar os embeddings dos currículos\n",
    "curriculos_embeddings = modelo.encode(curriculos_texto, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a similaridade do cosseno entre os embeddings dos editais e dos currículos\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similaridade = cosine_similarity(editais_embeddings.cpu().numpy(), curriculos_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o grafo de conhecimento\n",
    "import networkx as nx\n",
    "\n",
    "# Criar um grafo direcionado\n",
    "grafo = nx.DiGraph()\n",
    "\n",
    "# Adicionar nós para os editais\n",
    "for i, edital_embedding in enumerate(editais_embeddings):\n",
    "    grafo.add_node(f\"edital_{i}\")\n",
    "\n",
    "# Adicionar nós para os currículos\n",
    "for i, curriculo_embedding in enumerate(curriculos_embeddings):\n",
    "    grafo.add_node(f\"curriculo_{i}\")\n",
    "\n",
    "# Adicionar arestas com base na similaridade (limiar de 0.25)\n",
    "limiar_similaridade = 0.25\n",
    "for i, edital_embedding in enumerate(editais_embeddings):\n",
    "    for j, curriculo_embedding in enumerate(curriculos_embeddings):\n",
    "        if similaridade[i, j] > limiar_similaridade:\n",
    "            grafo.add_edge(f\"edital_{i}\", f\"curriculo_{j}\", weight=similaridade[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar as comunidades (utilizando o algoritmo Louvain)\n",
    "\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "# Encontrar as comunidades\n",
    "comunidades = list(greedy_modularity_communities(grafo)) # type: ignore\n",
    "comunidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exibir o grafo com Pyvis\n",
    "# from pyvis.network import Network\n",
    "\n",
    "# # Criar uma rede PyVis\n",
    "# net = Network(notebook=True, directed=True)\n",
    "\n",
    "# # Adicionar nós e arestas ao grafo PyVis\n",
    "# net.from_nx(grafo)\n",
    "\n",
    "# # Configurar a cor dos nós por comunidade\n",
    "# for i, comunidade in enumerate(comunidades):\n",
    "#     cor = f\"hsl({i * 360 / len(comunidades)}, 100%, 50%)\"\n",
    "#     for no in comunidade:\n",
    "#         net.get_node(no)[\"color\"] = cor\n",
    "\n",
    "# # Exibir o grafo\n",
    "# net.show(\"grafo_conhecimento.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomendar editais\n",
    "def recomendar_editais(curriculo_id, grafo, similaridade, top_n=5):\n",
    "    \"\"\"\n",
    "    Recomenda os editais mais propícios para um currículo com base no grafo de conhecimento.\n",
    "    \"\"\"\n",
    "    editais_similares = []\n",
    "    for no in grafo.neighbors(curriculo_id):\n",
    "        editais_similares.append((no, grafo.get_edge_data(curriculo_id, no)['weight']))\n",
    "    editais_similares.sort(key=lambda x: x[1], reverse=True)\n",
    "    return editais_similares[:top_n]\n",
    "\n",
    "# Exemplo de recomendação para o currículo 0\n",
    "curriculo_id = \"curriculo_0\"\n",
    "editais_recomendados = recomendar_editais(curriculo_id, grafo, similaridade)\n",
    "print(f\"Editais recomendados para o currículo {curriculo_id}: {editais_recomendados}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "print(f\"Criar Label 'Pesquisador' com as propriedades:\")\n",
    "print(f\"  Label: 'Pesquisador'\")\n",
    "print(f\"  {curriculos[k].get('Identificação')}\")\n",
    "subdict_formacaoacademica = {'Formação Acadêmica': [x.get('Descrição') for x in curriculos[k].get('Formação').get('Acadêmica')]}\n",
    "print(f\"  {subdict_formacaoacademica}\")\n",
    "print(f\"  {curriculos[k].get('Áreas')}\")\n",
    "print(f\"  {[x.get('Instituição') for x in curriculos[k].get('Atuação Profissional')]}\")\n",
    "print(f\"  {curriculos[k].get('Linhas de Pesquisa')}\")\n",
    "print(f\"  {curriculos[k].get('ProjetosPesquisa')}\")\n",
    "print(f\"  {curriculos[k].get('ProjetosExtensão')}\")\n",
    "print(f\"  {curriculos[k].get('ProjetosDesenvolvimento')}\")\n",
    "print(f\"  {curriculos[k].get('Patentes e registros')}\")\n",
    "print(f\"  {curriculos[k].get('Bancas')}\")\n",
    "print(f\"  {curriculos[k].get('Orientações')}\")\n",
    "print(f\"  {curriculos[k].get('JCR2')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
