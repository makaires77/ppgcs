{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge jupyterlab-drawio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Diagramas e mapas</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensões sobre pesquisa em Inteligência Artificial (IA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid \n",
    "graph LR\n",
    "    subgraph Racionalmente\n",
    "        RL(Sistemas para Raciocinar logicamente e gerar informações)\n",
    "        AO(Sistemas para Agir e atingir objetivos):::strong\n",
    "    end\n",
    "\n",
    "    subgraph Humanamente\n",
    "        PI(Sistemas para Processar informação como humanos)\n",
    "        CH(Sistemas para Emular ações humanas de comportamento)\n",
    "    end\n",
    "\n",
    "    P[Pensar]:::pensar -.-> PI & RL\n",
    "    A[Agir]:::agir -.-> CH & AO\n",
    "\n",
    "    style P fill:#f2e6ff,stroke:#9966ff,stroke-width:2px, color:#9966ff\n",
    "    style A fill:#e6f2ff,stroke:#6699ff,stroke-width:2px, color:#6699ff\n",
    "\n",
    "    classDef humanamente fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "    classDef strong fill:#ccf,stroke:#333,stroke-width:4px;\n",
    "\n",
    "    linkStyle 0,1 stroke:#333,stroke-width:1px;\n",
    "\n",
    "    style PI fill:#f2e6ff,stroke:#9966ff,stroke-width:2px\n",
    "    style CH fill:#e6f2ff,stroke:#6699ff,stroke-width:2px\n",
    "    style RL fill:#f2e6ff,stroke:#9966ff,stroke-width:2px\n",
    "    style AO fill:#e6f2ff,stroke:#6699ff,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapas mentais sobre pesquisa em IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "mindmap\n",
    "  root((Temas em Inteligência Artificial))\n",
    "    (Representação de Conhecimento)\n",
    "        Ontologias\n",
    "        Redes Semânticas\n",
    "        Lógica de Primeira Ordem\n",
    "        Grafos de Conhecimento\n",
    "    (Resolução de Problemas)\n",
    "        Busca em Grafos\n",
    "        Planejamento\n",
    "        Jogos\n",
    "    (Outras Áreas)\n",
    "        Visão Computacional\n",
    "        Geração de Texto\n",
    "        Robótica\n",
    "        ...\n",
    "    (Temas Transversais)\n",
    "        Ética e IA\n",
    "        Segurança em IA\n",
    "        IA e Sociedade\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "mindmap\n",
    "  root((Bases para Inteligência Artificial - IA))\n",
    "    (Fundamentos)\n",
    "        (Representação de Conhecimento e Raciocínio)\n",
    "        (Resolução de problemas)\n",
    "        (Aprendizado de Máquina)\n",
    "        (Processamento de Linguagem Natural - PLN)\n",
    "        (Modelos em Grafos)\n",
    "    (Áreas de Aplicação)\n",
    "        Sistemas Especialistas\n",
    "        Visão Computacional\n",
    "        Bioinformática\n",
    "        Automação\n",
    "        Finanças\n",
    "        Jogos\n",
    "    (Abordagens Metodológicas)\n",
    "        Deep Learning\n",
    "        Aprendizado Estruturado\n",
    "        Computação Evolutiva\n",
    "        Lógica Fuzzy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "mindmap\n",
    "  root((Inteligência Artificial - IA))\n",
    "    (Fundamentos)\n",
    "        (Representação de Conhecimento e Raciocínio)\n",
    "            Lógica de primeira ordem\n",
    "            Redes Semânticas\n",
    "            Ontologias\n",
    "            Raciocínio Probabilístico\n",
    "        (Resolução de problemas)\n",
    "            Busca e Planejamento\n",
    "            Algoritmos de Busca\n",
    "            Planejamento Clássico\n",
    "            Planejamento em Ambientes Incertos\n",
    "        (Aprendizado de Máquina)\n",
    "            Aprendizado Supervisionado\n",
    "            Aprendizado Não Supervisionado\n",
    "            Aprendizado por Reforço\n",
    "            Aprendizado Profundo - Deep Learning\n",
    "            Aprendizado Estruturado\n",
    "        (Processamento de Linguagem Natural - PLN)\n",
    "            Compreensão de Linguagem Natural\n",
    "            Geração de Linguagem Natural\n",
    "            Tradução Automática\n",
    "            Análise de Sentimentos\n",
    "        (Modelos em Grafos)\n",
    "            Redes Bayesianas\n",
    "            Modelos Markovianos Ocultos - HMMs\n",
    "            Conditional Random Fields - CRFs\n",
    "            Grafos de Conhecimento\n",
    "            Redes Neurais para Grafos - GNNs\n",
    "    (Áreas de Aplicação)\n",
    "        Visão Computacional\n",
    "            Reconhecimento de Imagens\n",
    "            Detecção de Objetos\n",
    "            Segmentação de Imagens\n",
    "        Robótica\n",
    "            Navegação\n",
    "            Planejamento de Movimento\n",
    "            Controle de Robôs\n",
    "            Interação Humano-Robô\n",
    "        Sistemas Especialistas\n",
    "        Jogos\n",
    "        Bioinformática\n",
    "        Finanças\n",
    "    (Abordagens Metodológicas)\n",
    "        Deep Learning\n",
    "            Redes Neurais Artificiais\n",
    "        Aprendizado Estruturado\n",
    "        Computação Evolutiva\n",
    "        Lógica Fuzzy\n",
    "    (Temas Transversais)\n",
    "        Ética e IA\n",
    "        Segurança em IA\n",
    "        IA e Sociedade\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapa mental em análises de grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "mindmap\n",
    "  root((Modelagem do Mundo Real em Grafos para Aplicações em Saúde))\n",
    "    (Grafos Simples)\n",
    "      Aplicações\n",
    "        Interações Medicamento-Medicamento - Efeitos Adversos\n",
    "        Co-ocorrência de Doenças - Comorbidades\n",
    "      Limitações\n",
    "        Não modela a direção ou força das relações\n",
    "    (Grafos Direcionados)\n",
    "      Aplicações\n",
    "        Propagação de Doenças Infecciosas - Transmissão\n",
    "        Vias Metabólicas - Reações Bioquímicas\n",
    "      Limitações\n",
    "        Não modela múltiplas relações entre os mesmos elementos\n",
    "    (Grafos Ponderados)\n",
    "      Aplicações\n",
    "        Redes de Interação Proteína-Proteína - Afinidade de Ligação\n",
    "        Redes de Coexpressão Gênica - Correlação\n",
    "      Limitações\n",
    "        Não modela diferentes tipos de relações simultaneamente\n",
    "    (Grafos Multicamadas)\n",
    "      Aplicações\n",
    "        Redes de Interação Fármaco-Alvo-Doença Múltiplos Mecanismos\n",
    "        Redes de Microbioma Humano \n",
    "        Nichos Tecnológicos\n",
    "      Limitações\n",
    "        Aumenta a complexidade da análise\n",
    "    (Hipergrafos)\n",
    "      Aplicações\n",
    "        Análise de Dados de Saúde Multimodais - Prontuários, Imagens, Genética\n",
    "        Modelagem de Fatores de Risco Complexos para Doenças Multifatoriais\n",
    "      Limitações\n",
    "        Dificuldade de visualização e análise\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagramas de blocos da metodologia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "graph BT\n",
    "        R(Feedback)\n",
    "    subgraph Extrair_e_preparar_dados_de_entrada\n",
    "        A[(Currículos Pesquisadores da ICT)]\n",
    "        B[(Intenções dos Pesquisadores da ICT)]\n",
    "        C[Processos em PDI]\n",
    "        D[Produtos Estratégicos CEIS]\n",
    "        E[Limpar e Normalizar Termos]\n",
    "    end\n",
    "\n",
    "    subgraph Gerar Embeddings em GPU\n",
    "        H[Embedding Multilingue Sentence Transformers]\n",
    "        I[Codificar Curriculos e Interesses dos Pesquisadores]\n",
    "        J[Codificar Produtos estratégicos para o CEIS]\n",
    "    end\n",
    "\n",
    "    subgraph Modelar_Grafo_de_Conhecimento\n",
    "        K[Gerar Entidades e Relacionamentos]\n",
    "        L[Grafo Multiplex dos Macroprocessos PDI no CEIS]\n",
    "    end\n",
    "\n",
    "    subgraph Analisar_Modelo_Grafo_com_ML/DL\n",
    "        M[Detectar Comunidades]\n",
    "        N[Analisar Similaridade com Redes Neurais em Grafos]\n",
    "        O[Identificar Lacunas]\n",
    "    end\n",
    "\n",
    "    subgraph Visualizar e Recomendar\n",
    "        P[Recomendar Alinhamento Competências/Produtos]\n",
    "        Q[Visualizar Resultados]\n",
    "    end\n",
    "\n",
    "    A --> E\n",
    "    B --> E\n",
    "    C --> E\n",
    "    D --> E\n",
    "    E --> H\n",
    "    H --> I\n",
    "    H --> J\n",
    "    I --> K\n",
    "    J --> K\n",
    "    N <--> M\n",
    "    L --> M\n",
    "    L --> N    \n",
    "    M --> O\n",
    "    N --> O\n",
    "    K --> L\n",
    "    O --> P\n",
    "    P --> Q\n",
    "    P -.- R\n",
    "    Q -.- R\n",
    "    R -.- L\n",
    "    R -.- J\n",
    "    R -.- I\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` mermaid\n",
    "graph BT\n",
    "    R(Feedback)\n",
    "    subgraph A._Extract_and_prepare_input_data\n",
    "        A[(ICT Researchers' Resumes)]\n",
    "        B[(ICT Researchers' Intentions)]\n",
    "        C[Processes in R&D]\n",
    "        D[CEIS Strategic Products]\n",
    "        E[Clean and Normalize Terms]\n",
    "    end\n",
    "\n",
    "    subgraph B._Generate_Embeddings_in_GPU\n",
    "        H[Multilingual Sentence Transformers Embedding]\n",
    "        I[Encode Researchers' Resumes and Interests]\n",
    "        J[Encode Strategic Products for CEIS]\n",
    "    end\n",
    "\n",
    "    subgraph C._Model_Knowledge_Graph\n",
    "        K[Generate Entities and Relationships]\n",
    "        L[Multilayer Graph of RDI Macroprocesses at CEIS]\n",
    "    end\n",
    "\n",
    "    subgraph D._Analyze_Graph_Model_with_ML/DL\n",
    "        M[Detect Communities]\n",
    "        N[Analyze Similarity with Graph Neural Networks]\n",
    "        O[Identify Gaps]\n",
    "    end\n",
    "\n",
    "    subgraph E._Visualize_and_Recommend\n",
    "        P[Recommend Skills/Products Alignment]\n",
    "        Q[Visualize Results]\n",
    "    end\n",
    "\n",
    "    A --> E\n",
    "    B --> E\n",
    "    C --> E\n",
    "    D --> E\n",
    "    E --> H\n",
    "    H --> I\n",
    "    H --> J\n",
    "    I --> K\n",
    "    J --> K\n",
    "    N <--> M\n",
    "    L --> M\n",
    "    L --> N    \n",
    "    M --> O\n",
    "    N --> O\n",
    "    K --> L\n",
    "    O --> P\n",
    "    P --> Q\n",
    "    P -.- R\n",
    "    Q -.- R\n",
    "    R -.- L\n",
    "    R -.- J\n",
    "    R -.- I\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização dos fluxos de dados em processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    CPU <--> Barramento;\n",
    "    GPU <--> Barramento;\n",
    "    Barramento <--> RAM;\n",
    "    SSD <--> Barramento;\n",
    "\n",
    "    subgraph \" \"\n",
    "        CPU[CPU]:::cpu\n",
    "        GPU[GPU]:::gpu\n",
    "        RAM[Memória RAM]:::ram\n",
    "        SSD[Armazenamento]:::ssd\n",
    "        Barramento[Barramento]:::bus\n",
    "    end\n",
    "    \n",
    "    classDef cpu fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "    classDef gpu fill:#ccf,stroke:#333,stroke-width:2px;\n",
    "    classDef ram fill:#ff9,stroke:#333,stroke-width:2px;\n",
    "    classDef ssd fill:#f0f,stroke:#333,stroke-width:2px;\n",
    "    classDef bus fill:#9fc,stroke:#333,stroke-width:2px;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    CPU_Processador -. Leitura .- Barramento;\n",
    "    CPU_Processador -. Escrita .- Barramento;\n",
    "    GPU_Aceleradora -. Leitura .- Barramento;\n",
    "    GPU_Aceleradora -. Escrita .- Barramento;\n",
    "    SSD -. Leitura .- Barramento;\n",
    "    SSD -. Escrita .- Barramento;\n",
    "    RAM -. Leitura .- Barramento;\n",
    "    RAM -. Escrita .- Barramento; \n",
    "    Barramento -. Leitura .- RAM;\n",
    "    Barramento -. Escrita .- RAM;        \n",
    "\n",
    "    subgraph \" \"\n",
    "        CPU_Processador[CPU_Processador]:::cpu\n",
    "        GPU_Aceleradora[GPU_Aceleradora]:::gpu\n",
    "        RAM[Memória RAM]:::ram\n",
    "        SSD[Armazenamento]:::ssd\n",
    "        Barramento[====__Barramento_PCIexpress__====]:::bus\n",
    "    end\n",
    "    \n",
    "    classDef cpu fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "    classDef gpu fill:#ccf,stroke:#333,stroke-width:2px;\n",
    "    classDef ram fill:#ff9,stroke:#333,stroke-width:2px;\n",
    "    classDef ssd fill:#f0f,stroke:#333,stroke-width:2px;\n",
    "    classDef bus fill:#9fc,stroke:#333,stroke-width:2px;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    CPU --> Barramento_PCIe;\n",
    "    GPU --> Barramento_PCIe;\n",
    "    Barramento_PCIe <--> RAM;\n",
    "    SSD1 --> Barramento_PCIe;\n",
    "    SSD2 --> Barramento_SATA;\n",
    "    Barramento_SATA --> Ponte_Southbridge;\n",
    "    Ponte_Southbridge --> Barramento_PCIe;\n",
    "    USB --> Ponte_Southbridge;\n",
    "\n",
    "    subgraph \" \"\n",
    "        CPU[CPU]:::cpu\n",
    "        GPU[GPU]:::gpu\n",
    "        RAM[Memória RAM]:::ram\n",
    "        SSD1[SSD NVMe]:::ssd\n",
    "        SSD2[SSD SATA]:::ssd\n",
    "        Barramento_PCIe[====__Barramento PCIe__====]:::bus\n",
    "        Barramento_SATA[---- Barramento SATA ----]:::bus\n",
    "        Ponte_Southbridge[Ponte Southbridge]:::chipset\n",
    "        USB[USB]:::usb\n",
    "    end\n",
    "    \n",
    "    classDef cpu fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "    classDef gpu fill:#ccf,stroke:#333,stroke-width:2px;\n",
    "    classDef ram fill:#ff9,stroke:#333,stroke-width:2px;\n",
    "    classDef ssd fill:#f0f,stroke:#333,stroke-width:2px;\n",
    "    classDef bus fill:#9fc,stroke:#333,stroke-width:2px;\n",
    "    classDef chipset fill:#ccc,stroke:#333,stroke-width:2px;\n",
    "    classDef usb fill:#9ff,stroke:#333,stroke-width:2px;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    CPU -->|\"Leitura/Escrita\"| Barramento_PCIe;\n",
    "    GPU -->|\"Leitura/Escrita\"| Barramento_PCIe;\n",
    "    Barramento_PCIe <-->|\"Leitura/Escrita\"| RAM;\n",
    "    SSD1 -->|\"Leitura/Escrita\"| Barramento_PCIe;\n",
    "    SSD2 -->|\"Leitura/Escrita\"| Barramento_SATA;\n",
    "    Barramento_SATA --> Ponte_Southbridge;\n",
    "    Ponte_Southbridge --> Barramento_PCIe;\n",
    "    USB --> Ponte_Southbridge;\n",
    "\n",
    "    subgraph \" \"\n",
    "        CPU[CPU]:::cpu\n",
    "        GPU[GPU]:::gpu\n",
    "        RAM[Memória RAM]:::ram\n",
    "        SSD1[SSD NVMe]:::ssd\n",
    "        SSD2[SSD SATA]:::ssd\n",
    "        Barramento_PCIe[====__Barramento PCIe__====]:::bus\n",
    "        Barramento_SATA[---- Barramento SATA ----]:::bus\n",
    "        Ponte_Southbridge[Ponte Southbridge]:::chipset\n",
    "        USB[USB]:::usb\n",
    "    end\n",
    "    \n",
    "    classDef cpu fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "    classDef gpu fill:#ccf,stroke:#333,stroke-width:2px;\n",
    "    classDef ram fill:#ff9,stroke:#333,stroke-width:2px;\n",
    "    classDef ssd fill:#f0f,stroke:#333,stroke-width:2px;\n",
    "    classDef bus fill:#9fc,stroke:#333,stroke-width:2px;\n",
    "    classDef chipset fill:#ccc,stroke:#333,stroke-width:2px;\n",
    "    classDef usb fill:#9ff,stroke:#333,stroke-width:2px;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    CPU -->|\"até 16 GB/s\"| Barramento_PCIe;\n",
    "    GPU -->|\"até 16 GB/s\"| Barramento_PCIe;\n",
    "    Barramento_PCIe <-->|\"até 64 GB/s\"| RAM;\n",
    "    SSD1 -->|\"até 4 GB/s\"| Barramento_PCIe;\n",
    "    SSD2 -->|\"até 0,6 GB/s\"| Barramento_SATA;\n",
    "    Barramento_SATA -->|\"até 0,55 GB/s\"| Ponte_Southbridge;\n",
    "    Ponte_Southbridge -->|\"até 5 GB/s\"| Barramento_PCIe;\n",
    "    USB -->|\"até 10 Gbps\"| Ponte_Southbridge;\n",
    "\n",
    "    subgraph \" \"\n",
    "        CPU[CPU]:::cpu\n",
    "        GPU[GPU]:::gpu\n",
    "        RAM[Memória RAM]:::ram\n",
    "        SSD1[SSD NVMe]:::ssd\n",
    "        SSD2[SSD SATA]:::ssd\n",
    "        Barramento_PCIe[====__Barramento PCIe__====]:::bus\n",
    "        Barramento_SATA[---- Barramento SATA ----]:::bus\n",
    "        Ponte_Southbridge[Ponte Southbridge]:::chipset\n",
    "        USB[USB]:::usb\n",
    "    end\n",
    "    \n",
    "    classDef cpu fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "    classDef gpu fill:#ccf,stroke:#333,stroke-width:2px;\n",
    "    classDef ram fill:#ff9,stroke:#333,stroke-width:2px;\n",
    "    classDef ssd fill:#f0f,stroke:#333,stroke-width:2px;\n",
    "    classDef bus fill:#9fc,stroke:#333,stroke-width:2px;\n",
    "    classDef chipset fill:#ccc,stroke:#333,stroke-width:2px;\n",
    "    classDef usb fill:#9ff,stroke:#333,stroke-width:2px;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na maioria dos casos, a comunicação entre CPU e GPU não é totalmente direta, mas sim mediada pela memória RAM e pelo barramento PCIe. Esse é o cenário mais comum em computadores pessoais e servidores convencionais.\n",
    "\n",
    "Como funciona a comunicação \"indireta\" via RAM:\n",
    "\n",
    "- A CPU processa dados e os envia para a memória RAM.\n",
    "- A GPU acessa esses dados na RAM através do barramento PCIe.\n",
    "- A GPU processa os dados (ex: renderiza gráficos) e grava os resultados na RAM.\n",
    "- A CPU acessa os resultados na RAM, completando o ciclo.\n",
    "\n",
    "Vantagens da comunicação via RAM:\n",
    "\n",
    "- Flexibilidade: Permite que múltiplas CPUs e GPUs acessem e compartilhem dados de forma organizada.\n",
    "- Escalabilidade: Facilita a expansão do sistema com mais memória ou GPUs.\n",
    "- Custo-benefício: É uma solução mais econômica em comparação com tecnologias de comunicação direta.\n",
    "\n",
    "Quando há comunicação direta?\n",
    "\n",
    "Em alguns casos específicos, existe sim comunicação direta entre CPU e GPU, sem a necessidade de passar pela RAM. Isso ocorre principalmente em sistemas de alto desempenho e para tarefas que exigem altíssima velocidade de transferência de dados, como:\n",
    "\n",
    "- Inteligência Artificial: Treinamento de modelos de deep learning e inferência em larga escala.\n",
    "- Computação Científica: Simulações complexas, análise de dados massivos, etc.\n",
    "- Processamento de imagens e vídeos: Edição e renderização de vídeos em alta resolução, etc.\n",
    "\n",
    "Tecnologias que permitem comunicação direta:\n",
    "\n",
    "- NVLink (NVIDIA): Conecta CPUs e GPUs NVIDIA com alta velocidade e baixa latência.\n",
    "- AMD Infinity Fabric: Interconecta CPUs, GPUs e outros componentes AMD.\n",
    "\n",
    "Ou seja, embora a comunicação direta entre CPU e GPU seja possível e vantajosa em cenários específicos, a comunicação via RAM e barramento PCIe ainda é a mais comum e eficiente na maioria das situações de computadores pessoais. A escolha da melhor abordagem depende dos requisitos de desempenho, custo e complexidade do sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    CPU -->|\"Leitura/Escrita\"| Barramento_PCIe;\n",
    "    GPU -->|\"Leitura/Escrita\"| Barramento_PCIe;\n",
    "    Barramento_PCIe <-->|\"Leitura/Escrita\"| RAM;\n",
    "    SSD1 -->|\"Leitura/Escrita\"| Barramento_PCIe;\n",
    "    SSD2 -->|\"Leitura/Escrita\"| Barramento_SATA;\n",
    "    Barramento_SATA --> Ponte_Southbridge;\n",
    "    Ponte_Southbridge --> Barramento_PCIe;\n",
    "    USB --> Ponte_Southbridge;\n",
    "    CPU <-->|\"NVLink/Infinity Fabric\"| GPU;\n",
    "\n",
    "    subgraph \" \"\n",
    "        CPU[CPU]:::cpu\n",
    "        GPU[GPU]:::gpu\n",
    "        RAM[Memória RAM]:::ram\n",
    "        SSD1[SSD NVMe]:::ssd\n",
    "        SSD2[SSD SATA]:::ssd\n",
    "        Barramento_PCIe[====__Barramento PCIe__====]:::bus\n",
    "        Barramento_SATA[---- Barramento SATA ----]:::bus\n",
    "        Ponte_Southbridge[Ponte Southbridge]:::chipset\n",
    "        USB[USB]:::usb\n",
    "    end\n",
    "    \n",
    "    classDef cpu fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "    classDef gpu fill:#ccf,stroke:#333,stroke-width:2px;\n",
    "    classDef ram fill:#ff9,stroke:#333,stroke-width:2px;\n",
    "    classDef ssd fill:#f0f,stroke:#333,stroke-width:2px;\n",
    "    classDef bus fill:#9fc,stroke:#333,stroke-width:2px;\n",
    "    classDef chipset fill:#ccc,stroke:#333,stroke-width:2px;\n",
    "    classDef usb fill:#9ff,stroke:#333,stroke-width:2px;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em um sistema pessoal real os dados são:\n",
    "\n",
    "- Velocidade da RAM: Velocidade máxima da RAM para 128 GB/s, pois a X570 suporta essa velocidade com overclocking.\n",
    "- PCIe 4.0: Versão do PCIe (4.0) no rótulo do barramento.\n",
    "- SATA III: Versão do SATA (III) no rótulo do barramento.\n",
    "- Ponte Southbridge: Em vez do nome genérico \"Ponte Southbridge\" é designado pelo chipset específico da placa mãe (X570).\n",
    "\n",
    "Observações:\n",
    "\n",
    "- Limitações: Mesmo com o PCIe 4.0, a velocidade de transferência de dados pode ser limitada por outros fatores, como a velocidade da RAM, do SSD e do processador.\n",
    "- CPU: Velocidade máxima do PCIe da CPU em 32 GB/s, dado que o Ryzen 7 5800X possui 20 lanes PCIe 4.0, que são divididas entre o slot da placa de vídeo (x16) e outros dispositivos.\n",
    "- Overclocking: A velocidade máxima da RAM de 128 GB/s pode ser atingida com overclocking. A velocidade real depende tando da configuração de memória na BIOS como das capacidades da placa mãe.\n",
    "\n",
    "Este diagrama atualizado reflete as velocidades de transferência da placa mãe Gigabyte X570 AORUS ULTRA, que equipa o ambeiente de testes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    CPU -->|\"PCIe 4.0 x16 até 32 GB/s\"| Barramento_PCIe;\n",
    "    GPU -->|\"PCIe 4.0 x16 até 32 GB/s\"| Barramento_PCIe;\n",
    "    Barramento_PCIe <-->|\"DDR4 até 128 GB/s (O.C.)\"| RAM;\n",
    "    SSD1 -->|\"PCIe 4.0 x4 até 8 GB/s\"| Barramento_PCIe;\n",
    "    SSD2 -->|\"SATA III até 600 MB/s\"| Barramento_SATA;\n",
    "    Barramento_SATA -->|\"até 600 MB/s\"| Ponte_Southbridge;\n",
    "    Ponte_Southbridge -->|\"PCIe 4.0 x4 até 8 GB/s\"| Barramento_PCIe;\n",
    "    USB -->|\"USB 3.2 Gen2x2 até 20 Gbps\"| Ponte_Southbridge;\n",
    "\n",
    "    subgraph \" \"\n",
    "        CPU[Ryzen 7 5800X]:::cpu\n",
    "        GPU[RTX 4080]:::gpu\n",
    "        RAM[64GB DDR4]:::ram\n",
    "        SSD1[SSD NVMe]:::ssd\n",
    "        SSD2[SSD SATA]:::ssd\n",
    "        Barramento_PCIe[====__Barramento_PCIe_4.0__====]:::bus\n",
    "        Barramento_SATA[---- Barramento SATA III ----]:::bus\n",
    "        Ponte_Southbridge[Chipset X570]:::chipset\n",
    "        USB[USB]:::usb\n",
    "    end\n",
    "    \n",
    "    classDef cpu fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "    classDef gpu fill:#ccf,stroke:#333,stroke-width:2px;\n",
    "    classDef ram fill:#ff9,stroke:#333,stroke-width:2px;\n",
    "    classDef ssd fill:#f0f,stroke:#333,stroke-width:2px;\n",
    "    classDef bus fill:#9fc,stroke:#333,stroke-width:2px;\n",
    "    classDef chipset fill:#ccc,stroke:#333,stroke-width:2px;\n",
    "    classDef usb fill:#9ff,stroke:#333,stroke-width:2px;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Etapas da Metodologia</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapas para gerar o grafo de conhecimento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar os dados dos arquivos JSON:\n",
    "\n",
    "- Carregar os dados dos currículos dos pesquisadores em um DataFrame cuDF.\n",
    "- Carregar os dados da Matriz CEIS em um DataFrame cuDF.\n",
    "- Carregar as relações para biológicos e pequenas moléculas em estruturas de dados Python (listas de dicionários).\n",
    "\n",
    "Criar o grafo de conhecimento usando aceleração de hardware:\n",
    "- Criar um grafo direcionado cuGraph.\n",
    "- Ingerir nós de Pesquisador, Produto CEIS e Etapas de desenvolvimento (biológicos e pequenas moléculas).\n",
    "- Ingerir atributos para os nós e arestas, como tipo de nó (\"pesquisador\", \"produto\", \"relacionamento\"), nome, ID, etc.\n",
    "\n",
    "Conectar os nós através dos relacionamentos:\n",
    "- Ingerir arestas para representar os relacionamentos entre as diversas entidades.\n",
    "- Conectar os nós de etapas entre si, seguindo as relações definidas nos arquivos JSON de bioprodutos e pequenas moléculas.\n",
    "- Conectar nós de Pesquisadores aos nós Produtos com base na similaridade entre dados do currículo, interesses e áreas de atuação do pesquisador e as Etapas de PDI, áreas de aplicação e necessidades relacionadas ao Produto.\n",
    "- Conectar os nós de Competências aos nós de Etapas, com base nos relacionamentos demandamos com base no tipo de produto (biológico ou pequena molécula).\n",
    "\n",
    "Calcular métricas de grafo:\n",
    "- Calcular métricas de centralidade, como grau de entrada e saída, para identificar os nós mais importantes no grafo.\n",
    "- Calcular métricas de caminho, como distância e betweenness centrality, para analisar as relações entre os nós.\n",
    "- Converter o grafo cuGraph para NetworkX:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar a Ingestão de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs.: Foi necessário adaptar o código de extração de nós e arestas da função construct_graph de acordo com a estrutura dos arquivos JSON obtidos da fase de levantamento de dados. Para certificar de que os arquivos JSON estejam sempre no formato correto, contendo informações sobre os nós (features) e as arestas (source e target) necessárias foram utilizaods esquemas de validação para garantir as entradas necessárias ao processamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimizar processamento de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otimização do código:\n",
    "\n",
    "- Vetorização: Foram utilizadas operações vetorizadas no PyTorch para aproveitar ao máximo a capacidade de processamento paralelo da GPU.\n",
    "- Tamanhos de lote: Foram realizados experimentados com diferentes tamanhos de lote para encontrar o valor ideal que maximiza a utilização da GPU e a velocidade de treinamento.\n",
    "- Transferência de dados: Foi minimizada a transferência de dados entre a CPU e a GPU, utilizando técnica de pin_memory e DataLoader com múltiplos workers.\n",
    "- Operações assíncronas: Foram utilizadas operações assíncronas para sobrepor a computação na GPU com a transferência de dados.\n",
    "\n",
    "A velocidade de treinamento depende de muitos fatores, dentre eles da complexidade do modelo de IA. Modelos mais complexos com mais camadas e parâmetros levam mais tempo para treinar. E por óbvio, quanto maiores os conjuntos de dados, maiores serão os tempo de treinamento, mas nos experimentos foi utilizado o mesmo conjunto de dados para avaliar cada um dos modelos. Além disso, a velocidade de treinamento pode ser afetada por outros fatores, como a otimização do código, a escolha do otimizador e a configuração dos hiperparâmetros para cada modelo.\n",
    "\n",
    "Detecção de gargalos por comparação com as velocidades teóricas:\n",
    "\n",
    "- Largura de banda da memória: Comparar a taxa de transferência de dados da RAM (DDR4 até 128 GB/s neste caso) com a taxa de acesso à memória do modelo durante o treinamento. Se a taxa de acesso à memória for significativamente menor que a largura de banda da RAM, pode haver um gargalo de memória.\n",
    "- Velocidade do barramento PCIe: Comparar a velocidade do barramento PCIe (até 32 GB/s neste caso) com a taxa de transferência de dados entre a CPU e a GPU durante o treinamento. Se a taxa de transferência for muito menor que a velocidade do PCIe, pode haver um gargalo no barramento.\n",
    "- Velocidade da GPU: Analisar a utilização da GPU (RTX 4080) durante o treinamento. Se a utilização estiver baixa, o modelo pode não estar sendo executado com a máxima eficiência na GPU.\n",
    "\n",
    "Para superar gargalo de memória,  o código foi otimizado para reduzir o acesso à memória, antes de avaliar a troca por RAM mais rápida; Para manter a utilização da GPU alta, foi aumentado o tamanho do lote e foram utilizadas técnicas de otimização de código para melhorar a performance na GPU. Para isso, foi utilizado PyTorch Profiler para identificar gargalos de desempenho de cada modelo; Foi monitorada a utilização da GPU com o NVIDIA SMI durante o treinamento; e a taxa de acesso à memória do modelo foi coomparada com a largura de banda da RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esquemas de validação JSON para dados de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install validclust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from git import Repo\n",
    "from model_evaluation import ModelPerformance, GNNModel, GKANModel, GraphDataset, JSONValidator\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "\n",
    "# Esquema JSON para validação (schema.json)\n",
    "schema_curriculos = {\n",
    "  \"type\": \"array\",\n",
    "  \"items\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"Identificação\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"Nome\": {\"type\": \"string\"},\n",
    "          \"ID Lattes\": {\"type\": \"string\"},\n",
    "          \"Última atualização\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"required\": [\"Nome\", \"ID Lattes\", \"Última atualização\"]\n",
    "      },\n",
    "      \"Idiomas\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"Idioma\": {\"type\": \"string\"},\n",
    "            \"Proficiência\": {\"type\": \"string\"}\n",
    "          },\n",
    "          \"required\": [\"Idioma\", \"Proficiência\"]\n",
    "        }\n",
    "      },\n",
    "      \"Formação\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"Acadêmica\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"Ano\": {\"type\": \"string\"},\n",
    "                \"Descrição\": {\"type\": \"string\"}\n",
    "              },\n",
    "              \"required\": [\"Ano\", \"Descrição\"]\n",
    "            }\n",
    "          },\n",
    "          \"Pos-Doc\": {\"type\": \"array\"},\n",
    "          \"Complementar\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"Ano\": {\"type\": \"string\"},\n",
    "                \"Descrição\": {\"type\": \"string\"}\n",
    "              },\n",
    "              \"required\": [\"Ano\", \"Descrição\"]\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"Acadêmica\", \"Pos-Doc\", \"Complementar\"]\n",
    "      },\n",
    "      \"Atuação Profissional\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"Instituição\": {\"type\": \"string\"},\n",
    "            \"Ano\": {\"type\": \"string\"},\n",
    "            \"Descrição\": {\"type\": \"string\"},\n",
    "            \"Outras informações\": {\"type\": \"string\"}\n",
    "          },\n",
    "          \"required\": [\"Instituição\", \"Ano\", \"Descrição\", \"Outras informações\"]\n",
    "        }\n",
    "      },\n",
    "      \"Linhas de Pesquisa\": {\"type\": \"array\"},\n",
    "      \"Áreas\": {\n",
    "        \"type\": \"object\",\n",
    "        \"patternProperties\": {\n",
    "          \"^[0-9]+\\\\.$\": {\"type\": \"string\"}\n",
    "        }\n",
    "      },\n",
    "      \"Produções\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"Artigos completos publicados em periódicos\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"ano\": {\"type\": \"string\"},\n",
    "                \"fator_impacto_jcr\": {\"type\": \"string\"},\n",
    "                \"ISSN\": {\"type\": \"string\"},\n",
    "                \"titulo\": {\"type\": \"string\"},\n",
    "                \"revista\": {\"type\": \"string\"},\n",
    "                \"autores\": {\"type\": \"string\"},\n",
    "                \"data_issn\": {\"type\": \"string\"},\n",
    "                \"DOI\": {\"type\": \"string\"},\n",
    "                \"Qualis\": {\"type\": \"string\"}\n",
    "              },\n",
    "              \"required\": [\"ano\", \"fator_impacto_jcr\", \"ISSN\", \"titulo\", \"revista\", \"autores\", \"data_issn\", \"DOI\", \"Qualis\"]\n",
    "            }\n",
    "          },\n",
    "          \"Resumos publicados em anais de congressos\": {\n",
    "            \"type\": \"object\",\n",
    "            \"patternProperties\": {\n",
    "              \"^[0-9]+\\\\.$\": {\"type\": \"string\"}\n",
    "            }\n",
    "          },\n",
    "          \"Apresentações de Trabalho\": {\n",
    "            \"type\": \"object\",\n",
    "            \"patternProperties\": {\n",
    "              \"^[0-9]+\\\\.$\": {\"type\": \"string\"}\n",
    "            }\n",
    "          },\n",
    "          \"Outras produções bibliográficas\": {\n",
    "            \"type\": \"object\",\n",
    "            \"patternProperties\": {\n",
    "              \"^[0-9]+\\\\.$\": {\"type\": \"string\"}\n",
    "            }\n",
    "          },\n",
    "          \"Entrevistas, mesas redondas, programas e comentários na mídia\": {\n",
    "            \"type\": \"object\",\n",
    "            \"patternProperties\": {\n",
    "              \"^[0-9]+\\\\.$\": {\"type\": \"string\"}\n",
    "            }\n",
    "          },\n",
    "          \"Demais tipos de produção técnica\": {\n",
    "            \"type\": \"object\",\n",
    "            \"patternProperties\": {\n",
    "              \"^[0-9]+\\\\.$\": {\"type\": \"string\"}\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"Artigos completos publicados em periódicos\", \n",
    "          # \"Apresentações de Trabalho\",\n",
    "          # 'Outras produções bibliográficas', \n",
    "          # \"Entrevistas, mesas redondas, programas e comentários na mídia\", \n",
    "          # \"Demais tipos de produção técnica\", \n",
    "          # \"Resumos publicados em anais de congressos\"\n",
    "          ]\n",
    "      },\n",
    "      \"ProjetosPesquisa\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"chave\": {\"type\": \"string\"},\n",
    "            \"titulo_projeto\": {\"type\": \"string\"},\n",
    "            \"descricao\": {\"type\": \"string\"}\n",
    "          },\n",
    "          \"required\": [\"chave\", \"titulo_projeto\", \"descricao\"]\n",
    "        }\n",
    "      },\n",
    "      \"ProjetosExtensão\": {\"type\": \"array\"},\n",
    "      \"ProjetosDesenvolvimento\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"chave\": {\"type\": \"string\"},\n",
    "            \"titulo_projeto\": {\"type\": \"string\"},\n",
    "            \"descricao\": {\"type\": \"string\"}\n",
    "          },\n",
    "          \"required\": [\"chave\", \"titulo_projeto\", \"descricao\"]\n",
    "        }\n",
    "      },\n",
    "      \"ProjetosOutros\": {\"type\": \"array\"},\n",
    "      \"Patentes e registros\": {\"type\": \"object\"},\n",
    "      \"Bancas\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"Participação em bancas de trabalhos de conclusão\": {\n",
    "            \"type\": \"object\",\n",
    "            \"patternProperties\": {\n",
    "              \"^[0-9]+\\\\.$\": { \"type\": \"string\" }\n",
    "            }\n",
    "          },\n",
    "          \"Participação em bancas de comissões julgadoras\": {\n",
    "            \"type\": \"object\",\n",
    "            \"patternProperties\": {\n",
    "              \"^[0-9]+\\\\.$\": { \"type\": \"string\" }\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          # \"Participação em bancas de trabalhos de conclusão\",\n",
    "          # \"Participação em bancas de comissões julgadoras\"\n",
    "        ]\n",
    "      },\n",
    "      \"Orientações\": {\"type\": \"array\"},\n",
    "      \"JCR2\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"doi\": { \"type\": [\"string\", \"null\"] },\n",
    "            \"impact-factor\": {\"type\": \"string\"},\n",
    "            \"original_title\": {\"type\": \"string\"}\n",
    "          },\n",
    "          \"required\": [\n",
    "            # \"doi\", \n",
    "            # \"impact-factor\", \n",
    "            # \"original_title\"\n",
    "            ]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\n",
    "      \"Identificação\", \n",
    "      \"Idiomas\", \n",
    "      \"Formação\", \n",
    "      \"Atuação Profissional\", \n",
    "      \"Linhas de Pesquisa\", \n",
    "      \"Áreas\", \n",
    "      \"Produções\", \n",
    "      \"ProjetosPesquisa\", \n",
    "      \"ProjetosExtensão\", \n",
    "      \"ProjetosDesenvolvimento\", \n",
    "      \"ProjetosOutros\", \n",
    "      \"Patentes e registros\", \n",
    "      \"Bancas\", \n",
    "      \"Orientações\", \n",
    "      \"JCR2\",\n",
    "      ]\n",
    "  }\n",
    "}\n",
    "\n",
    "# Salvar o esquema em um arquivo JSON\n",
    "with open('schema_curriculos.json', 'w') as f:\n",
    "    json.dump(schema_curriculos, f, indent=4)\n",
    "\n",
    "# Schema para dados sobre processos produtivos\n",
    "schema_process = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"nodes\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"id\": { \"type\": \"string\" },\n",
    "          \"label\": { \"type\": \"string\" }\n",
    "        },\n",
    "        \"required\": [\"id\", \"label\"]\n",
    "      }\n",
    "    },\n",
    "    \"edges\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"from\": { \"type\": \"string\" },\n",
    "          \"to\": { \"type\": \"string\" }\n",
    "        },\n",
    "        \"required\": [\"from\", \"to\"]\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"nodes\", \"edges\"]\n",
    "}   \n",
    "\n",
    "# Salvar o esquema em um arquivo JSON\n",
    "filename = 'schema_process.json'\n",
    "pathfilename = os.path.join(root_folder,'_data','in_json',filename)\n",
    "with open(pathfilename, 'w') as f:\n",
    "    json.dump(schema_process, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from graph_knowledge import GraphKnowledge as kg\n",
    "\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "filename = \"levantamento_interesses.xlsx\"\n",
    "\n",
    "input_path = os.path.join(root_folder, \"_data\",\"in_xls\")\n",
    "output_path = os.path.join(root_folder, \"_data\",\"in_json\")\n",
    "kg.generate_interesses_json(os.path.join(input_path,filename), os.path.join(output_path,\"input_interesses_pesquisadores.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validar arquivos de dados de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = JSONValidator()\n",
    "\n",
    "filenames = [\n",
    "    'input_curriculos.json',\n",
    "    'input_process_biologics.json',\n",
    "    'input_process_smallmolecules.json',\n",
    "    'input_gestao.json',\n",
    "    ]\n",
    "\n",
    "for filename in filenames:\n",
    "    validator.validate_input(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair competências dos arquivos JSON\n",
    "graph_data = []\n",
    "for json_file in json_files:\n",
    "    graph_data.extend(validator.extract_competencias(json_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptar o código de extração de nós e arestas na função construct_graph de acordo com a estrutura dos seus arquivos JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe para criar o dataset do PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A classe GraphConstructor recebe uma lista de arquivos JSON como entrada e constrói o grafo de conhecimento a partir dos dados extraídos.\n",
    "- A classe GraphDataset encapsula o grafo de conhecimento em um objeto Dataset do PyTorch, permitindo que ele seja usado no DataLoader da classe ModelPerformance.\n",
    "\n",
    "A seguir instanciamos as classes para gerar o dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, graph_data):\n",
    "        self.graph_data = graph_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # Retorna 1 pois temos apenas um grafo\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graph_data\n",
    "\n",
    "class GraphConstructor:\n",
    "    def __init__(self, json_files):\n",
    "        self.json_files = json_files\n",
    "        self.graph_data = None\n",
    "\n",
    "    def construct_graph(self):\n",
    "        nodes = []\n",
    "        edges = []\n",
    "        node_idx = 0\n",
    "\n",
    "        for json_file in self.json_files:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                # Extrair nós e arestas do JSON\n",
    "                for node_data in data['nodes']:\n",
    "                    nodes.append(node_data['features'])  # Assumindo que 'features' contém as features do nó\n",
    "                    node_idx += 1\n",
    "\n",
    "                for edge_data in data['edges']:\n",
    "                    source = edge_data['source']\n",
    "                    target = edge_data['target']\n",
    "                    edges.append([source, target])\n",
    "\n",
    "        # Criar o objeto Data do PyTorch Geometric\n",
    "        x = torch.tensor(nodes, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        self.graph_data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "        return self.graph_data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de arquivos JSON\n",
    "json_files = ['competencias.json', 'processos.json', 'gestao.json']\n",
    "\n",
    "# Criar o grafo de conhecimento\n",
    "graph_constructor = GraphConstructor(json_files)\n",
    "graph_data = graph_constructor.construct_graph()\n",
    "\n",
    "# Criar o dataset do PyTorch\n",
    "dataset = GraphDataset(graph_data)\n",
    "\n",
    "# ... usar o dataset na classe ModelPerformance ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de perda para aprendizado não-supervisionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No contexto de aprendizado não-supervisionado para alinhar competências da ICT aos produtos estratégicos, a escolha da função de perda considerou o objetivo de agrupar pesquisadores com competências semelhantes e identificar as lacunas em relação aos produtos estratégicos. Neste caso, não existe nesse cenário rótulos de classe pré-definidos. Portanto, as funções de perda tradicionais, como MSE ou Cross-Entropy, não são as mais adequadas.\n",
    "\n",
    "Considerando as opções de abordagem em redes neurais (com KANs, com Fourier e Híbrida) que serão utilizadas nos problemas de aprendizado não-supervisionado, a **Triplet Loss** surge como uma escolha promissora para esse tipo de problema, pois se concentra em aprender embeddings que agrupam nós semelhantes e separam nós distintos, o que se alinha com o objetivo de agrupar pesquisadores com competências semelhantes e identificar lacunas.\n",
    "\n",
    "O raciocínio sobre a Triplet Loss ser uma boa escolha para o aprendizado não-supervisionado em grafos com o objetivo de alinhar competências se aplica não só para abordagem com KANs mas também, de forma similar, se aplica bem, com algumas adaptações, para as outras abordagens de Rede Neural com Fourier e Rede Neural Híbrida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como aplicar a Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Definir trios:**\n",
    "    * **Âncora:** Um pesquisador.\n",
    "    * **Positivo:** Outro pesquisador com competências semelhantes à âncora, idealmente trabalhando em um produto estratégico que requer essas competências.\n",
    "    * **Negativo:** Um pesquisador com competências diferentes da âncora, preferencialmente que não esteja associado a nenhum produto estratégico que a âncora poderia contribuir.\n",
    "\n",
    "2. **Calcular perda:**\n",
    "    * A Triplet Loss busca minimizar a distância entre a âncora e o positivo, e maximizar a distância entre a âncora e o negativo.\n",
    "    * A função de perda é definida como:\n",
    "      \n",
    "<center>L = max(d(âncora, positivo) - d(âncora, negativo) + margem, 0)</center>\n",
    "      \n",
    "<center>onde 'd(a, b)' é a distância entre os embeddings de 'a' e 'b', e 'margem' é um hiperparâmetro que define a diferença mínima desejada entre as distâncias.</center>\n",
    "\n",
    "3. **Amostrar trios:**\n",
    "    * A escolha dos trios é crucial para o bom desempenho da Triplet Loss.\n",
    "    * Estratégias de amostragem como \"hard negative mining\" podem ser utilizadas para selecionar os trios mais informativos, que contribuem mais para o aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vantagens da Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Alinhamento com o objetivo:** A Triplet Loss foca em agrupar nós semelhantes e separar nós distintos, o que é diretamente relevante para o problema de alinhamento de competências.\n",
    "* **Consideração da estrutura do grafo:** A escolha dos trios pode levar em conta as relações entre os nós no grafo, como a colaboração entre pesquisadores ou a participação em projetos comuns.\n",
    "* **Flexibilidade:** A Triplet Loss pode ser combinada com diferentes arquiteturas de redes neurais em grafos, como GNNs, para capturar as informações do grafo de forma mais eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação da função de perda com Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar a Triplet Loss, foi utilizada biblioteca `PyTorch Metric Learning`, que oferece diversas funções de perda e métodos de mineração para facilitar o treinamento.\n",
    "\n",
    "Lembre-se que a escolha da função de perda é apenas um dos aspectos do problema. A arquitetura da rede neural, a estratégia de amostragem dos trios e a escolha dos hiperparâmetros também são importantes para o bom desempenho do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabelas LaTeX e mapas mentais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie uma instância da classe, passando o arquivo JSON e o arquivo BibTeX\n",
    "\n",
    "\n",
    "mindmap = MindmapGenerator('data.json', 'referencias.bib')\n",
    "\n",
    "# Gere o mapa mental\n",
    "mindmap.generate_mindmap('mindmap.png')\n",
    "\n",
    "# Gere a tabela LaTeX com as contagens de tipos de entrada do arquivo BibTeX\n",
    "mindmap.generate_latex_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Abordagens GNN com função perda Triplet Loss</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede KAN com função de perda por Triplet Loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A KAN foi responsável por gerar os embeddings dos nós do grafo, que foram utilizados pela Triplet Loss para calcular as distâncias entre os nós. A saída da KAN é um vetor de embedding para cada nó, representando suas características e relações no grafo.\n",
    "\n",
    "### Definição dos trios:\n",
    "\n",
    "A definição dos trios (âncora, positivo e negativo) segue a mesma lógica das outras abordagens:\n",
    "- Âncora: Um pesquisador.\n",
    "- Positivo: Outro pesquisador com competências semelhantes à âncora, idealmente associado a um produto estratégico que demanda essas competências.\n",
    "- Negativo: Um pesquisador com competências diferentes da âncora, preferencialmente não associado a produtos estratégicos que a âncora poderia contribuir.\n",
    "\n",
    "\n",
    "### Cálculo da perda:\n",
    "\n",
    "- A Triplet Loss foi calculada com base nos embeddings gerados pela Kolmogorov-Arnold Networks (KAN). \n",
    "\n",
    "- Quanto à amostragem dos trios, foi utilizada uma estratégia de amostragem eficiente, a \"hard negative mining\", para selecionar os trios mais informativos para o aprendizado.\n",
    "\n",
    "- A Triplet Loss foi utilizada para aprender embeddings que representem tanto as características estruturais do grafo quanto as relações de similaridade entre os nós, porém com o diferencial de aprender sobre as relações que ligam os nós (arestas).\n",
    "\n",
    "- Quanto à otimização, durante o treinamento, o otimizador ajustará os pesos da KAN para minimizar a Triplet Loss. A função de perda buscou minimizar a distância entre a âncora e o positivo, e maximizar a distância entre a âncora e o negativo. Isso gerou embeddings que agrupam pesquisadores com competências semelhantes e separam aqueles com competências distintas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural Fourier (Transformadas de Fourier + GNN):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Triplet Loss pode ser utilizada em conjunto com as Transformadas de Fourier para aprender embeddings que representem tanto as características estruturais do grafo quanto as relações de similaridade entre os nós.\n",
    "\n",
    "* As Transformadas de Fourier podem auxiliar na identificação de padrões e características relevantes na estrutura do grafo,  enquanto a Triplet Loss guia o aprendizado para agrupar nós com competências semelhantes.\n",
    "\n",
    "* A combinação dessas técnicas pode levar a um modelo mais robusto e capaz de capturar diferentes aspectos do problema de alinhamento de competências."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural Híbrida (Controle de Sincronização + GNN):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Triplet Loss foi utilizada para auxiliar na sincronização dos nós, incentivando o agrupamento de pesquisadores com competências semelhantes e a separação daqueles com competências diferentes.\n",
    "\n",
    "* A dinâmica de sincronização, baseada no valor de Fiedler, pode complementar a Triplet Loss, ajudando a identificar os nós mais importantes para o alinhamento de competências e guiando o aprendizado da rede.\n",
    "\n",
    "* A combinação da Triplet Loss com a GNN permite capturar as informações locais e globais do grafo,  enquanto a dinâmica de sincronização fornece uma perspectiva adicional sobre a importância dos nós e arestas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observações Gerais:**\n",
    "\n",
    "A escolha da função de perda é apenas um dos aspectos do problema. A arquitetura da rede neural, a estratégia de amostragem dos trios e a escolha dos hiperparâmetros também são importantes para o bom desempenho do modelo. Outras funções de perda, como a DGI ou a VGAE, também podem ser exploradas, especialmente se você desejar capturar a estrutura global do grafo e aprender representações mais robustas. É crucial avaliar o desempenho do modelo com diferentes métricas de avaliação, como as discutidas anteriormente, para garantir que ele esteja alinhando as competências da ICT aos produtos estratégicos de forma eficaz.\n",
    "\n",
    "A Triplet Loss é uma função de perda versátil que pode ser aplicada em diferentes abordagens de redes neurais em grafos. Para o problema de alinhamento de competências em aprendizado não-supervisionado, a escolha da melhor função de perda e da arquitetura do modelo dependerá das características específicas do seu problema e dos seus objetivos, mas pode-se observar de forma geral, que:\n",
    "\n",
    "* A escolha dos trios (âncora, positivo e negativo) é crucial para o bom desempenho da Triplet Loss. É importante definir uma estratégia de amostragem que leve em conta as características do grafo e o objetivo de alinhamento de competências.\n",
    "* A margem da Triplet Loss é um hiperparâmetro importante que deve ser ajustado para obter o melhor desempenho.\n",
    "* A Triplet Loss pode ser combinada com outras funções de perda, como a DGI ou a VGAE, para capturar diferentes aspectos do problema e melhorar o aprendizado das representações.\n",
    "* É fundamental avaliar o desempenho do modelo com diferentes métricas de avaliação,  além de analisar os embeddings e clusters gerados para garantir que o modelo esteja alinhando as competências de forma eficaz e interpretável.\n",
    "\n",
    "\n",
    "A interpretabilidade do modelo também é importante. Analise os embeddings gerados e os clusters formados para entender como o modelo está realizando o alinhamento de competências.\n",
    "Ao combinar a Triplet Loss com uma arquitetura de rede neural em grafos adequada e uma estratégia de amostragem eficiente, você poderá desenvolver um modelo de aprendizado não-supervisionado capaz de alinhar as competências da ICT aos produtos estratégicos e auxiliar na identificação de lacunas e oportunidades de desenvolvimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros iniciais e ajuste de hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A escolha dos parâmetros para cada modelo depende de diversos fatores, como o tamanho e a complexidade do grafo de conhecimento, a quantidade de dados de treinamento, a capacidade computacional disponível e o objetivo da análise. \n",
    "\n",
    "Partimos de alguns valores iniciais razoáveis para cada parâmetro para ajustá-los posteriormente com base nos resultados dos experimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `num_features`: Depende do número de features para cada nó do grafo (pesquisadores, competências e produtos). \n",
    "    * **Exemplo:** Se você tiver 10 features para cada pesquisador (e.g., anos de experiência, número de publicações, áreas de atuação), 5 features para cada competência (e.g., nível de proficiência, tipo de competência) e 3 features para cada produto estratégico (e.g., complexidade, área terapêutica), você pode concatenar esses features em um vetor de 18 dimensões.\n",
    "    * **Valor inicial:**  Número total de features extraídas para cada nó.\n",
    "\n",
    "* `hidden_dim`: Define a dimensão das camadas ocultas da KAN.\n",
    "    * **Valor inicial:**  64 ou 128.  Experimentar variações em potências de 2.\n",
    "\n",
    "* `num_classes`: Define o número de classes (clusters) que você deseja gerar.\n",
    "    * **Valor inicial:**  Mesmo valor utilizado no modelo híbrido.\n",
    "\n",
    "* `num_layers`: Define o número de camadas na KAN.\n",
    "    * **Valor inicial:**  3 ou 4 é um bom início. Escolher um valor que faça sentido para o problema.  Você pode usar técnicas como o método do cotovelo para auxiliar na escolha do número ideal de clusters.\n",
    "\n",
    "* `dropout`: Define a taxa de dropout.\n",
    "    * **Valor inicial:**  0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observações:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Ajuste dos parâmetros:**  É fundamental realizar experimentos e analisar os resultados para ajustar os parâmetros e encontrar a melhor configuração para cada modelo.\n",
    "* **Técnicas de otimização:**  Utilize técnicas como grid search ou random search para explorar diferentes combinações de parâmetros e encontrar a configuração ótima.\n",
    "* **Métricas de avaliação:**  Monitore as métricas de avaliação durante o treinamento e a validação para avaliar o desempenho do modelo e guiar o ajuste dos parâmetros.\n",
    "\n",
    "Estes foram apenas os valores iniciais. A melhor configuração para os parâmetros dependerá das características do grafo de conhecimento, da quantidade de dados de treinamento e do objetivo da análise. Esses valores foram buscados por otimização automatizada de hiperparâmetros dentro de espaços de busca relacionados com os valores inciais mostrados acima. A partir dos desse ajuste e de vários experimentos para medição de desempenho computacional é que se chegou aos hiperparâmetros adequados para obter o melhor desempenho de cada modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Visualização dos dados do Dataset</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar a estrutura de manipulação dos arquivos JSON e construir o grafo de análise, primeiro carregamos os dados dos arquivos JSON e, em seguida, criamos o grafo de conhecimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_knowledge import GraphKnowledge as kg\n",
    "kg.info_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pytorch-metric-learning\n",
    "\n",
    "# Importações necessárias\n",
    "import os\n",
    "import json\n",
    "import cudf\n",
    "import cugraph\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from git import Repo\n",
    "from pyvis.network import Network\n",
    "from IPython.display import HTML, display\n",
    "from semantic_matcher import RedeNeuralHibrida, RedeNeuralKAN, RedeNeuralFourier\n",
    "\n",
    "def display_full_width_df(df):\n",
    "    \"\"\"\n",
    "    Formats the DataFrame to occupy the entire width of the cell, uses line breaks, and justifies the text to the left.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be formatted.\n",
    "    \"\"\"\n",
    "\n",
    "    styled_df = df.to_html(classes='full-width-df')\n",
    "    css_style = \"\"\"\n",
    "    <style>\n",
    "    .full-width-df td {\n",
    "        word-wrap: break-word;\n",
    "        white-space: pre-wrap;\n",
    "        text-align: left;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    display(HTML(css_style + styled_df))\n",
    "\n",
    "    # .full-width-df {\n",
    "    #      width: 100%;\n",
    "    #  }\n",
    "    # return css_style + styled_df\n",
    "\n",
    "def recuperar_questionario():\n",
    "    filename = 'Alinhamento de competências em pesquisa às políticas para desenvolvimento do CEIS.xlsx'\n",
    "    pathfilename = os.path.join(xlsx_folder, filename)    \n",
    "    df_questionario = pd.read_excel(pathfilename).iloc[:, 7:]\n",
    "    print('Questionamentos realizados em levantamento junto aos pesquisadores da ICT:')\n",
    "    for n,i in enumerate(df_questionario.keys()):\n",
    "        if n==0:\n",
    "            print('\\nQuestões e palavras-chave de pesquisa:')\n",
    "        elif n==2:\n",
    "            print('\\nCompetências já dominadas e a evoluir:')\n",
    "        elif n==4:\n",
    "            print('\\nIntenção de atuar em atividades de desenvolvimento tencológico:')\n",
    "        elif n==5:\n",
    "            print('\\nNível de maturidade segmentado por tipo de tecnologias:')\n",
    "        elif n==11:\n",
    "            print('\\nConhecimentos sobre Estratégia para Desenvolvimento do CEIS, quanto:')\n",
    "        elif n==15:\n",
    "            print('\\nProdutos mais próximos das pesquisas na ICT:')\n",
    "        elif n==19:\n",
    "            print('\\nOcupação do tempo de trabalho por tipos de atividades de trabalho:')\n",
    "        elif n==25:\n",
    "            print('\\nSatisfação e identificação:')        \n",
    "        print(f\"  {n+1:2} {i}\")\n",
    "    return df_questionario\n",
    "\n",
    "def montar_dataframes_levantamento():\n",
    "    filename = 'Alinhamento de competências em pesquisa às políticas para desenvolvimento do CEIS.xlsx'\n",
    "    pathfilename = os.path.join(xlsx_folder, filename)\n",
    "    df_levantamento = pd.read_excel(pathfilename).iloc[:, 7:]\n",
    "    df_levantamento.columns=['questoes_pesquisa','palavras_chave','compet_dominadas','compet_desenvolver','avancar_desenv_desafios',\n",
    "                            'tec_diagnostico','tec_pesquisa','tec_terapeutica','tec_serviço','tec_social','tec_digital',\n",
    "                            'conhec_blocos','conhec_desafios','conhec_plataformas','conhec_produtos',\n",
    "                            'desafio_mais_proximo','produtos_bloco1','produtos_bloco2','adicional_contibuir',\n",
    "                            'tempo_lev_dados','tempo_analise_dados','tempo_debates_grupo','tempo_redigir_textos','tempo_reunioes','tempo_comunicacoes',                      \n",
    "                            'satisfacao','nome']\n",
    "\n",
    "    # df_levantamento.drop(columns=['tempo_fx1','tempo_fx2','tempo_fx3','tempo_fx4','tempo_fx5','tempo_fx6'], inplace=True)\n",
    "    print('Questionamentos realizados em levantamento junto aos pesquisadores da ICT:')\n",
    "    for n,i in enumerate(df_levantamento.keys()):\n",
    "        if n==0:\n",
    "            print('\\nQuestões e palavras-chave de pesquisa:')\n",
    "        elif n==2:\n",
    "            print('\\nCompetências já dominadas e a evoluir:')\n",
    "        elif n==4:\n",
    "            print('\\nIntenção de atuar em atividades de desenvolvimento tencológico:')\n",
    "        elif n==5:\n",
    "            print('\\nNível de maturidade segmentado por tipo de tecnologias:')\n",
    "        elif n==11:\n",
    "            print('\\nConhecimentos sobre Estratégia para Desenvolvimento do CEIS, quanto:')\n",
    "        elif n==15:\n",
    "            print('\\nProdutos mais próximos das pesquisas na ICT:')\n",
    "        elif n==19:\n",
    "            print('\\nOcupação do tempo de trabalho por tipos de atividades de trabalho:')\n",
    "        elif n==25:\n",
    "            print('\\nSatisfação e identificação:')        \n",
    "        print(f\"  {n+1:2} {i}\")\n",
    "\n",
    "    # Criar df_questoes com as 5 primeiras colunas do df_levantamento\n",
    "    df_questoes = df_levantamento.iloc[:, :5]\n",
    "\n",
    "    # Adicionar coluna 'id_resposta' ao df_questoes \n",
    "    df_questoes['id_resposta'] = df_levantamento.index\n",
    "\n",
    "    # Adicionar coluna 'nome_pesquisador' ao df_questoes com os valores da última coluna do df_levantamento \n",
    "    # df_questoes['nome_pesquisador'] = df_levantamento.iloc[:, -1]\n",
    "\n",
    "    # Montar dataframe com faixas de TRL por cada tipo de tecnologia\n",
    "    df_trls = df_levantamento.iloc[:,5:11]\n",
    "    df_trls['id_resposta'] = df_levantamento.index\n",
    "\n",
    "    # Montar dataframe sobre o conhecimento atual sobre estratégia do CEIS\n",
    "    df_estrat_ceis = df_levantamento.iloc[:,11:15]\n",
    "    df_estrat_ceis['id_resposta'] = df_levantamento.index\n",
    "\n",
    "    # Montar dataframe sobre os desafios e produtos mais próximos dentro da estratégia do CEIS\n",
    "    df_produtos_proximos = df_levantamento.iloc[:,15:19]\n",
    "    df_produtos_proximos['id_resposta'] = df_levantamento.index\n",
    "\n",
    "    # Montar dataframe sobre tempo destinado aos tipos de atividades típicas em pesquisa\n",
    "    df_tempo_ativ = df_levantamento.iloc[:,19:25]\n",
    "    df_tempo_ativ['id_resposta'] = df_levantamento.index\n",
    "\n",
    "    # Montar dataframe sobre satisfação com a coordenação de pesquisa e nome\n",
    "    df_satisf = df_levantamento.iloc[:,25:-1]\n",
    "\n",
    "    return df_levantamento, df_questoes, df_trls, df_estrat_ceis, df_produtos_proximos, df_tempo_ativ, df_satisf\n",
    "\n",
    "def listar_questoes_pesquisa(df_questoes):\n",
    "    print('Questões de pesquisa:')\n",
    "    pular=[np.nan,'','as principais questões científicas que norteiam minhas pesquisas na fiocruz ceará, relacionadas ao enfrentamento dos desafios em saúde, envolvem:','essas questões são centrais para o desenvolvimento de soluções que melhorem a vigilância epidemiológica e o acesso ao diagnóstico no brasil.']\n",
    "    for i in df_questoes.iloc[:,0]:\n",
    "        try:\n",
    "            if i.strip().lower() not in pular:\n",
    "                i = i.replace(';','\\n')\n",
    "                for sub_i in i.split('\\n'):\n",
    "                    if sub_i.strip().lower() not in pular:\n",
    "                        print(f'  {sub_i.strip().lower()}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def listar_palavras_chave(df_questoes):\n",
    "    print('Palavras-Chave:')\n",
    "    pular=['','as principais palavras-chave que podem associar meus temas de pesquisa com oportunidades de fomento que desejo monitorar são:']\n",
    "    for i in df_questoes.iloc[:,1]:\n",
    "        try:\n",
    "            if i.strip().lower() not in pular:\n",
    "                i = i.replace(';','\\n')\n",
    "                for sub_i in i.split('\\n'):\n",
    "                    if sub_i.strip().lower() not in pular:\n",
    "                        print(f'  {sub_i.strip().lower()}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def listar_desafios_proximos(df_produtos_proximos):\n",
    "    print('Desafios mais próximos das pesquisas na ICT:')\n",
    "    pular = ['', ' ', 'não encontrei produto algum na lista acima que corresponda com minhas pesquisas atuais e no futuro próximo']\n",
    "    for i in df_produtos_proximos.iloc[:,0]:\n",
    "        try:\n",
    "            # Verifica se a string 'i' não está vazia e não contém apenas espaços em branco\n",
    "            if i.strip() and i.lower().strip() not in pular:\n",
    "                i = i.replace(';','\\n')\n",
    "                for sub_i in i.split('\\n'):\n",
    "                    if sub_i.strip().lower() not in pular:\n",
    "                        print(f'  {sub_i.strip().lower()}')\n",
    "            print()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Divide a coluna de desafios em múltiplas colunas, separando os desafios por ponto e vírgula\n",
    "    df_produtos_proximos.iloc[:, 0] = df_produtos_proximos.iloc[:, 0].str.split(';')\n",
    "\n",
    "    # \"Explode\" o DataFrame para que cada desafio fique em uma linha separada\n",
    "    df_exploded = df_produtos_proximos.explode(df_produtos_proximos.columns[0])\n",
    "\n",
    "    # Filtra os valores vazios ou com apenas espaços em branco\n",
    "    df_exploded = df_exploded[df_exploded.iloc[:, 0].str.strip().astype(bool)]\n",
    "\n",
    "    # Calcula a frequência de cada desafio\n",
    "    df_sum = df_exploded[df_produtos_proximos.columns[0]].value_counts()\n",
    "\n",
    "    # Converte a Series para DataFrame e redefine o índice\n",
    "    df = pd.DataFrame(df_sum).reset_index()\n",
    "\n",
    "    # Renomeia as colunas para 'Desafio' e 'Total'\n",
    "    df.columns = ['Desafio', 'Total']\n",
    "    df_full = display_full_width_df(df)\n",
    "\n",
    "def listar_produtos_proximos_emergencias(df_produtos_proximos):\n",
    "    print('Produtos do Bloco 01 (Preparação do Sistema de Saúde para Emergências Sanitárias) mais próximos das pesquisas na ICT:')\n",
    "    pular = ['', 'não encontrei produto algum na lista acima que corresponda com minhas pesquisas atuais e no futuro próximo']\n",
    "    for i in df_produtos_proximos.iloc[:,1]:\n",
    "        try:\n",
    "            if i.lower().strip() not in pular:\n",
    "                i = i.replace(';','\\n')\n",
    "                for sub_i in i.split('\\n'):\n",
    "                    if sub_i.strip().lower() not in pular:\n",
    "                        print(f'  {sub_i.strip().lower()}')\n",
    "        except:\n",
    "            pass   \n",
    "\n",
    "def listar_produtos_proximos_agravos(df_produtos_proximos):\n",
    "    print('Produtos do Bloco 02 (Doenças e Agravos Críticos para o SUS) mais próximos das pesquisas na ICT:')\n",
    "    pular = ['','não encontrei agravo algum na lista acima que corresponda com minhas pesquisas atuais e no futuro próximo']\n",
    "    for i in df_produtos_proximos.iloc[:,2]:\n",
    "        try:\n",
    "            if i.strip().lower() not in pular:\n",
    "                i = i.replace(';','\\n')\n",
    "                for sub_i in i.split('\\n'):\n",
    "                    if sub_i.strip().lower() not in pular:\n",
    "                        print(f'  {sub_i.strip().lower()}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def produtos_extra(df_produtos_proximos):\n",
    "    print('Produtos extra estratégia do CEIS:')\n",
    "    pular = [np.nan,'não sei responder.','não consigo enxergar','já associado']\n",
    "    for i in df_produtos_proximos.iloc[:,3]:\n",
    "        try:\n",
    "            if i.strip().lower() not in pular:\n",
    "                i = i.replace(';','\\n')\n",
    "                for sub_i in i.split('\\n'):\n",
    "                    if sub_i.strip().lower() not in pular:\n",
    "                        print(f'  {sub_i.strip().lower()}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def df_to_json(dataframe):\n",
    "  # Converte o DataFrame para JSON no formato 'records' com 'lines=True' e UTF-8\n",
    "  json_data = dataframe.to_json(orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "  # Divide a string JSON em uma lista de strings, uma para cada linha\n",
    "  json_lines = json_data.splitlines()\n",
    "\n",
    "  # Converte cada linha em um objeto JSON\n",
    "  json_objects = []\n",
    "  for line in json_lines:\n",
    "    try:\n",
    "      json_objects.append(json.loads(line))\n",
    "    except json.JSONDecodeError as e:\n",
    "      print(f\"Erro ao analisar a linha: {line}\")\n",
    "      print(e)\n",
    "\n",
    "  # Imprime a lista de objetos JSON\n",
    "  return json_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados de levantamento de Questões e Palavras-Chave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_levantamento, df_questoes, df_trls, df_estrat_ceis, df_produtos_proximos, df_tempo_ativ, df_satisf = montar_dataframes_levantamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questionario = recuperar_questionario()\n",
    "# df_questionario.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_questoes = df_to_json(df_questoes)\n",
    "json_questoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listar_questoes_pesquisa(df_questoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listar_palavras_chave(df_questoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alinhamento com a estratégia de desenvolvimento do CEIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listar_desafios_proximos(df_produtos_proximos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listar_produtos_proximos_emergencias(df_produtos_proximos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_produtos_proximos = df_to_json(df_produtos_proximos)\n",
    "json_produtos_proximos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listar_produtos_proximos_agravos(df_produtos_proximos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produtos_extra(df_produtos_proximos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estrat_ceis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_produtos_proximos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tempo_ativ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_satisf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Lista de relacionamentos por tipo de produtos\")\n",
    "# for i,j in relacoes_biologicos.items():\n",
    "#     for k in j:\n",
    "#         print(k.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapas de Processos de Biológicos e Pequenas Moléculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lista de relacionamentos por tipo de produtos\")\n",
    "for i,j in relacoes_pequenas_moleculas.items():\n",
    "    for k in j:\n",
    "        rotulo = k.get('id')\n",
    "        if rotulo:\n",
    "            print(f\"  {rotulo}\")\n",
    "        else:\n",
    "            print(f\"{  k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lista de relacionamentos por tipo de produtos\")\n",
    "for i,j in relacoes_biologicos.items():\n",
    "    for k in j:\n",
    "        rotulo = k.get('id')\n",
    "        if rotulo:\n",
    "            print(f\"  {rotulo}\")\n",
    "        else:\n",
    "            print(f\"{  k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Preparar o Grafo de Conhecimento</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "grafo = nx.Graph()\n",
    "# ... (código para adicionar nós e arestas ao grafo)\n",
    "\n",
    "grafo_treino, grafo_teste = train_test_split(grafo, test_size=0.2)\n",
    "\n",
    "# Configuração dos parâmetros dos modelos\n",
    "parametros_iniciais = {\n",
    "    'num_features': ...,  # Número de features dos nós\n",
    "    'hidden_dim': 4,  # Dimensão da camada oculta\n",
    "    'num_classes': 7,  # Número de classes (clusters)\n",
    "    'dropout': 0.5  # Taxa de dropout\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Instanciar classes de cada modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_kan     = RedeNeuralKAN(parametros_iniciais)\n",
    "modelo_fourier = RedeNeuralFourier(parametros_iniciais)\n",
    "modelo_hibrido = RedeNeuralHibrida(parametros_iniciais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Treinar os modelos\n",
    "\n",
    "Utilizar o grafo de treinamento (grafo_treino) para treinar cada um dos modelos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Treinamento do modelo de GNN com Sincronização (híbrido)\n",
    "\n",
    "    Inspirado em https://iopscience.iop.org/article/10.1209/0295-5075/ad76d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_modelo(modelo, grafo_treino, epochs=100, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Treina um modelo de rede neural em grafos.\n",
    "\n",
    "    Args:\n",
    "      modelo: O modelo da rede neural.\n",
    "      grafo_treino: O grafo de conhecimento para treinamento.\n",
    "      epochs: O número de épocas de treinamento.\n",
    "      learning_rate: A taxa de aprendizado do otimizador.\n",
    "\n",
    "    Funcionamento:\n",
    "      Recebe o modelo, o grafo de treinamento, o número de épocas e a taxa de aprendizado como parâmetros.\n",
    "      Cria um otimizador Adam para atualizar os pesos do modelo.\n",
    "      Define a função de perda como CrossEntropyLoss (ajustável para a função de perda mais adequada ao problema).\n",
    "      Prepara os dados de treinamento (features, edge_index, edge_weight e labels, se houver).\n",
    "      Itera pelas épocas de treinamento, executando o modelo, calculando a perda, os gradientes e atualizando os pesos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Criar o otimizador\n",
    "        optimizer = torch.optim.Adam(modelo.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Definir a função de perda (exemplo: cross-entropy)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Preparar os dados de treinamento\n",
    "        x = torch.tensor(list(nx.get_node_attributes(grafo_treino, 'features').values()), dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(grafo_treino.edges()), dtype=torch.long).t().contiguous()\n",
    "        edge_weight = torch.tensor(list(nx.get_edge_attributes(grafo_treino, 'weight').values()), dtype=torch.float)\n",
    "        \n",
    "        # ... (código para obter os rótulos dos nós, se houver)\n",
    "        # labels = ...\n",
    "\n",
    "        # Treinar o modelo\n",
    "        modelo.train()  # Mudar para o modo de treinamento\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()  # Zerar os gradientes\n",
    "            out = modelo(x, edge_index, edge_weight)  # Executar o modelo\n",
    "            loss = criterion(out, labels)  # Calcular a perda\n",
    "            loss.backward()  # Calcular os gradientes\n",
    "            optimizer.step()  # Atualizar os pesos do modelo\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no treinamento do modelo: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Chamar a função para treinar o modelo_hibrido\n",
    "treinar_modelo(modelo_hibrido, grafo_treino, epochs=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Função de treinamento do modelo_kan_mse\n",
    "\n",
    "    Com função de perda Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_kan = nn.MSELoss()  \n",
    "\n",
    "def treinar_modelo_kan_mse(modelo, grafo_treino, epochs=100, learning_rate=0.01, criterion=criterion_kan):\n",
    "    \"\"\"\n",
    "    Treina um modelo de rede neural KAN em grafos.\n",
    "\n",
    "    Args:\n",
    "      modelo: O modelo da rede neural KAN.\n",
    "      grafo_treino: O grafo de conhecimento para treinamento.\n",
    "      epochs: O número de épocas de treinamento.\n",
    "      learning_rate: A taxa de aprendizado do otimizador.\n",
    "      criterion: A função de perda.\n",
    "    \n",
    "    Função de Perda: A função de perda utilizada foi o MSELoss. \n",
    "    \n",
    "    Pode-se ajustar para uma função de perda mais adequada ao problema, considerando aprendizado não-supervisionado. \n",
    "    Uma opção seria utilizar uma função de perda baseada na distância entre os embeddings de nós que deveriam estar no mesmo cluster.\n",
    "    Ou ainda, pode-se também usar função de perda baseada em Triplet Loss.\n",
    "    \n",
    "    Rótulos: Ajustar o código para obter os rótulos dos nós (labels) de acordo com o seu problema. \n",
    "    Como o problema é de aprendizado não-supervisionado, os rótulos podem ser definidos com base em alguma heurística ou conhecimento prévio sobre o problema.\n",
    "    \n",
    "    Hiperparâmetros: Ajustar o número de épocas (epochs) e a taxa de aprendizado (learning_rate) para obter o melhor desempenho do modelo.    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Criar o otimizador\n",
    "        optimizer = torch.optim.Adam(modelo.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Preparar os dados de treinamento\n",
    "        x = torch.tensor(list(nx.get_node_attributes(grafo_treino, 'features').values()), dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(grafo_treino.edges()), dtype=torch.long).t().contiguous()\n",
    "        edge_weight = torch.tensor(list(nx.get_edge_attributes(grafo_treino, 'weight').values()), dtype=torch.float)\n",
    "        \n",
    "        # ... (código para obter os rótulos dos nós, se houver - ajustar para o seu problema)\n",
    "        # labels = ...\n",
    "\n",
    "        # Treinar o modelo\n",
    "        modelo.train()  # Mudar para o modo de treinamento\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()  # Zerar os gradientes\n",
    "            out = modelo(x, edge_index, edge_weight)  # Executar o modelo\n",
    "            \n",
    "            # Calcular a perda (considerando que labels é um tensor com os valores desejados para os nós)\n",
    "            loss = criterion(out, labels)  \n",
    "            loss.backward()  # Calcular os gradientes\n",
    "            optimizer.step()  # Atualizar os pesos do modelo\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no treinamento do modelo KAN: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Chamar a função para treinar o modelo_kan\n",
    "treinar_modelo_kan_mse(modelo_kan, grafo_treino, epochs=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Função de treinamento do modelo_kan_triplet\n",
    "\n",
    "    Com função de perda por triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import losses\n",
    "\n",
    "# Exemplo de função para gerar trios (âncora, positivo, negativo)\n",
    "def gerar_triplets(grafo, embeddings):\n",
    "    \"\"\"\n",
    "    Gera trios (âncora, positivo, negativo) para a Triplet Loss.\n",
    "\n",
    "    Args:\n",
    "      grafo: O grafo de conhecimento.\n",
    "      embeddings: Os embeddings dos nós.\n",
    "\n",
    "    Returns:\n",
    "      Um tensor com os trios.\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    for no_ancora in grafo.nodes():\n",
    "        # Encontrar nós positivos (similares à âncora)\n",
    "        # ... (implementar lógica para encontrar nós positivos)\n",
    "\n",
    "        # Encontrar nós negativos (diferentes da âncora)\n",
    "        # ... (implementar lógica para encontrar nós negativos)\n",
    "\n",
    "        for no_positivo in nos_positivos:\n",
    "            for no_negativo in nos_negativos:\n",
    "                triplets.append([no_ancora, no_positivo, no_negativo])\n",
    "\n",
    "    return torch.tensor(triplets)\n",
    "\n",
    "def treinar_modelo_kan_triplet(modelo, grafo_treino, epochs=100, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Treina um modelo de rede neural KAN em grafos utilizando Triplet Loss.\n",
    "\n",
    "    Args:\n",
    "      modelo: O modelo da rede neural KAN.\n",
    "      grafo_treino: O grafo de conhecimento para treinamento.\n",
    "      epochs: O número de épocas de treinamento.\n",
    "      learning_rate: A taxa de aprendizado do otimizador.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Criar o otimizador\n",
    "        optimizer = torch.optim.Adam(modelo.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Preparar os dados de treinamento\n",
    "        x = torch.tensor(list(nx.get_node_attributes(grafo_treino, 'features').values()), dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(grafo_treino.edges()), dtype=torch.long).t().contiguous()\n",
    "        edge_weight = torch.tensor(list(nx.get_edge_attributes(grafo_treino, 'weight').values()), dtype=torch.float)\n",
    "\n",
    "        # Criar a função de perda Triplet Loss\n",
    "        loss_func = losses.TripletMarginLoss()\n",
    "\n",
    "        # Treinar o modelo\n",
    "        modelo.train()  # Mudar para o modo de treinamento\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()  # Zerar os gradientes\n",
    "            embeddings = modelo(x, edge_index, edge_weight)  # Executar o modelo\n",
    "            \n",
    "            # Gerar os trios (âncora, positivo, negativo), com lógica de acordo com o problema\n",
    "            triplets = gerar_triplets(grafo_treino, embeddings) \n",
    "\n",
    "            # Calcular a Triplet Loss\n",
    "            loss = loss_func(embeddings, triplets)  \n",
    "            loss.backward()  # Calcular os gradientes\n",
    "            optimizer.step()  # Atualizar os pesos do modelo\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no treinamento do modelo KAN: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Chamar a função para treinar o modelo_kan\n",
    "treinar_modelo_kan_triplet(modelo_kan, grafo_treino, epochs=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Função para treinamento do modelo_fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo_fourier\n",
    "criterion_fourier = nn.KLDivLoss() # Ajustar a função de perda para o modelo Fourier (exemplo: Kullback-Leibler Divergence)  \n",
    "\n",
    "def treinar_modelo_fourier(modelo, grafo_treino, epochs=100, learning_rate=0.01, criterion=criterion_fourier):\n",
    "    \"\"\"\n",
    "    Treina um modelo de rede neural Fourier em grafos.\n",
    "\n",
    "    Args:\n",
    "      modelo: O modelo da rede neural Fourier.\n",
    "      grafo_treino: O grafo de conhecimento para treinamento.\n",
    "      epochs: O número de épocas de treinamento.\n",
    "      learning_rate: A taxa de aprendizado do otimizador.\n",
    "      criterion: A função de perda.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Criar o otimizador\n",
    "        optimizer = torch.optim.Adam(modelo.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Preparar os dados de treinamento\n",
    "        x = torch.tensor(list(nx.get_node_attributes(grafo_treino, 'features').values()), dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(grafo_treino.edges()), dtype=torch.long).t().contiguous()\n",
    "        edge_weight = torch.tensor(list(nx.get_edge_attributes(grafo_treino, 'weight').values()), dtype=torch.float)\n",
    "        # ... (código para obter os rótulos dos nós, se houver - ajustar para o seu problema)\n",
    "        # labels = ...\n",
    "\n",
    "        # Treinar o modelo\n",
    "        modelo.train()  # Mudar para o modo de treinamento\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()  # Zerar os gradientes\n",
    "            out = modelo(x, edge_index, edge_weight)  # Executar o modelo\n",
    "\n",
    "            # Ajustar a saída do modelo e os rótulos para a KLDivLoss\n",
    "            # (a saída deve ser um tensor de probabilidades e os rótulos devem ser um tensor de índices)\n",
    "            out = torch.nn.functional.log_softmax(out, dim=1)  # Aplicar log_softmax na saída\n",
    "            # Converter labels para one-hot encoding e aplicar log_softmax (ajuste para o seu problema)\n",
    "            labels_onehot = torch.nn.functional.one_hot(labels, num_classes=out.shape[1]).float()\n",
    "            labels_onehot = torch.nn.functional.log_softmax(labels_onehot, dim=1) \n",
    "\n",
    "            # Calcular a perda \n",
    "            loss = criterion(out, labels_onehot)  \n",
    "            loss.backward()  # Calcular os gradientes\n",
    "            optimizer.step()  # Atualizar os pesos do modelo\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no treinamento do modelo Fourier: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Chamar a função para treinar o modelo_fourier\n",
    "treinar_modelo_fourier(modelo_fourier, grafo_treino, epochs=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Instanciar a classe de testes para realizar análises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie uma instância da classe TesteRedeNeural para avaliar o desempenho de cada modelo no grafo de teste (grafo_teste):\n",
    "teste_rede_neural = TesteRedeNeural(grafo_teste, parametros_modelo)  # Instanciar a classe de testes\n",
    "\n",
    "# Controle de Sincronização + GNN\n",
    "clusters_hibrido, embeddings_hibrido = modelo_hibrido.inferir(grafo_teste)  # Realizar inferência\n",
    "analises_hibrido = teste_rede_neural.testar_inferencia(clusters_hibrido, embeddings_hibrido)\n",
    "\n",
    "# KAN + GNN\n",
    "clusters_kan, embeddings_kan = modelo_kan.inferir(grafo_teste)\n",
    "analises_kan = teste_rede_neural.testar_inferencia(clusters_kan, embeddings_kan)\n",
    "\n",
    "# Transformadas de Fourier + GNN\n",
    "clusters_fourier, embeddings_fourier = modelo_fourier.inferir(grafo_teste)\n",
    "analises_fourier = teste_rede_neural.testar_inferencia(clusters_fourier, embeddings_fourier)\n",
    "\n",
    "# 6. Comparação dos resultados\n",
    "def gerar_tabela_latex(analises):\n",
    "    \"\"\"\n",
    "    Gera uma tabela LaTeX com os resultados das análises.\n",
    "\n",
    "    Args:\n",
    "      analises: Um dicionário contendo as análises dos modelos.\n",
    "\n",
    "    Returns:\n",
    "      Uma string com o código LaTeX da tabela.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(analises)\n",
    "    df.index = ['Híbrido', 'KAN', 'Fourier']\n",
    "    latex_table = df.to_latex(float_format=\"%.3f\", caption=\"Resultados das Análises\", label=\"tab:resultados\")\n",
    "    return latex_table\n",
    "\n",
    "# Agregar as análises em um dicionário\n",
    "analises = {\n",
    "    'Híbrido': analises_hibrido,\n",
    "    'KAN': analises_kan,\n",
    "    'Fourier': analises_fourier,\n",
    "}\n",
    "\n",
    "# Gerar a tabela LaTeX\n",
    "tabela_latex = gerar_tabela_latex(analises)\n",
    "\n",
    "# Exibir a tabela LaTeX\n",
    "print(tabela_latex)\n",
    "\n",
    "# Salvar a tabela em um arquivo .tex\n",
    "with open('resultados_analises.tex', 'w') as f:\n",
    "    f.write(tabela_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma instância da classe SemanticMatching\n",
    "from semantic_matcher import ENPreprocessor, SemanticMatcher\n",
    "tradutor = ENPreprocessor()\n",
    "matcher = SemanticMatcher(curriculos_data, matriz_ceis_data, relacoes_biologicos, relacoes_pequenas_moleculas)\n",
    "\n",
    "# Executar as etapas de processamento\n",
    "matcher.traduzir_nomes_produtos()\n",
    "matcher.extrair_caracteristicas()\n",
    "matcher.classificar_produtos()\n",
    "matcher.conectar_produtos_grafo()\n",
    "\n",
    "# Imprimir informações sobre os grafos\n",
    "print(\"Grafo de Biológicos:\")\n",
    "print(\"Número de nós:\", matcher.biologicos.number_of_nodes())\n",
    "print(\"Número de arestas:\", matcher.biologicos.number_of_edges())\n",
    "\n",
    "print(\"\\nGrafo de Pequenas Moléculas:\")\n",
    "print(\"Número de nós:\", matcher.pequenas_moleculas.number_of_nodes())\n",
    "print(\"Número de arestas:\", matcher.pequenas_moleculas.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "import cugraph\n",
    "import json\n",
    "from pyvis.network import Network\n",
    "\n",
    "json_folder = os.path.join(os.getcwd(),'_data','out_json')\n",
    "\n",
    "# Carregar os dados dos arquivos JSON com cuDF\n",
    "curriculos_df = cudf.read_json(os.path.join(json_folder,'curriculos.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_ceis_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cugraph\n",
    "import json\n",
    "import time\n",
    "from pyvis.network import Network\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Funções para calcular as transformadas de Fourier\n",
    "def naive_fourier(x, gridsize=300):\n",
    "    # ... (implementação da Naive Fourier Transform) ...\n",
    "    pass  # Substitua pelo código da transformada\n",
    "\n",
    "def gft(x, edge_index):\n",
    "    # ... (implementação da Graph Fourier Transform) ...\n",
    "    pass  # Substitua pelo código da transformada\n",
    "\n",
    "def wavelet_transform(x, edge_index, num_scales=5):\n",
    "    # ... (implementação da Wavelet Transform) ...\n",
    "    pass  # Substitua pelo código da transformada\n",
    "\n",
    "def fractional_fourier(x, alpha=0.5):\n",
    "    # ... (implementação da Fractional Fourier Transform) ...\n",
    "    pass  # Substitua pelo código da transformada\n",
    "\n",
    "def qft(x):\n",
    "  \"\"\"Calcula a QFT de um quaternion.\"\"\"\n",
    "  # ... (implementação da Transformada de Fourier Quaterniônica) ...\n",
    "  pass  # Substitua pelo código da transformada\n",
    "\n",
    "def conectar_pesquisadores_produtos(graph, curriculos_df, matriz_ceis_df, tipo_transformada):\n",
    "    \"\"\"Conecta pesquisadores a produtos com base na similaridade de áreas de atuação.\"\"\"\n",
    "\n",
    "    # Extrair áreas de atuação dos pesquisadores e produtos\n",
    "    pesquisadores_areas = curriculos_df['Áreas'].to_arrow().to_pylist()\n",
    "    produtos_areas = []\n",
    "    for bloco in matriz_ceis_df['blocos'].to_arrow().to_pylist():\n",
    "        for produto in bloco['produtos']:\n",
    "            produtos_areas.append(produto['nome'])  # Usando o nome do produto como representação da área\n",
    "\n",
    "    # Converter áreas de atuação em vetores numéricos (ex: usando word embeddings)\n",
    "    # ... (implementar lógica para converter áreas em vetores) ...\n",
    "\n",
    "    # Aplicar a transformada de Fourier selecionada\n",
    "    if tipo_transformada == 'naive':\n",
    "        pesquisadores_transformados = [naive_fourier(areas) for areas in pesquisadores_areas]\n",
    "        produtos_transformados = [naive_fourier(areas) for areas in produtos_areas]\n",
    "    elif tipo_transformada == 'gft':\n",
    "        # ... (aplicar GFT) ...\n",
    "        pass\n",
    "    elif tipo_transformada == 'wavelet':\n",
    "        # ... (aplicar Wavelet Transform) ...\n",
    "        pass\n",
    "    elif tipo_transformada == 'fractional':\n",
    "        # ... (aplicar Fractional Fourier Transform) ...\n",
    "        pass\n",
    "    elif tipo_transformada == 'qft':\n",
    "        # ... (aplicar QFT) ...\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Tipo de transformada inválido.\")\n",
    "\n",
    "    # Calcular a similaridade de cosseno entre os vetores transformados\n",
    "    similaridade = cosine_similarity(pesquisadores_transformados, produtos_transformados)\n",
    "\n",
    "    # Criar as arestas no grafo cuGraph com base na similaridade\n",
    "    limite_similaridade = 0.8  # Definir um limite para a similaridade\n",
    "    for i, pesquisador_id in enumerate(pesquisadores_ids):\n",
    "        for j, produto_id in enumerate(produtos_ids):\n",
    "            if similaridade[i, j] > limite_similaridade:\n",
    "                G.add_edge(pesquisador_id, produto_id, weight=similaridade[i, j])\n",
    "\n",
    "# --- Avaliação das abordagens ---\n",
    "\n",
    "resultados = {}\n",
    "tipos_transformadas = ['naive', 'gft', 'wavelet', 'fractional', 'qft']\n",
    "\n",
    "for tipo_transformada in tipos_transformadas:\n",
    "    inicio = time.time()\n",
    "\n",
    "    # Criar um novo grafo para cada tipo de transformada\n",
    "    G = cugraph.Graph()\n",
    "    G.add_nodes_from(pesquisadores_ids, tipo='pesquisador')\n",
    "    G.add_nodes_from(produtos_ids, tipo='produto')\n",
    "\n",
    "    # Conectar pesquisadores e produtos\n",
    "    conectar_pesquisadores_produtos(G, curriculos_df, matriz_ceis_df, tipo_transformada)\n",
    "\n",
    "    fim = time.time()\n",
    "    tempo_execucao = fim - inicio\n",
    "\n",
    "    # Calcular as métricas de avaliação\n",
    "    num_arestas = G.number_of_edges()\n",
    "\n",
    "    # ... (calcular precisão e recall usando um conjunto de dados de referência) ...\n",
    "\n",
    "    resultados[tipo_transformada] = {\n",
    "        'num_arestas': num_arestas,\n",
    "        'tempo_execucao': tempo_execucao,\n",
    "        # 'precisao': precisao,\n",
    "        # 'recall': recall\n",
    "    }\n",
    "\n",
    "# Imprimir os resultados\n",
    "print(resultados)\n",
    "\n",
    "# --- Visualização (opcional) ---\n",
    "# ... (criar gráficos com Altair para comparar as métricas) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pré-processamento dos dados com cuDF\n",
    "# (Extrair IDs e informações relevantes para os nós e arestas)\n",
    "pesquisadores_ids = curriculos_df['Identificação']['ID Lattes'].to_arrow().to_pylist()\n",
    "produtos_ids = []\n",
    "for bloco in matriz_ceis_df['blocos'].to_arrow().to_pylist():\n",
    "    for produto in bloco['produtos']:\n",
    "        produtos_ids.append(produto['id'])\n",
    "\n",
    "# Criar um grafo cuGraph\n",
    "G = cugraph.Graph()\n",
    "\n",
    "# Adicionar os nós ao grafo cuGraph\n",
    "G.add_nodes_from(pesquisadores_ids, tipo='pesquisador')\n",
    "G.add_nodes_from(produtos_ids, tipo='produto')\n",
    "\n",
    "# Criar as arestas entre pesquisadores e produtos (definir critérios)\n",
    "# ... (implementar lógica para conectar pesquisadores a produtos usando cuDF) ...\n",
    "# EXEMPLO: conectar pesquisadores a produtos com base na similaridade de áreas de atuação\n",
    "\n",
    "# Calcular métricas de grafo com cuGraph\n",
    "degree_centrality = cugraph.degree_centrality(G)\n",
    "\n",
    "# Converter o grafo cuGraph para NetworkX\n",
    "graph_nx = G.to_networkx()\n",
    "\n",
    "# Criar o grafo PyVis 'net' a partir do grafo NetworkX 'graph_nx'\n",
    "net = Network(notebook=True, directed=True)\n",
    "net.from_nx(graph_nx)\n",
    "\n",
    "# Personalizar a visualização (opcional)\n",
    "# ... (utilizar as métricas calculadas com cuGraph para definir o tamanho dos nós, cores, etc.) ...\n",
    "\n",
    "# Mostrar o grafo na célula do Jupyter\n",
    "net.show(\"graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "json_folder = os.path.join(os.getcwd(),'_data','out_json')\n",
    "\n",
    "# Carregar os dados dos arquivos JSON\n",
    "with open(os.path.join(json_folder,'curriculos.json'), 'r') as f:\n",
    "    curriculos_data = json.load(f)\n",
    "with open(os.path.join(json_folder,'matriz_ceis.json'), 'r') as f:\n",
    "    matriz_ceis_data = json.load(f)\n",
    "\n",
    "# Criar um grafo direcionado\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Adicionar os pesquisadores como nós\n",
    "for pesquisador in curriculos_data:\n",
    "    graph.add_node(pesquisador['Identificação']['ID Lattes'], tipo='pesquisador', **pesquisador)\n",
    "\n",
    "# Adicionar os produtos como nós\n",
    "for bloco in matriz_ceis_data['blocos']:\n",
    "    for produto in bloco['produtos']:\n",
    "        graph.add_node(produto['id'], tipo='produto', **produto)\n",
    "\n",
    "# Função para criar as arestas entre pesquisadores e produtos\n",
    "def create_edges(graph, pesquisador_node, produto_node):\n",
    "    \"\"\"Cria arestas entre pesquisadores e produtos com base em suas propriedades.\"\"\"\n",
    "    pesquisador_data = pesquisador_node[1]\n",
    "    produto_data = produto_node[1]\n",
    "\n",
    "    # Lógica para conectar pesquisadores a produtos (EXEMPLO)\n",
    "    # Verificar se as áreas de atuação do pesquisador são relevantes para o produto\n",
    "    for area in pesquisador_data['Áreas'].values():\n",
    "        if any(keyword in area for keyword in [\"Biotecnologia\", \"Saúde\", \"Química\", \"Biologia\"]):\n",
    "            graph.add_edge(pesquisador_node[0], produto_node[0])\n",
    "            return  # Criar apenas uma aresta por produto\n",
    "\n",
    "# Iterar pelos nós e criar as arestas\n",
    "for pesquisador_node in graph.nodes(data=True):\n",
    "    if pesquisador_node[1]['tipo'] == 'pesquisador':\n",
    "        for produto_node in graph.nodes(data=True):\n",
    "            if produto_node[1]['tipo'] == 'produto':\n",
    "                create_edges(graph, pesquisador_node, produto_node)\n",
    "\n",
    "# Imprimir os primeiros 5 nós e arestas\n",
    "print(\"Nós:\", list(graph.nodes(data=True))[:5])\n",
    "print(\"Arestas:\", list(graph.edges(data=True))[:5])\n",
    "\n",
    "# Mostrar o número de nós e arestas\n",
    "print(\"Número de nós:\", graph.number_of_nodes())\n",
    "print(\"Número de arestas:\", graph.number_of_edges())\n",
    "\n",
    "# Mostrar os tipos de nós presentes no grafo\n",
    "tipos_nos = set(no[1]['tipo'] for no in graph.nodes(data=True))\n",
    "print(\"Tipos de nós:\", tipos_nos)\n",
    "\n",
    "# Mostrar as propriedades dos nós\n",
    "print(\"Propriedades dos nós:\")\n",
    "for no in graph.nodes(data=True):\n",
    "    print(no)\n",
    "\n",
    "# Mostrar a distribuição das arestas entre os nós\n",
    "# (Exemplo: calcular o grau de cada nó)\n",
    "graus = dict(graph.degree())\n",
    "print(\"Distribuição de graus dos nós:\", graus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, directed=True)  # notebook=True para exibir no Jupyter, directed=True para grafo direcionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.from_nx(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir opções de layout\n",
    "net.repulsion(node_distance=200, central_gravity=0.2)\n",
    "\n",
    "# Configurar a física da visualização\n",
    "net.show_buttons(filter_=['physics'])  # Mostrar botões para controlar a física\n",
    "\n",
    "# Definir cores para os nós com base no tipo\n",
    "for node in net.nodes:\n",
    "    if node['tipo'] == 'pesquisador':\n",
    "        node['color'] = 'blue'\n",
    "    elif node['tipo'] == 'produto':\n",
    "        node['color'] = 'green'\n",
    "\n",
    "# Ajustar o tamanho dos nós com base no grau\n",
    "degrees = dict(graph.degree())\n",
    "for node in net.nodes:\n",
    "    node['size'] = degrees[node['id']] * 5  # Aumentar o tamanho proporcionalmente ao grau\n",
    "\n",
    "# net.show(\"graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "# Exibir o conteúdo do arquivo HTML na célula do Jupyter\n",
    "HTML(filename='graph.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cudf\n",
    "# import cugraph\n",
    "# import json\n",
    "# import networkx as nx\n",
    "# from pyvis.network import Network\n",
    "# from googletrans import Translator\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# class SemanticMatching:\n",
    "#     def __init__(self, curriculos_df, matriz_ceis_data, relacoes_biologicos,\n",
    "#                  relacoes_pequenas_moleculas):\n",
    "#         self.curriculos_df = curriculos_df\n",
    "#         self.matriz_ceis_data = matriz_ceis_data\n",
    "#         self.relacoes_biologicos = relacoes_biologicos\n",
    "#         self.relacoes_pequenas_moleculas = relacoes_pequenas_moleculas\n",
    "#         self.translator = Translator()\n",
    "#         self.produtos_df = self.criar_dataframe_produtos()\n",
    "#         self.biologicos, self.pequenas_moleculas = self.criar_grafos_relacionamentos()\n",
    "\n",
    "#     def criar_dataframe_produtos(self):\n",
    "#         json_folder = os.path.join(os.getcwd(),'_data','out_json')\n",
    "\n",
    "#         # Carregar a Matriz CEIS\n",
    "#         with open(os.path.join(json_folder,'matriz_ceis.json'), 'r') as f:\n",
    "#             matriz_ceis_data = json.load(f)\n",
    "\n",
    "#         # Extrair os dados dos produtos\n",
    "#         produtos = []\n",
    "#         for bloco in matriz_ceis_data['blocos']:\n",
    "#             for produto in bloco['produtos']:\n",
    "#                 produto['bloco_id'] = bloco['id']\n",
    "#                 produto['bloco_nome'] = bloco['titulo']\n",
    "#                 produtos.append(produto)\n",
    "\n",
    "#         # Retornar o DataFrame cuDF\n",
    "#         return cudf.DataFrame(produtos)\n",
    "\n",
    "#     def criar_grafos_relacionamentos(self):\n",
    "#         # ... (código para criar os grafos de biológicos e pequenas moléculas) ...\n",
    "#         biologicos = nx.DiGraph()\n",
    "#         for node in self.relacoes_biologicos[\"nodes\"]:\n",
    "#             biologicos.add_node(node['id'], **node)\n",
    "#         for edge in self.relacoes_biologicos[\"edges\"]:\n",
    "#             biologicos.add_edge(edge['from'], edge['to'])\n",
    "\n",
    "#         pequenas_moleculas = nx.DiGraph()\n",
    "#         for node in self.relacoes_pequenas_moleculas[\"nodes\"]:\n",
    "#             pequenas_moleculas.add_node(node['id'], **node)\n",
    "#         for edge in self.relacoes_pequenas_moleculas[\"edges\"]:\n",
    "#             pequenas_moleculas.add_edge(edge['from'], edge['to'])\n",
    "#         return biologicos, pequenas_moleculas\n",
    "\n",
    "#     def traduzir_nomes_produtos(self):\n",
    "#         # ... (código para traduzir os nomes dos produtos) ...\n",
    "#         self.produtos_df['nome_en'] = self.produtos_df['nome'].apply(\n",
    "#             lambda x: self.translator.translate(x, dest='en').text)\n",
    "\n",
    "#     def extrair_caracteristicas(self):\n",
    "#         # ... (código para extrair características semânticas) ...\n",
    "#         pass  # Implemente a extração de características aqui\n",
    "\n",
    "#     def classificar_produtos(self):\n",
    "#         # ... (código para classificar os produtos) ...\n",
    "#         pass  # Implemente a classificação dos produtos aqui\n",
    "\n",
    "#     def calcular_similaridade(self, produto, grafo, tipo_transformada):\n",
    "#         # ... (código para calcular similaridade usando a abordagem especificada) ...\n",
    "#         pass  # Implemente o cálculo de similaridade aqui\n",
    "\n",
    "#     def conectar_produtos_grafo(self):\n",
    "#         # ... (código para conectar os produtos aos grafos) ...\n",
    "#         pass  # Implemente a conexão dos produtos aos grafos aqui\n",
    "\n",
    "#     def avaliar_desempenho(self):\n",
    "#         # ... (código para avaliar o desempenho das abordagens) ...\n",
    "#         pass  # Implemente a avaliação de desempenho aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "yy = 2024\n",
    "print (f\"O calendário do ano {yy} é:\")\n",
    "print (calendar.calendar(yy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import calendar\n",
    "\n",
    "# yy = 2024\n",
    "# # mm = 10\n",
    "# # print(calendar.month(yy,mm))\n",
    "\n",
    "# for i in range(1,13):\n",
    "#     print(calendar.month(yy,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Define the CSS style for the DataFrame with the text wrapping in the lines and occupying the entire width of the cell\n",
    "# style = \"\"\"\n",
    "# <style>\n",
    "# .dataframe {\n",
    "#     width: 100%;\n",
    "# }\n",
    "\n",
    "# .dataframe td {\n",
    "#     word-wrap: break-word;\n",
    "#     white-space: pre-wrap;\n",
    "# }\n",
    "# </style>\n",
    "# \"\"\"\n",
    "\n",
    "# # Divide a coluna de desafios em múltiplas colunas, separando os desafios por ponto e vírgula\n",
    "# df_produtos_proximos.iloc[:, 0] = df_produtos_proximos.iloc[:, 0].str.split(';')\n",
    "\n",
    "# # \"Explode\" o DataFrame para que cada desafio fique em uma linha separada\n",
    "# df_exploded = df_produtos_proximos.explode(df_produtos_proximos.columns[0])\n",
    "\n",
    "# # Calcula a frequência de cada desafio\n",
    "# df_sum = df_exploded[df_produtos_proximos.columns[0]].value_counts()\n",
    "\n",
    "# # Converte a Series para DataFrame e redefine o índice\n",
    "# df = pd.DataFrame(df_sum).reset_index()\n",
    "\n",
    "# # Renomeia as colunas para 'Desafio' e 'Total'\n",
    "# df.columns = ['Desafio', 'Total']\n",
    "\n",
    "# # Exibe o DataFrame resultante\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.08",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
