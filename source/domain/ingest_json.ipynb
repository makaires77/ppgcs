{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Etapas da Metodologia</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapas para gerar o grafo de conhecimento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar os dados dos arquivos JSON:\n",
    "\n",
    "- Carregar os dados dos currículos dos pesquisadores em um DataFrame cuDF.\n",
    "- Carregar os dados da Matriz CEIS em um DataFrame cuDF.\n",
    "- Carregar as relações para biológicos e pequenas moléculas em estruturas de dados Python (listas de dicionários).\n",
    "\n",
    "Criar o grafo cuGraph:\n",
    "- Criar um grafo direcionado cuGraph.\n",
    "- Adicionar nós para cada pesquisador, produto da Matriz CEIS e elemento dos relacionamentos (biológicos e pequenas moléculas).\n",
    "- Definir atributos para os nós, como tipo de nó (\"pesquisador\", \"produto\", \"relacionamento\"), nome, ID, etc.\n",
    "\n",
    "Conectar os nós:\n",
    "- Conectar os nós de pesquisadores aos nós de produtos com base na similaridade entre as áreas de atuação do pesquisador e as áreas de aplicação do produto.\n",
    "- Conectar os nós de produtos aos nós de relacionamento com base no tipo de produto (biológico ou pequena molécula).\n",
    "- Conectar os nós de relacionamento entre si, seguindo as relações definidas nos arquivos JSON.\n",
    "\n",
    "Calcular métricas de grafo:\n",
    "- Calcular métricas de centralidade, como grau de entrada e saída, para identificar os nós mais importantes no grafo.\n",
    "- Calcular métricas de caminho, como distância e betweenness centrality, para analisar as relações entre os nós.\n",
    "- Converter o grafo cuGraph para NetworkX:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de perda adequada para aprendizado não-supervisionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No contexto de aprendizado não-supervisionado para alinhar competências da ICT aos produtos estratégicos, a escolha da função de perda considerou o objetivo de agrupar pesquisadores com competências semelhantes e identificar as lacunas em relação aos produtos estratégicos. Neste caso, não existe nesse cenário rótulos de classe pré-definidos. Portanto, as funções de perda tradicionais, como MSE ou Cross-Entropy, não são as mais adequadas.\n",
    "\n",
    "Considerando as opções de abordagem em redes neurais (com KANs, com Fourier e Híbrida) que serão utilizadas nos problemas de aprendizado não-supervisionado, a **Triplet Loss** surge como uma escolha promissora para esse tipo de problema, pois se concentra em aprender embeddings que agrupam nós semelhantes e separam nós distintos, o que se alinha com o objetivo de agrupar pesquisadores com competências semelhantes e identificar lacunas.\n",
    "\n",
    "O raciocínio sobre a Triplet Loss ser uma boa escolha para o aprendizado não-supervisionado em grafos com o objetivo de alinhar competências se aplica não só para abordagem com KANs mas também, de forma similar, se aplica bem, com algumas adaptações, para as outras abordagens de Rede Neural com Fourier e Rede Neural Híbrida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como aplicar a Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Definir trios:**\n",
    "    * **Âncora:** Um pesquisador.\n",
    "    * **Positivo:** Outro pesquisador com competências semelhantes à âncora, idealmente trabalhando em um produto estratégico que requer essas competências.\n",
    "    * **Negativo:** Um pesquisador com competências diferentes da âncora, preferencialmente que não esteja associado a nenhum produto estratégico que a âncora poderia contribuir.\n",
    "\n",
    "2. **Calcular perda:**\n",
    "    * A Triplet Loss busca minimizar a distância entre a âncora e o positivo, e maximizar a distância entre a âncora e o negativo.\n",
    "    * A função de perda é definida como:\n",
    "      \n",
    "<center>L = max(d(âncora, positivo) - d(âncora, negativo) + margem, 0)</center>\n",
    "      \n",
    "<center>onde 'd(a, b)' é a distância entre os embeddings de 'a' e 'b', e 'margem' é um hiperparâmetro que define a diferença mínima desejada entre as distâncias.</center>\n",
    "\n",
    "3. **Amostrar trios:**\n",
    "    * A escolha dos trios é crucial para o bom desempenho da Triplet Loss.\n",
    "    * Estratégias de amostragem como \"hard negative mining\" podem ser utilizadas para selecionar os trios mais informativos, que contribuem mais para o aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vantagens da Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Alinhamento com o objetivo:** A Triplet Loss foca em agrupar nós semelhantes e separar nós distintos, o que é diretamente relevante para o problema de alinhamento de competências.\n",
    "* **Consideração da estrutura do grafo:** A escolha dos trios pode levar em conta as relações entre os nós no grafo, como a colaboração entre pesquisadores ou a participação em projetos comuns.\n",
    "* **Flexibilidade:** A Triplet Loss pode ser combinada com diferentes arquiteturas de redes neurais em grafos, como GNNs, para capturar as informações do grafo de forma mais eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação da função de perda com Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar a Triplet Loss, foi utilizada biblioteca `PyTorch Metric Learning`, que oferece diversas funções de perda e métodos de mineração para facilitar o treinamento.\n",
    "\n",
    "Lembre-se que a escolha da função de perda é apenas um dos aspectos do problema. A arquitetura da rede neural, a estratégia de amostragem dos trios e a escolha dos hiperparâmetros também são importantes para o bom desempenho do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Abordagens GNN com função perda Triplet Loss</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede KAN com função de perda por Triplet Loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A KAN foi responsável por gerar os embeddings dos nós do grafo, que foram utilizados pela Triplet Loss para calcular as distâncias entre os nós. A saída da KAN é um vetor de embedding para cada nó, representando suas características e relações no grafo.\n",
    "\n",
    "### Definição dos trios:\n",
    "\n",
    "A definição dos trios (âncora, positivo e negativo) segue a mesma lógica das outras abordagens:\n",
    "- Âncora: Um pesquisador.\n",
    "- Positivo: Outro pesquisador com competências semelhantes à âncora, idealmente associado a um produto estratégico que demanda essas competências.\n",
    "- Negativo: Um pesquisador com competências diferentes da âncora, preferencialmente não associado a produtos estratégicos que a âncora poderia contribuir.\n",
    "\n",
    "\n",
    "### Cálculo da perda:\n",
    "\n",
    "- A Triplet Loss foi calculada com base nos embeddings gerados pela Kolmogorov-Arnold Networks (KAN). \n",
    "\n",
    "- Quanto à amostragem dos trios, foi utilizada uma estratégia de amostragem eficiente, a \"hard negative mining\", para selecionar os trios mais informativos para o aprendizado.\n",
    "\n",
    "- A Triplet Loss foi utilizada para aprender embeddings que representem tanto as características estruturais do grafo quanto as relações de similaridade entre os nós, porém com o diferencial de aprender sobre as relações que ligam os nós (arestas).\n",
    "\n",
    "- Quanto à otimização, durante o treinamento, o otimizador ajustará os pesos da KAN para minimizar a Triplet Loss. A função de perda buscou minimizar a distância entre a âncora e o positivo, e maximizar a distância entre a âncora e o negativo. Isso gerou embeddings que agrupam pesquisadores com competências semelhantes e separam aqueles com competências distintas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural Fourier (Transformadas de Fourier + GNN):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Triplet Loss pode ser utilizada em conjunto com as Transformadas de Fourier para aprender embeddings que representem tanto as características estruturais do grafo quanto as relações de similaridade entre os nós.\n",
    "\n",
    "* As Transformadas de Fourier podem auxiliar na identificação de padrões e características relevantes na estrutura do grafo,  enquanto a Triplet Loss guia o aprendizado para agrupar nós com competências semelhantes.\n",
    "\n",
    "* A combinação dessas técnicas pode levar a um modelo mais robusto e capaz de capturar diferentes aspectos do problema de alinhamento de competências."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural Híbrida (Controle de Sincronização + GNN):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Triplet Loss foi utilizada para auxiliar na sincronização dos nós, incentivando o agrupamento de pesquisadores com competências semelhantes e a separação daqueles com competências diferentes.\n",
    "\n",
    "* A dinâmica de sincronização, baseada no valor de Fiedler, pode complementar a Triplet Loss, ajudando a identificar os nós mais importantes para o alinhamento de competências e guiando o aprendizado da rede.\n",
    "\n",
    "* A combinação da Triplet Loss com a GNN permite capturar as informações locais e globais do grafo,  enquanto a dinâmica de sincronização fornece uma perspectiva adicional sobre a importância dos nós e arestas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observações Gerais:**\n",
    "\n",
    "A escolha da função de perda é apenas um dos aspectos do problema. A arquitetura da rede neural, a estratégia de amostragem dos trios e a escolha dos hiperparâmetros também são importantes para o bom desempenho do modelo. Outras funções de perda, como a DGI ou a VGAE, também podem ser exploradas, especialmente se você desejar capturar a estrutura global do grafo e aprender representações mais robustas. É crucial avaliar o desempenho do modelo com diferentes métricas de avaliação, como as discutidas anteriormente, para garantir que ele esteja alinhando as competências da ICT aos produtos estratégicos de forma eficaz.\n",
    "\n",
    "A Triplet Loss é uma função de perda versátil que pode ser aplicada em diferentes abordagens de redes neurais em grafos. Para o problema de alinhamento de competências em aprendizado não-supervisionado, a escolha da melhor função de perda e da arquitetura do modelo dependerá das características específicas do seu problema e dos seus objetivos, mas pode-se observar de forma geral, que:\n",
    "\n",
    "* A escolha dos trios (âncora, positivo e negativo) é crucial para o bom desempenho da Triplet Loss. É importante definir uma estratégia de amostragem que leve em conta as características do grafo e o objetivo de alinhamento de competências.\n",
    "* A margem da Triplet Loss é um hiperparâmetro importante que deve ser ajustado para obter o melhor desempenho.\n",
    "* A Triplet Loss pode ser combinada com outras funções de perda, como a DGI ou a VGAE, para capturar diferentes aspectos do problema e melhorar o aprendizado das representações.\n",
    "* É fundamental avaliar o desempenho do modelo com diferentes métricas de avaliação,  além de analisar os embeddings e clusters gerados para garantir que o modelo esteja alinhando as competências de forma eficaz e interpretável.\n",
    "\n",
    "\n",
    "A interpretabilidade do modelo também é importante. Analise os embeddings gerados e os clusters formados para entender como o modelo está realizando o alinhamento de competências.\n",
    "Ao combinar a Triplet Loss com uma arquitetura de rede neural em grafos adequada e uma estratégia de amostragem eficiente, você poderá desenvolver um modelo de aprendizado não-supervisionado capaz de alinhar as competências da ICT aos produtos estratégicos e auxiliar na identificação de lacunas e oportunidades de desenvolvimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros iniciais e ajuste de hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A escolha dos parâmetros para cada modelo depende de diversos fatores, como o tamanho e a complexidade do grafo de conhecimento, a quantidade de dados de treinamento, a capacidade computacional disponível e o objetivo da análise. \n",
    "\n",
    "Partimos de alguns valores iniciais razoáveis para cada parâmetro para ajustá-los posteriormente com base nos resultados dos experimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `num_features`: Depende do número de features para cada nó do grafo (pesquisadores, competências e produtos). \n",
    "    * **Exemplo:** Se você tiver 10 features para cada pesquisador (e.g., anos de experiência, número de publicações, áreas de atuação), 5 features para cada competência (e.g., nível de proficiência, tipo de competência) e 3 features para cada produto estratégico (e.g., complexidade, área terapêutica), você pode concatenar esses features em um vetor de 18 dimensões.\n",
    "    * **Valor inicial:**  Número total de features extraídas para cada nó.\n",
    "\n",
    "* `hidden_dim`: Define a dimensão das camadas ocultas da KAN.\n",
    "    * **Valor inicial:**  64 ou 128.  Experimentar variações em potências de 2.\n",
    "\n",
    "* `num_classes`: Define o número de classes (clusters) que você deseja gerar.\n",
    "    * **Valor inicial:**  Mesmo valor utilizado no modelo híbrido.\n",
    "\n",
    "* `num_layers`: Define o número de camadas na KAN.\n",
    "    * **Valor inicial:**  3 ou 4 é um bom início. Escolher um valor que faça sentido para o problema.  Você pode usar técnicas como o método do cotovelo para auxiliar na escolha do número ideal de clusters.\n",
    "\n",
    "* `dropout`: Define a taxa de dropout.\n",
    "    * **Valor inicial:**  0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observações:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Ajuste dos parâmetros:**  É fundamental realizar experimentos e analisar os resultados para ajustar os parâmetros e encontrar a melhor configuração para cada modelo.\n",
    "* **Técnicas de otimização:**  Utilize técnicas como grid search ou random search para explorar diferentes combinações de parâmetros e encontrar a configuração ótima.\n",
    "* **Métricas de avaliação:**  Monitore as métricas de avaliação durante o treinamento e a validação para avaliar o desempenho do modelo e guiar o ajuste dos parâmetros.\n",
    "\n",
    "Estes foram apenas os valores iniciais. A melhor configuração para os parâmetros dependerá das características do grafo de conhecimento, da quantidade de dados de treinamento e do objetivo da análise. Esses valores foram buscados por otimização automatizada de hiperparâmetros dentro de espaços de busca relacionados com os valores inciais mostrados acima. A partir dos desse ajuste e de vários experimentos para medição de desempenho computacional é que se chegou aos hiperparâmetros adequados para obter o melhor desempenho de cada modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Implementações em código</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar a estrutura de manipulação dos arquivos JSON e construir o grafo de análise, primeiro carregamos os dados dos arquivos JSON e, em seguida, criamos o grafo de conhecimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Síntese dos dados para criar o Grafo de Conhecimento:\n",
      "\n",
      "  Relacionamentos da Cadeia de Agregação de Valor em Produtos por tipo:\n",
      "       Biológicos:  2 chaves: dict_keys(['nodes', 'edges'])\n",
      "    Peq.Moléculas:  2 chaves: dict_keys(['nodes', 'edges'])\n",
      "\n",
      "  Dados das entidades em análise:\n",
      "      Matriz_CEIS:  2 blocos, contendo as chaves: ['bloco', 'id', 'titulo', 'produtos', 'desafios']\n",
      "       Currículos: 38 currículos, com 15 chaves em cada currículo\n",
      "\n",
      "  Dados sobre cada pesquisador:\n",
      "     Identificação\n",
      "     Idiomas\n",
      "     Formação\n",
      "     Atuação Profissional\n",
      "     Linhas de Pesquisa\n",
      "     Áreas\n",
      "     Produções\n",
      "     ProjetosPesquisa\n",
      "     ProjetosExtensão\n",
      "     ProjetosDesenvolvimento\n",
      "     ProjetosOutros\n",
      "     Patentes e registros\n",
      "     Bancas\n",
      "     Orientações\n",
      "     JCR2\n",
      "\n",
      "Lista de produtos por Bloco da Matriz CEIS:\n",
      "  Bloco: B01. PREPARAÇÃO DO SISTEMA DE SAÚDE PARA EMERGÊNCIAS SANITÁRIAS\n",
      "    Vacinas do PNI que demandem atualização tecnológica\n",
      "    Vacinas do PNI que possuem dependência externa\n",
      "    Vacina vírus sincicial respiratório (RSV)\n",
      "    Vacina Chikungunya\n",
      "    Vacina contra Dengue\n",
      "    Vacina Esquistossomose\n",
      "    Vacina Hanseníase\n",
      "    Vacina Leishmanioses\n",
      "    Vacina Zika\n",
      "    Vacina Herpes Zoster\n",
      "    Vacinas combinadas\n",
      "    Vacina associada a formas farmacêuticas não invasivas\n",
      "    Testes diagnósticos moleculares, por imunoensaio, rápidos de antígeno ou anticorpo, point-of-care, autoteste e outros testes diagnósticos in vitro\n",
      "    Kit de extração de ácidos nucléicos\n",
      "    Seringas e agulhas hipodérmicas\n",
      "    Soros Imunossupressores\n",
      "    Hemoderivados e bioprodutos\n",
      "    Novas tecnologias para hemoterapia\n",
      "    Serviços tecnológicos para hemoterapia\n",
      "    Desenvolvimento e produção local de tecnologias de informação e conectividade para centralização e padronização de ensaios sorológicos para triagem do sangue doado\n",
      "    Teste de amplificação de ácidos nucléicos de última geração\n",
      "    Tecnologias para inativação de patógenos em bolsas de sangue de doação voluntária\n",
      "    Máquina de aférese\n",
      "    IFA de base química ou biotecnológica de medicamentos demandados pelo SUS que apresentem dependência externa de insumos críticos da cadeia produtiva para todos os desafios em saúde definidos nesta matriz\n",
      "    Radiofármacos e produtos de medicina nuclear\n",
      "    IFA antimicrobianos\n",
      "    Produtos antimicrobianos demandados pelo SUS que não tenham produção nacional e para microorganismos resistentes\n",
      "    Insumos críticos da cadeia produtiva de dispositivos médicos\n",
      "    IFA obtidos por processos tecnológicos e industriais sustentáveis baseados na química verde\n",
      "    Fitoterápicos e produtos da biodiversidade apoiados pelo SUS\n",
      "    Desenvolvimento e produção local de tecnologias utilizadas na atenção à saúde (telessaude, telemonitoramento, telediagnóstico, entre outros)\n",
      "    Desenvolvimento e produção local de tecnologias de informação e conectividade para gestão de estoques\n",
      "    Desenvolvimento e produção local de tecnologias de informação e conectividade para rastreabilidade de órteses, próteses e meios auxiliares de locomoção dispensados pelo SUS\n",
      "    Desenvolvimento e produção local de tecnologias de informação e conectividade para a gestão de dados clínicos de prontuários eletrônicos\n",
      "    Desenvolvimento e produção local de tecnologias de informação e conectividade com padrões e interoperabilidade nos diversos níveis do SUS\n",
      "    Desenvolvimento e produção local de tecnologias de informação e conectividade para acompanhamento de aquisição, dispensação e desabastecimento de medicamentos\n",
      "    Equipamentos de uso médico adaptados para uso na atenção primária, atendimento pré-hospitalar e domiciliar\n",
      "    Equipamentos de uso médico e odontológico portáteis e com fontes alternativas de energia para uso em locais remotos e sem infraestrutura\n",
      "    Equipamentos para diagnóstico por imagem dotados de soluções integradas\n",
      "    Equipamentos para diagnóstico e terapia clínica e cirúrgica para oftalmologia\n",
      "    Dispositivos de tecnologia assistiva que auxiliem e aumentem a autonomia de pessoas com deficiências visual, auditiva e motora\n",
      "    Cimento de ionômero de vidro de alta viscosidade\n",
      "    Próteses cirúrgicas de ombro, quadril, fêmur, joelho, crânio e odontológicas, em titânio, porcelana ou biomateriais\n",
      "    Dispositivo de assistência ventricular, máquinas de perfusão para órgãos e oxigenação por membrana extracorpórea\n",
      "    Líquido para preservação de órgãos e córnea\n",
      "\n",
      "  Bloco: B02. DOENÇAS E AGRAVOS CRÍTICOS PARA O SUS\n",
      "    Medicamentos negligenciados e Novos esquemas terapêuticos para otimização do tratamento de Tuberculose, Doença de Chagas, Hanseníase, Esquistossomose, Leishmaniose, Malária e demais doenças elencadas pelo CIEDDS\n",
      "    Medicamentos e novos esquemas terapêuticos demandados pelo SUS para tratamento de HIV/AIDS e Hepatites virais\n",
      "    Medicamentos e formulações para tratamento da população pediátrica\n",
      "    Produtos e nutracêuticos para população pediátrica ou vulnerável\n",
      "    Plataforma de RT-PCR para diagnóstico de arboviroses, multiplex ou isolada\n",
      "    Testes diagnóstico moleculares, por imunoensaio, rápidos de antígeno ou anticorpo, point-of-care, autoteste e outros testes diagnóstico in vitro\n",
      "    Teste para determinação de sensibilidade aos antimicrobianos\n",
      "    Kit de extração de ácidos nucléicos\n",
      "    Inibidores de tirosina quinase\n",
      "    Inibidores de ciclina\n",
      "    Acetato de goserrelina\n",
      "    Pertuzumabe\n",
      "    Trastuzumabe deruxtecan\n",
      "    Pembrolizumabe\n",
      "    Blinatumomabe\n",
      "    Lenalidomida\n",
      "    BCG vesical\n",
      "    L-asparaginase/ Pegaspargase\n",
      "    Produtos de Terapia Gênica\n",
      "    Produtos e formulações para tratamento da população pediátrica\n",
      "    Equipamentos e plataformas para telecolposcopia, telepatologia e telerradiologia\n",
      "    Kit de autocoleta para detecção de HPV por biologia molecular\n",
      "    Teste point-of-care para o câncer de colo de útero\n",
      "    Testes diagnósticos moleculares e de anatomia patológica\n",
      "    Medicamentos e IFA utilizados pelo SUS que apresentem dependência externa de insumos críticos da cadeia produtiva\n",
      "    Espiral de platina\n",
      "    Próteses e outros dispositivos médicos implantáveis cardiovasculares\n",
      "    Testes diagnósticos para a dosagem e avaliação de marcadores cardíacos\n",
      "    Medicamentos e IFA utilizados pelo SUS que apresentem dependência externa de insumos críticos da cadeia produtiva\n",
      "    Insulinas e seus análogos\n",
      "    Desenvolvimento e produção local de tecnologias de informação e conectividade para o monitoramento da diabetes\n",
      "    Testes diagnóstico in vitro\n",
      "    Dispositivos médicos para o tratamento de úlceras no pé diabético\n",
      "    Medicamentos e IFA utilizados pelo SUS para doenças relacionadas ao envelhecimento que apresentem dependência externa de insumos críticos da cadeia produtiva\n",
      "    Conjunto de eletrodos e gerador para estimulação elétrica profunda cerebral\n",
      "    Desenvolvimento e produção local de tecnologias de informação e conectividade para melhoria da qualidade de vida da população idosa\n",
      "    Produtos para tratamento da Fibrose Cística: ivacaftor / elexacaftor, tezacaftor e ivacaftor\n",
      "    Produtos para tratamento da Atrofia Medular Espinhal: Risdiplam\n",
      "    Imunossupressores seletivos para Esclerose Múltipla e Hemoglobinúria paroxística noturna\n",
      "    Enzimas para tratamento de Mucopolissaridoses, Doença de Gaucher, Doença de Pompe e Fibrose Cística\n",
      "    Produtos para tratamento da Polineuropatia Amiloidótica Familiar\n",
      "    Produtos para tratamento da Acromegalia, Raquitismo e osteomalácia\n",
      "    Produtos de Terapia Gênica\n",
      "    Produtos e nutracêuticos para pacientes com erros inatos de metabolismo ou outras doenças raras\n",
      "    Equipamentos e insumos para o rastreamento e exames confirmatórios das doenças elegíveis à triagem neonatal\n",
      "    Sequenciamento completo do exoma\n",
      "    Medicamentos e IFA utilizados pelo SUS paradoenças autoimunes, respiratórias e transtornosmentais que apresentem dependência externa de insumos críticos da cadeia produtiva\n",
      "    Produtos de Terapia Gênica\n",
      "    Gerador de pulso para terapia de estimulação do nervo vago\n",
      "    Equipamentos e insumos para diálise peritoneal\n",
      "    Capilares, cateteres e máquina para hemodiálise\n",
      "    Teste rápido e kits diagnóstico para pré-eclâmpsia\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %pip install pytorch-metric-learning\n",
    "\n",
    "# Importações necessárias\n",
    "import os\n",
    "import json\n",
    "import cudf\n",
    "import cugraph\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from git import Repo\n",
    "from pyvis.network import Network\n",
    "from semantic_matcher import RedeNeuralHibrida, RedeNeuralKAN, RedeNeuralFourier\n",
    "\n",
    "# 1. Recuperar dados pré-processados\n",
    "# Informar caminho para arquivo CSV usando raiz do repositório Git como referência\n",
    "repo = Repo(search_parent_directories=True)\n",
    "root_folder = repo.working_tree_dir\n",
    "json_folder = os.path.join(root_folder, '_data', 'out_json')\n",
    "\n",
    "# Carregar os dados dos arquivos JSON\n",
    "with open(os.path.join(json_folder,'curriculos.json'), 'r') as f:\n",
    "    curriculos_data = json.load(f)\n",
    "with open(os.path.join(json_folder,'matriz_ceis.json'), 'r') as f:\n",
    "    matriz_ceis_data = json.load(f)\n",
    "with open(os.path.join(json_folder,'4dm_biologics.json'), 'r') as f:\n",
    "    relacoes_biologicos = json.load(f)\n",
    "with open(os.path.join(json_folder,'4dm_smallmolecules.json'), 'r') as f:\n",
    "    relacoes_pequenas_moleculas = json.load(f)\n",
    "\n",
    "print(\"Síntese dos dados para criar o Grafo de Conhecimento:\\n\")\n",
    "print(\"  Relacionamentos da Cadeia de Agregação de Valor em Produtos por tipo:\")\n",
    "print(f\"       Biológicos: {len(relacoes_biologicos):2} chaves: {relacoes_biologicos.keys()}\")\n",
    "print(f\"    Peq.Moléculas: {len(relacoes_pequenas_moleculas):2} chaves: {relacoes_pequenas_moleculas.keys()}\")\n",
    "print(\"\\n  Dados das entidades em análise:\")\n",
    "print(f\"      Matriz_CEIS: {len(matriz_ceis_data.get('blocos')):2} blocos, contendo as chaves: {list(matriz_ceis_data.get('blocos')[0].keys())}\")\n",
    "print(f\"       Currículos: {len(curriculos_data):2} currículos, com {len(curriculos_data[0])} chaves em cada currículo\")\n",
    "print(f\"\\n  Dados sobre cada pesquisador:\")\n",
    "for i in list(curriculos_data[0].keys()):\n",
    "    print(f\"     {i}\")\n",
    "\n",
    "print('\\nLista de produtos por Bloco da Matriz CEIS:')\n",
    "for n,b in enumerate(matriz_ceis_data.get('blocos')):\n",
    "    print(f\"  Bloco: {b.get('titulo')}\")\n",
    "    for p in b.get('produtos'):\n",
    "        print(f\"    {p.get('nome')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de relacionamentos por tipo de produtos\n",
      "  BasicScience\n",
      "  BiomarkerDevtProgram\n",
      "  AssayDevelopmentC\n",
      "  BiomedicalInformatics\n",
      "  HypothesisGeneration\n",
      "  DiseasePathophysiology\n",
      "  TherapeuticTargets\n",
      "  DataMining\n",
      "  NewIndicationRepurposingG\n",
      "  CellLines\n",
      "  AnimalModels\n",
      "  MolecularPathway\n",
      "  DataRepository\n",
      "  PrognosticPredictiveBiomarker\n",
      "  ResponseBiomarker\n",
      "  HTSAssay\n",
      "  CompoundLibraries\n",
      "  ChemInformatics\n",
      "  HTSSystem\n",
      "  Genotypes\n",
      "  Phenotypes\n",
      "  Qualification\n",
      "  QualifiedInvestigators\n",
      "  Hits\n",
      "  Misses\n",
      "  Biorepositories\n",
      "  Biospecimens\n",
      "  RegistriesEMRs\n",
      "  CompanionDiagnostics\n",
      "  SurrogateEndpoint\n",
      "  ResearcherInvestigator\n",
      "  IRBReview\n",
      "  IRBApproval\n",
      "  UnapprovedCompoundsNCENME\n",
      "  ClinicalCohorts\n",
      "  NaturalHistoryEpiStudiesE\n",
      "  ClinicalTrialPlanningPreparation\n",
      "  LeadOptimization\n",
      "  IND\n",
      "  ClinicalTrialPlanningPreparation\n",
      "  StudySponsor\n",
      "  OutcomesRatesEvents\n",
      "  LeadCompounds\n",
      "  PopulationSpecific\n",
      "  Recruitment\n",
      "  InformedConsent\n",
      "  RegulatoryComplianceStrategy\n",
      "  IdentificationOfTargetPopulation\n",
      "  InSilicoModeling\n",
      "  ParticipantEnrollment\n",
      "  ClinicalProgramTrialDesign\n",
      "  TherapeuticClinicalEndpoints\n",
      "  MedicinalChemistry\n",
      "  InVitroFunctionalSafetyScreening\n",
      "  ClinicalCohorts\n",
      "  FDAReview\n",
      "  LeadOptimizationD\n",
      "  NaturalHistoryEpiStudies\n",
      "  DiseasePathophysiologyA\n",
      "  ProcessChemistry\n",
      "  NonGMPSUPPLYEarlyCMC\n",
      "  InVivoPharmacologySafety\n",
      "  AnimalModels\n",
      "  RegulatoryStrategy\n",
      "  CMC\n",
      "  ClinicalTrialExecution\n",
      "  DataAnalysis\n",
      "  ClinicalStudyReport\n",
      "  DataSafetyMonitoringBoardDSMB\n",
      "  ClinicalTrialProtocol\n",
      "  ClinicalTrialSites\n",
      "  InvestigationalProduct\n",
      "  ClinicalTrialData\n",
      "  RegulatoryFiling\n",
      "  Approval\n",
      "  PostMarketSurveillance\n",
      "  PostMarketCommitments\n",
      "  PhaseIV\n",
      "  RealWorldEvidence\n",
      "  LabelExpansion\n",
      "  ClinicalTrialData\n",
      "  RealWorldData\n",
      "  AdverseEvents\n",
      "  Payers\n",
      "  ReimbursementPolicies\n",
      "  PricingContracting\n",
      "  HealthTechnologyAssessmentHTA\n",
      "  MarketingSales\n",
      "  Manufacturing\n",
      "  Distribution\n",
      "  SupplyChain\n",
      "  MedicalAffairs\n",
      "  Publication\n",
      "  HEOREvidenceGeneration\n",
      "  MedicalScienceLiaisons\n",
      "  KeyOpinionLeaders\n",
      "  ValueProposition\n",
      "  PatientAdvocacy\n",
      "  AccessPrograms\n",
      "  RealWorldEvidenceGeneration\n",
      "  DataSharing\n",
      "  ComparativeEffectivenessResearch\n",
      "  RegulatoryAgencies\n",
      "  RegulatoryGuidelines\n",
      "  Reimbursement\n",
      "  MarketAccess\n",
      "  Commercialization\n",
      "  PatientAccess\n",
      "{'from': 'BasicScience', 'to': 'BiomarkerDevtProgram'}\n",
      "{'from': 'BasicScience', 'to': 'AssayDevelopmentC'}\n",
      "{'from': 'BasicScience', 'to': 'BiomedicalInformatics'}\n",
      "{'from': 'BiomedicalInformatics', 'to': 'HypothesisGeneration'}\n",
      "{'from': 'HypothesisGeneration', 'to': 'DiseasePathophysiology'}\n",
      "{'from': 'HypothesisGeneration', 'to': 'TherapeuticTargets'}\n",
      "{'from': 'BiomedicalInformatics', 'to': 'DataMining'}\n",
      "{'from': 'DataMining', 'to': 'NewIndicationRepurposingG'}\n",
      "{'from': 'TherapeuticTargets', 'to': 'CellLines'}\n",
      "{'from': 'TherapeuticTargets', 'to': 'AnimalModels'}\n",
      "{'from': 'DiseasePathophysiology', 'to': 'MolecularPathway'}\n",
      "{'from': 'DataMining', 'to': 'DataRepository'}\n",
      "{'from': 'BiomarkerDevtProgram', 'to': 'PrognosticPredictiveBiomarker'}\n",
      "{'from': 'BiomarkerDevtProgram', 'to': 'ResponseBiomarker'}\n",
      "{'from': 'AssayDevelopmentC', 'to': 'HTSAssay'}\n",
      "{'from': 'HTSAssay', 'to': 'CompoundLibraries'}\n",
      "{'from': 'HTSAssay', 'to': 'ChemInformatics'}\n",
      "{'from': 'HTSAssay', 'to': 'HTSSystem'}\n",
      "{'from': 'CompoundLibraries', 'to': 'Genotypes'}\n",
      "{'from': 'CompoundLibraries', 'to': 'Phenotypes'}\n",
      "{'from': 'PrognosticPredictiveBiomarker', 'to': 'Qualification'}\n",
      "{'from': 'Qualification', 'to': 'QualifiedInvestigators'}\n",
      "{'from': 'HTSSystem', 'to': 'Hits'}\n",
      "{'from': 'HTSSystem', 'to': 'Misses'}\n",
      "{'from': 'ResponseBiomarker', 'to': 'Biorepositories'}\n",
      "{'from': 'ResponseBiomarker', 'to': 'Biospecimens'}\n",
      "{'from': 'ResponseBiomarker', 'to': 'RegistriesEMRs'}\n",
      "{'from': 'ResponseBiomarker', 'to': 'CompanionDiagnostics'}\n",
      "{'from': 'ResponseBiomarker', 'to': 'SurrogateEndpoint'}\n",
      "{'from': 'QualifiedInvestigators', 'to': 'ResearcherInvestigator'}\n",
      "{'from': 'ResearcherInvestigator', 'to': 'IRBReview'}\n",
      "{'from': 'IRBReview', 'to': 'IRBApproval'}\n",
      "{'from': 'IRBApproval', 'to': 'UnapprovedCompoundsNCENME'}\n",
      "{'from': 'UnapprovedCompoundsNCENME', 'to': 'ClinicalCohorts'}\n",
      "{'from': 'ClinicalCohorts', 'to': 'NaturalHistoryEpiStudiesE'}\n",
      "{'from': 'NaturalHistoryEpiStudiesE', 'to': 'ClinicalTrialPlanningPreparation'}\n",
      "{'from': 'ClinicalTrialPlanningPreparation', 'to': 'LeadOptimization'}\n",
      "{'from': 'LeadOptimization', 'to': 'IndustrialProcess'}\n",
      "{'from': 'IndustrialProcess', 'to': 'ClinicalTrialPlanningPreparation'}\n",
      "{'from': 'ClinicalTrialPlanningPreparation', 'to': 'StudySponsor'}\n",
      "{'from': 'ClinicalTrialPlanningPreparation', 'to': 'OutcomesRatesEvents'}\n",
      "{'from': 'LeadOptimization', 'to': 'LeadCompounds'}\n",
      "{'from': 'OutcomesRatesEvents', 'to': 'PopulationSpecific'}\n",
      "{'from': 'ClinicalTrialPlanningPreparation', 'to': 'Recruitment'}\n",
      "{'from': 'Recruitment', 'to': 'InformedConsent'}\n",
      "{'from': 'ClinicalTrialPlanningPreparation', 'to': 'RegulatoryComplianceStrategy'}\n",
      "{'from': 'ClinicalTrialPlanningPreparation', 'to': 'IdentificationOfTargetPopulation'}\n",
      "{'from': 'LeadCompounds', 'to': 'InSilicoModeling'}\n",
      "{'from': 'InformedConsent', 'to': 'ParticipantEnrollment'}\n",
      "{'from': 'RegulatoryComplianceStrategy', 'to': 'ClinicalProgramTrialDesign'}\n",
      "{'from': 'IdentificationOfTargetPopulation', 'to': 'TherapeuticClinicalEndpoints'}\n",
      "{'from': 'LeadCompounds', 'to': 'MedicinalChemistry'}\n",
      "{'from': 'LeadCompounds', 'to': 'InVitroFunctionalSafetyScreening'}\n",
      "{'from': 'ParticipantEnrollment', 'to': 'ClinicalCohorts'}\n",
      "{'from': 'ClinicalProgramTrialDesign', 'to': 'FDAReview'}\n",
      "{'from': 'MedicinalChemistry', 'to': 'LeadOptimizationD'}\n",
      "{'from': 'InVitroFunctionalSafetyScreening', 'to': 'NaturalHistoryEpiStudies'}\n",
      "{'from': 'NaturalHistoryEpiStudies', 'to': 'DiseasePathophysiologyA'}\n",
      "{'from': 'LeadOptimizationD', 'to': 'ProcessChemistry'}\n",
      "{'from': 'ProcessChemistry', 'to': 'NonGMPSUPPLYEarlyCMC'}\n",
      "{'from': 'InVitroFunctionalSafetyScreening', 'to': 'InVivoPharmacologySafety'}\n",
      "{'from': 'InVivoPharmacologySafety', 'to': 'AnimalModels'}\n",
      "{'from': 'NonGMPSUPPLYEarlyCMC', 'to': 'RegulatoryStrategy'}\n",
      "{'from': 'NonGMPSUPPLYEarlyCMC', 'to': 'CMC'}\n",
      "{'from': 'ParticipantEnrollment', 'to': 'ClinicalTrialExecution'}\n",
      "{'from': 'ClinicalTrialExecution', 'to': 'DataAnalysis'}\n",
      "{'from': 'DataAnalysis', 'to': 'ClinicalStudyReport'}\n",
      "{'from': 'ClinicalTrialExecution', 'to': 'DataSafetyMonitoringBoardDSMB'}\n",
      "{'from': 'ClinicalTrialExecution', 'to': 'ClinicalTrialProtocol'}\n",
      "{'from': 'ClinicalTrialProtocol', 'to': 'ClinicalTrialSites'}\n",
      "{'from': 'ClinicalTrialExecution', 'to': 'InvestigationalProduct'}\n",
      "{'from': 'DataAnalysis', 'to': 'ClinicalTrialData'}\n",
      "{'from': 'ClinicalTrialData', 'to': 'RegulatoryFiling'}\n",
      "{'from': 'RegulatoryFiling', 'to': 'Approval'}\n",
      "{'from': 'Approval', 'to': 'PostMarketSurveillance'}\n",
      "{'from': 'PostMarketSurveillance', 'to': 'PostMarketCommitments'}\n",
      "{'from': 'PostMarketSurveillance', 'to': 'PhaseIV'}\n",
      "{'from': 'PostMarketSurveillance', 'to': 'RealWorldEvidence'}\n",
      "{'from': 'PostMarketSurveillance', 'to': 'LabelExpansion'}\n",
      "{'from': 'PhaseIV', 'to': 'ClinicalTrialData'}\n",
      "{'from': 'RealWorldEvidence', 'to': 'RealWorldData'}\n",
      "{'from': 'RealWorldEvidence', 'to': 'AdverseEvents'}\n",
      "{'from': 'Approval', 'to': 'Reimbursement'}\n",
      "{'from': 'Reimbursement', 'to': 'Payers'}\n",
      "{'from': 'Reimbursement', 'to': 'ReimbursementPolicies'}\n",
      "{'from': 'Approval', 'to': 'MarketAccess'}\n",
      "{'from': 'MarketAccess', 'to': 'PricingContracting'}\n",
      "{'from': 'MarketAccess', 'to': 'HealthTechnologyAssessmentHTA'}\n",
      "{'from': 'Approval', 'to': 'Commercialization'}\n",
      "{'from': 'Commercialization', 'to': 'MarketingSales'}\n",
      "{'from': 'CMC', 'to': 'Manufacturing'}\n",
      "{'from': 'Manufacturing', 'to': 'Distribution'}\n",
      "{'from': 'Manufacturing', 'to': 'SupplyChain'}\n",
      "{'from': 'Commercialization', 'to': 'MedicalAffairs'}\n",
      "{'from': 'MedicalAffairs', 'to': 'Publication'}\n",
      "{'from': 'MedicalAffairs', 'to': 'HEOREvidenceGeneration'}\n",
      "{'from': 'MedicalAffairs', 'to': 'MedicalScienceLiaisons'}\n",
      "{'from': 'MedicalScienceLiaisons', 'to': 'KeyOpinionLeaders'}\n",
      "{'from': 'Commercialization', 'to': 'ValueProposition'}\n",
      "{'from': 'Commercialization', 'to': 'PatientAdvocacy'}\n",
      "{'from': 'Commercialization', 'to': 'AccessPrograms'}\n",
      "{'from': 'RealWorldEvidence', 'to': 'RealWorldEvidenceGeneration'}\n",
      "{'from': 'RealWorldEvidenceGeneration', 'to': 'DataSharing'}\n",
      "{'from': 'RealWorldEvidenceGeneration', 'to': 'ComparativeEffectivenessResearch'}\n",
      "{'from': 'RegulatoryFiling', 'to': 'RegulatoryAgencies'}\n",
      "{'from': 'RegulatoryAgencies', 'to': 'RegulatoryGuidelines'}\n",
      "{'from': 'Commercialization', 'to': 'PatientAccess'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Lista de relacionamentos por tipo de produtos\")\n",
    "for i,j in relacoes_pequenas_moleculas.items():\n",
    "    for k in j:\n",
    "        rotulo = k.get('id')\n",
    "        if rotulo:\n",
    "            print(f\"  {rotulo}\")\n",
    "        else:\n",
    "            print(f\"{  k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de relacionamentos por tipo de produtos\n",
      "  BasicScience\n",
      "  BiomedicalInformatics\n",
      "  DataMining\n",
      "  DataRepository\n",
      "  TargetPharmacology\n",
      "  TargetValidation\n",
      "  AssayDevelopment\n",
      "  Hit2Lead\n",
      "  LeadIdentification\n",
      "  LeadOptimization\n",
      "  CMC\n",
      "  ClinicalResearch\n",
      "  ClinicalTrials\n",
      "  RegulatoryReview\n",
      "  Postmarketing\n",
      "  PostmarketingStudies\n",
      "  MedicalLandscape\n",
      "  MedicalNeed\n",
      "  ClinicalPracticeGuidelines\n",
      "  HealthEconomics\n",
      "  Regulatory\n",
      "  RegulatoryAgencies\n",
      "  RegulatoryGuidelines\n",
      "  Reimbursement\n",
      "  Payers\n",
      "  ReimbursementPolicies\n",
      "  MarketAccess\n",
      "  Pricing\n",
      "  Commercialization\n",
      "  Marketing\n",
      "  PatientAccess\n",
      "  Manufacturing\n",
      "  SupplyChain\n",
      "  Distribution\n",
      "{'from': 'BasicScience', 'to': 'BiomedicalInformatics'}\n",
      "{'from': 'BiomedicalInformatics', 'to': 'DataMining'}\n",
      "{'from': 'DataMining', 'to': 'DataRepository'}\n",
      "{'from': 'BasicScience', 'to': 'TargetPharmacology'}\n",
      "{'from': 'TargetPharmacology', 'to': 'TargetValidation'}\n",
      "{'from': 'TargetValidation', 'to': 'AssayDevelopment'}\n",
      "{'from': 'AssayDevelopment', 'to': 'Hit2Lead'}\n",
      "{'from': 'Hit2Lead', 'to': 'LeadIdentification'}\n",
      "{'from': 'LeadIdentification', 'to': 'LeadOptimization'}\n",
      "{'from': 'LeadOptimization', 'to': 'CMC'}\n",
      "{'from': 'LeadOptimization', 'to': 'ClinicalResearch'}\n",
      "{'from': 'ClinicalResearch', 'to': 'ClinicalTrials'}\n",
      "{'from': 'ClinicalTrials', 'to': 'RegulatoryReview'}\n",
      "{'from': 'RegulatoryReview', 'to': 'Postmarketing'}\n",
      "{'from': 'Postmarketing', 'to': 'PostmarketingStudies'}\n",
      "{'from': 'Postmarketing', 'to': 'MedicalLandscape'}\n",
      "{'from': 'MedicalLandscape', 'to': 'MedicalNeed'}\n",
      "{'from': 'MedicalLandscape', 'to': 'ClinicalPracticeGuidelines'}\n",
      "{'from': 'MedicalLandscape', 'to': 'HealthEconomics'}\n",
      "{'from': 'RegulatoryReview', 'to': 'Regulatory'}\n",
      "{'from': 'Regulatory', 'to': 'RegulatoryAgencies'}\n",
      "{'from': 'Regulatory', 'to': 'RegulatoryGuidelines'}\n",
      "{'from': 'Postmarketing', 'to': 'Reimbursement'}\n",
      "{'from': 'Reimbursement', 'to': 'Payers'}\n",
      "{'from': 'Reimbursement', 'to': 'ReimbursementPolicies'}\n",
      "{'from': 'Postmarketing', 'to': 'MarketAccess'}\n",
      "{'from': 'MarketAccess', 'to': 'Pricing'}\n",
      "{'from': 'Postmarketing', 'to': 'Commercialization'}\n",
      "{'from': 'Commercialization', 'to': 'Marketing'}\n",
      "{'from': 'Commercialization', 'to': 'PatientAccess'}\n",
      "{'from': 'LeadOptimization', 'to': 'Manufacturing'}\n",
      "{'from': 'Manufacturing', 'to': 'SupplyChain'}\n",
      "{'from': 'SupplyChain', 'to': 'Distribution'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Lista de relacionamentos por tipo de produtos\")\n",
    "for i,j in relacoes_biologicos.items():\n",
    "    for k in j:\n",
    "        rotulo = k.get('id')\n",
    "        if rotulo:\n",
    "            print(f\"  {rotulo}\")\n",
    "        else:\n",
    "            print(f\"{  k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Lista de relacionamentos por tipo de produtos\")\n",
    "# for i,j in relacoes_biologicos.items():\n",
    "#     for k in j:\n",
    "#         print(k.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparar o Grafo de Conhecimento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grafo \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241m.\u001b[39mGraph()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ... (código para adicionar nós e arestas ao grafo)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m grafo_treino, grafo_teste \u001b[38;5;241m=\u001b[39m train_test_split(grafo, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "grafo = nx.Graph()\n",
    "# ... (código para adicionar nós e arestas ao grafo)\n",
    "\n",
    "grafo_treino, grafo_teste = train_test_split(grafo, test_size=0.2)\n",
    "\n",
    "# Configuração dos parâmetros dos modelos\n",
    "parametros_iniciais = {\n",
    "    'num_features': ...,  # Número de features dos nós\n",
    "    'hidden_dim': 4,  # Dimensão da camada oculta\n",
    "    'num_classes': 7,  # Número de classes (clusters)\n",
    "    'dropout': 0.5  # Taxa de dropout\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Criação dos modelos com instanciação das classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelo_kan     = RedeNeuralKAN(parametros_iniciais)\n",
    "modelo_fourier = RedeNeuralFourier(parametros_iniciais)\n",
    "modelo_hibrido = RedeNeuralHibrida(parametros_iniciais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Treinamento dos modelos\n",
    "\n",
    "Utilizar o grafo de treinamento (grafo_treino) para treinar cada um dos modelos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Treinamento do modelo de GNN com Sincronização (híbrido)\n",
    "\n",
    "    Inspirado em https://iopscience.iop.org/article/10.1209/0295-5075/ad76d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_modelo(modelo, grafo_treino, epochs=100, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Treina um modelo de rede neural em grafos.\n",
    "\n",
    "    Args:\n",
    "      modelo: O modelo da rede neural.\n",
    "      grafo_treino: O grafo de conhecimento para treinamento.\n",
    "      epochs: O número de épocas de treinamento.\n",
    "      learning_rate: A taxa de aprendizado do otimizador.\n",
    "\n",
    "    Funcionamento:\n",
    "      Recebe o modelo, o grafo de treinamento, o número de épocas e a taxa de aprendizado como parâmetros.\n",
    "      Cria um otimizador Adam para atualizar os pesos do modelo.\n",
    "      Define a função de perda como CrossEntropyLoss (ajustável para a função de perda mais adequada ao problema).\n",
    "      Prepara os dados de treinamento (features, edge_index, edge_weight e labels, se houver).\n",
    "      Itera pelas épocas de treinamento, executando o modelo, calculando a perda, os gradientes e atualizando os pesos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Criar o otimizador\n",
    "        optimizer = torch.optim.Adam(modelo.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Definir a função de perda (exemplo: cross-entropy)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Preparar os dados de treinamento\n",
    "        x = torch.tensor(list(nx.get_node_attributes(grafo_treino, 'features').values()), dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(grafo_treino.edges()), dtype=torch.long).t().contiguous()\n",
    "        edge_weight = torch.tensor(list(nx.get_edge_attributes(grafo_treino, 'weight').values()), dtype=torch.float)\n",
    "        \n",
    "        # ... (código para obter os rótulos dos nós, se houver)\n",
    "        # labels = ...\n",
    "\n",
    "        # Treinar o modelo\n",
    "        modelo.train()  # Mudar para o modo de treinamento\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()  # Zerar os gradientes\n",
    "            out = modelo(x, edge_index, edge_weight)  # Executar o modelo\n",
    "            loss = criterion(out, labels)  # Calcular a perda\n",
    "            loss.backward()  # Calcular os gradientes\n",
    "            optimizer.step()  # Atualizar os pesos do modelo\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no treinamento do modelo: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Chamar a função para treinar o modelo_hibrido\n",
    "treinar_modelo(modelo_hibrido, grafo_treino, epochs=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Função de treinamento do modelo_kan_mse\n",
    "\n",
    "    Com função de perda Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_kan = nn.MSELoss()  \n",
    "\n",
    "def treinar_modelo_kan_mse(modelo, grafo_treino, epochs=100, learning_rate=0.01, criterion=criterion_kan):\n",
    "    \"\"\"\n",
    "    Treina um modelo de rede neural KAN em grafos.\n",
    "\n",
    "    Args:\n",
    "      modelo: O modelo da rede neural KAN.\n",
    "      grafo_treino: O grafo de conhecimento para treinamento.\n",
    "      epochs: O número de épocas de treinamento.\n",
    "      learning_rate: A taxa de aprendizado do otimizador.\n",
    "      criterion: A função de perda.\n",
    "    \n",
    "    Função de Perda: A função de perda utilizada foi o MSELoss. \n",
    "    \n",
    "    Pode-se ajustar para uma função de perda mais adequada ao problema, considerando aprendizado não-supervisionado. \n",
    "    Uma opção seria utilizar uma função de perda baseada na distância entre os embeddings de nós que deveriam estar no mesmo cluster.\n",
    "    Ou ainda, pode-se também usar função de perda baseada em Triplet Loss.\n",
    "    \n",
    "    Rótulos: Ajustar o código para obter os rótulos dos nós (labels) de acordo com o seu problema. \n",
    "    Como o problema é de aprendizado não-supervisionado, os rótulos podem ser definidos com base em alguma heurística ou conhecimento prévio sobre o problema.\n",
    "    \n",
    "    Hiperparâmetros: Ajustar o número de épocas (epochs) e a taxa de aprendizado (learning_rate) para obter o melhor desempenho do modelo.    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Criar o otimizador\n",
    "        optimizer = torch.optim.Adam(modelo.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Preparar os dados de treinamento\n",
    "        x = torch.tensor(list(nx.get_node_attributes(grafo_treino, 'features').values()), dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(grafo_treino.edges()), dtype=torch.long).t().contiguous()\n",
    "        edge_weight = torch.tensor(list(nx.get_edge_attributes(grafo_treino, 'weight').values()), dtype=torch.float)\n",
    "        \n",
    "        # ... (código para obter os rótulos dos nós, se houver - ajustar para o seu problema)\n",
    "        # labels = ...\n",
    "\n",
    "        # Treinar o modelo\n",
    "        modelo.train()  # Mudar para o modo de treinamento\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()  # Zerar os gradientes\n",
    "            out = modelo(x, edge_index, edge_weight)  # Executar o modelo\n",
    "            \n",
    "            # Calcular a perda (considerando que labels é um tensor com os valores desejados para os nós)\n",
    "            loss = criterion(out, labels)  \n",
    "            loss.backward()  # Calcular os gradientes\n",
    "            optimizer.step()  # Atualizar os pesos do modelo\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no treinamento do modelo KAN: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Chamar a função para treinar o modelo_kan\n",
    "treinar_modelo_kan_mse(modelo_kan, grafo_treino, epochs=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Função de treinamento do modelo_kan_triplet\n",
    "\n",
    "    Com função de perda por triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import losses\n",
    "\n",
    "# Exemplo de função para gerar trios (âncora, positivo, negativo)\n",
    "def gerar_triplets(grafo, embeddings):\n",
    "    \"\"\"\n",
    "    Gera trios (âncora, positivo, negativo) para a Triplet Loss.\n",
    "\n",
    "    Args:\n",
    "      grafo: O grafo de conhecimento.\n",
    "      embeddings: Os embeddings dos nós.\n",
    "\n",
    "    Returns:\n",
    "      Um tensor com os trios.\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    for no_ancora in grafo.nodes():\n",
    "        # Encontrar nós positivos (similares à âncora)\n",
    "        # ... (implementar lógica para encontrar nós positivos)\n",
    "\n",
    "        # Encontrar nós negativos (diferentes da âncora)\n",
    "        # ... (implementar lógica para encontrar nós negativos)\n",
    "\n",
    "        for no_positivo in nos_positivos:\n",
    "            for no_negativo in nos_negativos:\n",
    "                triplets.append([no_ancora, no_positivo, no_negativo])\n",
    "\n",
    "    return torch.tensor(triplets)\n",
    "\n",
    "def treinar_modelo_kan_triplet(modelo, grafo_treino, epochs=100, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Treina um modelo de rede neural KAN em grafos utilizando Triplet Loss.\n",
    "\n",
    "    Args:\n",
    "      modelo: O modelo da rede neural KAN.\n",
    "      grafo_treino: O grafo de conhecimento para treinamento.\n",
    "      epochs: O número de épocas de treinamento.\n",
    "      learning_rate: A taxa de aprendizado do otimizador.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Criar o otimizador\n",
    "        optimizer = torch.optim.Adam(modelo.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Preparar os dados de treinamento\n",
    "        x = torch.tensor(list(nx.get_node_attributes(grafo_treino, 'features').values()), dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(grafo_treino.edges()), dtype=torch.long).t().contiguous()\n",
    "        edge_weight = torch.tensor(list(nx.get_edge_attributes(grafo_treino, 'weight').values()), dtype=torch.float)\n",
    "\n",
    "        # Criar a função de perda Triplet Loss\n",
    "        loss_func = losses.TripletMarginLoss()\n",
    "\n",
    "        # Treinar o modelo\n",
    "        modelo.train()  # Mudar para o modo de treinamento\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()  # Zerar os gradientes\n",
    "            embeddings = modelo(x, edge_index, edge_weight)  # Executar o modelo\n",
    "            \n",
    "            # Gerar os trios (âncora, positivo, negativo), com lógica de acordo com o problema\n",
    "            triplets = gerar_triplets(grafo_treino, embeddings) \n",
    "\n",
    "            # Calcular a Triplet Loss\n",
    "            loss = loss_func(embeddings, triplets)  \n",
    "            loss.backward()  # Calcular os gradientes\n",
    "            optimizer.step()  # Atualizar os pesos do modelo\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no treinamento do modelo KAN: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Chamar a função para treinar o modelo_kan\n",
    "treinar_modelo_kan_triplet(modelo_kan, grafo_treino, epochs=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Função para treinamento do modelo_fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo_fourier\n",
    "criterion_fourier = nn.KLDivLoss() # Ajustar a função de perda para o modelo Fourier (exemplo: Kullback-Leibler Divergence)  \n",
    "\n",
    "def treinar_modelo_fourier(modelo, grafo_treino, epochs=100, learning_rate=0.01, criterion=criterion_fourier):\n",
    "    \"\"\"\n",
    "    Treina um modelo de rede neural Fourier em grafos.\n",
    "\n",
    "    Args:\n",
    "      modelo: O modelo da rede neural Fourier.\n",
    "      grafo_treino: O grafo de conhecimento para treinamento.\n",
    "      epochs: O número de épocas de treinamento.\n",
    "      learning_rate: A taxa de aprendizado do otimizador.\n",
    "      criterion: A função de perda.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Criar o otimizador\n",
    "        optimizer = torch.optim.Adam(modelo.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Preparar os dados de treinamento\n",
    "        x = torch.tensor(list(nx.get_node_attributes(grafo_treino, 'features').values()), dtype=torch.float)\n",
    "        edge_index = torch.tensor(list(grafo_treino.edges()), dtype=torch.long).t().contiguous()\n",
    "        edge_weight = torch.tensor(list(nx.get_edge_attributes(grafo_treino, 'weight').values()), dtype=torch.float)\n",
    "        # ... (código para obter os rótulos dos nós, se houver - ajustar para o seu problema)\n",
    "        # labels = ...\n",
    "\n",
    "        # Treinar o modelo\n",
    "        modelo.train()  # Mudar para o modo de treinamento\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()  # Zerar os gradientes\n",
    "            out = modelo(x, edge_index, edge_weight)  # Executar o modelo\n",
    "\n",
    "            # Ajustar a saída do modelo e os rótulos para a KLDivLoss\n",
    "            # (a saída deve ser um tensor de probabilidades e os rótulos devem ser um tensor de índices)\n",
    "            out = torch.nn.functional.log_softmax(out, dim=1)  # Aplicar log_softmax na saída\n",
    "            # Converter labels para one-hot encoding e aplicar log_softmax (ajuste para o seu problema)\n",
    "            labels_onehot = torch.nn.functional.one_hot(labels, num_classes=out.shape[1]).float()\n",
    "            labels_onehot = torch.nn.functional.log_softmax(labels_onehot, dim=1) \n",
    "\n",
    "            # Calcular a perda \n",
    "            loss = criterion(out, labels_onehot)  \n",
    "            loss.backward()  # Calcular os gradientes\n",
    "            optimizer.step()  # Atualizar os pesos do modelo\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no treinamento do modelo Fourier: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Chamar a função para treinar o modelo_fourier\n",
    "treinar_modelo_fourier(modelo_fourier, grafo_treino, epochs=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Instanciar a classe de testes e realizar as análises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie uma instância da classe TesteRedeNeural para avaliar o desempenho de cada modelo no grafo de teste (grafo_teste):\n",
    "teste_rede_neural = TesteRedeNeural(grafo_teste, parametros_modelo)  # Instanciar a classe de testes\n",
    "\n",
    "# Controle de Sincronização + GNN\n",
    "clusters_hibrido, embeddings_hibrido = modelo_hibrido.inferir(grafo_teste)  # Realizar inferência\n",
    "analises_hibrido = teste_rede_neural.testar_inferencia(clusters_hibrido, embeddings_hibrido)\n",
    "\n",
    "# KAN + GNN\n",
    "clusters_kan, embeddings_kan = modelo_kan.inferir(grafo_teste)\n",
    "analises_kan = teste_rede_neural.testar_inferencia(clusters_kan, embeddings_kan)\n",
    "\n",
    "# Transformadas de Fourier + GNN\n",
    "clusters_fourier, embeddings_fourier = modelo_fourier.inferir(grafo_teste)\n",
    "analises_fourier = teste_rede_neural.testar_inferencia(clusters_fourier, embeddings_fourier)\n",
    "\n",
    "# 6. Comparação dos resultados\n",
    "def gerar_tabela_latex(analises):\n",
    "    \"\"\"\n",
    "    Gera uma tabela LaTeX com os resultados das análises.\n",
    "\n",
    "    Args:\n",
    "      analises: Um dicionário contendo as análises dos modelos.\n",
    "\n",
    "    Returns:\n",
    "      Uma string com o código LaTeX da tabela.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(analises)\n",
    "    df.index = ['Híbrido', 'KAN', 'Fourier']\n",
    "    latex_table = df.to_latex(float_format=\"%.3f\", caption=\"Resultados das Análises\", label=\"tab:resultados\")\n",
    "    return latex_table\n",
    "\n",
    "# Agregar as análises em um dicionário\n",
    "analises = {\n",
    "    'Híbrido': analises_hibrido,\n",
    "    'KAN': analises_kan,\n",
    "    'Fourier': analises_fourier,\n",
    "}\n",
    "\n",
    "# Gerar a tabela LaTeX\n",
    "tabela_latex = gerar_tabela_latex(analises)\n",
    "\n",
    "# Exibir a tabela LaTeX\n",
    "print(tabela_latex)\n",
    "\n",
    "# Salvar a tabela em um arquivo .tex\n",
    "with open('resultados_analises.tex', 'w') as f:\n",
    "    f.write(tabela_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma instância da classe SemanticMatching\n",
    "from semantic_matcher import ENPreprocessor, SemanticMatcher\n",
    "tradutor = ENPreprocessor()\n",
    "matcher = SemanticMatcher(curriculos_data, matriz_ceis_data, relacoes_biologicos, relacoes_pequenas_moleculas)\n",
    "\n",
    "# Executar as etapas de processamento\n",
    "matcher.traduzir_nomes_produtos()\n",
    "matcher.extrair_caracteristicas()\n",
    "matcher.classificar_produtos()\n",
    "matcher.conectar_produtos_grafo()\n",
    "\n",
    "# Imprimir informações sobre os grafos\n",
    "print(\"Grafo de Biológicos:\")\n",
    "print(\"Número de nós:\", matcher.biologicos.number_of_nodes())\n",
    "print(\"Número de arestas:\", matcher.biologicos.number_of_edges())\n",
    "\n",
    "print(\"\\nGrafo de Pequenas Moléculas:\")\n",
    "print(\"Número de nós:\", matcher.pequenas_moleculas.number_of_nodes())\n",
    "print(\"Número de arestas:\", matcher.pequenas_moleculas.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "import cugraph\n",
    "import json\n",
    "from pyvis.network import Network\n",
    "\n",
    "json_folder = os.path.join(os.getcwd(),'_data','out_json')\n",
    "\n",
    "# Carregar os dados dos arquivos JSON com cuDF\n",
    "curriculos_df = cudf.read_json(os.path.join(json_folder,'curriculos.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_ceis_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cugraph\n",
    "import json\n",
    "import time\n",
    "from pyvis.network import Network\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Funções para calcular as transformadas de Fourier\n",
    "def naive_fourier(x, gridsize=300):\n",
    "    # ... (implementação da Naive Fourier Transform) ...\n",
    "    pass  # Substitua pelo código da transformada\n",
    "\n",
    "def gft(x, edge_index):\n",
    "    # ... (implementação da Graph Fourier Transform) ...\n",
    "    pass  # Substitua pelo código da transformada\n",
    "\n",
    "def wavelet_transform(x, edge_index, num_scales=5):\n",
    "    # ... (implementação da Wavelet Transform) ...\n",
    "    pass  # Substitua pelo código da transformada\n",
    "\n",
    "def fractional_fourier(x, alpha=0.5):\n",
    "    # ... (implementação da Fractional Fourier Transform) ...\n",
    "    pass  # Substitua pelo código da transformada\n",
    "\n",
    "def qft(x):\n",
    "  \"\"\"Calcula a QFT de um quaternion.\"\"\"\n",
    "  # ... (implementação da Transformada de Fourier Quaterniônica) ...\n",
    "  pass  # Substitua pelo código da transformada\n",
    "\n",
    "def conectar_pesquisadores_produtos(graph, curriculos_df, matriz_ceis_df, tipo_transformada):\n",
    "    \"\"\"Conecta pesquisadores a produtos com base na similaridade de áreas de atuação.\"\"\"\n",
    "\n",
    "    # Extrair áreas de atuação dos pesquisadores e produtos\n",
    "    pesquisadores_areas = curriculos_df['Áreas'].to_arrow().to_pylist()\n",
    "    produtos_areas = []\n",
    "    for bloco in matriz_ceis_df['blocos'].to_arrow().to_pylist():\n",
    "        for produto in bloco['produtos']:\n",
    "            produtos_areas.append(produto['nome'])  # Usando o nome do produto como representação da área\n",
    "\n",
    "    # Converter áreas de atuação em vetores numéricos (ex: usando word embeddings)\n",
    "    # ... (implementar lógica para converter áreas em vetores) ...\n",
    "\n",
    "    # Aplicar a transformada de Fourier selecionada\n",
    "    if tipo_transformada == 'naive':\n",
    "        pesquisadores_transformados = [naive_fourier(areas) for areas in pesquisadores_areas]\n",
    "        produtos_transformados = [naive_fourier(areas) for areas in produtos_areas]\n",
    "    elif tipo_transformada == 'gft':\n",
    "        # ... (aplicar GFT) ...\n",
    "        pass\n",
    "    elif tipo_transformada == 'wavelet':\n",
    "        # ... (aplicar Wavelet Transform) ...\n",
    "        pass\n",
    "    elif tipo_transformada == 'fractional':\n",
    "        # ... (aplicar Fractional Fourier Transform) ...\n",
    "        pass\n",
    "    elif tipo_transformada == 'qft':\n",
    "        # ... (aplicar QFT) ...\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Tipo de transformada inválido.\")\n",
    "\n",
    "    # Calcular a similaridade de cosseno entre os vetores transformados\n",
    "    similaridade = cosine_similarity(pesquisadores_transformados, produtos_transformados)\n",
    "\n",
    "    # Criar as arestas no grafo cuGraph com base na similaridade\n",
    "    limite_similaridade = 0.8  # Definir um limite para a similaridade\n",
    "    for i, pesquisador_id in enumerate(pesquisadores_ids):\n",
    "        for j, produto_id in enumerate(produtos_ids):\n",
    "            if similaridade[i, j] > limite_similaridade:\n",
    "                G.add_edge(pesquisador_id, produto_id, weight=similaridade[i, j])\n",
    "\n",
    "# --- Avaliação das abordagens ---\n",
    "\n",
    "resultados = {}\n",
    "tipos_transformadas = ['naive', 'gft', 'wavelet', 'fractional', 'qft']\n",
    "\n",
    "for tipo_transformada in tipos_transformadas:\n",
    "    inicio = time.time()\n",
    "\n",
    "    # Criar um novo grafo para cada tipo de transformada\n",
    "    G = cugraph.Graph()\n",
    "    G.add_nodes_from(pesquisadores_ids, tipo='pesquisador')\n",
    "    G.add_nodes_from(produtos_ids, tipo='produto')\n",
    "\n",
    "    # Conectar pesquisadores e produtos\n",
    "    conectar_pesquisadores_produtos(G, curriculos_df, matriz_ceis_df, tipo_transformada)\n",
    "\n",
    "    fim = time.time()\n",
    "    tempo_execucao = fim - inicio\n",
    "\n",
    "    # Calcular as métricas de avaliação\n",
    "    num_arestas = G.number_of_edges()\n",
    "\n",
    "    # ... (calcular precisão e recall usando um conjunto de dados de referência) ...\n",
    "\n",
    "    resultados[tipo_transformada] = {\n",
    "        'num_arestas': num_arestas,\n",
    "        'tempo_execucao': tempo_execucao,\n",
    "        # 'precisao': precisao,\n",
    "        # 'recall': recall\n",
    "    }\n",
    "\n",
    "# Imprimir os resultados\n",
    "print(resultados)\n",
    "\n",
    "# --- Visualização (opcional) ---\n",
    "# ... (criar gráficos com Altair para comparar as métricas) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pré-processamento dos dados com cuDF\n",
    "# (Extrair IDs e informações relevantes para os nós e arestas)\n",
    "pesquisadores_ids = curriculos_df['Identificação']['ID Lattes'].to_arrow().to_pylist()\n",
    "produtos_ids = []\n",
    "for bloco in matriz_ceis_df['blocos'].to_arrow().to_pylist():\n",
    "    for produto in bloco['produtos']:\n",
    "        produtos_ids.append(produto['id'])\n",
    "\n",
    "# Criar um grafo cuGraph\n",
    "G = cugraph.Graph()\n",
    "\n",
    "# Adicionar os nós ao grafo cuGraph\n",
    "G.add_nodes_from(pesquisadores_ids, tipo='pesquisador')\n",
    "G.add_nodes_from(produtos_ids, tipo='produto')\n",
    "\n",
    "# Criar as arestas entre pesquisadores e produtos (definir critérios)\n",
    "# ... (implementar lógica para conectar pesquisadores a produtos usando cuDF) ...\n",
    "# EXEMPLO: conectar pesquisadores a produtos com base na similaridade de áreas de atuação\n",
    "\n",
    "# Calcular métricas de grafo com cuGraph\n",
    "degree_centrality = cugraph.degree_centrality(G)\n",
    "\n",
    "# Converter o grafo cuGraph para NetworkX\n",
    "graph_nx = G.to_networkx()\n",
    "\n",
    "# Criar o grafo PyVis 'net' a partir do grafo NetworkX 'graph_nx'\n",
    "net = Network(notebook=True, directed=True)\n",
    "net.from_nx(graph_nx)\n",
    "\n",
    "# Personalizar a visualização (opcional)\n",
    "# ... (utilizar as métricas calculadas com cuGraph para definir o tamanho dos nós, cores, etc.) ...\n",
    "\n",
    "# Mostrar o grafo na célula do Jupyter\n",
    "net.show(\"graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "json_folder = os.path.join(os.getcwd(),'_data','out_json')\n",
    "\n",
    "# Carregar os dados dos arquivos JSON\n",
    "with open(os.path.join(json_folder,'curriculos.json'), 'r') as f:\n",
    "    curriculos_data = json.load(f)\n",
    "with open(os.path.join(json_folder,'matriz_ceis.json'), 'r') as f:\n",
    "    matriz_ceis_data = json.load(f)\n",
    "\n",
    "# Criar um grafo direcionado\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Adicionar os pesquisadores como nós\n",
    "for pesquisador in curriculos_data:\n",
    "    graph.add_node(pesquisador['Identificação']['ID Lattes'], tipo='pesquisador', **pesquisador)\n",
    "\n",
    "# Adicionar os produtos como nós\n",
    "for bloco in matriz_ceis_data['blocos']:\n",
    "    for produto in bloco['produtos']:\n",
    "        graph.add_node(produto['id'], tipo='produto', **produto)\n",
    "\n",
    "# Função para criar as arestas entre pesquisadores e produtos\n",
    "def create_edges(graph, pesquisador_node, produto_node):\n",
    "    \"\"\"Cria arestas entre pesquisadores e produtos com base em suas propriedades.\"\"\"\n",
    "    pesquisador_data = pesquisador_node[1]\n",
    "    produto_data = produto_node[1]\n",
    "\n",
    "    # Lógica para conectar pesquisadores a produtos (EXEMPLO)\n",
    "    # Verificar se as áreas de atuação do pesquisador são relevantes para o produto\n",
    "    for area in pesquisador_data['Áreas'].values():\n",
    "        if any(keyword in area for keyword in [\"Biotecnologia\", \"Saúde\", \"Química\", \"Biologia\"]):\n",
    "            graph.add_edge(pesquisador_node[0], produto_node[0])\n",
    "            return  # Criar apenas uma aresta por produto\n",
    "\n",
    "# Iterar pelos nós e criar as arestas\n",
    "for pesquisador_node in graph.nodes(data=True):\n",
    "    if pesquisador_node[1]['tipo'] == 'pesquisador':\n",
    "        for produto_node in graph.nodes(data=True):\n",
    "            if produto_node[1]['tipo'] == 'produto':\n",
    "                create_edges(graph, pesquisador_node, produto_node)\n",
    "\n",
    "# Imprimir os primeiros 5 nós e arestas\n",
    "print(\"Nós:\", list(graph.nodes(data=True))[:5])\n",
    "print(\"Arestas:\", list(graph.edges(data=True))[:5])\n",
    "\n",
    "# Mostrar o número de nós e arestas\n",
    "print(\"Número de nós:\", graph.number_of_nodes())\n",
    "print(\"Número de arestas:\", graph.number_of_edges())\n",
    "\n",
    "# Mostrar os tipos de nós presentes no grafo\n",
    "tipos_nos = set(no[1]['tipo'] for no in graph.nodes(data=True))\n",
    "print(\"Tipos de nós:\", tipos_nos)\n",
    "\n",
    "# Mostrar as propriedades dos nós\n",
    "print(\"Propriedades dos nós:\")\n",
    "for no in graph.nodes(data=True):\n",
    "    print(no)\n",
    "\n",
    "# Mostrar a distribuição das arestas entre os nós\n",
    "# (Exemplo: calcular o grau de cada nó)\n",
    "graus = dict(graph.degree())\n",
    "print(\"Distribuição de graus dos nós:\", graus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, directed=True)  # notebook=True para exibir no Jupyter, directed=True para grafo direcionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.from_nx(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir opções de layout\n",
    "net.repulsion(node_distance=200, central_gravity=0.2)\n",
    "\n",
    "# Configurar a física da visualização\n",
    "net.show_buttons(filter_=['physics'])  # Mostrar botões para controlar a física\n",
    "\n",
    "# Definir cores para os nós com base no tipo\n",
    "for node in net.nodes:\n",
    "    if node['tipo'] == 'pesquisador':\n",
    "        node['color'] = 'blue'\n",
    "    elif node['tipo'] == 'produto':\n",
    "        node['color'] = 'green'\n",
    "\n",
    "# Ajustar o tamanho dos nós com base no grau\n",
    "degrees = dict(graph.degree())\n",
    "for node in net.nodes:\n",
    "    node['size'] = degrees[node['id']] * 5  # Aumentar o tamanho proporcionalmente ao grau\n",
    "\n",
    "# net.show(\"graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "# Exibir o conteúdo do arquivo HTML na célula do Jupyter\n",
    "HTML(filename='graph.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cudf\n",
    "# import cugraph\n",
    "# import json\n",
    "# import networkx as nx\n",
    "# from pyvis.network import Network\n",
    "# from googletrans import Translator\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# class SemanticMatching:\n",
    "#     def __init__(self, curriculos_df, matriz_ceis_data, relacoes_biologicos,\n",
    "#                  relacoes_pequenas_moleculas):\n",
    "#         self.curriculos_df = curriculos_df\n",
    "#         self.matriz_ceis_data = matriz_ceis_data\n",
    "#         self.relacoes_biologicos = relacoes_biologicos\n",
    "#         self.relacoes_pequenas_moleculas = relacoes_pequenas_moleculas\n",
    "#         self.translator = Translator()\n",
    "#         self.produtos_df = self.criar_dataframe_produtos()\n",
    "#         self.biologicos, self.pequenas_moleculas = self.criar_grafos_relacionamentos()\n",
    "\n",
    "#     def criar_dataframe_produtos(self):\n",
    "#         json_folder = os.path.join(os.getcwd(),'_data','out_json')\n",
    "\n",
    "#         # Carregar a Matriz CEIS\n",
    "#         with open(os.path.join(json_folder,'matriz_ceis.json'), 'r') as f:\n",
    "#             matriz_ceis_data = json.load(f)\n",
    "\n",
    "#         # Extrair os dados dos produtos\n",
    "#         produtos = []\n",
    "#         for bloco in matriz_ceis_data['blocos']:\n",
    "#             for produto in bloco['produtos']:\n",
    "#                 produto['bloco_id'] = bloco['id']\n",
    "#                 produto['bloco_nome'] = bloco['titulo']\n",
    "#                 produtos.append(produto)\n",
    "\n",
    "#         # Retornar o DataFrame cuDF\n",
    "#         return cudf.DataFrame(produtos)\n",
    "\n",
    "#     def criar_grafos_relacionamentos(self):\n",
    "#         # ... (código para criar os grafos de biológicos e pequenas moléculas) ...\n",
    "#         biologicos = nx.DiGraph()\n",
    "#         for node in self.relacoes_biologicos[\"nodes\"]:\n",
    "#             biologicos.add_node(node['id'], **node)\n",
    "#         for edge in self.relacoes_biologicos[\"edges\"]:\n",
    "#             biologicos.add_edge(edge['from'], edge['to'])\n",
    "\n",
    "#         pequenas_moleculas = nx.DiGraph()\n",
    "#         for node in self.relacoes_pequenas_moleculas[\"nodes\"]:\n",
    "#             pequenas_moleculas.add_node(node['id'], **node)\n",
    "#         for edge in self.relacoes_pequenas_moleculas[\"edges\"]:\n",
    "#             pequenas_moleculas.add_edge(edge['from'], edge['to'])\n",
    "#         return biologicos, pequenas_moleculas\n",
    "\n",
    "#     def traduzir_nomes_produtos(self):\n",
    "#         # ... (código para traduzir os nomes dos produtos) ...\n",
    "#         self.produtos_df['nome_en'] = self.produtos_df['nome'].apply(\n",
    "#             lambda x: self.translator.translate(x, dest='en').text)\n",
    "\n",
    "#     def extrair_caracteristicas(self):\n",
    "#         # ... (código para extrair características semânticas) ...\n",
    "#         pass  # Implemente a extração de características aqui\n",
    "\n",
    "#     def classificar_produtos(self):\n",
    "#         # ... (código para classificar os produtos) ...\n",
    "#         pass  # Implemente a classificação dos produtos aqui\n",
    "\n",
    "#     def calcular_similaridade(self, produto, grafo, tipo_transformada):\n",
    "#         # ... (código para calcular similaridade usando a abordagem especificada) ...\n",
    "#         pass  # Implemente o cálculo de similaridade aqui\n",
    "\n",
    "#     def conectar_produtos_grafo(self):\n",
    "#         # ... (código para conectar os produtos aos grafos) ...\n",
    "#         pass  # Implemente a conexão dos produtos aos grafos aqui\n",
    "\n",
    "#     def avaliar_desempenho(self):\n",
    "#         # ... (código para avaliar o desempenho das abordagens) ...\n",
    "#         pass  # Implemente a avaliação de desempenho aqui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.08",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
