{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montar lista de docentes e discentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import pandas as pd\n",
    "\n",
    "# Ignorar avisos de InsecureRequestWarning\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def extract_docente_info(soup, prefixes):\n",
    "    \"\"\"\n",
    "    Extrai informações dos docentes, incluindo nome, link, categoria e imagem.\n",
    "    :param soup: Objeto BeautifulSoup da página.\n",
    "    :param prefixes: Lista de prefixos dos links a serem considerados.\n",
    "    :return: Lista de dicionários com informações dos docentes.\n",
    "    \"\"\"\n",
    "    docentes_info = []\n",
    "    category = None\n",
    "\n",
    "    # Procurando por todas as divisões de docentes\n",
    "    for row in soup.find_all(\"div\", class_=\"et_pb_row\"):\n",
    "        h3 = row.find(\"h3\")\n",
    "        if h3:\n",
    "            category = h3.get_text().strip().upper()\n",
    "\n",
    "        # Procura por docentes na seção atual\n",
    "        for blurb in row.find_all(\"div\", class_=\"et_pb_blurb_container\"):\n",
    "            h4 = blurb.find(\"h4\")\n",
    "            if h4 and h4.span:\n",
    "                nome_tit = h4.span.get_text().strip()\n",
    "                nome = nome_tit.split(',')[0]\n",
    "                link_tag = blurb.find(\"a\", href=True)\n",
    "                link = link_tag['href'] if link_tag and any(prefix in link_tag['href'] for prefix in prefixes) else None\n",
    "                img_tag = blurb.find_previous_sibling(\"div\", class_=\"et_pb_main_blurb_image\")\n",
    "                img_link = img_tag.find(\"img\")[\"src\"] if img_tag and img_tag.find(\"img\") else None\n",
    "                docentes_info.append({\"nome\": nome, \"link\": link, \"categoria\": category, \"imagem\": img_link})\n",
    "\n",
    "    return docentes_info\n",
    "\n",
    "def listar_docentes(url):\n",
    "    \"\"\"\n",
    "    Lista os docentes da página fornecida, retornando um DataFrame com nome, categoria, link e imagem.\n",
    "    :param url: URL da página a ser analisada.\n",
    "    :return: DataFrame com as colunas nome, categoria, link e imagem.\n",
    "    \"\"\"\n",
    "    # Lista de prefixos desejados\n",
    "    prefixes = ['http://buscatextual.cnpq.br/buscatextual/', 'http://lattes.cnpq.br/']\n",
    "\n",
    "    # Headers para simular uma requisição de um navegador comum\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    # Enviando um pedido ao site com headers e desabilitando a verificação de SSL\n",
    "    response = requests.get(url, headers=headers, verify=False)\n",
    "\n",
    "    # Checa se a requisição foi bem sucedida\n",
    "    if response.status_code == 200:\n",
    "        # Analisa o conteúdo da página\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extrai informações dos docentes\n",
    "        docentes_info = extract_docente_info(soup, prefixes)\n",
    "\n",
    "        # Cria um DataFrame com as informações\n",
    "        df = pd.DataFrame(docentes_info)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Não foi possível acessar a página. Status code:\", response.status_code)\n",
    "        return pd.DataFrame()  # Retorna um DataFrame vazio em caso de falha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>link</th>\n",
       "      <th>categoria</th>\n",
       "      <th>imagem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adele Schwartz Benzaken</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Felipe Leão Gomes Murta</td>\n",
       "      <td>http://lattes.cnpq.br/7106266581552839</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flor Ernestina Martinez Espinosa</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jacqueline de Almeida Gonçalves Sachett</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Luiz Carlos de Lima Ferreira</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Marco Aurélio Sartim</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Maria das Graças Vale Barbosa Guerra</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stefanie Costa Pinto Lopes</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Camila Helena Aguiar Bôtto de Menezes</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fernando Fonseca de Almeida e Val</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gisely Cardoso de Melo</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>João Marcos Bemfica Barbosa Ferreira</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Manuela Berto Pucca</td>\n",
       "      <td>http://lattes.cnpq.br/2714810198631869</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Marcus Vinicius Guimarães de Lacerda</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Nagila Francinete Costa Secundino</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Vanderson de Souza Sampaio</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Djane Clarys Baia da Silva</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Flávia Regina Souza Ramos</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hector Henrique Ferreira Koolen</td>\n",
       "      <td>http://lattes.cnpq.br/2722430673503338</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Jorge Augusto de Oliveira Guerra</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Marcelo Cordeiro dos Santos</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Maria das Graças Costa Alecrim</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Paulo Filemon Paolucci Pimenta</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wuelton Marcelo Monteiro</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES PERMANENTES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Allyson Guimarães da Costa</td>\n",
       "      <td>None</td>\n",
       "      <td>DOCENTES COLABORADORES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sinésio Talhari</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES COLABORADORES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Carolina Chrusciak Talhari Cortez</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>DOCENTES COLABORADORES</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Henrique Manuel Condinho da Silveira</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>PROFESSOR VISITANTE</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       nome  \\\n",
       "0                   Adele Schwartz Benzaken   \n",
       "1                   Felipe Leão Gomes Murta   \n",
       "2          Flor Ernestina Martinez Espinosa   \n",
       "3   Jacqueline de Almeida Gonçalves Sachett   \n",
       "4              Luiz Carlos de Lima Ferreira   \n",
       "5                      Marco Aurélio Sartim   \n",
       "6      Maria das Graças Vale Barbosa Guerra   \n",
       "7                Stefanie Costa Pinto Lopes   \n",
       "8     Camila Helena Aguiar Bôtto de Menezes   \n",
       "9         Fernando Fonseca de Almeida e Val   \n",
       "10                   Gisely Cardoso de Melo   \n",
       "11     João Marcos Bemfica Barbosa Ferreira   \n",
       "12                      Manuela Berto Pucca   \n",
       "13     Marcus Vinicius Guimarães de Lacerda   \n",
       "14        Nagila Francinete Costa Secundino   \n",
       "15               Vanderson de Souza Sampaio   \n",
       "16               Djane Clarys Baia da Silva   \n",
       "17                Flávia Regina Souza Ramos   \n",
       "18          Hector Henrique Ferreira Koolen   \n",
       "19         Jorge Augusto de Oliveira Guerra   \n",
       "20              Marcelo Cordeiro dos Santos   \n",
       "21           Maria das Graças Costa Alecrim   \n",
       "22           Paulo Filemon Paolucci Pimenta   \n",
       "23                 Wuelton Marcelo Monteiro   \n",
       "24               Allyson Guimarães da Costa   \n",
       "25                          Sinésio Talhari   \n",
       "26        Carolina Chrusciak Talhari Cortez   \n",
       "27     Henrique Manuel Condinho da Silveira   \n",
       "\n",
       "                                                 link               categoria  \\\n",
       "0   http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "1              http://lattes.cnpq.br/7106266581552839    DOCENTES PERMANENTES   \n",
       "2   http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "3   http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "4   http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "5   http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "6   http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "7   http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "8   http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "9   http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "10  http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "11  http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "12             http://lattes.cnpq.br/2714810198631869    DOCENTES PERMANENTES   \n",
       "13  http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "14  http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "15  http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "16  http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "17  http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "18             http://lattes.cnpq.br/2722430673503338    DOCENTES PERMANENTES   \n",
       "19  http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "20  http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "21  http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "22  http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "23  http://buscatextual.cnpq.br/buscatextual/visua...    DOCENTES PERMANENTES   \n",
       "24                                               None  DOCENTES COLABORADORES   \n",
       "25  http://buscatextual.cnpq.br/buscatextual/visua...  DOCENTES COLABORADORES   \n",
       "26  http://buscatextual.cnpq.br/buscatextual/visua...  DOCENTES COLABORADORES   \n",
       "27  http://buscatextual.cnpq.br/buscatextual/visua...     PROFESSOR VISITANTE   \n",
       "\n",
       "                                               imagem  \n",
       "0   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "1   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "2   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "3   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "4   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "5   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "6   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "7   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "8   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "9   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "10  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "11  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "12  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "13  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "14  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "15  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "16  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "17  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "18  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "19  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "20  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "21  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "22  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "23  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "24  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "25  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "26  http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "27  http://ppgmt.uea.edu.br/wp-content/uploads/202...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# URL do site\n",
    "url = \"https://ppgmt.uea.edu.br/index.php/ppgmt-corpo-docente/\"\n",
    "\n",
    "# Chamando a função e exibindo o DataFrame\n",
    "df_docentes = listar_docentes(url)\n",
    "df_docentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_discente_info(soup):\n",
    "    \"\"\"\n",
    "    Extrai informações dos discentes, incluindo nome, link da turma e imagem.\n",
    "    :param soup: Objeto BeautifulSoup da página.\n",
    "    :return: Lista de dicionários com informações dos discentes.\n",
    "    \"\"\"\n",
    "    discentes_info = []\n",
    "    turma = None\n",
    "\n",
    "    # Procurando por todas as divisões de discentes\n",
    "    for row in soup.find_all(\"div\", class_=\"et_pb_row\"):\n",
    "        h3 = row.find(\"h3\")\n",
    "        if h3:\n",
    "            turma = h3.get_text().strip().upper()\n",
    "\n",
    "        # Procura por discentes na seção atual\n",
    "        for blurb in row.find_all(\"div\", class_=\"et_pb_blurb_container\"):\n",
    "            h4 = blurb.find(\"h4\")\n",
    "            if h4 and h4.span:\n",
    "                nome = h4.span.get_text().strip()\n",
    "                link_tag = blurb.find(\"a\", href=True)\n",
    "                link = link_tag['href'] if link_tag else None\n",
    "                img_tag = blurb.find_previous_sibling(\"div\", class_=\"et_pb_main_blurb_image\")\n",
    "                img_link = img_tag.find(\"img\")[\"src\"] if img_tag and img_tag.find(\"img\") else None\n",
    "                discentes_info.append({\"nome\": nome, \"turma\": turma, \"link\": link, \"imagem\": img_link})\n",
    "\n",
    "    return discentes_info\n",
    "\n",
    "def listar_discentes(url):\n",
    "    \"\"\"\n",
    "    Lista os discentes da página fornecida, retornando um DataFrame com nome, turma, link e imagem.\n",
    "    :param url: URL da página a ser analisada.\n",
    "    :return: DataFrame com as colunas nome, turma, link e imagem.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, verify=False)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        discentes_info = extract_discente_info(soup)\n",
    "        df = pd.DataFrame(discentes_info)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Não foi possível acessar a página. Status code:\", response.status_code)\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>turma</th>\n",
       "      <th>link</th>\n",
       "      <th>imagem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Titular: Doutoranda Lucyane Mendes Silva</td>\n",
       "      <td>REPRESENTANTES</td>\n",
       "      <td>http://lattes.cnpq.br/5403463588589325</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suplente: Mestranda Suzan Simões Vieira</td>\n",
       "      <td>REPRESENTANTES</td>\n",
       "      <td>http://lattes.cnpq.br/1019061653142832</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Agata Cristian Lima da Silva</td>\n",
       "      <td>TURMA MESTRADO 22 – 2023</td>\n",
       "      <td>http://lattes.cnpq.br/8286839416365441</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexandre de Oliveira Trindade</td>\n",
       "      <td>TURMA MESTRADO 22 – 2023</td>\n",
       "      <td>http://lattes.cnpq.br/7002309653421543</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ana Carolina Azevedo Furtado</td>\n",
       "      <td>TURMA MESTRADO 22 – 2023</td>\n",
       "      <td>http://lattes.cnpq.br/3974378040642731</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Maianne Yasmin Oliveira Dias</td>\n",
       "      <td>TURMA DE MESTRADO 20 – 2021</td>\n",
       "      <td>http://lattes.cnpq.br/3258254616135748</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Malena Vanessa Grados Vasquez</td>\n",
       "      <td>TURMA DE MESTRADO 20 – 2021</td>\n",
       "      <td>http://lattes.cnpq.br/9685744753514121</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Marcos Gabriel Maciel Salazar</td>\n",
       "      <td>TURMA DE MESTRADO 20 – 2021</td>\n",
       "      <td>http://lattes.cnpq.br/7599043185740642</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Reinan Brotas Ferreira</td>\n",
       "      <td>TURMA DE MESTRADO 20 – 2021</td>\n",
       "      <td>http://lattes.cnpq.br/9311997177488570</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Yasmim Vieira da Rocha</td>\n",
       "      <td>TURMA DE MESTRADO 20 – 2021</td>\n",
       "      <td>http://lattes.cnpq.br/7269825939833754</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        nome                        turma  \\\n",
       "0   Titular: Doutoranda Lucyane Mendes Silva               REPRESENTANTES   \n",
       "1    Suplente: Mestranda Suzan Simões Vieira               REPRESENTANTES   \n",
       "2               Agata Cristian Lima da Silva     TURMA MESTRADO 22 – 2023   \n",
       "3             Alexandre de Oliveira Trindade     TURMA MESTRADO 22 – 2023   \n",
       "4               Ana Carolina Azevedo Furtado     TURMA MESTRADO 22 – 2023   \n",
       "..                                       ...                          ...   \n",
       "63              Maianne Yasmin Oliveira Dias  TURMA DE MESTRADO 20 – 2021   \n",
       "64             Malena Vanessa Grados Vasquez  TURMA DE MESTRADO 20 – 2021   \n",
       "65             Marcos Gabriel Maciel Salazar  TURMA DE MESTRADO 20 – 2021   \n",
       "66                    Reinan Brotas Ferreira  TURMA DE MESTRADO 20 – 2021   \n",
       "67                    Yasmim Vieira da Rocha  TURMA DE MESTRADO 20 – 2021   \n",
       "\n",
       "                                      link  \\\n",
       "0   http://lattes.cnpq.br/5403463588589325   \n",
       "1   http://lattes.cnpq.br/1019061653142832   \n",
       "2   http://lattes.cnpq.br/8286839416365441   \n",
       "3   http://lattes.cnpq.br/7002309653421543   \n",
       "4   http://lattes.cnpq.br/3974378040642731   \n",
       "..                                     ...   \n",
       "63  http://lattes.cnpq.br/3258254616135748   \n",
       "64  http://lattes.cnpq.br/9685744753514121   \n",
       "65  http://lattes.cnpq.br/7599043185740642   \n",
       "66  http://lattes.cnpq.br/9311997177488570   \n",
       "67  http://lattes.cnpq.br/7269825939833754   \n",
       "\n",
       "                                               imagem  \n",
       "0   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "1   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "2                                                None  \n",
       "3                                                None  \n",
       "4                                                None  \n",
       "..                                                ...  \n",
       "63                                               None  \n",
       "64                                               None  \n",
       "65                                               None  \n",
       "66                                               None  \n",
       "67                                               None  \n",
       "\n",
       "[68 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_discentes_mestrado = \"https://ppgmt.uea.edu.br/index.php/ppgmt-corpo-discente-mestrado/\"\n",
    "\n",
    "# Chamando a função e exibindo o DataFrame\n",
    "df_discentes_mestrado = listar_discentes(url_discentes_mestrado)\n",
    "df_discentes_mestrado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>turma</th>\n",
       "      <th>link</th>\n",
       "      <th>imagem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Titular: Doutoranda Lucyane Mendes Silva</td>\n",
       "      <td>REPRESENTANTES</td>\n",
       "      <td>http://lattes.cnpq.br/5403463588589325</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suplente: Mestranda Suzan Simões Vieira</td>\n",
       "      <td>REPRESENTANTES</td>\n",
       "      <td>http://lattes.cnpq.br/1019061653142832</td>\n",
       "      <td>http://ppgmt.uea.edu.br/wp-content/uploads/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gabriela Maciel Alencar</td>\n",
       "      <td>TURMA DOUTORADO 18 – 2023</td>\n",
       "      <td>#</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thiago Serrão Pinto</td>\n",
       "      <td>TURMA DOUTORADO 18 – 2023</td>\n",
       "      <td>#</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ana Ruth Lima Arcanjo</td>\n",
       "      <td>TURMA DOUTORADO 17 – 2022</td>\n",
       "      <td>http://lattes.cnpq.br/5272630311018526</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>João Arthur Alcântara de Lima</td>\n",
       "      <td>TURMA DOUTORADO 13 – 2018</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Marília Rosa Abtibol Bernardino</td>\n",
       "      <td>TURMA DOUTORADO 13 – 2018</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Paula Rita Leite da Silva</td>\n",
       "      <td>TURMA DOUTORADO 13 – 2018</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Rodrigo Maciel Alencar</td>\n",
       "      <td>TURMA DOUTORADO 13 – 2018</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Silmara Navarro Pennini</td>\n",
       "      <td>TURMA DOUTORADO 13 – 2018</td>\n",
       "      <td>http://buscatextual.cnpq.br/buscatextual/visua...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        nome                      turma  \\\n",
       "0   Titular: Doutoranda Lucyane Mendes Silva             REPRESENTANTES   \n",
       "1    Suplente: Mestranda Suzan Simões Vieira             REPRESENTANTES   \n",
       "2                    Gabriela Maciel Alencar  TURMA DOUTORADO 18 – 2023   \n",
       "3                        Thiago Serrão Pinto  TURMA DOUTORADO 18 – 2023   \n",
       "4                      Ana Ruth Lima Arcanjo  TURMA DOUTORADO 17 – 2022   \n",
       "..                                       ...                        ...   \n",
       "65             João Arthur Alcântara de Lima  TURMA DOUTORADO 13 – 2018   \n",
       "66           Marília Rosa Abtibol Bernardino  TURMA DOUTORADO 13 – 2018   \n",
       "67                 Paula Rita Leite da Silva  TURMA DOUTORADO 13 – 2018   \n",
       "68                    Rodrigo Maciel Alencar  TURMA DOUTORADO 13 – 2018   \n",
       "69                   Silmara Navarro Pennini  TURMA DOUTORADO 13 – 2018   \n",
       "\n",
       "                                                 link  \\\n",
       "0              http://lattes.cnpq.br/5403463588589325   \n",
       "1              http://lattes.cnpq.br/1019061653142832   \n",
       "2                                                   #   \n",
       "3                                                   #   \n",
       "4              http://lattes.cnpq.br/5272630311018526   \n",
       "..                                                ...   \n",
       "65  http://buscatextual.cnpq.br/buscatextual/visua...   \n",
       "66  http://buscatextual.cnpq.br/buscatextual/visua...   \n",
       "67  http://buscatextual.cnpq.br/buscatextual/visua...   \n",
       "68  http://buscatextual.cnpq.br/buscatextual/visua...   \n",
       "69  http://buscatextual.cnpq.br/buscatextual/visua...   \n",
       "\n",
       "                                               imagem  \n",
       "0   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "1   http://ppgmt.uea.edu.br/wp-content/uploads/202...  \n",
       "2                                                None  \n",
       "3                                                None  \n",
       "4                                                None  \n",
       "..                                                ...  \n",
       "65                                               None  \n",
       "66                                               None  \n",
       "67                                               None  \n",
       "68                                               None  \n",
       "69                                               None  \n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_discentes_doutorado = 'https://ppgmt.uea.edu.br/index.php/ppgmt-corpo-discente-doutorado/'\n",
    "\n",
    "# Chamando a função e exibindo o DataFrame\n",
    "df_discentes_doutorado = listar_discentes(url_discentes_doutorado)\n",
    "df_discentes_doutorado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração dos dados do Lattes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import h5py\n",
    "import logging\n",
    "import requests\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, sqlite3, asyncio\n",
    "import os, re, time, traceback, json\n",
    "import warnings, csv, sys, pip, string\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from string import Formatter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from typing import List, Optional, Dict, Union\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "from pyjarowinkler.distance import get_jaro_distance\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from neo4j import GraphDatabase\n",
    "from flask import render_template_string\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common import exceptions\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException, \n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException,\n",
    "    TimeoutException,\n",
    "    WebDriverException\n",
    ")\n",
    "\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "delay = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import platform\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from neo4j import GraphDatabase\n",
    "from selenium import webdriver\n",
    "from typing import Any, List, Dict, Optional\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "class Neo4jPersister:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(value: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Converts a given value to its most primitive form suitable for storage.\n",
    "        Parameters:\n",
    "        - value: Any, the value to be converted.\n",
    "        Returns:\n",
    "        - Any: The converted value in its most primitive form.\n",
    "        \"\"\"\n",
    "        if isinstance(value, str):\n",
    "            return value  # Directly return the string, ensuring it remains a simple string.\n",
    "        elif isinstance(value, list):\n",
    "            return [Neo4jPersister.convert_to_primitives(elem) for elem in value]\n",
    "        elif isinstance(value, dict):\n",
    "            return {key: Neo4jPersister.convert_to_primitives(val) for key, val in value.items()}\n",
    "        elif isinstance(value, set):\n",
    "            return list(value)  # Convert set to list for JSON serializability.\n",
    "        # ... (handle here other types if necessary)\n",
    "        else:\n",
    "            return value  # Return the value as-is if it's already in a primitive form.\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return Neo4jPersister.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise\n",
    "\n",
    "    def persist_data(self, data_dict, label):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MERGE (node:{label}) SET node = $props\"\n",
    "            session.run(query, props=data_dict_primitives)\n",
    "\n",
    "    def update_data(self, node_id, data_dict):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MATCH (node) WHERE id(node) = {node_id} SET node += $props\"\n",
    "            session.run(query, props=data_dict_primitives)\n",
    "\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "class ParseSoup:\n",
    "    def __init__(self, driver):\n",
    "        self.configure_logging()\n",
    "        self.base_url = 'http://buscatextual.cnpq.br'\n",
    "        self.session = requests.Session()\n",
    "        self.failed_extractions = []\n",
    "        self.driver = driver\n",
    "        self.delay = 10\n",
    "        self.soup = None\n",
    "\n",
    "    def configure_logging(self):\n",
    "        # Configura o logging para usar um novo arquivo de log, substituindo o antigo\n",
    "        logging.basicConfig(filename='lattes_scraper.log', level=logging.INFO, filemode='w')\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self  # the object to bind to the variable in the `as` clause\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.driver.quit()\n",
    "        self.driver = None\n",
    "\n",
    "    def to_json(self, data_dict: Dict, filename: str) -> None:\n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(data_dict, f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to JSON: {e}\")\n",
    "\n",
    "    def to_hdf5(self, processed_data: List[Dict], hdf5_filename: str) -> None:\n",
    "        try:\n",
    "            with h5py.File(hdf5_filename, 'w') as hdf5_file:\n",
    "                for i, data in enumerate(processed_data):\n",
    "                    # Serializa o dicionário como uma string JSON antes de armazená-lo.\n",
    "                    serialized_data = json.dumps(data)\n",
    "                    hdf5_file.create_dataset(str(i), data=serialized_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to HDF5: {e}\")\n",
    "\n",
    "    def dictlist_to_json(self, data_list: List[Dict], filename: str) -> None:\n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(data_list, f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to JSON: {e}\")\n",
    "\n",
    "    def dictlist_to_hdf5(self, data_list: List[Dict], filename: str, directory=None) -> None:\n",
    "        try:\n",
    "            converter = DictToHDF5(data_list)\n",
    "            converter.create_dataset(filename, directory)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to HDF5: {e}\")\n",
    "    \n",
    "    def format_string(self, input_str):\n",
    "        # Verifica se a entrada é uma string de oito dígitos\n",
    "        if input_str and len(input_str) == 9:\n",
    "            return input_str\n",
    "        elif input_str and len(input_str) == 8:\n",
    "            # Divide a string em duas partes\n",
    "            part1 = input_str[:4]\n",
    "            part2 = input_str[4:]\n",
    "            # Concatena as duas partes com um hífen\n",
    "            formatted_str = f\"{part1}-{part2}\"\n",
    "            return formatted_str\n",
    "        else:\n",
    "            return input_str\n",
    "            \n",
    "    def extract_tit1_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        # Títulos contendo subseções\n",
    "        tit1a = ['Identificação','Endereço','Formação acadêmica/titulação','Pós-doutorado','Formação Complementar',\n",
    "                'Linhas de pesquisa','Projetos de pesquisa','Projetos de extensão','Projetos de desenvolvimento', 'Revisor de periódico','Revisor de projeto de fomento','Áreas de atuação','Idiomas','Inovação']\n",
    "        tit1b = ['Atuação Profissional'] # dados com subseções\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            # Verifique se o título está na lista 'tit1'\n",
    "            if titulo in tit1a:\n",
    "                if verbose:\n",
    "                    print(titulo)\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    divs_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "                    divs_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "                    keys = []\n",
    "                    vals = []\n",
    "                    for i, j in zip(divs_layout_cell_3, divs_layout_cell_9):\n",
    "                        if divs_layout_cell_3 and divs_layout_cell_9:\n",
    "                            key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                            key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                            keys.append(key_text)\n",
    "                            val = j.find('div', class_='layout-cell-pad-5')\n",
    "                            val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                            vals.append(val_text)\n",
    "                            if verbose:\n",
    "                                print(f'      {key_text:>3}: {val_text}')\n",
    "                    agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                    data_dict[titulo] = Neo4jPersister.convert_to_primitives(agg_dict)\n",
    "            if titulo in tit1b:\n",
    "                if verbose:\n",
    "                    print(titulo)\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")               \n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = []\n",
    "                        if verbose:\n",
    "                            print(section_name)\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "                            if 'layout-cell-3' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-9' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "                                    if verbose:\n",
    "                                        print(len(current_data.values()), key, val)\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_data:\n",
    "                                        data_dict[titulo][section_name].append(current_data)  # Armazenamos os dados em uma lista\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "        return data_dict\n",
    "\n",
    "    def extract_tit2_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        database = ''\n",
    "        total_trab_text = 0\n",
    "        total_cite_text = 0\n",
    "        num_fator_h = 0\n",
    "        data_wos_text = ''\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        tit2 = ['Produções', 'Bancas', 'Orientações']\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            # Verifique se o título está na lista 'tit2'\n",
    "            if titulo in tit2:\n",
    "                if verbose:\n",
    "                    print(f'Título: {titulo}')\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = {}\n",
    "                        if verbose:\n",
    "                            print(f'Seção: {section_name}')\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_subsection = None\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                        if section_name == 'Produção bibliográfica':\n",
    "                            subsections = section.find_next_siblings('div', class_='cita-artigos')\n",
    "                            if verbose:\n",
    "                                print(len(subsections), 'subseções')                       \n",
    "                            for subsection in subsections:                            \n",
    "                                if subsection:\n",
    "                                    subsection_name = subsection.find('b').get_text().strip()\n",
    "                                    if verbose:\n",
    "                                        print(f'    Subseção: {subsection_name}') # nomes de subseção como ocorrências \n",
    "                                        print(f'    {len(subsection)} divs na subseção {subsection_name}')                                \n",
    "                                    if subsection_name == 'Citações':\n",
    "                                        current_subsection = subsection_name\n",
    "                                        data_dict[titulo][section_name]['Citações'] = {}\n",
    "                                        sub_section_list = []  \n",
    "                                        ## Extrair quantidade de citações e fator H das divs de subseção com classe lyout-cell-12\n",
    "                                        next_siblings = subsection.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "                                        for sibling in next_siblings:\n",
    "                                            citation_counts = sibling.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que tem os Valores de Citações\n",
    "                                            if citation_counts:\n",
    "                                                for i in citation_counts:\n",
    "                                                    database = i.get_text()\n",
    "                                                    total_trab = i.find_next_sibling(\"div\", class_=\"trab\")\n",
    "                                                    if total_trab:\n",
    "                                                        total_trab_text = total_trab.get_text().split(\"Total de trabalhos:\")[1]\n",
    "                                                    total_cite = i.find_next_sibling(\"div\", class_=\"cita\")\n",
    "                                                    if total_cite:\n",
    "                                                        total_cite_text = total_cite.get_text().split(\"Total de citações:\")[1]\n",
    "                                                    fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "                                                    num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "                                                    data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\")\n",
    "                                                    if data_wos:\n",
    "                                                        try:\n",
    "                                                            data_wos_text = data_wos.get_text().split(\"Data:\")[1].strip()\n",
    "                                                        except:\n",
    "                                                            data_wos_text = data_wos.get_text()\n",
    "                                                    # Converta os valores para tipos de dados adequados\n",
    "                                                    total_trab = int(total_trab_text)\n",
    "                                                    total_cite = int(total_cite_text)\n",
    "                                                    citation_numbers = {\n",
    "                                                        \"Database\": database,\n",
    "                                                        \"Total de trabalhos\": total_trab,\n",
    "                                                        \"Total de citações\": total_cite,\n",
    "                                                        \"Índice_H\": num_fator_h,\n",
    "                                                        \"Data\": data_wos_text\n",
    "                                                    }\n",
    "                                                    # Verifique se a subseção atual já existe no dicionário\n",
    "                                                    if 'Citações' not in data_dict[titulo][section_name]:\n",
    "                                                        data_dict[titulo][section_name]['Citações'] = {}  # Inicialize como uma lista vazia\n",
    "                                                    data_dict[titulo][section_name]['Citações'] = citation_numbers\n",
    "                                                    if verbose:\n",
    "                                                        print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')    \n",
    "                            ## Encontrar a div irmã de div subseção com classe layout-cell-12 com artigos\n",
    "                            vals_jcr = []\n",
    "                            div_artigo_geral = data_cell.findChild(\"div\", id=\"artigos-completos\")\n",
    "                            if verbose:\n",
    "                                print(f'Encontrada {len(div_artigo_geral)} div geral de artigos')\n",
    "                            if div_artigo_geral:\n",
    "                                divs_artigos = div_artigo_geral.find_all('div', class_='artigo-completo')\n",
    "                                if verbose:\n",
    "                                    print(len(divs_artigos), 'divs de artigos')\n",
    "                                current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                                if divs_artigos:                              \n",
    "                                    for div_artigo in divs_artigos:\n",
    "                                        data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = {}\n",
    "                                        ## Extrair filhos da classes de artigos completos que estão à frente\n",
    "                                        sibling = div_artigo.findChild()\n",
    "                                        while sibling:\n",
    "                                            classes = sibling.get('class', [])\n",
    "                                            if 'layout-cell-1' in classes:  # Data key\n",
    "                                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                                sibling = sibling.find_next_sibling()\n",
    "                                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                                    info_dict = {\n",
    "                                                        'data-issn': 'NULL',\n",
    "                                                        'impact-factor': 'NULL',  \n",
    "                                                        'jcr-year': 'NULL',\n",
    "                                                    }\n",
    "                                                    # Remova as tags span da div\n",
    "                                                    for span in sibling.find_all('span'):\n",
    "                                                        span.extract()\n",
    "                                                    val_text = sibling.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "                                                    current_data[key] = val_text\n",
    "                                                    if verbose:\n",
    "                                                        print(len(current_data.values()), key, val)\n",
    "                                                    sup_element = sibling.find('sup')\n",
    "                                                    if sup_element:\n",
    "                                                        raw_jcr_data = sup_element.get_text()\n",
    "                                                        # print('sup_element:',sup_element)\n",
    "                                                        img_element = sup_element.find('img')\n",
    "                                                        # print('img_element:',img_element)\n",
    "                                                        if img_element:\n",
    "                                                            original_title = img_element.get('original-title')\n",
    "                                                            if original_title:\n",
    "                                                                info_list = original_title.split('<br />') if original_title.split('<br />') else original_title\n",
    "                                                                if info_list != 'NULL':\n",
    "                                                                    issn = self.format_string(img_element.get('data-issn'))\n",
    "                                                                    if verbose:\n",
    "                                                                        print(f'impact-factor: {info_list[1].split(\": \")[1]}')\n",
    "                                                                    info_dict = {\n",
    "                                                                        'data-issn': issn,\n",
    "                                                                        'impact-factor': info_list[1].split(': ')[1],\n",
    "                                                                        'jcr-year': info_list[1].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')',''),\n",
    "                                                                        'journal': info_list[0],\n",
    "                                                                    }\n",
    "                                                            else:\n",
    "                                                                if verbose:\n",
    "                                                                    print('Entrou no primeiro Else')\n",
    "                                                                issn = self.format_string(img_element.get('data-issn'))\n",
    "                                                                info_dict = {\n",
    "                                                                    'data-issn': issn,\n",
    "                                                                    'impact-factor': 'NULL',\n",
    "                                                                    'jcr-year': 'NULL',\n",
    "                                                                    'journal': 'NULL',\n",
    "                                                                }\n",
    "                                                    else:\n",
    "                                                        if verbose:\n",
    "                                                                    print('Entrou no segundo Else')\n",
    "                                                        info_dict = {\n",
    "                                                            'data-issn': 'NULL',\n",
    "                                                            'impact-factor': 'NULL',\n",
    "                                                            'jcr-year': 'NULL',\n",
    "                                                            'journal': 'NULL',\n",
    "                                                        }\n",
    "                                                    vals_jcr.append(info_dict)\n",
    "                                                    if verbose:\n",
    "                                                        print(f'         {info_dict}')\n",
    "                                                if 'JCR' not in data_dict:\n",
    "                                                    data_dict['JCR'] = []\n",
    "                                                if verbose:\n",
    "                                                    print(len(vals_jcr))\n",
    "                                                data_dict['JCR'] = vals_jcr\n",
    "                                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                                next_sibling = sibling.find_next_sibling()\n",
    "                                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                                    sibling = None\n",
    "                                                else:\n",
    "                                                    if current_data:\n",
    "                                                        converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                                        data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = converted_data\n",
    "                                            if sibling:\n",
    "                                                sibling = sibling.find_next_sibling()\n",
    "                        else:\n",
    "                            while sibling:\n",
    "                                classes = sibling.get('class', [])\n",
    "                                if 'cita-artigos' in classes:  # Subsection start\n",
    "                                    subsection_name = sibling.find('b').get_text().strip()\n",
    "                                    current_subsection = subsection_name\n",
    "                                    if verbose:\n",
    "                                        print(f'    Subseção: {subsection_name}')\n",
    "                                    data_dict[titulo][section_name][current_subsection] = {}\n",
    "                                    current_data = {}  # Inicializamos o dicionário de dados da subseção atual\n",
    "                                elif 'layout-cell-1' in classes:  # Data key\n",
    "                                    key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                    sibling = sibling.find_next_sibling()\n",
    "                                    if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                        val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                        current_data[key] = val\n",
    "                                elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Subsection or section end\n",
    "                                    next_sibling = sibling.find_next_sibling()\n",
    "                                    if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                        sibling = None\n",
    "                                    else:\n",
    "                                        if current_subsection:\n",
    "                                            data_dict[titulo][section_name][current_subsection] = Neo4jPersister.convert_to_primitives(current_data)  # Armazenamos os dados da subseção atual\n",
    "                                if sibling:\n",
    "                                    sibling = sibling.find_next_sibling()\n",
    "        \n",
    "        # Verifique se os dados dos tooltips estão presentes no objeto soup\n",
    "        if 'tooltips' in soup.attrs:\n",
    "            tooltips_data = soup.attrs['tooltips']\n",
    "            agg = []\n",
    "            for tooltip in tooltips_data:\n",
    "                agg_data = {}\n",
    "                # Extração do ano JCR a partir do \"original_title\"\n",
    "                if tooltip.get(\"original_title\"):\n",
    "                    jcr_year = tooltip[\"original_title\"].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')','')\n",
    "                    agg_data[\"jcr-ano\"] = jcr_year\n",
    "                # Adicionar todas as chaves e valores do tooltip ao dicionário agg_data\n",
    "                for key, value in tooltip.items():\n",
    "                    agg_data[key] = value\n",
    "                agg.append(agg_data)\n",
    "            data_dict['JCR2'] = agg\n",
    "        else:\n",
    "            print('Não foram achados os dados de tooltip')\n",
    "            print(soup.attrs)\n",
    "        return data_dict\n",
    "\n",
    "    def extract_tit3_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        # Títulos da seção 'Eventos'\n",
    "        tit3 = ['Eventos']\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            # Verifique se o título está na lista 'tit3'\n",
    "            if titulo in tit3:\n",
    "                if verbose:\n",
    "                    print(f'Título: {titulo}')\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = []\n",
    "                        if verbose:\n",
    "                            print(section_name)\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "                            if 'layout-cell-1' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "                                    if verbose:\n",
    "                                        print(len(current_data.values()), key, val)\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_data:\n",
    "                                        converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                        data_dict[titulo][section_name] = converted_data\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "        return data_dict\n",
    "\n",
    "    def extract_data(self, soup):\n",
    "        \"\"\"\n",
    "        Aggregates data from various dictionary sources into a consolidated nested dictionary, \n",
    "        ensuring that all nested lists within the dictionaries are transformed into nested dictionaries.\n",
    "        Parameters:\n",
    "        - soup: BeautifulSoup object, representing the parsed HTML content.\n",
    "        Returns:\n",
    "        - dict: An aggregated dictionary containing the consolidated data.\n",
    "        \"\"\"\n",
    "        self.soup = soup\n",
    "        \n",
    "        def convert_list_to_dict(lst):\n",
    "            \"\"\"\n",
    "            Converts a list into a dictionary with indices as keys.\n",
    "            \n",
    "            Parameters:\n",
    "            - lst: list, input list to be transformed.\n",
    "            \n",
    "            Returns:\n",
    "            - dict: Transformed dictionary.\n",
    "            \"\"\"\n",
    "            return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "        def merge_dict(d1, d2):\n",
    "            \"\"\"\n",
    "            Recursively merges two dictionaries, transforming nested lists into dictionaries.\n",
    "            Parameters:\n",
    "            - d1: dict, the primary dictionary into which data is merged.\n",
    "            - d2: dict or list, the secondary dictionary or list from which data is sourced.\n",
    "            Returns:\n",
    "            - None\n",
    "            \"\"\"\n",
    "            # If d2 is a list, convert it to a dictionary first\n",
    "            if isinstance(d2, list):\n",
    "                d2 = convert_list_to_dict(d2)\n",
    "            \n",
    "            for key, value in d2.items():\n",
    "                if isinstance(value, list):\n",
    "                    d2[key] = convert_list_to_dict(value)\n",
    "                if key in d1 and isinstance(d1[key], dict) and isinstance(value, dict):\n",
    "                    merge_dict(d1[key], value)\n",
    "                else:\n",
    "                    d1[key] = value\n",
    "\n",
    "        # Extract necessary information from soup\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "        name = info_list[0]\n",
    "\n",
    "        # Initialization of the aggregated_data dictionary\n",
    "        aggregated_data = {\"labels\": \"Person\", \"name\": name, \"InfPes\": info_list, \"Resumo\": [elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()]}\n",
    "\n",
    "        # Data extraction and merging\n",
    "        for data_extraction_func in [self.extract_tit1_soup, self.extract_tit2_soup, self.extract_tit3_soup]:\n",
    "            extracted_sections = data_extraction_func(soup)\n",
    "            for title, data in extracted_sections.items():\n",
    "                if title not in aggregated_data:\n",
    "                    aggregated_data[title] = {}\n",
    "                merge_dict(aggregated_data[title], data)\n",
    "        return aggregated_data\n",
    "\n",
    "    def convert_list_to_dict(self, lst: List[Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Converts a list into a dictionary with indices as keys.\n",
    "        Parameters:\n",
    "        - lst: List[Any], input list to be transformed.\n",
    "        Returns:\n",
    "        - Dict[str, Any]: Transformed dictionary.\n",
    "        \"\"\"\n",
    "        return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "    def preprocess_data(self, extracted_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Preprocesses the extracted data to ensure that it is in the desired format.\n",
    "        Parameters:\n",
    "        - extracted_data: Dict[str, Any], the data to be preprocessed.\n",
    "        Returns:\n",
    "        - Dict[str, Any]: Preprocessed data.\n",
    "        \"\"\"\n",
    "        return self._recursive_preprocessing(extracted_data)\n",
    "    \n",
    "    def _recursive_preprocessing(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Recursively preprocesses a dictionary, applying conversion methods to its elements.\n",
    "        Parameters:\n",
    "        - data: Dict[str, Any], the data dictionary that needs to be preprocessed.\n",
    "        Returns:\n",
    "        - Dict[str, Any]: The preprocessed data dictionary.\n",
    "        \"\"\"\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, str):\n",
    "                data[key] = self._handle_string_value(value)\n",
    "            elif isinstance(value, list):\n",
    "                data[key] = self._handle_list_value(value)\n",
    "            elif isinstance(value, dict):\n",
    "                data[key] = self._recursive_preprocessing(value)\n",
    "            elif isinstance(value, set):\n",
    "                data[key] = self._handle_set_value(value)\n",
    "        return data\n",
    "\n",
    "    def _handle_set_value(self, value: set) -> list:\n",
    "        \"\"\"\n",
    "        Handles set type values during preprocessing by converting them to lists.\n",
    "        Parameters:\n",
    "        - value: set, the set value to be preprocessed.\n",
    "        Returns:\n",
    "        - list: The preprocessed value as a list.\n",
    "        \"\"\"\n",
    "        return list(value)\n",
    "    \n",
    "    def _handle_string_value(self, value: str) -> Any:\n",
    "        \"\"\"\n",
    "        Handles string type values during preprocessing.\n",
    "        Parameters:\n",
    "        - value: str, the string value to be preprocessed.\n",
    "        Returns:\n",
    "        - Any: The preprocessed value.\n",
    "        \"\"\"\n",
    "        return Neo4jPersister.convert_to_primitives(value)\n",
    "    \n",
    "    def _handle_list_value(self, value: list) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Handles list type values during preprocessing.\n",
    "        Parameters:\n",
    "        - value: list, the list value to be preprocessed.\n",
    "        Returns:\n",
    "        - Dict[str, Any]: The preprocessed value as a dictionary.\n",
    "        \"\"\"\n",
    "        return self.convert_list_to_dict(Neo4jPersister.convert_to_primitives(value))\n",
    "\n",
    "    def dias_desde_atualizacao(self, data_atualizacao_str):\n",
    "        # Converte a data de atualização em um objeto datetime\n",
    "        data_atualizacao = datetime.strptime(data_atualizacao_str, '%d/%m/%Y')\n",
    "        \n",
    "        # Obtém a data atual\n",
    "        data_atual = datetime.now()\n",
    "        \n",
    "        # Calcula a diferença em dias\n",
    "        diferenca_dias = (data_atual - data_atualizacao).days\n",
    "        return diferenca_dias\n",
    "\n",
    "    def extrair_data_atualizacao(self, dic):\n",
    "        info_atualizacao = dic.get('InfPes', {}).get('3', '')\n",
    "        data_atualizacao = re.search(r'\\d{2}/\\d{2}/\\d{4}', info_atualizacao)\n",
    "        if data_atualizacao:\n",
    "            data_atualizacao = data_atualizacao.group()\n",
    "        else:\n",
    "            info_atualizacao = dic.get('InfPes', {}).get('4', '')\n",
    "            data_atualizacao = re.search(r'\\d{2}/\\d{2}/\\d{4}', info_atualizacao)\n",
    "            data_atualizacao = data_atualizacao.group()\n",
    "        return data_atualizacao if data_atualizacao else None\n",
    "\n",
    "    def extrair_idlattes(self, dic):\n",
    "        id_lattes_info = dic.get('InfPes', {}).get('2', '')\n",
    "        id_lattes = re.search(r'ID Lattes: (\\d+)', id_lattes_info)\n",
    "        ''\n",
    "        if id_lattes:\n",
    "            id_lattes = id_lattes.group(1)\n",
    "        else:\n",
    "            id_lattes = re.search(r'Endereço para acessar este CV: http%3A//lattes.cnpq.br/(\\d+)', id_lattes_info)\n",
    "            id_lattes = id_lattes.group(1)\n",
    "        return id_lattes\n",
    "\n",
    "    def process_single_result(self, extracted_data: Dict, json_filename: str, hdf5_filename: str) -> Optional[Dict]:\n",
    "        try:\n",
    "            # Pre-process data to convert lists and strings to dictionaries\n",
    "            preprocessed_data = self.preprocess_data(extracted_data)\n",
    "\n",
    "            # Use the extrair_idlattes method\n",
    "            id_lattes = self.extrair_idlattes(preprocessed_data)\n",
    "\n",
    "            processed_data = {\n",
    "                'idlattes': id_lattes,\n",
    "                'name': preprocessed_data.get('name', 'N/A'),\n",
    "                'Áreas de atuação': preprocessed_data.get('Áreas de atuação', 'N/A'),\n",
    "                # 'publications': int(preprocessed_data.get('publications', 0)),\n",
    "            }\n",
    "\n",
    "            # Save processed data\n",
    "            self.to_json([processed_data], json_filename)\n",
    "            self.to_hdf5([processed_data], hdf5_filename)\n",
    "            print(processed_data)\n",
    "            return preprocessed_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during single result processing:\\n {e}\")\n",
    "            return None\n",
    "\n",
    "    # montar datasets somente com dicionários com as publicações\n",
    "    def process_all_results(self, \n",
    "                            all_extracted_data: List[Dict], \n",
    "                            json_filename: str, \n",
    "                            hdf5_filename: str) -> List[Dict]:\n",
    "        successful_processed_data = []\n",
    "        for extracted_data in all_extracted_data:\n",
    "            processed_data = self.process_single_result(extracted_data, json_filename, hdf5_filename)\n",
    "            if processed_data is not None:\n",
    "                successful_processed_data.append(processed_data)\n",
    "            else:\n",
    "                self.failed_extractions.append(extracted_data)\n",
    "        if self.failed_extractions:\n",
    "            logging.info(\"Retrying failed extractions...\")\n",
    "            for failed_data in self.failed_extractions:\n",
    "                processed_data = self.process_single_result(failed_data, json_filename, hdf5_filename)\n",
    "                if processed_data is not None:\n",
    "                    successful_processed_data.append(processed_data)\n",
    "        self.to_json(successful_processed_data, json_filename)\n",
    "        self.to_hdf5(successful_processed_data, hdf5_filename)\n",
    "        return successful_processed_data\n",
    "\n",
    "def connect_driver(caminho):\n",
    "    '''\n",
    "    Conecta ao servidor do CNPq para busca de currículo\n",
    "    '''\n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    # options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    # driver   = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        # Caminho para o seu chromedriver\n",
    "        if platform.system() == \"Windows\":\n",
    "            driver_path=caminho+'chromedriver/chromedriver.exe'\n",
    "        else:\n",
    "            driver_path=caminho+'chromedriver/chromedriver'\n",
    "    except:\n",
    "        print(\"Não foi possíve estabelecer uma conexão, verifique o chromedriver\")\n",
    "    \n",
    "    # print(driver_path)\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service)    \n",
    "    url_busca = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    driver.get(url_busca) # acessa a url de busca do CNPQ   \n",
    "    driver.set_window_position(-20, -10)\n",
    "    driver.set_window_size(170, 1896)\n",
    "    driver.mouse = webdriver.ActionChains(driver)\n",
    "    return driver\n",
    "\n",
    "class LattesScraper:\n",
    "    def __init__(self, driver, institution, unit, term):\n",
    "        self.base_url = 'http://buscatextual.cnpq.br'\n",
    "        self.session = requests.Session()\n",
    "        self.driver = driver\n",
    "        self.delay = 10\n",
    "\n",
    "    def wait_for_element(self, css_selector: str, ignored_exceptions=None):\n",
    "        \"\"\"\n",
    "        Waits for the element specified by the CSS selector to load.\n",
    "        :param css_selector: CSS selector of the element to wait for\n",
    "        :param ignored_exceptions: List of exceptions to ignore\n",
    "        \"\"\"\n",
    "        WebDriverWait(self.driver, self.delay, ignored_exceptions=ignored_exceptions).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "    def paginar(self, driver):\n",
    "        '''\n",
    "        Helper function to page results on the search page\n",
    "        '''\n",
    "        numpaginas = []\n",
    "        css_paginacao = \"div.paginacao:nth-child(2)\"\n",
    "        try:\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "            paginacao = self.driver.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "            paginas = paginacao.text.split(' ')\n",
    "            remover = ['', 'anterior', '...']\n",
    "            numpaginas = [x for x in paginas if x not in remover]\n",
    "        except Exception as e:\n",
    "            print('  ERRO!! Ao rodar função paginar():', e)\n",
    "        return numpaginas\n",
    "\n",
    "    def retry(self, func, expected_ex_type=Exception, limit=0, wait_ms=200,\n",
    "              wait_increase_ratio=2, on_exhaust=\"throw\"):\n",
    "        attempt = 1\n",
    "        while True:\n",
    "            try:\n",
    "                return func()\n",
    "            except Exception as ex:\n",
    "                if not isinstance(ex, expected_ex_type):\n",
    "                    raise ex\n",
    "                if 0 < limit <= attempt:\n",
    "                    if on_exhaust == \"throw\":\n",
    "                        raise ex\n",
    "                    return on_exhaust\n",
    "                attempt += 1\n",
    "                time.sleep(wait_ms / 1000)\n",
    "                wait_ms *= wait_increase_ratio\n",
    "\n",
    "    def find_terms(self, NOME, instituicao, unidade, termo, delay, limite):\n",
    "        \"\"\"\n",
    "        Função para manipular o HTML até abir a página HTML de cada currículo   \n",
    "        Parâmeteros:\n",
    "            - NOME: É o nome completo de cada pesquisador\n",
    "            - Instituição, unidade e termo: Strings a buscar no currículo para reduzir duplicidades\n",
    "            - driver (webdriver object): The Selenium webdriver object.\n",
    "            - limite (int): Número máximo de tentativas em casos de erro.\n",
    "            - delay (int): tempo em milisegundos a esperar nas operações de espera.\n",
    "        Retorna:\n",
    "            elm_vinculo, np.NaN, np.NaN, np.NaN, driver.\n",
    "        Em caso de erro retorna:\n",
    "            None, NOME, np.NaN, e, driver\n",
    "        \"\"\"\n",
    "        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "        # Inicializando variáveis para evitar UnboundLocalError\n",
    "        elm_vinculo = None\n",
    "        qte_resultados = 0\n",
    "        ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "        duvidas   = []\n",
    "        force_break_loop = False\n",
    "        try:\n",
    "            # Wait and fetch the number of results\n",
    "            css_resultados = \".resultado\"\n",
    "            WebDriverWait(self.driver, delay, ignored_exceptions=ignored_exceptions).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "            resultados = self.driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "            ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "            try:\n",
    "                css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "                WebDriverWait(self.driver, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                div_element = soup.find('div', {'class': 'tit_form'})\n",
    "                match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "                if match:\n",
    "                    qte_resultados = int(match.group(1))\n",
    "                    # print(f'{qte_resultados} resultados para {NOME}')\n",
    "                else:\n",
    "                    return None, NOME, np.NaN, 'Currículo não encontrado', self.driver\n",
    "            except Exception as e1:\n",
    "                print('  ERRO!! Currículo não disponível no Lattes')\n",
    "                return None, NOME, np.NaN, e1, self.driver\n",
    "            \n",
    "            ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "            ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "            numpaginas = self.paginar(self.driver)\n",
    "            if numpaginas == [] and qte_resultados==1:\n",
    "                # capturar link para o primeiro nome resultado da busca\n",
    "                try:\n",
    "                    css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                    WebDriverWait(self.driver, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                    elm_vinculo  = self.driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                    nome_vinculo = elm_vinculo.text\n",
    "                except Exception as e2:\n",
    "                    print('  ERRO!! Ao encontrar o primeiro resultado da lista de nomes:', e2)\n",
    "                    \n",
    "                    # Call the handle stale file_error function\n",
    "                    if self.handle_stale_file_error(self.driver):\n",
    "                        # If the function returns True, it means the error was resolved.\n",
    "                        # try to get the nome_vinculo again:\n",
    "                        try:\n",
    "                            elm_vinculo  = self.driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                            nome_vinculo = elm_vinculo.text\n",
    "                        except Exception as e3:\n",
    "                            print('  ERRO!! Servidor CNPq indisponível no momento, tentar em alguns minutos:', e3)\n",
    "                            return None, NOME, np.NaN, e3, self.driver\n",
    "                    else:\n",
    "                        # If the function returns False, it means the error was not resolved within the given retries.\n",
    "                        return None, NOME, np.NaN, e2, self.driver\n",
    "\n",
    "                    print('  Não foi possível extrair por falha no servidor do CNPq:',e)\n",
    "                    return None, NOME, np.NaN, e2, self.driver\n",
    "                # print('Clicar no nome único:', nome_vinculo)\n",
    "                try:\n",
    "                    self.retry(ActionChains(self.driver).click(elm_vinculo).perform(),\n",
    "                        wait_ms=20,\n",
    "                        limit=limite,\n",
    "                        on_exhaust=(f'  Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "                except Exception as e4:\n",
    "                    print('  ERRO!! Ao clicar no único nome encontrado anteriormente',e)\n",
    "                    return None, NOME, np.NaN, e4, self.driver\n",
    "            \n",
    "            ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "            else:\n",
    "                print(f'       {qte_resultados:>3} homônimos de: {NOME}')\n",
    "                numpaginas = self.paginar(self.driver)\n",
    "                numpaginas.append('próximo')\n",
    "                iteracoes=0\n",
    "                ## iterar em cada página de resultados\n",
    "                pagin = qte_resultados//10+1\n",
    "                count = None\n",
    "                found = None\n",
    "                for i in range(pagin+1):\n",
    "                    # print(i,'/',pagin)\n",
    "                    iteracoes+=1\n",
    "                    try:\n",
    "                        numpaginas = self.paginar(self.driver)\n",
    "                        # print(f'       Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                        css_resultados = \".resultado\"\n",
    "                        WebDriverWait(self.driver, delay).until(\n",
    "                            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                        resultados = self.driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                    except Exception as e:\n",
    "                        print('  ERRO!! Ao paginar:',e)\n",
    "                    ## iterar em cada resultado\n",
    "                    for n,i in enumerate(resultados):\n",
    "                        linhas = i.text.split('\\n\\n')\n",
    "                        # print(linhas)\n",
    "                        if 'Stale file handle' in str(linhas):\n",
    "                            return np.NaN, NOME, np.NaN, 'Stale file handle', self.driver\n",
    "                        for m,linha in enumerate(linhas):\n",
    "                            # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                            # print('Conteúdo da linha:',linha.lower())\n",
    "                            # print(linha)\n",
    "                            try:\n",
    "                                if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                    # print('Vínculo encontrado!')\n",
    "                                    count=m\n",
    "                                    while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                        count-=1\n",
    "                                    # print('       Identificado vínculo no resultado:', m+1)\n",
    "                                    found = m+1\n",
    "                                    # nome_vinculo = linhas[count].replace('\\n','\\n       ').strip()\n",
    "                                    # print(f'       Achado: {nome_vinculo}')\n",
    "                                    try:\n",
    "                                        css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                        # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                        WebDriverWait(self.driver, delay).until(\n",
    "                                            EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                        elm_vinculo  = self.driver.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                        nome_vinculo = elm_vinculo.text\n",
    "                                        # print('Elemento retornado:',nome_vinculo)\n",
    "                                        self.retry(ActionChains(self.driver).click(elm_vinculo).perform(),\n",
    "                                            wait_ms=200,\n",
    "                                            limit=limite,\n",
    "                                            on_exhaust=(f'  Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                    except Exception as e5:\n",
    "                                        print('  ERRO!! Ao achar o link do nome com múltiplos resultados')\n",
    "                                        return np.NaN, NOME, np.NaN, e5, self.driver\n",
    "                                    force_break_loop = True\n",
    "                                    break\n",
    "                            except Exception as e6:\n",
    "                                traceback_str = ''.join(traceback.format_tb(e6.__traceback__))\n",
    "                                print('  ERRO!! Ao procurar vínculo com currículos achados')    \n",
    "                                print(e6,traceback_str)\n",
    "                            ## Caso percorra toda lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                            if m==(qte_resultados):\n",
    "                                print(f'Nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                                duvidas.append(NOME)\n",
    "                                # clear_output(wait=True)\n",
    "                                # driver.quit()\n",
    "                                continue\n",
    "                        if force_break_loop:\n",
    "                            break\n",
    "                    try:\n",
    "                        prox = self.driver.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                        prox.click()\n",
    "                    except:\n",
    "                        continue\n",
    "                if count:\n",
    "                    nome_vinculo = linhas[count].replace('\\n','\\n       ').strip()\n",
    "                    print(f'       Escolhido homônimo {found}: {nome_vinculo}')\n",
    "                else:\n",
    "                    print(f'       Não foi possível identificar o vínculo de: {NOME}')\n",
    "                    duvidas.append(NOME)\n",
    "            try:\n",
    "                elm_vinculo.text\n",
    "                # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "            except:\n",
    "                return None, NOME, np.NaN, 'Vínculo não encontrado', self.driver\n",
    "        except exceptions.TimeoutException:\n",
    "            print(\"  ERRO!! O tempo limite de espera foi atingido.\")\n",
    "            return None, NOME, np.NaN, \"TimeoutException\", self.driver\n",
    "        except exceptions.WebDriverException as e7:\n",
    "            print(\"  ERRO!! Problema ao interagir com o driver.\")\n",
    "            return None, NOME, np.NaN, e7, self.driver\n",
    "        except Exception as e8:\n",
    "            print(\"  ERRO 8!! Um erro inesperado ocorreu.\")\n",
    "            print(f'  {e8}')\n",
    "            return None, NOME, np.NaN, e8, self.driver\n",
    "        # Verifica antes de retornar para garantir que elm_vinculo foi definido\n",
    "        if elm_vinculo is None:\n",
    "            print(\"Vínculo não foi definido.\")\n",
    "            return None, NOME, np.NaN, 'Vínculo não encontrado', self.driver\n",
    "        # Retorna a saída de sucesso\n",
    "        return elm_vinculo, np.NaN, np.NaN, np.NaN, self.driver\n",
    "\n",
    "    def handle_stale_file_error(self, max_retries=5, retry_interval=10):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                error_div = self.driver.find_element(By.CSS_SELECTOR, 'resultado')\n",
    "                linha1 = error_div.fidChild('li')\n",
    "                if 'Stale file handle' in linha1.text:\n",
    "                    time.sleep(retry_interval)\n",
    "                else:\n",
    "                    return True\n",
    "            except NoSuchElementException:\n",
    "                return True\n",
    "        return False\n",
    "       \n",
    "    def extract_data_from_cvuri(self, element) -> dict:\n",
    "        \"\"\"\n",
    "        Extracts data from the cvuri attribute of the given element.\n",
    "        :param element: WebElement object\n",
    "        :return: Dictionary of extracted data\n",
    "        \"\"\"\n",
    "        cvuri = element.get_attribute('cvuri')\n",
    "        parsed_url = urlparse(cvuri)\n",
    "        params = parse_qs(parsed_url.query)\n",
    "        data_dict = {k: v[0] for k, v in params.items()}\n",
    "        return data_dict\n",
    "\n",
    "    def fill_name(self, NOME):\n",
    "        '''\n",
    "        Move cursor to the search field and fill in the specified name.\n",
    "        '''\n",
    "        if self.driver is None:\n",
    "            logging.error(\"O driver não foi inicializado corretamente.\")\n",
    "            return\n",
    "        try:\n",
    "            nome = lambda: self.driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "            nome().send_keys(Keys.CONTROL + \"a\")\n",
    "            nome().send_keys(NOME)\n",
    "        except Exception as e:\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(f'  ERRO!! Ao colar nome para buscar.') #, {traceback_str}\n",
    "        try:            \n",
    "            seletorcss = 'div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "            \n",
    "            seletorcss = \"#botaoBuscaFiltros\"\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        except Exception as e:\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(f'  ERRO!! Ao clicar no botão Buscar.\\n{e}, {traceback_str}')\n",
    "\n",
    "    def return_search_page(self):\n",
    "        url_busca = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "        self.driver.get(url_busca) # acessa a url de busca do CNPQ        \n",
    "\n",
    "    # Clicar no nome achado e clicar no botão \"Abrir Currículo\" da janela pop-up\n",
    "    def check_and_click_vinculo(self, elm_vinculo):\n",
    "        if elm_vinculo is None:\n",
    "            logging.error(\"Vínculo não encontrado, passando para o próximo nome...\")\n",
    "            self.return_search_page()\n",
    "            \n",
    "        try:\n",
    "            logging.info(f'Vínculo encontrado no currículo de nome: {elm_vinculo.text}')\n",
    "        except AttributeError:\n",
    "            logging.error(\"Vínculo não encontrado, passando para o próximo nome...\")\n",
    "            self.return_search_page()\n",
    "\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            # Aguardar até estar pronto para ser clicado       \n",
    "            btn_abrir_curriculo = WebDriverWait(self.driver, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "            time.sleep(0.5)\n",
    "            try:\n",
    "                # Clicar no botão para abrir o currículo       \n",
    "                ActionChains(self.driver).click(btn_abrir_curriculo).perform()\n",
    "            except TimeoutException:\n",
    "                print(\"       New window did not open. Clicking again.\")\n",
    "                ActionChains(self.driver).click(btn_abrir_curriculo).perform()  # Reattempting the click\n",
    "        except WebDriverException:\n",
    "            self.return_search_page()\n",
    "            logging.error('Falha ao clicar no link do nome.')\n",
    "        \n",
    "    def switch_to_new_window(self):\n",
    "        window_before = self.driver.current_window_handle\n",
    "        WebDriverWait(self.driver, self.delay).until(EC.number_of_windows_to_be(2))\n",
    "        window_after = self.driver.window_handles\n",
    "        new_window = [x for x in window_after if x != window_before][0]\n",
    "        self.driver.switch_to.window(new_window)\n",
    "\n",
    "    def switch_back_to_original_window(self):\n",
    "        current_window = self.driver.current_window_handle\n",
    "        original_window = [x for x in self.driver.window_handles if x != current_window][0]\n",
    "        self.driver.close() # Close the current window\n",
    "        self.driver.switch_to.window(original_window) # Switch back to the original window\n",
    "\n",
    "    def extract_tooltip_data(self, retries=3, delay=0.2) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Extracts tooltip data from articles section using Selenium with retry logic.\n",
    "        :param retries: Number of retries if element is not interactable.\n",
    "        :param delay: Wait time before retrying.\n",
    "        :return: List of dictionaries containing the extracted tooltip data.\n",
    "        \"\"\"\n",
    "        tooltip_data_list = []\n",
    "\n",
    "        try:\n",
    "            self.wait_for_element(\"#artigos-completos img.ajaxJCR\", [TimeoutException])\n",
    "            layout_cells = self.driver.find_elements(By.CSS_SELECTOR, '#artigos-completos .layout-cell-11 .layout-cell-pad-5')\n",
    "            for cell in layout_cells:\n",
    "                tooltip_data = {}\n",
    "                try:\n",
    "                    elem_citado = cell.find_element(By.CSS_SELECTOR, '.citado')\n",
    "                    tooltip_data.update(self.extract_data_from_cvuri(elem_citado))\n",
    "                except (ElementNotInteractableException, NoSuchElementException):\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    doi_elem = cell.find_element(By.CSS_SELECTOR, \"a.icone-producao.icone-doi\")\n",
    "                    tooltip_data[\"doi\"] = doi_elem.get_attribute(\"href\")\n",
    "                except NoSuchElementException:\n",
    "                    tooltip_data[\"doi\"] = None\n",
    "\n",
    "                current_retries = retries\n",
    "                while current_retries > 0:\n",
    "                    try:\n",
    "                        self.wait_for_element(\"img.ajaxJCR\", [TimeoutException])\n",
    "                        tooltip_elem = cell.find_element(By.CSS_SELECTOR, \"img.ajaxJCR\")\n",
    "                        if tooltip_elem.is_displayed() and tooltip_elem.size['height'] > 0:\n",
    "                            ActionChains(self.driver).move_to_element(tooltip_elem).perform()\n",
    "                            original_title = tooltip_elem.get_attribute(\"original-title\")\n",
    "                            match = re.search(r\"Fator de impacto \\(JCR \\d{4}\\): (\\d+\\.\\d+)\", original_title)\n",
    "                            tooltip_data[\"impact-factor\"] = match.group(1) if match else None\n",
    "                            tooltip_data[\"original_title\"] = original_title.split('<br />')[0].strip()\n",
    "                            break  # Saída do loop se a ação foi bem-sucedida\n",
    "                    except (ElementNotInteractableException, NoSuchElementException, TimeoutException):\n",
    "                        time.sleep(delay)  # Espera antes de tentar novamente\n",
    "                        current_retries -= 1  # Decrementa a contagem de retentativas\n",
    "\n",
    "                tooltip_data_list.append(tooltip_data)\n",
    "\n",
    "            print(f'       {len(tooltip_data_list):>003} artigos extraídos')\n",
    "            logging.info(f'{len(tooltip_data_list):>003} artigos extraídos')\n",
    "\n",
    "        except TimeoutException as e:\n",
    "            logging.error(\"Sem resposta antes do timeout\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro inesperado ao extrair tooltips: {e}\")\n",
    "\n",
    "        return tooltip_data_list\n",
    "\n",
    "\n",
    "    def search_profile(self, name, instituicao, unidade, termo):\n",
    "        try:\n",
    "            # Find terms to interact with the web page and extract the profile\n",
    "            profile_element, _, _, _, _ = self.find_terms(\n",
    "                name, \n",
    "                instituicao,  \n",
    "                unidade,  \n",
    "                termo,  \n",
    "                10,  \n",
    "                3  \n",
    "            )\n",
    "            # print('Elemento encontrado:', profile_element)\n",
    "            if profile_element:\n",
    "                return profile_element\n",
    "            else:\n",
    "                logging.info(f'Currículo não encontrado: {name}')\n",
    "                self.return_search_page()\n",
    "\n",
    "        except requests.HTTPError as e:\n",
    "            logging.error(f\"HTTPError occurred: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro inesperado ao buscar: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "    def scrape(self, driver, name_list, instituicao, unidade, termo, json_filename, hdf5_filename):\n",
    "        dict_list=[]\n",
    "        for k, name in enumerate(name_list):\n",
    "            try:\n",
    "                print(f'{k+1:>2}/{len(name_list)}: {name}')\n",
    "                self.fill_name(name)\n",
    "                elm_vinculo = self.search_profile(name, instituicao, unidade, termo)\n",
    "                \n",
    "                # Clica no link do nome e no botão Abrir Currículo\n",
    "                self.check_and_click_vinculo(elm_vinculo)\n",
    "                \n",
    "                # Muda para a nova janela aberta com o currículo\n",
    "                self.switch_to_new_window()\n",
    "                \n",
    "                if elm_vinculo:\n",
    "                    try:\n",
    "                        tooltip_data_list = self.extract_tooltip_data()\n",
    "                    except:\n",
    "                        print(f\"Erro ao extrair tooltips, tentando novamente...\")\n",
    "                        tooltip_data_list = self.extract_tooltip_data()\n",
    "                    \n",
    "                    page_source = driver.page_source\n",
    "                    if page_source is not None:\n",
    "                        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                        soup.attrs['tooltips'] = tooltip_data_list                 \n",
    "                        if soup:\n",
    "                            # print('Extraindo dados do objeto Soup...')\n",
    "                            parse_soup_instance = ParseSoup(driver)\n",
    "                            data = parse_soup_instance.extract_data(soup)\n",
    "                            # Chama métodos de conversão de dicionário individual\n",
    "                            # parse_soup_instance.to_json(data, json_filename)\n",
    "                            # parse_soup_instance.to_hdf5(data, hdf5_filename)\n",
    "                            dict_list.append(data)\n",
    "                    else:\n",
    "                        logging.error(f\"Could not get soup for profile: {name}\")\n",
    "                else:\n",
    "                    logging.error(f\"Currículo não encontrado para: {name}\")\n",
    "\n",
    "                # Fechar janela do currículo e voltar para página de busca\n",
    "                self.switch_back_to_original_window()\n",
    "\n",
    "                # Clicar no botão para fechar janela pop-up\n",
    "                btn_fechar_curriculo = WebDriverWait(driver, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnfechar\")))\n",
    "                ActionChains(driver).click(btn_fechar_curriculo).perform()    \n",
    "\n",
    "                self.return_search_page()\n",
    "                # logging.info('Successfully restarded extraction.')\n",
    "            except TimeoutException as e:\n",
    "                logging.error(f\"Sem resposta antes do timeout para: {name}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erro inesperado ao extrair para: {name}: {str(e)}\")\n",
    "        driver.quit()\n",
    "        return dict_list\n",
    "    \n",
    "# if __name__ == \"__main__\":   \n",
    "#     drives=['C:/Users/','E:/','./home/']\n",
    "#     pastas=['marcos.aires/', 'marco/']\n",
    "#     pastasraiz=['kgfioce','fioce']\n",
    "#     pasta_dados = './../data/'\n",
    "#     pastaraiz = 'fioce'\n",
    "#     caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "#     pathzip, pathcsv, pathjson, pathfig, pathaux, pathout = preparar_pastas(caminho)\n",
    "\n",
    "#     instituicao = 'Fundação Oswaldo Cruz'\n",
    "#     unidade = 'Fiocruz Ceará'\n",
    "#     termo = 'Ministerio da Saude'\n",
    "#     driver = connect_driver(caminho)\n",
    "#     t0 = time.time()\n",
    "#     scraper = LattesScraper(driver, instituicao, unidade, termo)\n",
    "#     dict_list = scraper.scrape(driver, lista_nomes, instituicao, unidade, termo,\n",
    "#                                pasta_dados+\"output.json\", pasta_dados+\"output.hdf5\")\n",
    "    \n",
    "#     print(f'{tempo(t0,time.time())} para busca e extração de dados de {len(lista_nomes)} nomes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sistema operacional Linux...\n",
      "Pasta armazenagem local /home/mak/fioce/\n",
      "\n",
      "Pasta para os xml já existe!\n",
      "Pasta para os CSV já existe!\n",
      "Pasta para os JSON já existe!\n",
      "Pasta para figuras já existe!\n",
      "Pasta para saídas já existe!\n",
      "\n",
      "Caminho da pasta raiz /home/mak/fioce/\n",
      "Caminho arquivos  XML /home/mak/fioce/xml_zip/\n",
      "Caminho arquivos JSON /home/mak/fioce/json/\n",
      "Caminho arquivos  CSV /home/mak/fioce/csv/\n",
      "Caminho para  figuras /home/mak/fioce/fig/\n",
      "Pasta arquivos saídas /home/mak/fioce/output/\n",
      "Conectando com o servidor do CNPq...\n",
      " 1/28: Adele Schwartz Benzaken\n",
      "       132 artigos extraídos\n",
      " 2/28: Felipe Leão Gomes Murta\n",
      "       024 artigos extraídos\n",
      " 3/28: Flor Ernestina Martinez Espinosa\n",
      "       045 artigos extraídos\n",
      " 4/28: Jacqueline de Almeida Gonçalves Sachett\n",
      "       098 artigos extraídos\n",
      " 5/28: Luiz Carlos de Lima Ferreira\n",
      "         2 homônimos de: Luiz Carlos de Lima Ferreira\n",
      "       Não foi possível identificar o vínculo de: Luiz Carlos de Lima Ferreira\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Luiz Carlos de Lima Ferreira: Message: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/28: Marco Aurélio Sartim\n",
      "       042 artigos extraídos\n",
      " 7/28: Maria das Graças Vale Barbosa Guerra\n",
      "       095 artigos extraídos\n",
      " 8/28: Stefanie Costa Pinto Lopes\n",
      "       048 artigos extraídos\n",
      " 9/28: Camila Helena Aguiar Bôtto de Menezes\n",
      "       027 artigos extraídos\n",
      "10/28: Fernando Fonseca de Almeida e Val\n",
      "       056 artigos extraídos\n",
      "11/28: Gisely Cardoso de Melo\n",
      "       098 artigos extraídos\n",
      "12/28: João Marcos Bemfica Barbosa Ferreira\n",
      "       046 artigos extraídos\n",
      "13/28: Manuela Berto Pucca\n",
      "       090 artigos extraídos\n",
      "14/28: Marcus Vinicius Guimarães de Lacerda\n",
      "       451 artigos extraídos\n",
      "15/28: Nagila Francinete Costa Secundino\n",
      "       074 artigos extraídos\n",
      "16/28: Vanderson de Souza Sampaio\n",
      "       107 artigos extraídos\n",
      "17/28: Djane Clarys Baia da Silva\n",
      "       060 artigos extraídos\n",
      "18/28: Flávia Regina Souza Ramos\n",
      "       211 artigos extraídos\n",
      "19/28: Hector Henrique Ferreira Koolen\n",
      "       121 artigos extraídos\n",
      "20/28: Jorge Augusto de Oliveira Guerra\n",
      "       070 artigos extraídos\n",
      "21/28: Marcelo Cordeiro dos Santos\n",
      "         7 homônimos de: Marcelo Cordeiro dos Santos\n",
      "       Não foi possível identificar o vínculo de: Marcelo Cordeiro dos Santos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Marcelo Cordeiro dos Santos: Message: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/28: Maria das Graças Costa Alecrim\n",
      "       096 artigos extraídos\n",
      "23/28: Paulo Filemon Paolucci Pimenta\n",
      "       164 artigos extraídos\n",
      "24/28: Wuelton Marcelo Monteiro\n",
      "       346 artigos extraídos\n",
      "25/28: Allyson Guimarães da Costa\n",
      "       086 artigos extraídos\n",
      "26/28: Sinésio Talhari\n",
      "       135 artigos extraídos\n",
      "27/28: Carolina Chrusciak Talhari Cortez\n",
      "       086 artigos extraídos\n",
      "28/28: Henrique Manuel Condinho da Silveira\n",
      "       054 artigos extraídos\n",
      "\n",
      "00h 42m 18s para busca de 28 nomes com extração de dados de 26 dicionários\n"
     ]
    }
   ],
   "source": [
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    from string import Formatter\n",
    "    \n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    \n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    \n",
    "    return f.format(fmt, **values)\n",
    "\n",
    "def tempo(start, end):\n",
    "    from datetime import timedelta\n",
    "        \n",
    "    t=end-start\n",
    "\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "def definir_sistema(pastaraiz):\n",
    "    \"\"\"\n",
    "    Function to define the system and prepare local folders.\n",
    "\n",
    "    Parameters:\n",
    "    pastaraiz (str): Root folder for the operation.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the path (caminho), drive, and user (usuario).\n",
    "\n",
    "    Raises:\n",
    "    EnvironmentError: If the operating system is not recognized.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    caminho = ''\n",
    "    drive = ''\n",
    "    usuario = ''\n",
    "\n",
    "    sistema_operacional = sys.platform\n",
    "\n",
    "    try:\n",
    "        if 'linux' in sistema_operacional:\n",
    "            print('Sistema operacional Linux...')\n",
    "            linux_users = ['mak/', 'marcos/']\n",
    "            drive = '/home/'\n",
    "            for user in linux_users:\n",
    "                temp_caminho = os.path.join(drive, user, pastaraiz)\n",
    "                if os.path.isdir(temp_caminho):\n",
    "                    usuario = user\n",
    "                    caminho = os.path.join(drive, usuario, pastaraiz, '')\n",
    "                    break\n",
    "\n",
    "        elif 'win32' in sistema_operacional:\n",
    "            print('Sistema operacional Windows...')\n",
    "            windows_users = ['\\\\Users\\\\marco\\\\', '\\\\Users\\\\marcos.aires\\\\']\n",
    "            drive = 'C:'\n",
    "            # print('Procurando em:')\n",
    "            for user in windows_users:\n",
    "                temp_caminho = os.path.join(drive, user, pastaraiz)\n",
    "                print(f\"  {temp_caminho}\")\n",
    "                if os.path.isdir(temp_caminho):\n",
    "                    usuario = user\n",
    "                    caminho = os.path.join(drive, usuario, pastaraiz, '')\n",
    "                    break\n",
    "                else:\n",
    "                    pathzip, pathcsv, pathjson, pathfig, caminho, pathout = preparar_pastas(pastaraiz)\n",
    "                    if os.path.isdir(caminho):\n",
    "                        usuario = user\n",
    "        else:\n",
    "            raise EnvironmentError('SO não reconhecido')\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print('  ERRO!! Diretório não encontrado!')\n",
    "        print(e)\n",
    "    except EnvironmentError as e:\n",
    "        print('  ERRO!! Sistema Operacional não suportado!')\n",
    "        print(e)\n",
    "\n",
    "    if not caminho:\n",
    "        print('  ERRO!! Caminho não foi definido!')\n",
    "\n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "\n",
    "    return caminho, drive, usuario\n",
    "\n",
    "def preparar_pastas(caminho):\n",
    "    # caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    # caminho = drive+':/'+usuario+pastaraiz\n",
    "    # caminho = drive+':/'+pastaraiz\n",
    "\n",
    "    # Check if caminho is empty or None\n",
    "    if not caminho:\n",
    "        raise ValueError(\"Variável 'caminho' vazia. Não é possível criar os diretórios.\")\n",
    "\n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'xml_zip/'\n",
    "    pathcsv  = caminho+'csv/'\n",
    "    pathjson = caminho+'json/'\n",
    "    pathfig  = caminho+'fig/'\n",
    "    pathaux  = caminho\n",
    "    pathout  = caminho+'output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XML', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    print()\n",
    "    \n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout\n",
    "\n",
    "\n",
    "drives=['C:/Users/','E:/','./home/']\n",
    "pastas=['marcos.aires/', 'marco/','mak/']\n",
    "pastaraiz='ppgmt_uea'\n",
    "caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "\n",
    "pathzip, pathcsv, pathjson, pathfig, pathaux, pathout = preparar_pastas(caminho)\n",
    "\n",
    "lista_nomes = df_docentes['nome']\n",
    "\n",
    "# Comentado por economia de tempo, meia hora para montar 50 currículos\n",
    "pasta_dados = './../data/'\n",
    "instituicao = 'Universidade Federal do Amazonas'\n",
    "unidade = 'Universidade do Estado do Amazonas'\n",
    "termo = 'Medicina Tropical'\n",
    "driver = connect_driver(caminho)\n",
    "t0 = time.time()\n",
    "scraper = LattesScraper(driver, instituicao, unidade, termo)\n",
    "\n",
    "# Extrai e monta a lista de dicionários\n",
    "dict_list = scraper.scrape(driver, lista_nomes, instituicao, unidade, termo,\n",
    "                            pasta_dados+\"output.json\", pasta_dados+\"output.hdf5\")\n",
    "\n",
    "print(f'\\n{tempo(t0,time.time())} para busca de {len(lista_nomes)} nomes com extração de dados de {len(dict_list)} dicionários')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 dicionários montados\n",
      " 0C 132A 132T Dif:000 Adele Schwartz Benzaken \n",
      " 1C 024A 024T Dif:000 Felipe Leão Gomes Murta \n",
      " 2C 045A 045T Dif:000 Flor Ernestina Martinez Espinosa \n",
      " 3C 098A 098T Dif:000 Jacqueline de Almeida Gonçalves Sachett \n",
      " 4C 042A 042T Dif:000 Marco Aurélio Sartim \n",
      " 5C 095A 095T Dif:000 Maria das Graças Vale Barbosa Guerra \n",
      " 6C 048A 048T Dif:000 Stefanie Costa Pinto Lopes \n",
      " 7C 027A 027T Dif:000 Camila Helena Aguiar Bôtto de Menezes \n",
      " 8C 056A 056T Dif:000 Fernando Fonseca de Almeida e Val \n",
      " 9C 098A 098T Dif:000 Gisely Cardoso de Melo \n",
      "10C 046A 046T Dif:000 Joao Marcos Bemfica Barbosa Ferreira \n",
      "11C 090A 090T Dif:000 Manuela Berto Pucca \n",
      "12C 451A 451T Dif:000 Marcus Vinícius Guimarães de Lacerda \n",
      "13C 074A 074T Dif:000 Nagila Francinete Costa Secundino \n",
      "14C 107A 107T Dif:000 Vanderson de Souza Sampaio \n",
      "15C 060A 060T Dif:000 Djane Clarys Baia da Silva \n",
      "16C 211A 211T Dif:000 Flávia Regina Souza Ramos \n",
      "17C 121A 121T Dif:000 Hector Henrique Ferreira Koolen \n",
      "18C 070A 070T Dif:000 Jorge Augusto de Oliveira Guerra \n",
      "19C 096A 096T Dif:000 Maria das Graças Costa Alecrim \n",
      "20C 164A 164T Dif:000 Paulo Filemon Paolucci Pimenta \n",
      "21C 346A 346T Dif:000 Wuelton Marcelo Monteiro \n",
      "22C 086A 086T Dif:000 Allyson Guimarães da Costa \n",
      "23C 135A 135T Dif:000 Sinésio Talhari \n",
      "24C 086A 086T Dif:000 Carolina Chrusciak Talhari Cortez \n",
      "25C 054A 054T Dif:000 Henrique Manuel Condinho da Silveira \n",
      "\n",
      "Total de artigos em todos períodos: 2862\n",
      "Total de títulos em todos períodos: 2862\n",
      "Arquivo JSON salvo em: ./../data/dict_list_uea.json\n"
     ]
    }
   ],
   "source": [
    "## Contagem de artigos para simples confererência\n",
    "print(f'{len(dict_list)} dicionários montados')\n",
    "qte_artigos=0\n",
    "qte_titulos=0\n",
    "for k,i in enumerate(dict_list):\n",
    "    try:\n",
    "        qte_jcr = len(i['JCR'])\n",
    "    except:\n",
    "        qte_jcr = 0\n",
    "    try:\n",
    "       qte_jcr2 = len(i['JCR2'])\n",
    "    except:\n",
    "       qte_jcr2 = 0\n",
    "    qte_artigos+=qte_jcr\n",
    "    qte_titulos+=qte_jcr2\n",
    "    status=qte_jcr2-qte_jcr\n",
    "    print(f\"{k:>2}C {qte_jcr:>03}A {qte_jcr2:>03}T Dif:{status:>03} {i['name']} \")\n",
    "\n",
    "print(f'\\nTotal de artigos em todos períodos: {qte_artigos}')\n",
    "print(f'Total de títulos em todos períodos: {qte_titulos}')\n",
    "\n",
    "# Função para salvar a lista de dicionários em um arquivo .json\n",
    "def save_to_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "data=dict_list\n",
    "file_path='./../data/dict_list_uea.json'\n",
    "save_to_json(data, file_path)\n",
    "\n",
    "print(f\"Arquivo JSON salvo em: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "folder='./../data/'\n",
    "def load_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Carrega um arquivo JSON e retorna seu conteúdo.\n",
    "    Parâmetros:\n",
    "        file_path (str): O caminho para o arquivo JSON.\n",
    "    Retorna:\n",
    "        dict: O conteúdo do arquivo JSON.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def list_json(folder):\n",
    "    for i in os.listdir(folder):\n",
    "        try:\n",
    "            ext = i.split('.')[1]\n",
    "            if ext == 'json':\n",
    "                print(i)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 currículos carregados em lista de dicionários\n"
     ]
    }
   ],
   "source": [
    "# Carregar o conteúdo do arquivo 'output.json' para a variável dict_list\n",
    "dict_list = load_from_json('./../data/dict_list_uea.json')\n",
    "print(f\"{len(dict_list)} currículos carregados em lista de dicionários\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class ArticlesCounter:\n",
    "    def __init__(self, dict_list):\n",
    "        self.data_list = dict_list\n",
    "\n",
    "    def dias_desde_atualizacao(self, data_atualizacao_str):\n",
    "        # Converte a data de atualização em um objeto datetime\n",
    "        data_atualizacao = datetime.strptime(data_atualizacao_str, '%d/%m/%Y')\n",
    "        \n",
    "        # Obtém a data atual\n",
    "        data_atual = datetime.now()\n",
    "        \n",
    "        # Calcula a diferença em dias\n",
    "        diferenca_dias = (data_atual - data_atualizacao).days if data_atualizacao else None\n",
    "        return diferenca_dias\n",
    "\n",
    "    def extrair_idlattes(self, dic):\n",
    "        id_lattes_info = dic.get('InfPes', {}).get('2', '')\n",
    "        id_lattes = re.search(r'ID Lattes: (\\d+)', id_lattes_info)\n",
    "        if id_lattes:\n",
    "            id_lattes = id_lattes.group(1)\n",
    "        else:\n",
    "            id_lattes = re.search(r'Endereço para acessar este CV: http%3A//lattes.cnpq.br/(\\d+)', id_lattes_info)\n",
    "            id_lattes = id_lattes.group(1)\n",
    "        return id_lattes\n",
    "\n",
    "    def extrair_data_atualizacao(self, dict_list):\n",
    "        ids_lattes_grupo=[]\n",
    "        nomes_curriculos=[]\n",
    "        dts_atualizacoes=[]\n",
    "        tempos_defasagem=[]\n",
    "        qtes_artcomplper=[]\n",
    "        for dic in dict_list:\n",
    "            info_nam = dic.get('name',{})\n",
    "            nomes_curriculos.append(info_nam)\n",
    "            info_pes = dic.get('InfPes', {})\n",
    "            for line in info_pes:\n",
    "                try:\n",
    "                    id_pattern = re.search(r'ID Lattes: (\\d+)', line)\n",
    "                    dt_pattern = re.search(r'\\d{2}/\\d{2}/\\d{4}', line)\n",
    "                    id_lattes =  id_pattern.group(1) if id_pattern else None\n",
    "                    if id_lattes:\n",
    "                        ids_lattes_grupo.append(id_lattes)\n",
    "                    data_atualizacao = dt_pattern.group() if dt_pattern else None\n",
    "                    if data_atualizacao:\n",
    "                        dts_atualizacoes.append(data_atualizacao)\n",
    "                        tempo_atualizado = self.dias_desde_atualizacao(data_atualizacao)\n",
    "                        tempos_defasagem.append(tempo_atualizado)                    \n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    # print(e)\n",
    "\n",
    "            info_art = dic.get('Produções', {}).get('Produção bibliográfica', {}).get('Artigos completos publicados em periódicos', {})\n",
    "            qtes_artcomplper.append(len(info_art.values()))\n",
    "\n",
    "        dtf_atualizado = pd.DataFrame({\"id_lattes\": ids_lattes_grupo,\n",
    "                                       \"curriculos\": nomes_curriculos, \n",
    "                                       \"ultima_atualizacao\": dts_atualizacoes,\n",
    "                                       \"dias_defasagem\": tempos_defasagem,\n",
    "                                       \"qte_artigos_periodicos\": qtes_artcomplper,\n",
    "                                       })\n",
    "        return dtf_atualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_lattes</th>\n",
       "      <th>curriculos</th>\n",
       "      <th>ultima_atualizacao</th>\n",
       "      <th>dias_defasagem</th>\n",
       "      <th>qte_artigos_periodicos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5458052030678047</td>\n",
       "      <td>Adele Schwartz Benzaken</td>\n",
       "      <td>12/11/2023</td>\n",
       "      <td>5</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7106266581552839</td>\n",
       "      <td>Felipe Leão Gomes Murta</td>\n",
       "      <td>31/10/2023</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6327051322950104</td>\n",
       "      <td>Flor Ernestina Martinez Espinosa</td>\n",
       "      <td>06/11/2023</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0998695766654411</td>\n",
       "      <td>Jacqueline de Almeida Gonçalves Sachett</td>\n",
       "      <td>09/11/2023</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0196692401515816</td>\n",
       "      <td>Marco Aurélio Sartim</td>\n",
       "      <td>03/11/2023</td>\n",
       "      <td>14</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2940481324985304</td>\n",
       "      <td>Maria das Graças Vale Barbosa Guerra</td>\n",
       "      <td>09/11/2023</td>\n",
       "      <td>8</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9573217074348653</td>\n",
       "      <td>Stefanie Costa Pinto Lopes</td>\n",
       "      <td>11/08/2023</td>\n",
       "      <td>98</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2650427081350436</td>\n",
       "      <td>Camila Helena Aguiar Bôtto de Menezes</td>\n",
       "      <td>06/11/2023</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7664318780210160</td>\n",
       "      <td>Fernando Fonseca de Almeida e Val</td>\n",
       "      <td>08/11/2023</td>\n",
       "      <td>9</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5566457348830121</td>\n",
       "      <td>Gisely Cardoso de Melo</td>\n",
       "      <td>09/11/2023</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4440401798628933</td>\n",
       "      <td>Joao Marcos Bemfica Barbosa Ferreira</td>\n",
       "      <td>19/09/2023</td>\n",
       "      <td>59</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2714810198631869</td>\n",
       "      <td>Manuela Berto Pucca</td>\n",
       "      <td>01/11/2023</td>\n",
       "      <td>16</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8492376468047417</td>\n",
       "      <td>Marcus Vinícius Guimarães de Lacerda</td>\n",
       "      <td>16/11/2023</td>\n",
       "      <td>1</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0641258278314799</td>\n",
       "      <td>Nagila Francinete Costa Secundino</td>\n",
       "      <td>08/11/2023</td>\n",
       "      <td>9</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0039836167659650</td>\n",
       "      <td>Vanderson de Souza Sampaio</td>\n",
       "      <td>05/06/2023</td>\n",
       "      <td>165</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1446146671158093</td>\n",
       "      <td>Djane Clarys Baia da Silva</td>\n",
       "      <td>28/10/2023</td>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3027670493001716</td>\n",
       "      <td>Flávia Regina Souza Ramos</td>\n",
       "      <td>06/11/2023</td>\n",
       "      <td>11</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2722430673503338</td>\n",
       "      <td>Hector Henrique Ferreira Koolen</td>\n",
       "      <td>12/11/2023</td>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8416964690912995</td>\n",
       "      <td>Jorge Augusto de Oliveira Guerra</td>\n",
       "      <td>27/09/2023</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6000588372204544</td>\n",
       "      <td>Maria das Graças Costa Alecrim</td>\n",
       "      <td>02/08/2023</td>\n",
       "      <td>107</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4592140991723664</td>\n",
       "      <td>Paulo Filemon Paolucci Pimenta</td>\n",
       "      <td>08/11/2023</td>\n",
       "      <td>9</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4986967857234820</td>\n",
       "      <td>Wuelton Marcelo Monteiro</td>\n",
       "      <td>13/11/2023</td>\n",
       "      <td>4</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7531662673281014</td>\n",
       "      <td>Allyson Guimarães da Costa</td>\n",
       "      <td>08/11/2023</td>\n",
       "      <td>9</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4447075735878106</td>\n",
       "      <td>Sinésio Talhari</td>\n",
       "      <td>31/03/2021</td>\n",
       "      <td>961</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4041028384017266</td>\n",
       "      <td>Carolina Chrusciak Talhari Cortez</td>\n",
       "      <td>31/10/2023</td>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4984598689941154</td>\n",
       "      <td>Henrique Manuel Condinho da Silveira</td>\n",
       "      <td>03/08/2023</td>\n",
       "      <td>106</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id_lattes                               curriculos  \\\n",
       "0   5458052030678047                  Adele Schwartz Benzaken   \n",
       "1   7106266581552839                  Felipe Leão Gomes Murta   \n",
       "2   6327051322950104         Flor Ernestina Martinez Espinosa   \n",
       "3   0998695766654411  Jacqueline de Almeida Gonçalves Sachett   \n",
       "4   0196692401515816                     Marco Aurélio Sartim   \n",
       "5   2940481324985304     Maria das Graças Vale Barbosa Guerra   \n",
       "6   9573217074348653               Stefanie Costa Pinto Lopes   \n",
       "7   2650427081350436    Camila Helena Aguiar Bôtto de Menezes   \n",
       "8   7664318780210160        Fernando Fonseca de Almeida e Val   \n",
       "9   5566457348830121                   Gisely Cardoso de Melo   \n",
       "10  4440401798628933     Joao Marcos Bemfica Barbosa Ferreira   \n",
       "11  2714810198631869                      Manuela Berto Pucca   \n",
       "12  8492376468047417     Marcus Vinícius Guimarães de Lacerda   \n",
       "13  0641258278314799        Nagila Francinete Costa Secundino   \n",
       "14  0039836167659650               Vanderson de Souza Sampaio   \n",
       "15  1446146671158093               Djane Clarys Baia da Silva   \n",
       "16  3027670493001716                Flávia Regina Souza Ramos   \n",
       "17  2722430673503338          Hector Henrique Ferreira Koolen   \n",
       "18  8416964690912995         Jorge Augusto de Oliveira Guerra   \n",
       "19  6000588372204544           Maria das Graças Costa Alecrim   \n",
       "20  4592140991723664           Paulo Filemon Paolucci Pimenta   \n",
       "21  4986967857234820                 Wuelton Marcelo Monteiro   \n",
       "22  7531662673281014               Allyson Guimarães da Costa   \n",
       "23  4447075735878106                          Sinésio Talhari   \n",
       "24  4041028384017266        Carolina Chrusciak Talhari Cortez   \n",
       "25  4984598689941154     Henrique Manuel Condinho da Silveira   \n",
       "\n",
       "   ultima_atualizacao  dias_defasagem  qte_artigos_periodicos  \n",
       "0          12/11/2023               5                     132  \n",
       "1          31/10/2023              17                      24  \n",
       "2          06/11/2023              11                      45  \n",
       "3          09/11/2023               8                      98  \n",
       "4          03/11/2023              14                      42  \n",
       "5          09/11/2023               8                      95  \n",
       "6          11/08/2023              98                      48  \n",
       "7          06/11/2023              11                      27  \n",
       "8          08/11/2023               9                      56  \n",
       "9          09/11/2023               8                      98  \n",
       "10         19/09/2023              59                      46  \n",
       "11         01/11/2023              16                      90  \n",
       "12         16/11/2023               1                     451  \n",
       "13         08/11/2023               9                      74  \n",
       "14         05/06/2023             165                     107  \n",
       "15         28/10/2023              20                      60  \n",
       "16         06/11/2023              11                     211  \n",
       "17         12/11/2023               5                     121  \n",
       "18         27/09/2023              51                      70  \n",
       "19         02/08/2023             107                      96  \n",
       "20         08/11/2023               9                     164  \n",
       "21         13/11/2023               4                     346  \n",
       "22         08/11/2023               9                      86  \n",
       "23         31/03/2021             961                     135  \n",
       "24         31/10/2023              17                      86  \n",
       "25         03/08/2023             106                      54  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atualizador = ArticlesCounter(dict_list)\n",
    "dtf_atualizado = atualizador.extrair_data_atualizacao(dict_list)\n",
    "dtf_atualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
