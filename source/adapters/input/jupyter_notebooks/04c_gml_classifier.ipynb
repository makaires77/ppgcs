{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <center><center><img src=\"https://raw.githubusercontent.com/makaires77/fioce/master/assets/logo_fioce.png\" \n",
    "style=\"height:150px\" alt=\"Logo_Fiocruz\"></center></center> -->\n",
    "\n",
    "<center><center><img src=\"https://user-images.githubusercontent.com/61051085/81343928-3ce9d500-908c-11ea-9850-0210b4e94ba0.jpg\" \n",
    "style=\"height:150px\" alt=\"Logo_Unifor\"></center></center>\n",
    "\n",
    "## <center>Graph Machine Learning para análise de agrupamento dinâmico<br /></center>\n",
    "\n",
    "    Antonio Marcos Aires Barbosa (Fiocruz Ceará)\n",
    "\n",
    "**Introdução**\n",
    "\n",
    "A análise de Grafos permite obter insights como produtos de análises em contextos da realidade com base em modelos capazes de lidar dados heterogêneos e relações complexas.\n",
    "\n",
    "\n",
    "Nesta etapa do trabalho propomos modelos de classificação com Graph Machine Learning para suportar análise dos dados de pesquisa acadêmica tendo como fonte de dados os currículo Lattes de servidores da unidade Fiocruz Ceará."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boas Dicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boas práticas em Orientação a Objeto e Engenharia de Software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs.: Decorator Pattern - Uso de métodos de instância, ou decoradores @staticmethod e @classmethod\n",
    "\n",
    "Em Python e em muitas outras linguagens orientadas a objetos, um método de instância é definido como um método que opera no contexto de uma instância específica de uma classe e que não possui um decorador especial que altera esse comportamento. Podemos citar as características principais de um método de instância como:\n",
    "\n",
    "    Métodos de instância (sem decorador)\n",
    "Contexto de Instância: Um método de instância opera em uma instância específica de uma classe. Ele tem acesso ao estado da instância através do argumento self, que é uma referência ao objeto da classe.\n",
    "\n",
    "Sem Decoradores Específicos: Geralmente, um método de instância não é decorado com @staticmethod ou @classmethod. Portanto, ele não tem as características especiais de acesso ou comportamento que esses decoradores fornecem.\n",
    "\n",
    "Uso de self: Em Python, o primeiro parâmetro de um método de instância é convencionalmente chamado self, que é uma referência automática à instância que está chamando o método. Esse parâmetro é fornecido automaticamente pelo Python quando o método é chamado em uma instância da classe.\n",
    "\n",
    "Acesso a Atributos de Instância: Métodos de instância podem acessar e modificar atributos da instância (self.atributo), o que os torna adequados para operações que dependem do estado específico da instância.\n",
    "\n",
    "Chamada: São chamados a partir de uma instância da classe, não diretamente da classe. Por exemplo: instancia.metodo().\n",
    "\n",
    "Portanto, um método que não possui um decorador @staticmethod ou @classmethod em uma classe Python é, por padrão, um método de instância. No entanto, é importante notar que o Python também permite criar decoradores personalizados, então a presença de um decorador não necessariamente exclui um método de ser um método de instância; isso depende do que o decorador faz.\n",
    "\n",
    "A escolha entre usar métodos de instância, @staticmethod e @classmethod depende principalmente de se você precisa acessar ou modificar dados pertencentes a uma instância específica, à classe como um todo, ou se precisa apenas de uma função lógica que se encaixa no contexto da classe, mas que opera de forma independente dela. Em python há vários decoradores como @property, @setter e @deletter para tornar as coisas práticas, mas os mais usados são @staticmethod e @classmethod:\n",
    "\n",
    "Comparativo com @staticmethod e @classmethod\n",
    "\n",
    "    @staticmethod:\n",
    "\n",
    "Não têm acesso ao estado específico de uma instância (self) nem ao estado da classe (cls).\n",
    "São como funções regulares, mas são agrupadas dentro da classe por uma questão de organização e contexto lógico.\n",
    "\n",
    "    @classmethod:\n",
    "\n",
    "Têm acesso ao estado da classe através do parâmetro cls.\n",
    "Podem modificar o estado da classe, que afetará todas as instâncias daquela classe.\n",
    "São usados para métodos que têm mais a ver com a classe em geral do que com uma instância específica.\n",
    "\n",
    "    Métodos de Instância (métodos sem decoradores):\n",
    "\n",
    "Têm acesso ao estado específico da instância através de self.\n",
    "Podem ler ou modificar o estado da instância.\n",
    "São a escolha padrão para a maioria das ações que os objetos da classe precisam realizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo prático de aplicação dos princípios e boas práticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A utilização de @classmethod e @staticmethod em Python, como demonstrado na classe JSONFileManager, assegura vários princípios importantes em programação e design de software:\n",
    "\n",
    "Encapsulamento: Encapsular a lógica relacionada a arquivos JSON dentro de uma única classe melhora a organização do código e facilita a manutenção. Cada método na classe JSONFileManager tem uma responsabilidade específica, seja carregar dados, listar arquivos ou comparar conteúdos.\n",
    "\n",
    "Coesão: A classe mantém uma alta coesão, pois todos os métodos estão intimamente relacionados à manipulação e análise de arquivos JSON. Isso torna a classe mais compreensível e reutilizável.\n",
    "\n",
    "Separação de Preocupações: Cada método lida com uma tarefa específica. Por exemplo, load_json foca em carregar um arquivo JSON, enquanto compare_dictionaries se dedica a comparar dois dicionários. Esta separação torna o código mais fácil de entender e testar.\n",
    "\n",
    "Flexibilidade e Extensibilidade: A classe pode ser facilmente estendida para incluir novos métodos relacionados a arquivos JSON. Por exemplo, poderia ser adicionado um método para salvar dados em formato JSON, mantendo a consistência com as funcionalidades existentes.\n",
    "\n",
    "Reutilização de Código: Métodos como load_json podem ser reutilizados em várias partes da classe, reduzindo a duplicação de código. Além disso, a classe em si pode ser reutilizada em diferentes partes de um projeto ou em projetos futuros que lidam com arquivos JSON.\n",
    "\n",
    "Independência: Usar métodos estáticos para operações que não dependem do estado da classe ou de suas instâncias permite que esses métodos sejam chamados sem a necessidade de criar um objeto da classe, o que é útil para operações utilitárias que são genéricas e independentes.\n",
    "\n",
    "Princípio da Responsabilidade Única: Cada método na classe é responsável por uma única ação. Isso torna a classe mais fácil de entender e manter, e também facilita a depuração e o teste do código.\n",
    "\n",
    "Princípio de Aberto/Fechado: A classe pode ser estendida sem modificar seu comportamento existente, o que é um aspecto fundamental do princípio de aberto/fechado em design de software. Métodos adicionais podem ser acrescentados para expandir a funcionalidade, enquanto o código existente permanece inalterado.\n",
    "\n",
    "Esses princípios são fundamentais para criar um código limpo, eficiente e fácil de manter, que são metas desejáveis em qualquer projeto de desenvolvimento de software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literatura recomendada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas obras são frequentemente citadas em cursos universitários de ciência da computação e engenharia de software, e são consideradas leituras essenciais para profissionais que desejam aprofundar seu conhecimento em princípios de design e práticas de desenvolvimento de software. No âmbito tanto acadêmico, como  profissional de computação, vários princípios de design e engenharia de software são amplamente reconhecidos e recomendados. Muitos desses princípios são discutidos em detalhes em literaturas fundamentais. Algumas das melhores fontes nessa temática incluem:\n",
    "\n",
    "<b>\"Design Patterns: Elements of Reusable Object-Oriented Software\"</b> Erich Gamma, Richard Helm, Ralph Johnson e John Vlissides\n",
    "\n",
    " (Conhecidos como o \"Gang of Four\" ou GoF). Este livro é fundamental para entender padrões de design em programação orientada a objetos e aborda princípios como encapsulamento, reutilização de código e separação de preocupações.\n",
    "\n",
    "<b>\"Clean Code: A Handbook of Agile Software Craftsmanship\"</b> por Robert C. Martin. Este livro oferece uma exploração aprofundada sobre como escrever código limpo e manutenível, abordando princípios como responsabilidade única e coesão.\n",
    "\n",
    "<b>\"Refactoring: Improving the Design of Existing Code\" por Martin Fowler</b>. Fowler discute técnicas para melhorar a estrutura interna do software sem alterar seu comportamento externo, uma abordagem fundamental para manter a flexibilidade e extensibilidade do código.\n",
    "\n",
    "<b>\"The Pragmatic Programmer: Your Journey to Mastery\"</b> por Andrew Hunt e David Thomas. Este livro é uma coleção de dicas e práticas para programadores que desejam melhorar sua habilidade em desenvolvimento de software, incluindo aspectos como encapsulamento e princípios de design sólido.\n",
    "\n",
    "<b>\"Object-Oriented Software Construction\"</b> por Bertrand Meyer. Este livro é conhecido por introduzir o princípio do Design by Contract, além de discutir extensivamente sobre a construção de software orientado a objetos.\n",
    "\n",
    "<b>\"Software Engineering: A Practitioner's Approach\"</b> por Roger S. Pressman. Um texto clássico para cursos de engenharia de software, cobrindo uma ampla gama de tópicos desde princípios de design até técnicas de gerenciamento de projetos de software.\n",
    "\n",
    "<b>\"Patterns of Enterprise Application Architecture\"</b> por Martin Fowler. Este livro trata de padrões arquitetônicos em aplicações empresariais, discutindo como os princípios de design podem ser aplicados em sistemas de grande escala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar JSON local para dicionário em memória"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular o número de comparações necessárias para conferir cada par de arquivos é necessário comparar um conjunto de $(n)$ arquivos dois a dois, sem repetição, é a fórmula para calcular o número de combinações de $(n)$ elementos tomados 2 a 2. Em termos de combinações, isso é representado por $( C(n,2) )$, que é calculado pela fórmula:\n",
    "\n",
    "$$\n",
    "C(n, 2) = \\frac{n!}{2!(n - 2)!}\n",
    "$$\n",
    "\n",
    "Onde \\( n! \\) (lê-se \"n fatorial\") é o produto de todos os números inteiros positivos até \\( n \\). A fórmula pode ser simplificada para:\n",
    "\n",
    "$$\n",
    "C(n, 2) = \\frac{n \\times (n - 1)}{2}\n",
    "$$\n",
    "\n",
    "Essa é a fórmula para calcular o número de pares únicos que podem ser formados a partir de um conjunto de \\( n \\) itens. Por exemplo, se você tiver 5 arquivos e quiser saber quantas comparações são necessárias para comparar cada par uma única vez, você usaria esta fórmula para calcular:\n",
    "\n",
    "$$\n",
    "C(5, 2) = \\frac{5 \\times (5 - 1)}{2} = \\frac{5 \\times 4}{2} = 10\n",
    "$$\n",
    "\n",
    "Portanto, são necessárias 10 comparações.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "class JSONFileManager:\n",
    "    data_folder = './../data/'\n",
    "\n",
    "    @staticmethod\n",
    "    def load_json(file):\n",
    "        with open(file, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_dictionaries(dict1, dict2, path=data_folder):\n",
    "        differences = []\n",
    "        for key in set(dict1.keys()).union(dict2.keys()):\n",
    "            if key in dict1 and key not in dict2:\n",
    "                differences.append(f\"    Divergência em {path}: Chave '{key}' presente no primeiro dicionário, mas ausente no segundo\")\n",
    "            elif key not in dict1 and key in dict2:\n",
    "                differences.append(f\"    Divergência em {path}: Chave '{key}' presente no segundo dicionário, mas ausente no primeiro\")\n",
    "            elif dict1.get(key) != dict2.get(key):\n",
    "                differences.append(f\"    Divergência em {path}: Diferença no valor da chave '{key}', {dict1.get(key)} != {dict2.get(key)}\")\n",
    "        return differences\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_lists_of_dictionaries(list1, list2, path=data_folder):\n",
    "        differences = []\n",
    "\n",
    "        # Transforma as listas em conjuntos de elementos únicos para comparação\n",
    "        set1 = {json.dumps(d, sort_keys=True) for d in list1}\n",
    "        set2 = {json.dumps(d, sort_keys=True) for d in list2}\n",
    "\n",
    "        unique_in_list1 = set1 - set2\n",
    "        unique_in_list2 = set2 - set1\n",
    "\n",
    "        if unique_in_list1 or unique_in_list2:\n",
    "            differences.append(f\"    Divergência no conteúdo das listas em {path}\")\n",
    "            differences.append(f\"    {len(unique_in_list1):>2} dicionário(s) únicos na primeira lista\")\n",
    "            differences.append(f\"    {len(unique_in_list2):>2} dicionário(s) únicos na segunda lista\")\n",
    "\n",
    "        return differences\n",
    "\n",
    "    @classmethod\n",
    "    def list_relevant_json_files(cls):\n",
    "        relevant_files = [f for f in os.listdir(cls.data_folder) if f.startswith('output_') and f.endswith('.json')]\n",
    "        return relevant_files\n",
    "\n",
    "    @classmethod\n",
    "    def compare_all_json_pairs(cls, show_details=False):\n",
    "        files = cls.list_relevant_json_files()\n",
    "        total_comparisons = 0\n",
    "        identical_files = []\n",
    "        divergent_files = []\n",
    "\n",
    "        for file1, file2 in combinations(files, 2):\n",
    "            total_comparisons += 1\n",
    "            dataset1 = cls.load_json(cls.data_folder + file1)\n",
    "            dataset2 = cls.load_json(cls.data_folder + file2)\n",
    "            differences = JSONFileManager.compare_lists_of_dictionaries(dataset1, dataset2)\n",
    "\n",
    "            if len(differences) == 0:\n",
    "                identical_files.append((file1, file2))\n",
    "            else:\n",
    "                divergent_files.append((file1, file2))\n",
    "                if show_details:\n",
    "                    print(f\"Comparando: {file1} e {file2}\")\n",
    "                    print(f\"Encontradas {len(differences)} diferenças entre {file1} e {file2}.\")\n",
    "                    for diff in differences:\n",
    "                        print(diff)\n",
    "\n",
    "        # Exibir resumo ao final\n",
    "        print(\"\\nTotal de arquivos de datasets:\", len(files))\n",
    "        print(\"\\nResumo das Comparações:\")\n",
    "        print(f\"Total de comparações por pares realizadas: {total_comparisons}\")\n",
    "        print(f\"Quantidade de pares de arquivos idênticos: {len(identical_files)}\")\n",
    "        if identical_files:\n",
    "            print(\"Pares idênticos:\")\n",
    "            for file_pair in identical_files:\n",
    "                print(\"   '\", file_pair[0], \"'\")\n",
    "                print(\"   '\", file_pair[1], \"'\")\n",
    "                print()\n",
    "\n",
    "        print(f\"Quantidade de pares de arquivos divergentes: {len(divergent_files)}\")\n",
    "        if divergent_files:\n",
    "            for file_pair in divergent_files:\n",
    "                print(\"   '\", file_pair[0], \"'\")\n",
    "                print(\"   '\", file_pair[1], \"'\")\n",
    "                print()\n",
    "\n",
    "        cls.infer_problematic_file(divergent_files)\n",
    "\n",
    "    @classmethod\n",
    "    def infer_problematic_file(cls, divergent_files):\n",
    "        file_counts = Counter(file for pair in divergent_files for file in pair)\n",
    "        most_common = file_counts.most_common(1)\n",
    "\n",
    "        if most_common:\n",
    "            problematic_file, count = most_common[0]\n",
    "            print(f\"\\nVerifique o arquivo abaixo, esteve presente em {count} pares divergentes:\")\n",
    "            print(f\"    '{problematic_file}'\")\n",
    "        else:\n",
    "            print(\"\\nNão foi possível inferir um arquivo problemático.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_specific_files(file1, file2, show_details=True):\n",
    "        print(f\"Comparando: {file1} e {file2}\")\n",
    "        dataset1 = JSONFileManager.load_json(JSONFileManager.data_folder + file1)\n",
    "        dataset2 = JSONFileManager.load_json(JSONFileManager.data_folder + file2)\n",
    "        differences = JSONFileManager.compare_lists_of_dictionaries(dataset1, dataset2)\n",
    "        num_differences = len(differences)\n",
    "\n",
    "        if num_differences == 0:\n",
    "            print(f\"Os arquivos {file1} e {file2} são idênticos.\")\n",
    "        else:\n",
    "            print(f\"Encontradas {num_differences} diferenças entre {file1} e {file2}.\")\n",
    "\n",
    "            if show_details:\n",
    "                print(\"    Detalhes das diferenças:\")\n",
    "                for diff in differences:\n",
    "                    print(f\"    {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total de arquivos de datasets: 6\n",
      "\n",
      "Resumo das Comparações:\n",
      "Total de comparações por pares realizadas: 15\n",
      "Quantidade de pares de arquivos idênticos: 10\n",
      "Pares idênticos:\n",
      "   ' output_py_gpu_multithreads.json '\n",
      "   ' output_py_cpu_multithreads.json '\n",
      "\n",
      "   ' output_py_gpu_multithreads.json '\n",
      "   ' output_py_gpu_singlethread.json '\n",
      "\n",
      "   ' output_py_gpu_multithreads.json '\n",
      "   ' output_py_cpu_singlethread.json '\n",
      "\n",
      "   ' output_py_gpu_multithreads.json '\n",
      "   ' output_multithreads_py.json '\n",
      "\n",
      "   ' output_py_cpu_multithreads.json '\n",
      "   ' output_py_gpu_singlethread.json '\n",
      "\n",
      "   ' output_py_cpu_multithreads.json '\n",
      "   ' output_py_cpu_singlethread.json '\n",
      "\n",
      "   ' output_py_cpu_multithreads.json '\n",
      "   ' output_multithreads_py.json '\n",
      "\n",
      "   ' output_py_gpu_singlethread.json '\n",
      "   ' output_py_cpu_singlethread.json '\n",
      "\n",
      "   ' output_py_gpu_singlethread.json '\n",
      "   ' output_multithreads_py.json '\n",
      "\n",
      "   ' output_py_cpu_singlethread.json '\n",
      "   ' output_multithreads_py.json '\n",
      "\n",
      "Quantidade de pares de arquivos divergentes: 5\n",
      "   ' output_py_gpu_multithreads.json '\n",
      "   ' output_go_cpu_multithreads.json '\n",
      "\n",
      "   ' output_go_cpu_multithreads.json '\n",
      "   ' output_py_cpu_multithreads.json '\n",
      "\n",
      "   ' output_go_cpu_multithreads.json '\n",
      "   ' output_py_gpu_singlethread.json '\n",
      "\n",
      "   ' output_go_cpu_multithreads.json '\n",
      "   ' output_py_cpu_singlethread.json '\n",
      "\n",
      "   ' output_go_cpu_multithreads.json '\n",
      "   ' output_multithreads_py.json '\n",
      "\n",
      "\n",
      "Verifique o arquivo abaixo, esteve presente em 5 pares divergentes:\n",
      "    'output_go_cpu_multithreads.json'\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "filemanager = JSONFileManager()\n",
    "filemanager.compare_all_json_pairs(show_details=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparando: output_go_cpu_multithreads.json e output_py_gpu_multithreads.json\n",
      "Encontradas 3 diferenças entre output_go_cpu_multithreads.json e output_py_gpu_multithreads.json.\n",
      "    Detalhes das diferenças:\n",
      "        Divergência no conteúdo das listas em ./../data/\n",
      "        34 dicionário(s) únicos na primeira lista\n",
      "        34 dicionário(s) únicos na segunda lista\n"
     ]
    }
   ],
   "source": [
    "filemanager.compare_specific_files('output_go_cpu_multithreads.json', \n",
    "                                   'output_py_gpu_multithreads.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar a classe JSONFileManager\n",
    "filemanager = JSONFileManager()\n",
    "\n",
    "# Carregar os datasets\n",
    "dataset_stpycpu = filemanager.load_json('output_py_cpu_singlethread.json')\n",
    "dataset_stpygpu = filemanager.load_json('output_py_gpu_singlethread.json')\n",
    "dataset_mtpycpu = filemanager.load_json('output_multithreads_py.json')\n",
    "dataset_mtgocpu = filemanager.load_json('output_multithreading_golang.json')\n",
    "\n",
    "# Comparar os datasets\n",
    "differences_stpy = filemanager.compare_lists_of_dictionaries(dataset_stpycpu, dataset_stpygpu)\n",
    "differences_mtpy_mtgo = filemanager.compare_lists_of_dictionaries(dataset_mtpycpu, dataset_mtgocpu)\n",
    "\n",
    "file1 = 'output_py_cpu_singlethread.json'\n",
    "file2 = 'output_multithreads_py.json'\n",
    "# Imprimir as diferenças encontradas (se houver)\n",
    "print(f\"Diferenças entre {file1} e {file2}:\")\n",
    "for diff in differences_stpy:\n",
    "    print(diff)\n",
    "\n",
    "print(f\"\\nDiferenças entre {file1} e output_multithreading_golang.json:\")\n",
    "for diff in differences_mtpy_mtgo:\n",
    "    print(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stpycpu[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mtpycpu[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o conteúdo do arquivo 'output.json' escolhido para popular a variável dict_list\n",
    "# Instanciar a classe JSONFileManager\n",
    "filemanager = JSONFileManager()\n",
    "\n",
    "source_file = 'dict_list_fioce.json'\n",
    "dataset = filemanager.load_json(source_file)\n",
    "print(f\"{len(dataset)} currículos carregados em lista de dicionários\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 4: Modelo de Graph Machine Learning</b>\n",
    "\n",
    "(Final do Upstream)\n",
    "\n",
    "    Aplicação: Segmentar e classificar publicações na Classificação do CNPq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No domínio da computação e, especificamente, no contexto de processamento de dados e fluxos de trabalho, \"upstream\" e \"downstream\" são frequentemente utilizados para descrever a origem e o destino dos dados ou a sequência de operações. As traduções mais adequadas para o português, considerando esse contexto, seriam:\n",
    "\n",
    "- **Upstream**: \"Montante\" ou \"a montante\"\n",
    "  - Refere-se à **origem dos dados** ou a etapas anteriores em um processo ou fluxo de trabalho. Por exemplo, em desenvolvimento de software, um projeto \"upstream\" é o projeto principal de onde vem o código-fonte.\n",
    "\n",
    "- **Downstream**: \"Jusante\" ou \"a jusante\"\n",
    "  - Refere-se ao **destino dos dados** ou a etapas subsequentes em um processo ou fluxo de trabalho. No contexto de desenvolvimento de software, um projeto \"downstream\" é geralmente um projeto derivado que pega o código-fonte de um projeto \"upstream\" e o adapta ou expande.\n",
    "\n",
    "Esses termos são emprestados da terminologia fluvial, onde \"a montante\" refere-se à parte do rio de onde a água flui e \"a jusante\" refere-se à direção para a qual a água está fluindo.\n",
    "\n",
    "No entanto, é importante notar que, em muitos contextos técnicos e literaturas em português, especialmente em campos como TI e desenvolvimento de software, os termos \"upstream\" e \"downstream\" são frequentemente mantidos em sua forma original em inglês devido à sua aceitação e compreensão amplamente difundidas na comunidade técnica.\n",
    "\n",
    "Portanto, na literatura acadêmica relativa à inteligência artificial (IA), as terminologias \"upstream\" e \"downstream\" são frequentemente empregadas para descrever diferentes etapas ou componentes do pipeline de processamento ou tarefas relacionadas. A divisão entre atividades \"upstream\" e \"downstream\" pode ser entendida da seguinte forma:\n",
    "\n",
    "\n",
    "### Processos de Upstream:\n",
    "Os processos contendo as atividades, não só de IA, mas envolvendo tratamento de dados para gerar conhecimento, relacionados ao \"upstream\" normalmente se referem às etapas iniciais do pipeline ou processo:\n",
    "\n",
    "1. **Delinear os problemas**: Esta fase por vezes não é mencionada nas aplicações práticas, por já ter sido feita previamente, mas sem saber onde se quer, e mais ainda, porque se quer chegar a algum resultado nenhum empreendimento será eficiente, e só terá sucesso esporádico e insustentável a longo prazo.\n",
    "\n",
    "2. **Revisar Literatura**: Com base nos temas e problemas em estudos são efetuadas buscas e prospecções de informação para situar o estado da arte no mundo para tratamento de problemas semelhantes, seja no contexto, ou nas técnicas utilizadas para tratar as questões relacionadas com os problemas.\n",
    "\n",
    "3. **Estabelecer hipóteses**: Mais voltada para aplicações acadêmicas onde é preciso estabelecer bem os pontos de partida, para permitir a evolução futura, esta é a fase onde se condensam as visões de mundo em hipóteses a serem testadas com base em experimentação controlada.\n",
    "\n",
    "4. **Coletar Dados**: Esta é a fase onde os dados brutos são coletados de várias fontes. Pode ser através de sensores, scraping da web, bancos de dados existentes, entre outros.\n",
    "\n",
    "5. **Pré-processar Dados**: Uma vez coletados, os dados passam por várias transformações para garantir sua qualidade. Isso inclui limpeza, normalização, tratamento de valores faltantes, etc.\n",
    "\n",
    "6. **Analisar Exploratoriamente os Dados (AED)**: Os dados são explorados para entender suas características, distribuições e potenciais anomalias. Isso pode envolver visualização de dados, cálculo de estatísticas descritivas, etc.\n",
    "\n",
    "7. **Engendrar Características (Feature Engineering)**: Baseando-se na AED, características relevantes são extraídas ou construídas a partir dos dados brutos para alimentar modelos de IA.\n",
    "\n",
    "### Processos Downstream:\n",
    "Já os processos com as atividades \"downstream\" são aquelas que ocorrem após as etapas iniciais e estão mais orientadas para a aplicação ou utilização do modelo de IA:\n",
    "\n",
    "1. **Modelar**: São pensados arranjos de dados, na forma de interpretações da realidade, de acordo com os objetivos pretendidos para as aplicações da solução em estudo.\n",
    "\n",
    "2. **Treinar**: Os algoritmos de IA são aplicados aos esquemas de dados preparados para treinar os modelos. Isso envolve a seleção de algoritmos, treinamento e validação dos modelos.\n",
    "\n",
    "3. **Avaliar**: Uma vez treinado, o desempenho do modelo é avaliado usando métricas adequadas, e comparado com benchmarks ou modelos de base.\n",
    "\n",
    "4. **Implantar**: Os modelos que atendem aos critérios de avaliação são então implantados em ambientes de produção para serem usados em aplicações reais.\n",
    "\n",
    "5. **Monitorar e Manter**: Depois de implantados, os modelos precisam ser monitorados para garantir que continuem a fornecer resultados precisos. Eles podem precisar ser re-treinados ou ajustados com base em novos dados.\n",
    "\n",
    "6. **Aplicar na realidade**: Isso se refere ao uso real do modelo em aplicações específicas, seja em sistemas de recomendação, diagnósticos médicos, carros autônomos, etc.\n",
    "\n",
    "7. **Evoluir**: A partir da análise constante dos dados e dos resultados gerados, define-se como refatorar os softwares, adaptar as análises a fim de evoluir o modelo para melhorar seus resultados.\n",
    "\n",
    "Em resumo, as atividades \"upstream\" estão mais centradas na preparação e garantia da qualidade dos dados, enquanto as atividades \"downstream\" estão focadas na criação, avaliação e aplicação de modelos de IA. \n",
    "\n",
    "A distinção entre essas duas categorias ajuda os profissionais e pesquisadores a estruturar e organizar processos complexos envolvidos no desenvolvimento e implantação de soluções de IA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular similaridade e persistir relações semânticas -> Neo4j\n",
    "\n",
    "    Neo4jService\n",
    "    CosineSimilarityRelationship\n",
    "    HierarchicalSemanticMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import psutil\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Configuração do logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "\n",
    "class Neo4jService:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def execute_query(self, query, parameters=None):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(query, parameters)\n",
    "            return [record.data() for record in result]\n",
    "\n",
    "    # Anteriormente: session.read_transaction(...)\n",
    "    # Atualizado para: session.execute_read(...)\n",
    "    def execute_read(self, query, parameters=None):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(query, parameters)\n",
    "            return [record.data() for record in result]\n",
    "\n",
    "    # Anteriormente: session.write_transaction(...)\n",
    "    # Atualizado para: session.execute_write(...)\n",
    "    def execute_write(self, query, parameters=None):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(query, parameters)\n",
    "            return [record.data() for record in result]\n",
    "\n",
    "    def run_clustering_algorithm(self, projection_name, node_labels, relationship_types):\n",
    "        # Execute a clustering algorithm like K-Means\n",
    "        k = 8  # For the eight 'GrandeÁrea' clusters\n",
    "        self.neo4j_service.run_kmeans(projection_name, k)\n",
    "        # Fetch the cluster labels from the graph\n",
    "        cluster_labels_query = \"MATCH (n) RETURN id(n) AS id, n.clusterId AS cluster\"\n",
    "        results = self.execute_read(cluster_labels_query)\n",
    "        clusters = {record['id']: record['cluster'] for record in results}\n",
    "        return clusters\n",
    "\n",
    "\n",
    "    def find_available_procedures(self):\n",
    "        query = \"SHOW PROCEDURES\"\n",
    "        return self.execute_read(query)\n",
    "    \n",
    "    def find_available_functions(self):\n",
    "        query = \"SHOW FUNCTIONS\"\n",
    "        return self.execute_read(query)\n",
    "\n",
    "    def find_related_procedures(self, keyword):\n",
    "        # Recuperamos todos os procedimentos disponíveis\n",
    "        all_procedures = self.find_available_procedures()\n",
    "\n",
    "        # Filtramos a lista de procedimentos com base na palavra-chave\n",
    "        related_procedures = [proc for proc in all_procedures if keyword.lower() in proc['name'].lower()]\n",
    "\n",
    "        return related_procedures\n",
    "\n",
    "    def get_procedure_signatures(self, procedures_list):\n",
    "        \"\"\"\n",
    "        Retrieves the signatures for a list of procedures.\n",
    "\n",
    "        :param procedures_list: A list of strings representing the procedure names to fetch signatures for.\n",
    "        :return: A dictionary with the procedure names as keys and their signatures as values.\n",
    "        \"\"\"\n",
    "        # Start a string to build the Cypher query for multiple procedures\n",
    "        if len(procedures_list) > 1:\n",
    "            query_conditions = \" OR \".join([f\"name = '{procedure}'\" for procedure in procedures_list])\n",
    "        else:\n",
    "            query_conditions = f\"name = '{procedures_list[0]}'\"\n",
    "\n",
    "        get_signatures_query = f\"\"\"\n",
    "        SHOW PROCEDURES\n",
    "        YIELD name, signature\n",
    "        WHERE {query_conditions}\n",
    "        RETURN name, signature;\n",
    "        \"\"\"\n",
    "\n",
    "        # print(get_signatures_query)\n",
    "\n",
    "        # Execute the query and fetch the results\n",
    "        try:\n",
    "            results = self.execute_read(get_signatures_query)\n",
    "            print(f\"Quantidade de resultados encontrados: {len(results)}\")\n",
    "            # print(results)\n",
    "            signatures = {record[\"name\"]: record[\"signature\"] for record in results}\n",
    "            # for i,j in results:\n",
    "            #     print(f\"PROCEDIMENTO:{i}\")\n",
    "            #     print(f\"  ASSINATURA:{j}\")\n",
    "            #     print()\n",
    "            pprint(signatures,width=140)\n",
    "            return signatures\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching procedure signatures: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def project_graph(self, projection_name, node_labels, relationship_types):\n",
    "        logging.info(f\"Projecting graph {projection_name} with labels {node_labels} and relationships {relationship_types}\")\n",
    "        exists_query = f\"\"\"\n",
    "        CALL gds.graph.exists($projection_name)\n",
    "        YIELD exists\n",
    "        RETURN exists\n",
    "        \"\"\"\n",
    "        exists_result = self.execute_read(exists_query, {'projection_name': projection_name})\n",
    "        exists = False if not exists_result else exists_result[0]['exists']\n",
    "        \n",
    "        if not exists:\n",
    "            create_projection_query = \"\"\"\n",
    "            CALL gds.graph.project($projection_name, $node_labels, $relationship_types)\n",
    "            \"\"\"\n",
    "            self.execute_write(create_projection_query, {\n",
    "                'projection_name': projection_name,\n",
    "                'node_labels': node_labels,\n",
    "                'relationship_types': relationship_types\n",
    "            })            \n",
    "        else:\n",
    "            print(f\"Graph projection '{projection_name}' already exists.\")\n",
    "\n",
    "        # Após a criação ou confirmação da projeção, conte as relações SIMILAR\n",
    "        similar_relations_query = \"MATCH ()-[r:SIMILAR]->() RETURN COUNT(r) AS similarCount\"\n",
    "        similar_count_result = self.execute_read(similar_relations_query)\n",
    "        similar_count = similar_count_result[0]['similarCount'] if similar_count_result else 0\n",
    "        logging.info(f\"{similar_count:>4} relacionamentos SIMILAR na projeção {projection_name}\")            \n",
    "\n",
    "    def run_kmeans(self, projection_name, k):\n",
    "        \"\"\"\n",
    "        Executa o algoritmo K-Means na projeção especificada do grafo.\n",
    "\n",
    "        :param projection_name: O nome da projeção do grafo onde o K-Means será executado.\n",
    "        :param k: O número de clusters para o K-Means.\n",
    "        \"\"\"\n",
    "        kmeans_query = f\"\"\"\n",
    "        CALL gds.kmeans.write('{projection_name}', {{\n",
    "            nodeProjection: ['Publicacao', 'Especialidade', 'Subárea'],\n",
    "            nodeProperties: 'embedding',\n",
    "            relationshipTypes: ['SIMILAR'],\n",
    "            writeProperty: 'clusterId',\n",
    "            k: $k,\n",
    "            randomSeed: 42\n",
    "        }})\n",
    "        YIELD nodePropertiesWritten, modelInfo\n",
    "        RETURN nodePropertiesWritten, modelInfo.centroids, modelInfo.iterations, modelInfo.didConverge\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.execute_write(kmeans_query, {'projection_name': projection_name, 'k': k})\n",
    "            success = result.single()['didConverge']\n",
    "            return success\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while running kmeans: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    def run_louvain(self, projection_name):\n",
    "        \"\"\"\n",
    "        Executa o algoritmo de Louvain na projeção especificada do grafo.\n",
    "\n",
    "        :param projection_name: O nome da projeção do grafo onde o Louvain será executado.\n",
    "        \"\"\"\n",
    "        louvain_query = \"\"\"\n",
    "        CALL gds.louvain.write($projection_name, {\n",
    "            nodeLabels: ['Publicacao', 'Especialidade', 'Subárea'],  // Especifica os rótulos dos nós\n",
    "            relationshipTypes: ['SIMILAR'],        // Especifica os tipos de relações\n",
    "            nodeWeightProperty: 'weight',          // A propriedade de nó que contém o peso para o algoritmo de Louvain\n",
    "            relationshipWeightProperty: 'weight',  // A propriedade da relação que contém o peso\n",
    "            writeProperty: 'communityId'           // Propriedade do nó onde o ID da comunidade será armazenado\n",
    "        })\n",
    "        YIELD communityCount, modularity, ranLevels, writeProperty\n",
    "        RETURN communityCount, modularity, ranLevels, writeProperty\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Processar e retornar o resultado conforme necessário\n",
    "            result = self.execute_write(louvain_query, {'projection_name': projection_name})\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while running Louvain: {e}\")\n",
    "            return None\n",
    "\n",
    "    def run_label_propagation(self, projection_name):\n",
    "        \"\"\"\n",
    "        Executa o algoritmo de propagação de rótulos (label propagation) na projeção especificada do grafo.\n",
    "\n",
    "        :param projection_name: O nome da projeção do grafo onde a propagação de rótulos será executada.\n",
    "        \"\"\"\n",
    "        label_propagation_query = \"\"\"\n",
    "        CALL gds.labelPropagation.write($projection_name, {\n",
    "            nodeLabels: ['Publicacao', 'Especialidade', 'Subárea'],  // Especifica os rótulos dos nós\n",
    "            relationshipTypes: ['SIMILAR'],  // Especifica os tipos de relações\n",
    "            writeProperty: 'communityLabel'  // Propriedade do nó onde o rótulo da comunidade será armazenado\n",
    "        })\n",
    "        YIELD communityCount, ranIterations, didConverge, writeProperty\n",
    "        RETURN communityCount, ranIterations, didConverge, writeProperty\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.execute_write(label_propagation_query, {'projection_name': projection_name})\n",
    "            # Processar e retornar o resultado conforme necessário\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while running Label Propagation: {e}\")\n",
    "            return None\n",
    "\n",
    "    def optimize_and_evaluate_modularity(self, graph_name):\n",
    "        # Otimiza a modularidade e retorna as estatísticas\n",
    "        optimize_query = f\"\"\"\n",
    "        CALL gds.modularityOptimization.write('{graph_name}', {{\n",
    "            writeProperty: 'communityIdOptimized'\n",
    "        }})\n",
    "        YIELD communityCount, modularidade, ranLevels\n",
    "        RETURN communityCount, modularidade, ranLevels\n",
    "        \"\"\"\n",
    "        optimization_result = self.execute_write(optimize_query)\n",
    "        \n",
    "        # Calcula estatísticas de modularidade após a otimização\n",
    "        stats_query = f\"\"\"\n",
    "        CALL gds.modularity.stats('{graph_name}', {{\n",
    "            nodeLabels: ['Publicacao', 'Especialidade', 'Subárea'],\n",
    "            relationshipTypes: ['SIMILAR']\n",
    "        }})\n",
    "        YIELD modularity\n",
    "        RETURN modularity\n",
    "        \"\"\"\n",
    "        stats_result = self.execute_read(stats_query)\n",
    "        \n",
    "        return {\n",
    "            'optimization': optimization_result.single() if optimization_result.peek() else None,\n",
    "            'modularityStats': stats_result.single()['modularity'] if stats_result.peek() else None\n",
    "        }\n",
    "\n",
    "    def drop_named_graph(self, projection_name):\n",
    "        drop_graph_query = f\"\"\"\n",
    "        CALL gds.graph.drop($projection_name)\n",
    "        \"\"\"\n",
    "        self.execute_write(drop_graph_query, {\n",
    "            'projection_name': projection_name\n",
    "        })\n",
    "\n",
    "    def calculate_modularity(self, projection_name, community_detection_method='kmeans', k=8):\n",
    "        \"\"\"\n",
    "        Identifica comunidades usando o método especificado e calcula a modularidade do grafo.\n",
    "\n",
    "        :param projection_name: O nome da projeção do grafo.\n",
    "        :param community_detection_method: O método de detecção de comunidades a ser usado ('kmeans' por padrão).\n",
    "        :param k: O número de comunidades a identificar se o método kmeans for utilizado.\n",
    "        :return: A modularidade média do grafo.\n",
    "        \"\"\"\n",
    "        if community_detection_method == 'kmeans':\n",
    "            success = self.run_kmeans(projection_name, k)\n",
    "            if not success:\n",
    "                print(f\"KMeans failed to converge for projection {projection_name}\")\n",
    "                return None\n",
    "            community_property = 'clusterId'\n",
    "        else:\n",
    "            # Implementação para outros métodos de detecção de comunidades pode ser adicionada aqui\n",
    "            raise NotImplementedError(f\"Community detection method {community_detection_method} is not implemented.\")\n",
    "\n",
    "        modularity_query = f\"\"\"\n",
    "        CALL gds.modularity.stream($projection_name, {{\n",
    "            nodeLabels: ['Publicacao', 'Especialidade', 'Subárea'],\n",
    "            relationshipTypes: ['SIMILAR'],\n",
    "            communityProperty: $community_property\n",
    "        }})\n",
    "        YIELD communityId, modularity\n",
    "        RETURN AVG(modularity) AS averageModularity\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'projection_name': projection_name,\n",
    "            'community_property': community_property\n",
    "        }\n",
    "        result = self.execute_read(modularity_query, params)\n",
    "        if result:  # Verifique se há resultados antes de tentar acessar\n",
    "            average_modularity = result.single().get('averageModularity')\n",
    "            return average_modularity\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def evaluate_classification_accuracy(self, projection_name):\n",
    "        \"\"\"\n",
    "        Avalia a precisão da classificação das publicações em GrandeÁrea com base nas relações SIMILAR.\n",
    "        \"\"\"\n",
    "        accuracy_query = \"\"\"\n",
    "        MATCH (p:Publicacao)-[:SIMILAR]->(s:Subárea)-[:CONTÉM_SUBÁREA]->(a:Área)-[:CONTÉM_ÁREA]->(g:GrandeÁrea)\n",
    "        WITH p, COLLECT(DISTINCT g) AS matchedGrandeAreas\n",
    "        MATCH (p)-[:CONTÉM_GRANDEÁREA]->(expected:GrandeÁrea)\n",
    "        WITH p, matchedGrandeAreas, COLLECT(DISTINCT expected) AS expectedGrandeAreas\n",
    "        WITH p, matchedGrandeAreas = expectedGrandeAreas AS isCorrect\n",
    "        WITH SUM(CASE WHEN isCorrect THEN 1 ELSE 0 END) AS correctClassifications, COUNT(p) AS totalPublicacoes\n",
    "        RETURN correctClassifications, \n",
    "            1.0 * correctClassifications / totalPublicacoes AS accuracy\n",
    "        \"\"\"\n",
    "        result = self.execute_read(accuracy_query)\n",
    "        if result:  # Assume there is always at least one result\n",
    "            return {\n",
    "                \"correctClassifications\": result[0]['correctClassifications'],\n",
    "                \"accuracy\": result[0]['accuracy']\n",
    "            }\n",
    "        else:\n",
    "            return {\"correctClassifications\": 0, \"accuracy\": 0}\n",
    "\n",
    "    def calculate_graph_density(self, projection_name):\n",
    "        \"\"\"\n",
    "        Calcula a densidade da projeção gráfica especificada.\n",
    "\n",
    "        :param project_name: O nome da projeção do grafo a ser avaliada.\n",
    "        :return: A densidade do grafo.\n",
    "        \"\"\"\n",
    "        density_query = f\"\"\"\n",
    "        MATCH (n)--(m)\n",
    "        WHERE (n:Publicacao OR n:Especialidade OR n:Subárea) AND (m:Publicacao OR m:Especialidade OR m:Subárea)\n",
    "        WITH COUNT(DISTINCT n) AS numNodes, COUNT(DISTINCT (n)-[:SIMILAR]-(m)) AS numEdges\n",
    "        RETURN (2.0 * numEdges) / (numNodes * (numNodes - 1)) AS density\n",
    "        \"\"\"\n",
    "        result = self.execute_read(density_query, {'projection_name': projection_name})\n",
    "        return result.single()['density'] if result.peek() else 0\n",
    "\n",
    "    def calculate_degree_distribution(self, projection_name):\n",
    "        \"\"\"\n",
    "        Calcula a distribuição de graus da projeção gráfica especificada.\n",
    "\n",
    "        :param project_name: O nome da projeção do grafo a ser avaliada.\n",
    "        :return: Um dicionário com a distribuição de graus do grafo.\n",
    "        \"\"\"\n",
    "        degree_query = f\"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE n:Publicacao OR n:Especialidade OR n:Subárea\n",
    "        WITH n, SIZE((n)-[:SIMILAR]-()) AS degree\n",
    "        RETURN degree, COUNT(n) AS count\n",
    "        ORDER BY degree\n",
    "        \"\"\"\n",
    "        result = self.execute_read(degree_query, {'projection_name': projection_name})\n",
    "        return {record['degree']: record['count'] for record in result} if result.peek() else {}\n",
    "\n",
    "    def calculate_overlap_score(self, projection_name):\n",
    "        \"\"\"\n",
    "        Calcula o score de sobreposição para cada publicação em uma projeção nomeada do grafo,\n",
    "        indicando com quantas outras publicações ela compartilha subáreas ou especialidades similares.\n",
    "\n",
    "        :param projection_name: O nome da projeção do grafo onde a sobreposição será calculada.\n",
    "        \"\"\"\n",
    "        overlap_score_query = f\"\"\"\n",
    "        USE gds.graph('{projection_name}')  // Use a projeção nomeada\n",
    "        MATCH (p:Publicacao)-[:SIMILAR]->(subject)\n",
    "        WITH p, collect(subject) AS similarSubjects\n",
    "        \n",
    "        UNWIND similarSubjects AS simSubject\n",
    "        MATCH (simSubject)<-[:SIMILAR]-(otherPub:Publicacao)\n",
    "        WITH p, simSubject, collect(otherPub) AS otherPubsSharingSubject\n",
    "        \n",
    "        RETURN id(p) AS PublicationID, id(simSubject) AS SubjectID, size(otherPubsSharingSubject) AS OverlapScore\n",
    "        ORDER BY PublicationID, OverlapScore DESC\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Executar a consulta e retornar o resultado\n",
    "            result = self.execute_read(overlap_score_query)\n",
    "            # Converter o resultado em uma lista de dicionários\n",
    "            return [record for record in result]\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while calculating overlap score: {e}\")\n",
    "            return []\n",
    "\n",
    "    def count_isolated_nodes(self, label):\n",
    "        \"\"\"\n",
    "        Conta os nós do tipo especificado que não têm nenhuma aresta ligada a eles.\n",
    "\n",
    "        :param label: O rótulo do nó a ser verificado.\n",
    "        :return: O número de nós isolados.\n",
    "        \"\"\"\n",
    "        isolated_nodes_query = f\"\"\"\n",
    "        MATCH (n:{label})\n",
    "        WHERE NOT (n)--()\n",
    "        RETURN COUNT(n) AS isolatedCount\n",
    "        \"\"\"\n",
    "        result = self.execute_read(isolated_nodes_query)\n",
    "        if result:\n",
    "            return result[0]['isolatedCount']\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def count_excessive_similar_edges(self, label, max_edges=2):\n",
    "        excessive_edges_query = f\"\"\"\n",
    "        MATCH (n:{label})-[:SIMILAR]->(m)\n",
    "        WITH n, COUNT(m) AS relCount\n",
    "        WHERE relCount > $max_edges\n",
    "        RETURN COUNT(n) AS excessiveCount\n",
    "        \"\"\"\n",
    "        result = self.execute_read(excessive_edges_query, {'max_edges': max_edges})\n",
    "        # Como execute_read retorna uma lista de dicionários, obtenha o primeiro item da lista e então o valor de 'excessiveCount'\n",
    "        return result[0]['excessiveCount'] if result else 0\n",
    "\n",
    "    def evaluate_projection(self, projection_name):\n",
    "        logging.info(f\"Starting evaluation for projection: {projection_name}\")\n",
    "\n",
    "        metrics = {\n",
    "            'correctClassifications': 0,\n",
    "            'accuracy': 0.0,\n",
    "            'isolated_nodes': 0,\n",
    "            'excessive_similar_edges': 0,\n",
    "            'modularity': 0.0,\n",
    "            'overlap_score': 0.0,\n",
    "            'classification_accuracy': 0.0\n",
    "        }\n",
    "        aggregate_score = 0.0\n",
    "\n",
    "        try:\n",
    "            node_count_query = \"MATCH (p:Publicacao) RETURN COUNT(p) AS nodeCount\"\n",
    "            node_count_result = self.execute_read(node_count_query)\n",
    "            node_count = node_count_result[0]['nodeCount'] if node_count_result else 0\n",
    "            logging.info(f\"Node count for projection {projection_name}: {node_count}\")\n",
    "            # Log a contagem de relações SIMILAR\n",
    "            similar_relations_query = \"MATCH ()-[r:SIMILAR]->() RETURN COUNT(r) AS similarCount\"\n",
    "            similar_count_result = self.execute_read(similar_relations_query)\n",
    "            similar_count = similar_count_result[0]['similarCount'] if similar_count_result else 0\n",
    "            logging.info(f\"Number of SIMILAR relationships in projection {projection_name}: {similar_count}\")\n",
    "\n",
    "            if node_count == 0:\n",
    "                logging.error(f\"No nodes found in projection: {projection_name}. Division by zero error is likely.\")\n",
    "                return {'error': 'Division by zero'}, 0\n",
    "\n",
    "            accuracy_query = \"\"\"\n",
    "            MATCH (p:Publicacao)-[:SIMILAR]->(s:Subárea)-[:CONTÉM_SUBÁREA]->(a:Área)-[:CONTÉM_ÁREA]->(g:GrandeÁrea)\n",
    "            WITH p, COLLECT(DISTINCT g) AS matchedGrandeAreas\n",
    "            MATCH (p)-[:CONTÉM_GRANDEÁREA]->(expected:GrandeÁrea)\n",
    "            WITH p, matchedGrandeAreas, COLLECT(DISTINCT expected) AS expectedGrandeAreas\n",
    "            WHERE expectedGrandeAreas = matchedGrandeAreas\n",
    "            RETURN COUNT(DISTINCT p) AS correctClassifications, \n",
    "                1.0 * COUNT(DISTINCT p) / $node_count AS accuracy\n",
    "            \"\"\"\n",
    "            accuracy_result = self.execute_read(accuracy_query, {'node_count': node_count})\n",
    "            if accuracy_result:\n",
    "                metrics['correctClassifications'] = accuracy_result[0]['correctClassifications']\n",
    "                metrics['accuracy'] = accuracy_result[0]['accuracy']\n",
    "                logging.info(f\"Correct classifications: {metrics['correctClassifications']}, Accuracy: {metrics['accuracy']}\")\n",
    "\n",
    "            # Computar outras métricas e atualizar dicionário das métricas\n",
    "            params = {'node_count': node_count}\n",
    "            accuracy_result = self.execute_read(accuracy_query, params)\n",
    "            correct_classifications = accuracy_result[0]['correctClassifications'] if accuracy_result else 0\n",
    "            accuracy = accuracy_result[0]['accuracy'] if accuracy_result else 0.0\n",
    "\n",
    "            # Agregue as métricas em um dicionário\n",
    "            metrics = {\n",
    "                'correctClassifications': correct_classifications,\n",
    "                'accuracy': accuracy,\n",
    "                'isolated_nodes': self.count_isolated_nodes('Publicacao'),\n",
    "                'excessive_similar_edges': self.count_excessive_similar_edges('Publicacao'),\n",
    "                'modularity': self.calculate_modularity(projection_name),\n",
    "                'overlap_score': self.calculate_overlap_score(projection_name),\n",
    "                'classification_accuracy': self.evaluate_classification_accuracy(projection_name)\n",
    "            }\n",
    "            # Calcule a pontuação agregada baseada em suas métricas\n",
    "            aggregate_score = (metrics['accuracy']  # ou qualquer fórmula que você usa para calcular a pontuação agregada\n",
    "                            - metrics.get('isolated_nodes', 0)\n",
    "                            - metrics.get('excessive_similar_edges', 0)\n",
    "                            + metrics.get('modularity', 0)\n",
    "                            + metrics.get('overlap_score', 0)\n",
    "                            + metrics.get('classification_accuracy', 0))\n",
    "\n",
    "            # # Calculate aggregate score based on your metrics\n",
    "            # aggregate_score = (\n",
    "            #     metrics['accuracy']\n",
    "            #     - metrics['isolated_nodes']\n",
    "            #     - metrics['excessive_similar_edges']\n",
    "            #     + metrics['modularity']\n",
    "            #     + metrics['overlap_score']\n",
    "            #     + metrics['classification_accuracy']\n",
    "            # )\n",
    "\n",
    "            # Inclua a chave 'aggregate_score' no dicionário metrics\n",
    "            metrics['aggregate_score'] = aggregate_score\n",
    "\n",
    "            logging.info(f\"Finished evaluation for projection: {projection_name} with metrics: {metrics}\")\n",
    "            return metrics, aggregate_score\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while evaluating projection {projection_name}: {e}\")\n",
    "            metrics['error'] = str(e)\n",
    "\n",
    "        logging.info(f\"Finished evaluation for projection: {projection_name} with metrics: {metrics}\")\n",
    "        return metrics, aggregate_score\n",
    "\n",
    "    def list_projections(self):\n",
    "        \"\"\"\n",
    "        Lists all the graph projections in the connected Neo4j database.\n",
    "\n",
    "        Returns:\n",
    "            A list of dictionaries, each representing a graph projection with its details.\n",
    "        \"\"\"\n",
    "        list_projections_query = \"\"\"\n",
    "        CALL gds.graph.list()\n",
    "        YIELD graphName, nodeCount, relationshipCount, schema\n",
    "        RETURN graphName, nodeCount, relationshipCount, schema;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.execute_read(list_projections_query)\n",
    "            return [record for record in result]\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while listing projections: {e}\")\n",
    "            return []\n",
    "\n",
    "    def check_and_drop_projection(self, projection_name):\n",
    "        # Verifica se a projeção existe\n",
    "        exists_query = \"CALL gds.graph.exists($projection_name) YIELD exists RETURN exists\"\n",
    "        exists_result = self.execute_read(exists_query, {'projection_name': projection_name})\n",
    "        \n",
    "        # Como execute_read retorna uma lista, devemos verificar se a lista não está vazia\n",
    "        if exists_result and exists_result[0]['exists']:  # Acessar o primeiro resultado e a chave 'exists'\n",
    "            drop_query = \"CALL gds.graph.drop($projection_name)\"\n",
    "            self.execute_write(drop_query, {'projection_name': projection_name})\n",
    "            print(f\"Graph projection '{projection_name}' was dropped.\")\n",
    "        else:\n",
    "            print(f\"Graph projection '{projection_name}' does not exist.\")\n",
    "\n",
    "    def list_projections(self):\n",
    "        \"\"\"\n",
    "        Lists all the graph projections in the connected Neo4j database.\n",
    "\n",
    "        Returns:\n",
    "            A list of dictionaries, each representing a graph projection with its details.\n",
    "        \"\"\"\n",
    "        list_projections_query = \"\"\"\n",
    "        CALL gds.graph.list()\n",
    "        YIELD graphName, nodeCount, relationshipCount, schema\n",
    "        RETURN graphName, nodeCount, relationshipCount, schema;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.execute_read(list_projections_query)\n",
    "            return [record for record in result]\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while listing projections: {e}\")\n",
    "            return []\n",
    "\n",
    "    def drop_all_projections(self):\n",
    "        \"\"\"\n",
    "        Lista e apaga todas as projeções do grafo existentes no Neo4j.\n",
    "        \"\"\"\n",
    "        # Obter a lista de todas as projeções\n",
    "        all_projections = self.list_projections()\n",
    "        \n",
    "        # Iterar sobre a lista e apagar cada projeção\n",
    "        for projection in all_projections:\n",
    "            projection_name = projection['graphName']\n",
    "            self.check_and_drop_projection(projection_name)\n",
    "\n",
    "class CosineSimilarityRelationship:        \n",
    "    def __init__(self, neo4j_service, model_name=\"default_model\"):\n",
    "        self.neo4j_service = neo4j_service\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def cosine_similarity(self, a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "    def normalize_similarity(self, similarity):\n",
    "        return (similarity + 1) / 2\n",
    "\n",
    "    def get_memory_usage(self):\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_MB = process.memory_info().rss / (1024 ** 2)\n",
    "        return memory_MB\n",
    "\n",
    "    def get_all_embeddings(self, label):\n",
    "        return self.neo4j_service.execute_query(f\"MATCH (n:{label}) RETURN id(n) AS id, n.embedding AS embedding, n.name AS name\")\n",
    "\n",
    "    def process_similarity_for_nodes(self, source_embeddings, target_embeddings, source_label, target_label, threshold, batch_size):\n",
    "        relationships_created_count = 0\n",
    "        node_pairs_count = 0\n",
    "        try:\n",
    "            for source in tqdm(source_embeddings, desc=f\"Processing similarity between {source_label} and {target_label}\"):\n",
    "                for target in target_embeddings:\n",
    "                    similarity = self.cosine_similarity(np.array(source[\"embedding\"]), np.array(target[\"embedding\"]))\n",
    "                    node_pairs_count += 1\n",
    "                    if similarity > threshold:\n",
    "                        normalized_similarity = self.normalize_similarity(similarity)\n",
    "                        query = f\"\"\"\n",
    "                        MATCH (source:{source_label}) WHERE id(source) = $source_id\n",
    "                        MATCH (target:{target_label}) WHERE id(target) = $target_id\n",
    "                        MERGE (source)-[:SIMILAR {{score: $similarity, weight: $normalized_similarity}}]->(target)\n",
    "                        \"\"\"\n",
    "                        self.neo4j_service.execute_query(query, {\n",
    "                            'source_id': source[\"id\"], 'target_id': target[\"id\"],\n",
    "                            'similarity': similarity, 'normalized_similarity': normalized_similarity\n",
    "                        })\n",
    "                        relationships_created_count += 1\n",
    "                        if relationships_created_count % batch_size == 0:\n",
    "                            logging.info(f\"Committed {batch_size} relationships for {source_label}/{target_label}.\")\n",
    "            return node_pairs_count, relationships_created_count\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred: {e}\")\n",
    "            return -1\n",
    "\n",
    "    def create_similarity_embeddings_relationships(self, threshold=0.7, batch_size=3000):\n",
    "        start_time = time.time()\n",
    "        initial_memory = self.get_memory_usage()\n",
    "        try:\n",
    "            pub_embeddings = self.get_all_embeddings(\"Publicacao\")\n",
    "            esp_embeddings = self.get_all_embeddings(\"Especialidade\")\n",
    "            sub_embeddings = self.get_all_embeddings(\"Subárea\")\n",
    "\n",
    "            total_pub = len(pub_embeddings)\n",
    "            total_esp = len(esp_embeddings)\n",
    "            total_sub = len(sub_embeddings)\n",
    "            logging.info(f\"Nodes: Publicacao {total_pub}, Especialidade {total_esp}, Subárea {total_sub}\")\n",
    "\n",
    "            pub_sub_pairs, pub_sub_rels = self.process_similarity_for_nodes(pub_embeddings, esp_embeddings, \"Publicacao\", \"Especialidade\", threshold, batch_size)\n",
    "            pub_esp_pairs, pub_esp_rels = self.process_similarity_for_nodes(pub_embeddings, sub_embeddings, \"Publicacao\", \"Subárea\", threshold, batch_size)\n",
    "            logging.info(\"Completed creating similarity relationships.\")\n",
    "        \n",
    "            final_memory = self.get_memory_usage()\n",
    "            end_time = time.time()\n",
    "            memory_difference = final_memory - initial_memory\n",
    "            processing_time = end_time - start_time\n",
    "            logging.info(f\"RAM Consumption: {np.round(memory_difference,2)} MB\")\n",
    "            logging.info(f\"Processing Time: {np.round(processing_time,2)} seconds\")\n",
    "            logging.info(f\"Current Memory Usage: {np.round(final_memory,2)} MB\")\n",
    "            logging.info(f\"Execution time for similarity calculations and relationship creation: {np.round(processing_time, 2)} seconds\")\n",
    "            logging.info(f\"Similarity threshold: {threshold}\")\n",
    "            logging.info(f\"Total node pairs analyzed: {pub_sub_pairs + pub_esp_pairs}\")\n",
    "            logging.info(f\"Node pairs Publicacao/Subárea: {pub_sub_pairs}\")\n",
    "            logging.info(f\"Node pairs Publicacao/Especialidade: {pub_esp_pairs}\")\n",
    "            logging.info(f\"Total relationships created: {pub_sub_rels + pub_esp_rels}\")\n",
    "            logging.info(f\"Relationships stabilished on nodes Publicacao/Subárea: {pub_sub_rels}\")\n",
    "            logging.info(f\"Relationships stabilished on Publicacao/Especialidade: {pub_esp_rels}\")\n",
    "        \n",
    "            return pub_esp_rels + pub_sub_rels\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while creating similarity relationships: {e}\")\n",
    "            return -1\n",
    "\n",
    "    def run_similarity_operations(self, threshold=0.7):\n",
    "        self.create_similarity_embeedings_relationships(threshold)\n",
    "        \n",
    "# Configuração inicial e parâmetros do Neo4j\n",
    "neo4j_uri = 'bolt://localhost:7687'\n",
    "neo4j_user = 'neo4j'\n",
    "neo4j_password = 'password'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Configuração do logging\n",
    "logging.basicConfig(filename='HierarchicalSemanticMatcher.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# Carregar o modelo e o tokenizer do BERT Multilíngue\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "class HierarchicalSemanticMatcher:\n",
    "    def __init__(self, neo4j_uri, neo4j_user, neo4j_password):\n",
    "        self.neo4j_service = Neo4jService(neo4j_uri, neo4j_user, neo4j_password)\n",
    "        self.cosine_similarity_service = CosineSimilarityRelationship(self.neo4j_service)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_embedding(text):\n",
    "        \"\"\"\n",
    "        Obtenha o embedding de um texto usando o modelo BERT Multilíngue.\n",
    "        Args:\n",
    "        - text (str): O texto a ser vetorizado.\n",
    "        Returns:\n",
    "        - List[float]: O embedding do texto.\n",
    "        \"\"\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True, padding=\"max_length\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "\n",
    "    # Vetorizar títulos de especialidades em dados já no Neo4j com vetores pré-treinados\n",
    "    def vectorize_specialty_names(self):\n",
    "        logging.info(\"Iniciando vetorização dos nomes das especialidades...\")\n",
    "        query = \"MATCH (e:Especialidade) RETURN e.name AS name\"\n",
    "        result = self.neo4j_service.execute_read(query)\n",
    "        embeddings = []\n",
    "\n",
    "        # Converter resultados em uma lista para poder usar o tqdm\n",
    "        result_list = [record['name'] for record in result]\n",
    "        \n",
    "        logging.info(f\"Total de nomes a serem vetorizados: {len(result_list)}\")\n",
    "\n",
    "        if not result_list:\n",
    "            logging.warning(\"Nenhum nome de especialidade foi recuperado da base de dados.\")\n",
    "            return embeddings\n",
    "\n",
    "        for name in tqdm(result_list, desc=\"Vetorizando nomes de especialidades\"):\n",
    "            embedding = self.get_embedding(name) # vetoriza\n",
    "            embeddings.append({\"name\": name, \"embedding\": embedding})\n",
    "\n",
    "            # translated_name = self._translate_text(name) # traduz\n",
    "            # embedding = self.get_embedding(translated_name) # vetoriza\n",
    "            # embeddings.append({\"name\": translated_name, \"embedding\": embedding})            \n",
    "\n",
    "            # Atualizar nome no nó 'Especialidade' com o nome traduzido\n",
    "            # update_query = \"\"\"\n",
    "            # MATCH (e:Especialidade {name: $original_name})\n",
    "            # SET e.name = $translated_name\n",
    "            # \"\"\"\n",
    "            # self._execute_query(update_query, parameters={\"original_name\": name, \"translated_name\": translated_name})\n",
    "        \n",
    "        logging.info(f\"Vetorização concluída. {len(embeddings)} nomes foram vetorizados.\")\n",
    "        return embeddings\n",
    "\n",
    "    # Persistir vetores gerados no Neo4j\n",
    "    def update_specialty_nodes_with_embeddings(self, specialty_embeddings):\n",
    "        \"\"\"\n",
    "        Atualiza os nós 'Especialidade' com os embeddings fornecidos.\n",
    "        Args:\n",
    "        - specialty_embeddings (List[Dict]): Lista de dicionários contendo os nomes (traduzidos) e seus embeddings.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        UNWIND $specialty_embeddings AS specialty_data\n",
    "        MATCH (e:Especialidade {name: specialty_data.name})\n",
    "        SET e.embedding = specialty_data.embedding\n",
    "        \"\"\"\n",
    "        self.neo4j_service.execute_write(query, parameters={\"specialty_embeddings\": specialty_embeddings})\n",
    "\n",
    "    # Recuperar e Vetorizar títulos de subáreas em dados já no Neo4j usando vetores pré-treinados\n",
    "    def vectorize_subarea_names(self):\n",
    "        logging.info(\"Iniciando vetorização dos nomes das subareas...\")\n",
    "        query = \"MATCH (e:Subárea) RETURN e.name AS name\"\n",
    "        result = self.neo4j_service.execute_read(query)\n",
    "        embeddings = []\n",
    "\n",
    "        # Converter resultados em uma lista para poder usar o tqdm\n",
    "        result_list = [record['name'] for record in result]\n",
    "        \n",
    "        logging.info(f\"Total de nomes a serem vetorizados: {len(result_list)}\")\n",
    "\n",
    "        if not result_list:\n",
    "            logging.warning(\"Nenhum nome de subárea foi recuperado da base de dados.\")\n",
    "            return embeddings\n",
    "\n",
    "        for name in tqdm(result_list, desc=\"Vetorizando nomes de subáreas\"):\n",
    "            embedding = self.get_embedding(name) # vetoriza o nome de subárea em português\n",
    "            embeddings.append({\"name\": name, \"embedding\": embedding})\n",
    "            \n",
    "            # translated_name = self._translate_text(name) # traduz\n",
    "            # embedding = self.get_embedding(translated_name) # vetoriza o nome de subárea\n",
    "            # embeddings.append({\"name\": translated_name, \"embedding\": embedding})\n",
    "\n",
    "            # Atualizar nome no nó 'Subárea' com o nome traduzido\n",
    "            # update_query = \"\"\"\n",
    "            # MATCH (e:Subárea {name: $original_name})\n",
    "            # SET e.name = $translated_name\n",
    "            # \"\"\"\n",
    "            # self._execute_query(update_query, parameters={\"original_name\": name, \"translated_name\": translated_name})\n",
    "        \n",
    "        logging.info(f\"Vetorização concluída. {len(embeddings)} nomes foram vetorizados.\")\n",
    "        return embeddings\n",
    "\n",
    "    # Persistir vetores gerados no Neo4j\n",
    "    def update_subarea_nodes_with_embeddings(self, subarea_embeddings):\n",
    "        \"\"\"\n",
    "        Atualiza os nós 'Subárea' com os embeddings fornecidos.\n",
    "        Args:\n",
    "        - subarea_embeddings (List[Dict]): Lista de dicionários contendo os nomes (traduzidos) e seus embeddings.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        UNWIND $subarea_embeddings AS subarea_data\n",
    "        MATCH (s:Subárea {name: subarea_data.name})\n",
    "        SET s.embedding = subarea_data.embedding\n",
    "        \"\"\"\n",
    "        self.neo4j_service.execute_write(query, parameters={\"subarea_embeddings\": subarea_embeddings})\n",
    "\n",
    "    def extract_titles_from_data(self, data_dict):\n",
    "        \"\"\"\n",
    "        Extract titles from the given data dictionary structure.\n",
    "        Args:\n",
    "        - data_dict (Dict): The dictionary structure containing the data.\n",
    "        Returns:\n",
    "        - List[Dict]: A list of dictionaries with titles and their embeddings.\n",
    "        \"\"\"\n",
    "        # Extrair títulos dos dicionários\n",
    "        titles = [entry['titulo'] for entry in data_dict.values()]\n",
    "        \n",
    "        # Obter embeddings para cada título\n",
    "        embeddings = [HierarchicalSemanticMatcher.get_embedding(title) for title in titles]\n",
    "        \n",
    "        # Combinar títulos e seus embeddings em um único dicionário\n",
    "        return [{\"title\": title, \"embedding\": embedding} for title, embedding in zip(titles, embeddings)]\n",
    "\n",
    "    def add_publications_with_embeddings(self, titles_list):\n",
    "        \"\"\"\n",
    "        Adicione nós de 'Publicação' com embeddings de títulos ao banco de dados Neo4j.\n",
    "        Args:\n",
    "        - titles_list (List[Dict]): Uma lista de dicionários contendo títulos e seus embeddings.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        UNWIND $titles_list AS title_data\n",
    "        MERGE (p:Publicacao {titulo: title_data.title})\n",
    "        SET p.embedding = title_data.embedding\n",
    "        \"\"\"\n",
    "        self.neo4j_service.execute_write(query, parameters={\"titles_list\": titles_list})\n",
    "\n",
    "    def add_titles_with_embeddings(self, titles):\n",
    "        query = \"\"\"\n",
    "        UNWIND $titles AS title\n",
    "        MERGE (t:Title {name: title.title})\n",
    "        SET t.embedding = title.embedding\n",
    "        \"\"\"\n",
    "        self.neo4j_service.execute_write(query, titles=titles)\n",
    "\n",
    "    def find_most_similar_node(self, article_embedding_tensor, node_embeddings):\n",
    "        # Initialize the highest similarity and most similar node id\n",
    "        highest_similarity = -1\n",
    "        most_similar_node_id = None\n",
    "        \n",
    "        # Convert article_embedding_tensor to the appropriate shape for cosine_similarity\n",
    "        article_embedding_tensor = article_embedding_tensor.view(1, -1)\n",
    "\n",
    "        # Iterate over node embeddings and calculate similarity\n",
    "        for node in node_embeddings:\n",
    "            node_embedding_tensor = torch.tensor(node['embedding'], dtype=torch.float).view(1, -1)\n",
    "            similarity = cosine_similarity(article_embedding_tensor, node_embedding_tensor)\n",
    "            if similarity.item() > highest_similarity:\n",
    "                highest_similarity = similarity.item()\n",
    "                most_similar_node_id = node['id']\n",
    "        \n",
    "        return most_similar_node_id\n",
    "\n",
    "    def find_grande_area(self, node_id):\n",
    "        query = \"\"\"\n",
    "        MATCH (node)-[:CONTÉM_ESPECIALIDADE|CONTÉM_SUBÁREA*]->(area:Área)-[:CONTÉM_ÁREA]->(grandeArea:GrandeÁrea)\n",
    "        WHERE id(node) = $node_id\n",
    "        RETURN grandeArea.name AS grande_area_name\n",
    "        \"\"\"\n",
    "        result = self.neo4j_service.execute_read(query, {'node_id': node_id})\n",
    "        if result:  # Check if the result is not empty\n",
    "            return result[0]['grande_area_name']  # Access the first record's 'grande_area_name'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def consult_grande_area(self, article_title):\n",
    "        # Gera o embedding do título do artigo\n",
    "        article_embedding = self.get_embedding(article_title)\n",
    "        article_embedding_tensor = torch.tensor([article_embedding], dtype=torch.float)\n",
    "\n",
    "        # Obtém os embeddings das especialidades e subáreas\n",
    "        specialty_embeddings = self.cosine_similarity_service.get_all_embeddings(\"Especialidade\")\n",
    "        subarea_embeddings = self.cosine_similarity_service.get_all_embeddings(\"Subárea\")\n",
    "\n",
    "        # Encontra o ID do nó mais similar\n",
    "        most_similar_node_id = self.find_most_similar_node(article_embedding_tensor, specialty_embeddings + subarea_embeddings)\n",
    "\n",
    "        # Encontra a Grande Área correspondente\n",
    "        grande_area = self.find_grande_area(most_similar_node_id)\n",
    "        \n",
    "        return grande_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas projeções limpas com sucesso na base de dados!\n"
     ]
    }
   ],
   "source": [
    "neo4j_service = Neo4jService(neo4j_uri, neo4j_user, neo4j_password)\n",
    "\n",
    "try:\n",
    "    neo4j_service.drop_all_projections()\n",
    "    print(\"Todas projeções limpas com sucesso na base de dados!\")\n",
    "except:\n",
    "    print(\"Não foi possível conectar ao Neo4j.\")\n",
    "    print(\"Verifique se o DBMS está ativo e se a base de dados foi inicializada com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não foi possível pesquisar, dados inexistentes.\n",
      "Gere pelo menos um ciclo de treinamento do modelo para efetuar classificações.\n"
     ]
    }
   ],
   "source": [
    "# Título do artigo que se deseja consultar no banco de dados.\n",
    "title = \"Bioinformática estrutural e biotecnologia para saúde pública\"\n",
    "\n",
    "# Crie uma instância da classe HierarchicalSemanticMatcher\n",
    "matcher = HierarchicalSemanticMatcher(neo4j_uri, neo4j_user, neo4j_password)\n",
    "\n",
    "# Faça a previsão da Grande Área\n",
    "try:\n",
    "    grande_area_encontrada = matcher.consult_grande_area(title)\n",
    "\n",
    "    # Imprima o resultado da previsão\n",
    "    print(f\"A informação atual para Grande Área no banco de dados para o artigo\\n  '{title}'\\nGrande Área encontrada:\\n  {grande_area_encontrada}\")\n",
    "except:\n",
    "    print(\"Não foi possível pesquisar, dados inexistentes.\")\n",
    "    print(\"Gere pelo menos um ciclo de treinamento do modelo para efetuar classificações.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listar procedimentos e assinaturas atuais Neo4j GDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigar existência dos procedimento atualizados no Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utilização da função para listar todos os procedimentos e funções disponíveis\n",
    "# neo4j_service = Neo4jService(uri, user, password)\n",
    "# procedures = neo4j_service.find_available_procedures()\n",
    "# functions = neo4j_service.find_available_functions()\n",
    "\n",
    "# # Agora, podemos imprimir ou verificar manualmente a lista de todos procedimentos\n",
    "# for proc in procedures:\n",
    "#     print(proc['name'])\n",
    "\n",
    "# for func in functions:\n",
    "#     print(func['name'])\n",
    "\n",
    "# neo4j_service.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listar procedimentos das biliotecas instaladas no Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Buscar procedimentos vigentes no Neo4j relacionados com uma string\n",
    "# neo4j_service = Neo4jService(neo4j_uri, neo4j_user, neo4j_password)\n",
    "# create_related_procedures = neo4j_service.find_related_procedures('create')\n",
    "# for proc in create_related_procedures:\n",
    "#     print(proc['name'])\n",
    "\n",
    "# neo4j_service.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utilização da função para buscar procedimentos relacionados com uma string\n",
    "# create_related_procedures = neo4j_service.find_related_procedures('graph')\n",
    "# for proc in create_related_procedures:\n",
    "#     print(proc['name'])\n",
    "\n",
    "# neo4j_service.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utilização da função para buscar procedimentos relacionados com uma string\n",
    "# create_related_procedures = neo4j_service.find_related_procedures('louvain')\n",
    "# for proc in create_related_procedures:\n",
    "#     print(proc['name'])\n",
    "\n",
    "# neo4j_service.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utilização da função para buscar procedimentos relacionados com uma string\n",
    "# create_related_procedures = neo4j_service.find_related_procedures('propagation')\n",
    "# for proc in create_related_procedures:\n",
    "#     print(proc['name'])\n",
    "\n",
    "# neo4j_service.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utilização da função para buscar procedimentos relacionados com uma string\n",
    "# create_related_procedures = neo4j_service.find_related_procedures('kmeans')\n",
    "# for proc in create_related_procedures:\n",
    "#     print(proc['name'])\n",
    "\n",
    "# neo4j_service.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utilização da função para buscar procedimentos relacionados com uma string\n",
    "# create_related_procedures = neo4j_service.find_related_procedures('NodeProperties')\n",
    "# for proc in create_related_procedures:\n",
    "#     print(proc['name'])\n",
    "\n",
    "# neo4j_service.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utilização da função para buscar procedimentos relacionados com uma string\n",
    "# create_related_procedures = neo4j_service.find_related_procedures('node')\n",
    "# for proc in create_related_procedures:\n",
    "#     print(proc['name'])\n",
    "\n",
    "# neo4j_service.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilização da função para buscar procedimentos relacionados com uma string\n",
    "create_related_procedures = neo4j_service.find_related_procedures('modularity')\n",
    "\n",
    "# Agora, podemos imprimir a lista de procedimentos relacionados com 'create'\n",
    "for proc in create_related_procedures:\n",
    "    print(proc['name'])\n",
    "\n",
    "neo4j_service.close()\n",
    "\n",
    "# Método antigo: gds.beta.modularity.stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listar assinaturas de procedimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilização da função para buscar assinaturas dos procedimentos em uma lista\n",
    "procedures_list = ['gds.graph.nodeProperties.stream']\n",
    "procedures_signatures = neo4j_service.get_procedure_signatures(procedures_list)\n",
    "\n",
    "neo4j_service.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilização da função para buscar assinaturas dos procedimentos em uma lista\n",
    "procedures_list = ['gds.modularity.stream']\n",
    "procedures_signatures = neo4j_service.get_procedure_signatures(procedures_list)\n",
    "\n",
    "neo4j_service.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes unitários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from neo4j import GraphDatabase\n",
    "# from dynagml import Neo4jService  # Ajuste o import para o seu caso\n",
    "\n",
    "class TestNeo4jService(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Conectar ao banco de dados de teste do Neo4j\n",
    "        cls.neo4j_service = Neo4jService(neo4j_uri, neo4j_user, neo4j_password)\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        # Fechar a conexão com o banco de dados\n",
    "        cls.neo4j_service.close()\n",
    "\n",
    "    def test_project_graph(self):\n",
    "        # Testar a projeção do grafo\n",
    "        projection_name = 'testProjection'\n",
    "        node_labels = ['Node']\n",
    "        relationship_types = ['RELATIONSHIP']\n",
    "        self.neo4j_service.project_graph(projection_name, node_labels, relationship_types)\n",
    "        # Verificar se a projeção foi criada corretamente. \n",
    "        # Eliminar projeções de teste caso a base de dados seja permanente.\n",
    "\n",
    "    def test_run_kmeans(self):\n",
    "        # Testar o algoritmo K-Means\n",
    "        projection_name = 'testProjection'\n",
    "        k = 2\n",
    "        self.neo4j_service.run_kmeans(projection_name, k)\n",
    "        # Verificar se as propriedades de cluster foram escritas corretamente nos nós\n",
    "\n",
    "    def test_run_louvain(self):\n",
    "        # Testar o algoritmo de Louvain\n",
    "        projection_name = 'testProjection'\n",
    "        communities = self.neo4j_service.run_louvain(projection_name)\n",
    "        # Verificar se as comunidades foram retornadas corretamente\n",
    "\n",
    "    def test_run_label_propagation(self):\n",
    "        # Testar o algoritmo de propagação de rótulos\n",
    "        projection_name = 'testProjection'\n",
    "        self.neo4j_service.run_label_propagation(projection_name)\n",
    "        # Verificar se os rótulos foram escritos corretamente nos nós\n",
    "\n",
    "    def test_drop_named_graph(self):\n",
    "        # Testar a exclusão de um grafo nomeado\n",
    "        projection_name = 'testProjection'\n",
    "        self.neo4j_service.drop_named_graph(projection_name)\n",
    "        # Verificar se o grafo nomeado foi excluído corretamente\n",
    "\n",
    "# Adicionar mais testes conforme necessário...\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     unittest.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar, Treinar e avaliar um modelo GCN\n",
    "\n",
    "    GCN_weight\n",
    "    TrainingPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN_noweight(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, nome_modelo):\n",
    "        super(GCN_noweight, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GCN_weight(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, nome_modelo):\n",
    "        super(GCN_weight, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        # Verificar se há arestas antes de usar o peso da aresta na convolução\n",
    "        if edge_index.numel() > 0:\n",
    "            x = torch.relu(self.conv1(x, edge_index, edge_weight))\n",
    "            x = self.conv2(x, edge_index, edge_weight)\n",
    "        else:\n",
    "            logging.warning(\"No edges present. Skipping edge-weighted convolutions.\")\n",
    "            x = torch.relu(self.conv1(x, edge_index))\n",
    "            x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Configuração inicial do logging.\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class TrainingPipeline:\n",
    "    def __init__(self, neo4j_service, input_dim, hidden_dim, output_dim, model_name):\n",
    "        self.neo4j_service = neo4j_service\n",
    "        self.cosine_similarity_service = CosineSimilarityRelationship(neo4j_service)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.nome_modelo = model_name        \n",
    "        self.model = GCN_weight(input_dim, hidden_dim, output_dim, model_name)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "        self.loss_history = []\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.fig, self.ax = plt.subplots(figsize=(19, 6))\n",
    "        self.ax.set_xlabel('Epoch')\n",
    "        self.ax.set_ylabel('Loss')\n",
    "        self.line, = self.ax.plot([], [], 'b-', label='Training Loss')\n",
    "        self.ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def create_similarity_graph(self, threshold):\n",
    "        logging.info(\"Criando relações de similaridade no Neo4j...\")\n",
    "        self.cosine_similarity_service.create_similarity_embeddings_relationships(threshold)\n",
    "\n",
    "    def load_data_from_neo4j(self):\n",
    "        logging.info(\"Carregando dados do Neo4j...\")\n",
    "        nodes = self.cosine_similarity_service.get_all_embeddings(\"Publicacao\")\n",
    "        x = torch.tensor([node[\"embedding\"] for node in nodes], dtype=torch.float).to(self.device)\n",
    "\n",
    "        edge_index   = torch.empty((2, 0), dtype=torch.long, device=self.device)\n",
    "        edge_weights = torch.empty((0,), dtype=torch.float, device=self.device)\n",
    "\n",
    "        if nodes:\n",
    "            node_id_to_graph_id = {node['id']: idx for idx, node in enumerate(nodes)}\n",
    "            for node in tqdm(nodes, desc=\"Loading edges\"):\n",
    "                queries = [\n",
    "                    f\"\"\"\n",
    "                    MATCH (p1:Publicacao)-[r:SIMILAR]->(s:Subárea) WHERE id(p1) = {node['id']}\n",
    "                    RETURN id(s) AS target, r.weight AS weight\n",
    "                    \"\"\",\n",
    "                    f\"\"\"\n",
    "                    MATCH (p1:Publicacao)-[r:SIMILAR]->(e:Especialidade) WHERE id(p1) = {node['id']}\n",
    "                    RETURN id(e) AS target, r.weight AS weight\n",
    "                    \"\"\"\n",
    "                ]\n",
    "                for query in queries:\n",
    "                    edges = self.neo4j_service.execute_read(query)\n",
    "                    for edge in edges:\n",
    "                        graph_id = node_id_to_graph_id.get(node['id'])\n",
    "                        target_graph_id = node_id_to_graph_id.get(edge['target'])\n",
    "                        if graph_id is not None and target_graph_id is not None:\n",
    "                            new_edge_index = torch.tensor([[graph_id], [target_graph_id]], dtype=torch.long, device=self.device)\n",
    "                            new_edge_weights = torch.tensor([edge['weight']], dtype=torch.float, device=self.device)\n",
    "                            edge_index = torch.cat([edge_index, new_edge_index], dim=1)\n",
    "                            edge_weights = torch.cat([edge_weights, new_edge_weights], dim=0)\n",
    "\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_weights).to(self.device)\n",
    "\n",
    "    def compute_loss(self, output, data):\n",
    "        embeddings = output\n",
    "        edge_index = data.edge_index\n",
    "        edge_weight = data.edge_attr\n",
    "\n",
    "        # Calcular função de perda usando uma abordagem contrastiva.\n",
    "        if data.edge_index.shape[1] > 0:  # Verifique se existem arestas.\n",
    "            positive_score = torch.exp(-torch.norm(embeddings[edge_index[0]] - embeddings[edge_index[1]], dim=1))\n",
    "            positive_score = torch.clamp(positive_score, min=1e-10)  # Evite log(0)\n",
    "            loss = -torch.log(positive_score).mean()\n",
    "        else:\n",
    "            # Caso não existam arestas, usar métrica alternativa, como a variância dos embeddings, por exemplo.\n",
    "            loss = torch.var(embeddings)\n",
    "\n",
    "        # Verificar se a perda é nan.\n",
    "        if torch.isnan(loss):\n",
    "            logging.error(\"Perda calculada como nan durante o treinamento.\")\n",
    "            loss = torch.tensor(0.0, device=output.device)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self, data, epochs, early_stopping_patience=10):\n",
    "        logging.info(\"Iniciando o treinamento...\")\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        self.setup_plot()  # Configurar plotagem.\n",
    "\n",
    "       # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Recreate the model and optimizer, and move them to GPU\n",
    "        self.model = GCN_weight(self.input_dim, self.hidden_dim, self.output_dim, self.nome_modelo).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Make sure your data is on the correct device\n",
    "        data = data.to(self.device)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.compute_loss(output, data)\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                logging.error(f'Loss inválida detectada na época {epoch+1}: {loss}')\n",
    "                break  # Interrompe o treinamento se uma perda inválida for detectada.\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Sincronizar CUDA\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize(self.device)\n",
    "\n",
    "            # Atualizar histórico da função de perda com valores válidos.\n",
    "            loss_value = loss.item()\n",
    "            self.loss_history.append(loss_value)\n",
    "            logging.info(f'Época {epoch+1}/{epochs}, Perda: {loss_value}')\n",
    "\n",
    "            if loss_value < best_loss:\n",
    "                best_loss = loss_value\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                logging.info(f\"Parada antecipada acionada na época {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "            # Atualizar linha de perda no grafo.\n",
    "            self.plot_loss(epoch)\n",
    "\n",
    "        plt.ioff()  # Desativar modo interativo após o treinamento\n",
    "\n",
    "    def setup_plot(self):\n",
    "        # Configurar eixos e legenda para plotagem.\n",
    "        self.ax.set_xlabel('Epoch')\n",
    "        self.ax.set_ylabel('Loss')\n",
    "        # self.line, = self.ax.plot([], [], 'b-', label='Training Loss')\n",
    "        self.ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss(self, epoch):\n",
    "        # Atualizar dados da linha com a nova perda.\n",
    "        self.line.set_xdata(range(epoch + 1))\n",
    "        self.line.set_ydata(self.loss_history)\n",
    "\n",
    "        # Re-calcular limites com base nos novos dados.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view(True, True, True)\n",
    "\n",
    "        # Atualizar o gráfico com os novos dados.\n",
    "        clear_output(wait=True)  # Limpar saída antes de desenhar o novo gráfico.\n",
    "        display(self.fig)  # Exibir o gráfico atualizado.\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        torch.save(self.model.state_dict(), file_path)\n",
    "        logging.info(f\"Model saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1900x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 01:14:26,361 INFO:Criando relações de similaridade no Neo4j...\n",
      "2024-01-11 01:14:26,431 INFO:Nodes: Publicacao 0, Especialidade 861, Subárea 340\n",
      "Processing similarity between Publicacao and Especialidade: 0it [00:00, ?it/s]\n",
      "Processing similarity between Publicacao and Subárea: 0it [00:00, ?it/s]\n",
      "2024-01-11 01:14:26,434 INFO:Completed creating similarity relationships.\n",
      "2024-01-11 01:14:26,435 INFO:RAM Consumption: 0.38 MB\n",
      "2024-01-11 01:14:26,436 INFO:Processing Time: 0.07 seconds\n",
      "2024-01-11 01:14:26,436 INFO:Current Memory Usage: 1370.86 MB\n",
      "2024-01-11 01:14:26,436 INFO:Execution time for similarity calculations and relationship creation: 0.07 seconds\n",
      "2024-01-11 01:14:26,437 INFO:Similarity threshold: 0.75\n",
      "2024-01-11 01:14:26,437 INFO:Total node pairs analyzed: 0\n",
      "2024-01-11 01:14:26,437 INFO:Node pairs Publicacao/Subárea: 0\n",
      "2024-01-11 01:14:26,438 INFO:Node pairs Publicacao/Especialidade: 0\n",
      "2024-01-11 01:14:26,438 INFO:Total relationships created: 0\n",
      "2024-01-11 01:14:26,438 INFO:Relationships stabilished on nodes Publicacao/Subárea: 0\n",
      "2024-01-11 01:14:26,439 INFO:Relationships stabilished on Publicacao/Especialidade: 0\n",
      "2024-01-11 01:14:26,439 INFO:Carregando dados do Neo4j...\n",
      "2024-01-11 01:14:26,444 INFO:Iniciando o treinamento...\n",
      "2024-01-11 01:14:26,527 WARNING:No edges present. Skipping edge-weighted convolutions.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got -2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     data \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mload_data_from_neo4j()\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     22\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(e)\n",
      "Cell \u001b[0;32mIn[5], line 138\u001b[0m, in \u001b[0;36mTrainingPipeline.train\u001b[0;34m(self, data, epochs, early_stopping_patience)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 138\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(output, data)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(loss):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m, in \u001b[0;36mGCN_weight.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo edges present. Skipping edge-weighted convolutions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:223\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    220\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m gcn_norm(  \u001b[38;5;66;03m# yapf: disable\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m         edge_index, edge_weight, \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimproved, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_self_loops, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow, x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got -2)"
     ]
    }
   ],
   "source": [
    "nome_modelo = 'model01'\n",
    "input_dim = 768  # O tamanho dos embeddings de entrada\n",
    "hidden_dim = 64  # Tamanho da camada oculta\n",
    "output_dim = 8   # Número esperado de comunidades/clusters\n",
    "\n",
    "# Inicializar o serviço Neo4j com seus próprios parâmetros de conexão\n",
    "neo4j_service = Neo4jService(neo4j_uri, neo4j_user, neo4j_password)\n",
    "\n",
    "# Instanciar a classe TrainingPipeline passando o serviço Neo4j\n",
    "pipeline = TrainingPipeline(neo4j_service, input_dim, hidden_dim, output_dim, model_name=nome_modelo)\n",
    "\n",
    "\n",
    "# Criar no Neo4j um grafo de similaridade com o limiar definido\n",
    "similarity_threshold = 0.75\n",
    "pipeline.create_similarity_graph(similarity_threshold)\n",
    "\n",
    "# Carregar os dados do Neo4j e treinar o modelo\n",
    "try:\n",
    "    data = pipeline.load_data_from_neo4j()\n",
    "    pipeline.train(data, epochs=150, early_stopping_patience=20)\n",
    "except ValueError as e:\n",
    "    logging.error(e)\n",
    "\n",
    "# Salvar o modelo em um arquivo\n",
    "model_save_path = f'./../models/{nome_modelo}.pt'\n",
    "pipeline.save_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_service = Neo4jService(neo4j_uri, neo4j_user, neo4j_password)\n",
    "neo4j_service.drop_all_projections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliações do treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do desempenho de aprendizado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando a figura do gráfico de perda, percebemos o decaimento rápido e então a estabilização, o que é um bom sinal de que o modelo está aprendendo. No entanto, sem mais contextos, como o valor da perda em cada época e comparação com um conjunto de validação, é difícil fazer uma avaliação completa. Aqui estão alguns pontos a considerar:\n",
    "\n",
    "Decaimento da Perda: \n",
    "\n",
    "    Se a perda decresce e se estabiliza, isso indica que o modelo está convergindo. Entretanto, é necessário garantir que não haja \"overfitting\" ou \"underfitting\".\n",
    "\n",
    "Comparação com Dados de Validação: \n",
    "\n",
    "    Idealmente, você deveria monitorar a perda em um conjunto de validação separado para garantir que a diminuição na perda está correlacionada com uma melhoria na capacidade de generalização do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do desempenho do clustering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualização de Clusters: \n",
    "\n",
    "    Para tarefas de clustering, visualizar como os dados são agrupados em um espaço de baixa dimensão (usando técnicas como PCA ou t-SNE) pode ajudar a avaliar a qualidade dos clusters formados.\n",
    "\n",
    "Métricas de Clustering: \n",
    "    \n",
    "    Métricas como Silhouette Score, Davies-Bouldin Index ou Dunn Index podem dar uma ideia quantitativa de quão bem separados e coesos são os clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação geral no contexto do problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo treinado, que é uma Rede Neural Gráfica (GNN), que como demonstrado na curva de aprendizado aprendeu a representar as relações e características dos nós em um espaço de vetores incorporados (embeddings) onde a estrutura do grafo é preservada. Em outras palavras, o modelo foi capaz de entender e capturar padrões complexos de conectividade entre os nós, bem como a importância relativa das arestas, e representar essas informações em um espaço vetorial de baixa dimensão.\n",
    "\n",
    "Em detalhes o que foi aprendido e como aplicamos esse aprendizado ao problema de clusterização dinâmica no grafo:\n",
    "\n",
    "1. **Representações de nós (Node Embeddings)**: O modelo aprendeu a mapear nós em um espaço de embeddings onde a proximidade entre pontos reflete a semelhança ou a conexão no grafo. Isso é particularmente útil para clusterização, pois nós similares ou relacionados terão representações próximas uns dos outros no espaço de embeddings.\n",
    "\n",
    "2. **Importância das Arestas (Edge Weights)**: Ao usar uma GNN com capacidade de ponderar arestas, o modelo leva em consideração a força da relação entre dois nós. Isso permite que o modelo diferencie entre conexões fortes e fracas, o que pode ser crucial para determinar os limites entre clusters.\n",
    "\n",
    "3. **Clusterização Dinâmica**: Com as representações aprendidas, é possível aplicar métodos de clusterização, como K-means ou algoritmos baseados em densidade como DBSCAN, para agrupar os nós em clusters. A clusterização dinâmica se refere à capacidade de ajustar os clusters à medida que novos dados são incorporados ou à medida que o grafo muda ao longo do tempo. Com os embeddings gerados pela GNN, você pode reagrupar os nós de maneira eficiente sem ter que reexecutar o algoritmo de clusterização do zero.\n",
    "\n",
    "4. **Interpretação**: Os embeddings gerados pelo modelo podem ser visualizados usando técnicas de redução de dimensionalidade, como t-SNE ou PCA, o que pode fornecer insights visuais sobre a estrutura do grafo e a formação de clusters.\n",
    "\n",
    "5. **Aplicação ao Problema**: Para o problema de clusterização dinâmica no contexto específico da clusterização das publicações nas grandes áreas do CNPq, usamos os embeddings para calcular a aproximação semântica entre os títulos das publicações com as subáreas e especialidades do CNPq. Com base nessa similaridade semântica aprendida pelo modelo, pudemos identificar grupos de publicações que estão semanticamente relacionadas a certas subáreas ou especialidades, e por conseguinte aos demais níveis hierárquicos, áreas e grandes áreas da árvore do conhecimento.\n",
    "\n",
    "6. **Atualizações e Manutenção**: À medida que o grafo evolui com novos nós de novas publicações, ou até por mudanças em subáreas ou especialidades, novas arestas são identificadas pelo modelo, e podemos atualizar os embeddings com incrementos de treinamento ou treinar o modelo novamente, se necessário, dependendo da magnitude das mudanças. Isso ajuda a manter a precisão dos clusters ao longo do tempo.\n",
    "\n",
    "A partir daqui vamos aplicar o aprendizado ao seu problema de clusterização dinâmica:\n",
    "\n",
    "- Extrair os embeddings do modelo treinado para todos os nós de interesse.\n",
    "- Aplicar um algoritmo de clusterização aos embeddings para formar grupos.\n",
    "- Avaliar a qualidade dos clusters usando métricas como Silhouette Score ou similaridade interna dos clusters.\n",
    "- Interpretar os clusters formados em termos do problema de domínio, o que pode envolver a compreensão de como as publicações estão agrupadas em torno de subáreas e especialidades.\n",
    "- Utilizar os clusters para recomendar, classificar ou filtrar informações de acordo com as necessidades dos usuários ou do sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificar titulos a partir do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferencePipeline:\n",
    "    def __init__(self, neo4j_uri, neo4j_user, neo4j_password, model_path):\n",
    "        self.neo4j_service = Neo4jService(neo4j_uri, neo4j_user, neo4j_password)\n",
    "        self.cosine_similarity_service = CosineSimilarityRelationship(self.neo4j_service)\n",
    "        self.matcher = HierarchicalSemanticMatcher(neo4j_uri, neo4j_user, neo4j_password)\n",
    "        self.model = self.load_model(model_path)\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        model = GCN_weight(input_dim, hidden_dim, output_dim, model_name)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def predict_grande_area(self, title):\n",
    "        # Step 1: Retrieve embeddings for all \"Especialidade\" and \"Subárea\" nodes\n",
    "        specialty_embeddings = self.cosine_similarity_service.get_all_embeddings(\"Especialidade\")\n",
    "        subarea_embeddings = self.cosine_similarity_service.get_all_embeddings(\"Subárea\")\n",
    "        combined_embeddings = specialty_embeddings + subarea_embeddings\n",
    "        \n",
    "        # Step 2: Compute cosine similarity and find the highest similarity node\n",
    "        most_similar_node_id = None\n",
    "        highest_similarity = -float('inf')\n",
    "        most_similar_node_label = None\n",
    "        most_similar_node_name = None\n",
    "        \n",
    "        # Convert title to tensor\n",
    "        article_embedding_tensor = torch.tensor(HierarchicalSemanticMatcher.get_embedding(title), dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "        for node in combined_embeddings:\n",
    "            node_embedding_tensor = torch.tensor(node['embedding'], dtype=torch.float).unsqueeze(0)\n",
    "            similarity = torch.nn.functional.cosine_similarity(article_embedding_tensor, node_embedding_tensor)\n",
    "            if similarity.item() > highest_similarity:\n",
    "                highest_similarity = similarity.item()\n",
    "                most_similar_node_id = node['id']\n",
    "                most_similar_node_label = \"Especialidade\" if node in specialty_embeddings else \"Subárea\"\n",
    "                most_similar_node_name = node.get('name', 'Unknown')\n",
    "\n",
    "        logging.info(f\"Maior índice de similaridade calculado: {highest_similarity} id({most_similar_node_id})\")\n",
    "        print(f\"       Label nó mais similar: {most_similar_node_label}\\n        Nome nó mais similar: {most_similar_node_name}\")\n",
    "\n",
    "        # Step 4: Query the associated \"Grande Área\" from the Neo4j database\n",
    "        grande_area_query = f\"\"\"\n",
    "        MATCH (n) WHERE ID(n) = {most_similar_node_id}\n",
    "        OPTIONAL MATCH (n)-[:CONTÉM_ESPECIALIDADE|CONTÉM_SUBÁREA*0..]-(area:Área)-[:CONTÉM_ÁREA]-(grandeArea:GrandeÁrea)\n",
    "        RETURN COALESCE(grandeArea.name, 'Unknown') AS grande_area_name\n",
    "        \"\"\"\n",
    "        result = self.neo4j_service.execute_read(grande_area_query)\n",
    "        grande_area_name = result[0]['grande_area_name'] if result else 'Não foi possível encontrar um resultado'\n",
    "        \n",
    "        return grande_area_name\n",
    "\n",
    "\n",
    "# Use a path to your saved model\n",
    "nome_modelo = 'model01.pt'\n",
    "model_path = './../models/'+nome_modelo\n",
    "input_dim  = 768  # O tamanho dos embeddings de entrada\n",
    "hidden_dim = 64   # Tamanho da camada oculta\n",
    "output_dim = 8    # Número esperado de comunidades/clusters\n",
    "\n",
    "# Inicialize a pipeline de inferência\n",
    "inference_pipeline = InferencePipeline(neo4j_uri, neo4j_user, neo4j_password, model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer inferência para um título fornecido\n",
    "title = \"Bioinformática estrutural e biotecnologia para saúde pública\"\n",
    "grande_area_predita = inference_pipeline.predict_grande_area(title)\n",
    "print(f\" Grande Área nó mais similar: {grande_area_predita}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer inferência para um título fornecido\n",
    "title_en = \"bioinformatics and biotechnology for public health\"\n",
    "grande_area_predita = inference_pipeline.predict_grande_area(title_en)\n",
    "print(f\" Grande Área nó mais similar: {grande_area_predita}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregar feedback do usuário para aprendizagem por reforço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "class IncrementalTrainer:\n",
    "    def __init__(self, n_clusters, model=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        if model is None:\n",
    "            self.model = KMeans(n_clusters=self.n_clusters)\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "    def incremental_train(self, new_data):\n",
    "        \"\"\"\n",
    "        Treina o modelo incrementalmente com os novos dados.\n",
    "        \n",
    "        :param new_data: Os novos dados para treinar o modelo. Deve ser um array 2D.\n",
    "        \"\"\"\n",
    "        # Verificar se o modelo já foi inicialmente treinado\n",
    "        if not hasattr(self.model, 'cluster_centers_'):\n",
    "            # Se não foi, treinar com os novos dados\n",
    "            self.model.fit(new_data)\n",
    "        else:\n",
    "            # Caso contrário, atualizar o modelo com os novos dados\n",
    "            initial_centers = self.model.cluster_centers_\n",
    "            updated_data = np.concatenate((initial_centers, new_data))\n",
    "            self.model.fit(updated_data)\n",
    "\n",
    "        # Salvar o estado atualizado do modelo, se necessário\n",
    "        # self.save_model_state()\n",
    "\n",
    "    def save_model_state(self):\n",
    "        # Implementar o método para salvar o estado do modelo\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReinforcementLearningInterface:\n",
    "#     def __init__(self, neo4j_service, matcher):\n",
    "#         self.neo4j_service = neo4j_service\n",
    "#         self.matcher = matcher\n",
    "\n",
    "#     def find_similar_specialties(self, title, top_n=3):\n",
    "#         # Gerar o embedding do título\n",
    "#         article_embedding = self.matcher.get_embedding(title)\n",
    "#         article_embedding_tensor = torch.tensor(article_embedding, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "#         # Obter todos os embeddings de \"Especialidade\"\n",
    "#         specialty_embeddings = self.neo4j_service.get_all_embeddings(\"Especialidade\")\n",
    "        \n",
    "#         # Calcular a similaridade\n",
    "#         similarities = []\n",
    "#         for specialty in specialty_embeddings:\n",
    "#             specialty_embedding_tensor = torch.tensor(specialty['embedding'], dtype=torch.float).unsqueeze(0)\n",
    "#             similarity = torch.nn.functional.cosine_similarity(article_embedding_tensor, specialty_embedding_tensor)\n",
    "#             similarities.append((specialty['id'], similarity.item()))\n",
    "\n",
    "#         # Ordenar e obter os três primeiros\n",
    "#         sorted_specialties = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "#         return [(self.neo4j_service.get_node_name(specialty_id), similarity) for specialty_id, similarity in sorted_specialties]\n",
    "\n",
    "#     def get_node_name(self, node_id):\n",
    "#         query = \"MATCH (n) WHERE id(n) = $node_id RETURN n.name AS name\"\n",
    "#         result = self.neo4j_service.execute_read(query, {'node_id': node_id})\n",
    "#         return result[0]['name'] if result else \"Unknown\"\n",
    "\n",
    "#     def user_selection(self, options):\n",
    "#         print(\"Escolha a especialidade preferida:\")\n",
    "#         for idx, (name, similarity) in enumerate(options):\n",
    "#             print(f\"{idx+1}: {name} (Similaridade: {similarity:.2f})\")\n",
    "#         choice = int(input(\"Digite o número da especialidade escolhida: \"))\n",
    "#         return options[choice-1][0]  # Retorna o nome da especialidade escolhida\n",
    "\n",
    "#     def update_embeddings(self, specialty_embedding, article_embedding, learning_rate=0.01):\n",
    "#         \"\"\"\n",
    "#         Ajusta o embedding de uma especialidade escolhida, movendo-a em direção ao\n",
    "#         embedding de um artigo.\n",
    "\n",
    "#         :param specialty_embedding: O embedding atual da especialidade escolhida.\n",
    "#         :param article_embedding: O embedding do artigo relacionado.\n",
    "#         :param learning_rate: A taxa de aprendizado que determina o quão rápido os embeddings são ajustados.\n",
    "#         \"\"\"\n",
    "#         # Converter embeddings para tensores do PyTorch\n",
    "#         specialty_tensor = torch.tensor(specialty_embedding, dtype=torch.float)\n",
    "#         article_tensor = torch.tensor(article_embedding, dtype=torch.float)\n",
    "\n",
    "#         # Calcular o vetor de diferença e atualizar o embedding da especialidade\n",
    "#         difference = article_tensor - specialty_tensor\n",
    "#         adjusted_embedding = specialty_tensor + learning_rate * difference\n",
    "\n",
    "#         # Converter o tensor ajustado de volta para uma lista para armazenamento\n",
    "#         updated_embedding = adjusted_embedding.tolist()\n",
    "#         return updated_embedding\n",
    "\n",
    "#     def update_model(self, chosen_specialty_name):\n",
    "#         # Obter o ID do nó da especialidade escolhida\n",
    "#         specialty_id = self.neo4j_service.get_node_id(chosen_specialty_name)\n",
    "#         specialty_embedding = self.neo4j_service.get_embedding(specialty_id)\n",
    "        \n",
    "#         # Implementar lógica para ajustar embeddings/modelo\n",
    "#         # Isso poderia ser feito, por exemplo, movendo o embedding do nó escolhido\n",
    "#         # mais perto do embedding do artigo no espaço de embedding, e/ou\n",
    "#         # movendo embeddings de nós não escolhidos mais longe.\n",
    "        \n",
    "#         self.update_embeddings(specialty_embedding)\n",
    "        \n",
    "#         # Salvar os embeddings atualizados de volta no banco de dados\n",
    "#         self.neo4j_service.save_embedding(specialty_id, specialty_embedding)\n",
    "        \n",
    "#         # Se o modelo precisar de treinamento incremental, isso também poderia ser acionado aqui\n",
    "#         self.model.incremental_train(new_data)\n",
    "        \n",
    "#         # Salvar o modelo atualizado\n",
    "#         self.model.save('path_to_saved_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt-get install python3-tk\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import tkinter.font as tkfont\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class RecommendationSystem:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self.neo4j_service = Neo4jService(uri, user, password)\n",
    "        self.cosine_similarity_service = CosineSimilarityRelationship(self.neo4j_service)\n",
    "        self.matcher = HierarchicalSemanticMatcher(uri, user, password)\n",
    "        self.setup_ui()\n",
    "\n",
    "    def setup_ui(self):\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Recomendação de Especialidade\")\n",
    "        self.submit_button = tk.Button(self.root, text=\"Salvar Escolha\", command=self.save_choice)\n",
    "        self.root.resizable(True, True)\n",
    "        self.tree = ttk.Treeview(self.root, columns=('Especialidade', 'Subárea', 'Área', 'GrandeÁrea'), show='headings')\n",
    "        self.tree.heading('Especialidade', text='Especialidade')\n",
    "        self.tree.heading('Subárea', text='Subárea')\n",
    "        self.tree.heading('Área', text='Área')\n",
    "        self.tree.heading('GrandeÁrea', text='GrandeÁrea')\n",
    "        self.tree.column('Especialidade', width=240, anchor='center')\n",
    "        self.tree.column('Subárea', width=120, anchor='center')\n",
    "        self.tree.column('Área', width=120, anchor='center')\n",
    "        self.tree.column('GrandeÁrea', width=120, anchor='center')\n",
    "        self.tree.pack(fill='both', expand=True)\n",
    "        self.submit_button.pack(fill='x', expand=False)\n",
    "        self.tree.bind('<<TreeviewSelect>>', self.on_selection)\n",
    "\n",
    "    def predict_speciality(self, title):\n",
    "        # Obter os embeddings\n",
    "        specialty_embeddings = self.cosine_similarity_service.get_all_embeddings(\"Especialidade\")\n",
    "        subarea_embeddings = self.cosine_similarity_service.get_all_embeddings(\"Subárea\")\n",
    "\n",
    "        # Calcular a similaridade\n",
    "        article_embedding = HierarchicalSemanticMatcher.get_embedding(title)\n",
    "        similarities = [(node, torch.nn.functional.cosine_similarity(\n",
    "            torch.tensor(article_embedding, dtype=torch.float).unsqueeze(0),\n",
    "            torch.tensor(node['embedding'], dtype=torch.float).unsqueeze(0)\n",
    "        ).item()) for node in (specialty_embeddings + subarea_embeddings)]\n",
    "\n",
    "        # Ordenar e pegue os três primeiros\n",
    "        top_three = sorted(similarities, key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "        # Extrair os nomes das especialidades e busque informações adicionais\n",
    "        top_three_details = []\n",
    "        for node, _ in top_three:\n",
    "            specialty_id = node['id']\n",
    "            details_query = f\"\"\"\n",
    "            MATCH (specialty:Especialidade) WHERE id(specialty) = {specialty_id}\n",
    "            OPTIONAL MATCH (specialty)-[:CONTÉM_ESPECIALIDADE|CONTÉM_SUBÁREA*0..]-(subarea:Subárea)\n",
    "            OPTIONAL MATCH (subarea)-[:CONTÉM_SUBÁREA*0..]-(area:Área)\n",
    "            OPTIONAL MATCH (area)-[:CONTÉM_ÁREA]-(grandeArea:GrandeÁrea)\n",
    "            RETURN specialty.name AS specialty, subarea.name AS subarea, \n",
    "                   area.name AS area, grandeArea.name AS grandeArea\n",
    "            \"\"\"\n",
    "            result = self.driver.session().run(details_query)\n",
    "            if result.peek():\n",
    "                record = result.single()\n",
    "                top_three_details.append({\n",
    "                    'Especialidade': record['specialty'],\n",
    "                    'Subárea': record['subarea'],\n",
    "                    'Área': record['area'],\n",
    "                    'GrandeÁrea': record['grandeArea']\n",
    "                })\n",
    "            else:\n",
    "                top_three_details.append({\n",
    "                    'Especialidade': node['name'],\n",
    "                    'Subárea': 'Desconhecido',\n",
    "                    'Área': 'Desconhecido',\n",
    "                    'GrandeÁrea': 'Desconhecido'\n",
    "                })\n",
    "\n",
    "        return top_three_details\n",
    "\n",
    "    def on_selection(self, event):\n",
    "        selected_item = self.tree.selection()[0]\n",
    "        self.choice = self.tree.item(selected_item, 'values')\n",
    "        print(f\"Você selecionou: {self.choice}\")\n",
    "\n",
    "    def adjust_column_width(self):\n",
    "        for col in self.tree['columns']:\n",
    "            max_width = 0\n",
    "            for item in self.tree.get_children():\n",
    "                item_width = tkfont.Font().measure(self.tree.item(item, 'values')[self.tree['columns'].index(col)])\n",
    "                if item_width > max_width:\n",
    "                    max_width = item_width\n",
    "            self.tree.column(col, width=max_width)\n",
    "\n",
    "    def adjust_window_size(self):\n",
    "        self.root.update_idletasks()\n",
    "        width = max(self.tree.winfo_width(), 600)  # Garante uma largura mínima\n",
    "        height = self.tree.winfo_height() + self.submit_button.winfo_height()\n",
    "        self.root.geometry(f\"{width}x{height}\")\n",
    "\n",
    "    # Exemplo pra apenas atribuir a propriedade save_choice o valor True no nó Especialidade escolhido\n",
    "    # def save_choice(self):\n",
    "    #     if hasattr(self, 'choice') and self.choice:\n",
    "    #         with self.driver.session() as session:\n",
    "    #             session.run(\"MATCH (n:Especialidade) WHERE n.name = $name SET n.user_choice = $choice\", \n",
    "    #                         {\"name\": self.choice[0], \"choice\": True})\n",
    "    #         self.root.destroy()\n",
    "    #     else:\n",
    "    #         print(\"Nenhuma escolha foi feita.\")\n",
    "\n",
    "    def save_choice(self, publication_id):\n",
    "        if hasattr(self, 'choice') and self.choice:\n",
    "            with self.driver.session() as session:\n",
    "                # Cria a nova relação SIMILAR e remove as antigas\n",
    "                session.run(\"\"\"\n",
    "                MATCH (p:Publicacao) WHERE id(p) = $pub_id\n",
    "                MATCH (e:Especialidade) WHERE e.name = $esp_name\n",
    "                MERGE (p)-[:SIMILAR]->(e)\n",
    "                WITH p, e\n",
    "                MATCH (p)-[r:SIMILAR]->(n)\n",
    "                WHERE n <> e\n",
    "                DELETE r\n",
    "                \"\"\", {\"pub_id\": publication_id, \"esp_name\": self.choice[0]})\n",
    "\n",
    "            print(f\"Escolha salva para a publicação: {publication_id}\")\n",
    "            self.root.destroy()\n",
    "        else:\n",
    "            print(\"Nenhuma escolha foi feita.\")\n",
    "\n",
    "    def run_analysis_and_offer_options(self, title):\n",
    "        options = self.predict_speciality(title)\n",
    "        for option in options:\n",
    "            self.tree.insert('', 'end', values=(option['Especialidade'], option['Subárea'], option['Área'], option['GrandeÁrea']))\n",
    "        self.adjust_column_width()\n",
    "        self.adjust_window_size()\n",
    "        self.root.mainloop()\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    system = RecommendationSystem(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n",
    "    title = \"Bioinformática estrutural e biotecnologia para saúde pública\"\n",
    "    system.run_analysis_and_offer_options(title)\n",
    "    system.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimizar busca por melhor Threshold com GML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos pesos aprendidos com a GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Para exibir os parâmetros do modelo:\n",
    "# for name, param in pipeline.model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Ajuste automatizado de hiperparâmetros</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função objetivo para otimizar hiperparâmetros de clusterização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para elaborar uma função objetivo para a otimização de hiperparâmetros em um contexto de aprendizado de máquina com grafos, vamos considerar os seguintes critérios:\n",
    "\n",
    "    Quantidade de Nós Isolados: Minimizar o número de nós Publicacao que estão isolados, ou seja, que não possuem relações SIMILAR, para que a classificação tenha abrangência maior, uma cobertura melhor da amostra de publicações.\n",
    "\n",
    "    Modularidade do Grafo: Maximizar a modularidade do grafo, indicando uma estrutura comunitária forte onde nós dentro do mesmo cluster são densamente conectados.\n",
    "\n",
    "    Quantidade de Arestas SIMILAR: Garantir que cada nó Publicacao tenha um número ótimo de no máximo duas arestas do tipo SIMILAR, visando obter uma maior especificidade para a classificação.\n",
    "\n",
    "A função objetivo combina o resultado desses critérios a partir do cálculo nas funções auxiliares definidas:\n",
    "\n",
    "    calculate_isolation(nodes): Retorna a porcentagem de nós isolados.\n",
    "    calculate_modularity(graph): Retorna o valor da modularidade do grafo.\n",
    "    count_excessive_similarity_edges(nodes): Retorna a porcentagem de nós com mais de duas arestas SIMILAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatizamos a busca por melhores valores dos hiperparâmetros usando o Optuna, uma estrutura de otimização de hiperparâmetros versátil útil em muitos contextos de aprendizado de máquina, incluindo a otimização em grafos. No presente estudo que envolve a clusterização em grafos de conhecimento no âmbito de pesquisa acadêmica e científica, Optuna foi utilizado para ajustar automaticamente hiperparâmetros como thresholds de similaridade, o número de clusters, e os hiperparâmetros de modelos de aprendizado profundo como taxas de aprendizado, número de camadas e unidades em redes neurais.\n",
    "\n",
    "Para integrar o Optuna realizamos os passos de:\n",
    "\n",
    "1. **Delimitar o Espaço de Hiperparâmetros:**\n",
    "   Definimos inicialmente o espaço de busca dos hiperparâmetros para a otimização incluindo o intervalo de 0.1 a 0.9 para o threshold de similaridade que controla a criação de arestas do tipo SIMILAR.\n",
    "\n",
    "2. **Definir a Função Objetivo:**\n",
    "   Criamos uma função objetivo que utiliza esses hiperparâmetros para construir o grafo, aplicamos três algoritmos de clusterização, e calculamos as métricas de quantidade de nós Publicacao isolados, a modularidade do grafo, e a coerência da classificação das publicações com a hierarquia de áreas do conhecimento do CNPq.\n",
    "\n",
    "3. **Rodar a Otimização:**\n",
    "   Executamos iterativamente as buscas de otimização, onde cada tentativa corresponde a uma combinação de hiperparâmetros do espaço definido. Escolhemos automatizadamente as combinações com base em métodos de otimização bayesiana, grade ou aleatórios, para obter a combinação de hiperparâmetros que maximiza o valor retornado pela função objetivo.\n",
    "\n",
    "4. **Analisar Resultados:**\n",
    "   Visualizamos e analisamos os resultados da otimização. O resultado dessas análises permitiu o melhor entendimento de como diferentes hiperparâmetros afetam a performance do modelo e a qualidade dos clusters.\n",
    "\n",
    "Esta fase de estudo de engenharia de hiperparâmetros pode substituir, ou trabalhar em conjunto, com processos manuais, ou semi-automatizados, de busca de hiperparâmetros visando aumentar a eficiência e eficácia do pipeline de otimização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação otimização dos hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "model_name = 'HiperparameterSearch'\n",
    "# Buscar procedimentos vigentes no Neo4j relacionados com uma string\n",
    "neo4j_service = Neo4jService(neo4j_uri, neo4j_user, neo4j_password)\n",
    "simil_service = CosineSimilarityRelationship(neo4j_uri, neo4j_user, neo4j_password, model_name)\n",
    "\n",
    "# Definir listas globais para armazenar métricas de desempenho para plotagem\n",
    "performances = []\n",
    "global_performances = []\n",
    "trial_numbers = []\n",
    "\n",
    "# Função objectivo modificada para adicionar logs\n",
    "def objective(trial):\n",
    "    threshold = trial.suggest_float('threshold', 0.1, 0.9, step=0.05)\n",
    "    projection_name = f\"projection_{threshold:.2f}\"\n",
    "    \n",
    "    # Crie relacionamentos SIMILAR com base no threshold aqui\n",
    "    simil_service.run_similarity_operations(threshold)\n",
    "\n",
    "    # Projeta o grafo antes de calcular as métricas\n",
    "    node_labels = ['Publicacao', 'Subárea', 'Especialidade']\n",
    "    relationship_types = ['SIMILAR']\n",
    "    neo4j_service.project_graph(projection_name, node_labels, relationship_types)\n",
    "\n",
    "    # Avalia a projeção do grafo\n",
    "    metrics, aggregate_score = neo4j_service.evaluate_projection(projection_name)\n",
    "    print(f'Campos de métricas avaliados:{metrics.keys()}')\n",
    "    print(f'Valores das métricas obtidos:{metrics.values()}')\n",
    "\n",
    "    # Verifique se ocorreu um erro durante a avaliação\n",
    "    if 'error' in metrics:\n",
    "        logging.error(f\"Error during evaluation: {metrics['error']}\")\n",
    "        return float('-inf')  # Retorne um valor que indique erro na otimização\n",
    "\n",
    "    # Remove a projeção após a avaliação\n",
    "    neo4j_service.drop_named_graph(projection_name)\n",
    "    logging.info(f\"Dropped graph projection: {projection_name}\")\n",
    "    \n",
    "    performance = metrics['aggregate_score']\n",
    "    print(f'Performance:{performance}')\n",
    "\n",
    "    global_performance = metrics.get('global_performance', 0)\n",
    "    print(f'Global Performance:{global_performance}')\n",
    "    \n",
    "    update_plots(trial.number, performance, global_performance)\n",
    "    \n",
    "    return performance\n",
    "\n",
    "\n",
    "def update_plots(trial_number, performance, global_performance):\n",
    "    # Adicione o desempenho às listas para plotagem\n",
    "    performances.append(performance)\n",
    "    global_performances.append(global_performance)\n",
    "    trial_numbers.append(trial_number)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(19, 6))\n",
    "\n",
    "    # Plot da linha de desempenho\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(trial_numbers, performances, label='Desempenho dos Parâmetros', marker='o')\n",
    "    plt.title('Desempenho dos Parâmetros por Trial')\n",
    "    plt.xlabel('Trial')\n",
    "    plt.ylabel('Desempenho')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot da linha de desempenho global\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(trial_numbers, global_performances, label='Desempenho Global', marker='x')\n",
    "    plt.title('Desempenho Global por Trial')\n",
    "    plt.xlabel('Trial')\n",
    "    plt.ylabel('Desempenho Global')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Iniciar estudo do Optuna e otimização\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_service = Neo4jService(neo4j_uri, neo4j_user, neo4j_password)\n",
    "neo4j_service.drop_all_projections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances = []\n",
    "global_performances = []\n",
    "trial_numbers = []\n",
    "\n",
    "# Criação do estudo Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Imprime o melhor resultado\n",
    "print('Melhor threshold:', study.best_params['threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicação do elementos para clusterização dinâmica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interação entre as classes de clusterização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe `ClusterOptimizer` é complementar à `DynamicClusterManager`. A classe `ClusterOptimizer` é especializada na otimização dos parâmetros de clusterização e na escolha do melhor algoritmo de clusterização para o conjunto de dados específico, resultados esses que serão utilizados na `DynamicClusterManager` que é responsável por gerenciar a clusterização dinâmica em si, incluindo a criação de clusters, atualização e remoção baseada em certos eventos ou critérios.\n",
    "\n",
    "Descrição de cada uma das classes:\n",
    "\n",
    "- **ClusterOptimizer**: Esta classe foca em determinar o melhor valor de threshold para as similaridades e decidir qual algoritmo de clusterização produz os melhores clusters em termos de coerência interna e alinhamento com a estrutura de `GrandeÁrea` do CNPq.\n",
    "\n",
    "- **DynamicClusterManager**: Após a otimização, esta classe utiliza os insights e configurações definidas pela `ClusterOptimizer` para gerenciar os clusters. Ela lida com tarefas como a formação inicial dos clusters, a manutenção dinâmica (como adicionar ou remover nós), e a aplicação de mudanças nos clusters conforme novos dados são inseridos ou atualizados no sistema.\n",
    "\n",
    "Portanto, enquanto a `DynamicClusterManager` é mais sobre a execução e manutenção contínua dos clusters, a `ClusterOptimizer` é sobre a análise prévia e a preparação dos parâmetros para que a clusterização seja a mais eficiente e significativa possível.\n",
    "\n",
    "Temos então um fluxo de trabalho onde a `ClusterOptimizer` é usada periodicamente para reavaliar e ajustar os parâmetros de clusterização (por exemplo, quando novos dados são adicionados ou em intervalos regulares para refinar a clusterização com base em novas informações). Após cada otimização, `DynamicClusterManager` toma o controle para aplicar e gerenciar os clusters de acordo com os parâmetros otimizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de Louvain: \n",
    "\n",
    "Utilizamos na função assign_clusters a detecção do algoritmo de Louvain para identificar as comunidades no grafo. O algoritmo de Louvain é uma heurística popular para a detecção de comunidades que é baseada na otimização da modularidade de uma rede.\n",
    "\n",
    "Utilizamos o algoritmo de Louvain diretamente nas projeções de grafo geradas no neo4j, usando os embeddings de nós para criar um grafo de similaridade a partir da similaridade entre os embeddings de nós."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação das classes\n",
    "\n",
    "    ClusterOptimizer\n",
    "    DynamicClusterManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusterOptimizer:\n",
    "    def __init__(self, neo4j_service, model, start_threshold=0.1, end_threshold=0.9, step=0.1):\n",
    "        self.neo4j_service = neo4j_service\n",
    "        self.model = model\n",
    "        self.start_threshold = start_threshold\n",
    "        self.end_threshold = end_threshold\n",
    "        self.step = step\n",
    "        self.best_threshold = None\n",
    "        self.best_score = -1\n",
    "\n",
    "    def optimize_threshold(self):\n",
    "        for threshold in np.arange(self.start_threshold, self.end_threshold, self.step):\n",
    "            # Create SIMILAR relationships based on the current threshold\n",
    "            self.neo4j_service.create_similarity_relationships(threshold)\n",
    "            \n",
    "            # Run clustering algorithm and obtain cluster labels\n",
    "            projection_name = f\"projection_{threshold}\"\n",
    "            node_labels, relationship_types = self.prepare_graph_projection(projection_name)\n",
    "            clusters = self.run_clustering_algorithm(projection_name, node_labels, relationship_types)\n",
    "            \n",
    "            # Evaluate the clusters\n",
    "            score = silhouette_score(self.model.embeddings, clusters)\n",
    "            \n",
    "            # Check if the current score is the best\n",
    "            if score > self.best_score:\n",
    "                self.best_score = score\n",
    "                self.best_threshold = threshold\n",
    "                \n",
    "            # Log the results\n",
    "            print(f\"Threshold: {threshold}, Silhouette Score: {score}\")\n",
    "\n",
    "        print(f\"Best threshold: {self.best_threshold}, Best Silhouette Score: {self.best_score}\")\n",
    "        return self.best_threshold\n",
    "\n",
    "    def prepare_graph_projection(self, projection_name):\n",
    "        # Use gds.graph.project to create the graph projection\n",
    "        node_labels = ['Node']  # Replace with your actual node labels\n",
    "        relationship_types = ['SIMILAR']  # Replace with your actual relationship types\n",
    "        self.neo4j_service.project_graph(projection_name, node_labels, relationship_types)\n",
    "        return node_labels, relationship_types\n",
    "\n",
    "    def run_kmeans(self, projection_name, k):\n",
    "        return self.neo4j_service.run_kmeans(projection_name, k)\n",
    "\n",
    "    def run_louvain(self, projection_name):\n",
    "        return self.neo4j_service.run_louvain(projection_name)\n",
    "\n",
    "    def run_label_propagation(self, projection_name):\n",
    "        return self.neo4j_service.run_label_propagation(projection_name)\n",
    "\n",
    "    def apply_clustering_algorithm(self, projection_name, algorithm='kmeans', **kwargs):\n",
    "        if algorithm == 'kmeans':\n",
    "            clusters = self.run_kmeans(projection_name, **kwargs)\n",
    "        elif algorithm == 'louvain':\n",
    "            clusters = self.run_louvain(projection_name)\n",
    "        elif algorithm == 'label_propagation':\n",
    "            clusters = self.run_label_propagation(projection_name)\n",
    "        else:\n",
    "            raise ValueError(f\"Algoritmo {algorithm} não suportado.\")\n",
    "        return clusters\n",
    "\n",
    "    def cleanup(self, projection_name):\n",
    "        self.neo4j_service.drop_named_graph(projection_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicClusterManager:\n",
    "    def __init__(self, neo4j_uri, neo4j_user, neo4j_password, model):\n",
    "        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "        self.model = model\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    def fetch_embeddings(self):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"MATCH (n) WHERE EXISTS(n.embedding) RETURN id(n) AS node_id, n.embedding AS embedding\")\n",
    "            embeddings = {record['node_id']: record['embedding'] for record in result}\n",
    "        return embeddings\n",
    "\n",
    "    def cluster_embeddings(self, embeddings, n_clusters):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        node_ids = list(embeddings.keys())\n",
    "        embedding_vectors = [embeddings[node_id] for node_id in node_ids]\n",
    "        labels = kmeans.fit_predict(embedding_vectors)\n",
    "        return dict(zip(node_ids, labels))\n",
    "\n",
    "    def evaluate_clusters(self, embeddings, labels):\n",
    "        embedding_vectors = [embeddings[node_id] for node_id in labels.keys()]\n",
    "        score = silhouette_score(embedding_vectors, list(labels.values()))\n",
    "        return score\n",
    "\n",
    "    def interpret_clusters(self, labels):\n",
    "        \"\"\"\n",
    "        Interpreta os clusters com base em métricas definidas, otimizando o número de nós Publicacao isolados,\n",
    "        a modularidade do grafo, e limitando o número de arestas SIMILAR para cada nó Publicacao.\n",
    "\n",
    "        :param labels: Um dicionário de node_id para label de cluster.\n",
    "        \"\"\"\n",
    "        # Obtendo informações do grafo para avaliação\n",
    "        graph_info = self.neo4j_service.fetch_graph_info()\n",
    "        isolated_publications = self.neo4j_service.count_isolated_nodes('Publicacao')\n",
    "        graph_modularity = self.neo4j_service.calculate_modularity(labels)\n",
    "        excessive_similar_edges = self.neo4j_service.count_excessive_similar_edges('Publicacao', max_edges=2)\n",
    "\n",
    "        # Definindo a função objetivo\n",
    "        objective_score = self.calculate_objective_score(isolated_publications, graph_modularity, excessive_similar_edges)\n",
    "\n",
    "        return {\n",
    "            'isolated_publications': isolated_publications,\n",
    "            'graph_modularity': graph_modularity,\n",
    "            'excessive_similar_edges': excessive_similar_edges,\n",
    "            'objective_score': objective_score\n",
    "        }\n",
    "\n",
    "    def calculate_objective_score(self, isolated_count, modularity, excessive_edge_count):\n",
    "        \"\"\"\n",
    "        Calcula uma pontuação de objetivo com base nas métricas.\n",
    "\n",
    "        :param isolated_count: Número de publicações isoladas.\n",
    "        :param modularity: Modularidade do grafo.\n",
    "        :param excessive_edge_count: Número de nós Publicacao com mais de duas arestas SIMILAR.\n",
    "        :return: Uma pontuação de objetivo composta que reflete a qualidade da clusterização.\n",
    "        \"\"\"\n",
    "        # Ajustar esses pesos com base na importância relativa de cada métrica\n",
    "        weight_isolated = -1  # Penaliza publicações isoladas\n",
    "        weight_modularity = 1  # Valoriza a modularidade\n",
    "        weight_excessive_edges = -0.5  # Penaliza excesso de arestas SIMILAR\n",
    "\n",
    "        # Calcula a pontuação do objetivo\n",
    "        score = (weight_isolated * isolated_count +\n",
    "                 weight_modularity * modularity +\n",
    "                 weight_excessive_edges * excessive_edge_count)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def apply_clusters(self, labels):\n",
    "        with self.driver.session() as session:\n",
    "            for node_id, cluster_label in labels.items():\n",
    "                session.run(\"MATCH (n) WHERE id(n) = $node_id SET n.cluster = $cluster_label\", node_id=node_id, cluster_label=cluster_label)\n",
    "\n",
    "    def run_clustering_pipeline(self, n_clusters):\n",
    "        try:\n",
    "            self.logger.info(\"Fetching embeddings from Neo4j...\")\n",
    "            embeddings = self.fetch_embeddings()\n",
    "\n",
    "            self.logger.info(\"Clustering embeddings...\")\n",
    "            labels = self.cluster_embeddings(embeddings, n_clusters)\n",
    "\n",
    "            self.logger.info(\"Evaluating clusters...\")\n",
    "            score = self.evaluate_clusters(embeddings, labels)\n",
    "            self.logger.info(f\"Silhouette Score: {score}\")\n",
    "\n",
    "            self.logger.info(\"Interpreting clusters...\")\n",
    "            self.interpret_clusters(labels)\n",
    "\n",
    "            self.logger.info(\"Applying clusters to the graph...\")\n",
    "            self.apply_clusters(labels)\n",
    "\n",
    "            self.logger.info(\"Clustering pipeline completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred during the clustering pipeline: {e}\")\n",
    "            raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interação com os dados do Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para montar o dataset a ser utilizado pelo GMLTrainer como uma lista de objetos Data do PyTorch Geometric, contendo os embeddings carregados, e usar os algoritmos do Neo4j para realizar os cálculos nas projeções do grafo, implementamos a classe DataLoaderExtension. \n",
    "\n",
    "Para utilizar o treinamento dinâmico, em vez de calcular a modularidade dentro da classe, focamos em extrair os dados necessários do Neo4j e converter esses dados em um formato que o PyTorch Geometric possa utilizar.\n",
    "\n",
    "A classe DataLoaderExtension é capaz de extrair os dados de embeddings de nós e arestas do Neo4j, convertê-los em tensores do PyTorch e encapsulá-los em um objeto Data do PyTorch Geometric. A função create_data_loader cria um DataLoader que pode ser usado para iterar sobre o conjunto de dados durante o treinamento do modelo.\n",
    "\n",
    "Para calcular a modularidade diretamente no Neo4j, a função calculate_modularity_neo4j faz uso da implementação do algoritmo de Louvain do Neo4j, que é eficiente e evita a necessidade de transportar grandes quantidades de dados do Neo4j para a aplicação Python.\n",
    "\n",
    "Essa abordagem permite aproveitar a potência computacional do Neo4j para realizar cálculos complexos de grafos e usar diretamente os resultados desses cálculos na criação de modelos de aprendizado de máquina em grafos com o PyTorch Geometric.\n",
    "\n",
    "Observação sobre tráfego entre CPU e GPU:\n",
    "\n",
    "Caso o modelo default seja implementado, certamente ocorrerá um erro em tempo de execução parecido com este:\n",
    "\n",
    "    RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned \n",
    " \n",
    "O que indica que o DataLoader está tentando fixar (pin) uma tensor que já está na GPU, enquanto o pinning de memória é uma operação que só pode ser feita com tensores na CPU. O pinning de memória é útil para transferências de dados mais rápidas da CPU para a GPU, mas não é aplicável a tensores que já estão na GPU.\n",
    "\n",
    "Para evitar ou corrigir o erro, garantimos que o pinning só seja feito em tensores que estão na CPU e somente se isso for necessário, porque em alguns casos essa tarefa de pinning só cabe em alguns caso de uso, casos como ausẽncia de uma GPU, ou quando os cálculos forem computacionalmente proibitivos de ser realizados na GPU. \n",
    "\n",
    "No caso específico do GML descrito neste trabalho já estamos carregando os dados diretamente na GPU, não sendo necessário assim o pinning na maioria dos passos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberar toda a memória cache não alocada na GPU para tornar a memória visível ao gerenciador de memória do CUDA\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "class BaselineClusterer(Neo4jService):\n",
    "    def __init__(self, uri, user, password):\n",
    "        super().__init__(uri, user, password)\n",
    "    \n",
    "    def create_weighted_named_projection(self, label, relationship_type, relationship_property, projection_name):\n",
    "        self.check_and_drop_projection(projection_name)\n",
    "        create_projection_query = f\"\"\"\n",
    "        CALL gds.graph.project(\n",
    "            '{projection_name}',\n",
    "            {{\n",
    "                {label}: {{}}\n",
    "            }},\n",
    "            {{\n",
    "                {relationship_type}: {{\n",
    "                    type: '{relationship_type}',\n",
    "                    properties: '{relationship_property}',\n",
    "                    orientation: 'UNDIRECTED'\n",
    "                }}\n",
    "            }}\n",
    "        )\n",
    "        YIELD graphName\n",
    "        RETURN graphName\n",
    "        \"\"\"\n",
    "        self.execute_write(create_projection_query)\n",
    "\n",
    "    def check_and_drop_projection(self, projection_name, db='neo4j'):\n",
    "        exists_query = \"CALL gds.graph.exists($projection_name) YIELD exists\"\n",
    "        drop_query = \"CALL gds.graph.drop($projection_name)\"\n",
    "        with self._driver.session(database=db) as session:\n",
    "            exists = session.run(exists_query, projection_name=projection_name).single()[0]\n",
    "            if exists:\n",
    "                session.run(drop_query, projection_name=projection_name)\n",
    "\n",
    "    def generate_embeddings(self, projection_name, algorithm='fastRP', embedding_size=128):\n",
    "        embeddings_query = f\"\"\"\n",
    "        CALL gds.{algorithm}.write(\n",
    "            '{projection_name}',\n",
    "            {{\n",
    "                writeProperty: 'embedding',\n",
    "                embeddingDimension: {embedding_size}\n",
    "            }}\n",
    "        )\n",
    "        YIELD nodePropertiesWritten\n",
    "        RETURN nodePropertiesWritten\n",
    "        \"\"\"\n",
    "        return self.execute_write(embeddings_query)\n",
    "\n",
    "    def extract_embeddings(self, projection_name):\n",
    "        get_embeddings_query = f\"\"\"\n",
    "        CALL gds.graph.nodeProperties.stream(\n",
    "            '{projection_name}',\n",
    "            ['embedding']\n",
    "        )\n",
    "        YIELD nodeId, nodeProperty, propertyValue\n",
    "        WHERE nodeProperty = 'embedding'\n",
    "        RETURN nodeId, propertyValue AS embedding\n",
    "        \"\"\"\n",
    "        return self.execute_read(get_embeddings_query)\n",
    "\n",
    "    def apply_clustering(self, projection_name, n_clusters=8):\n",
    "        embeddings_data = self.extract_embeddings(projection_name)\n",
    "        if not embeddings_data:\n",
    "            print(\"Embeddings do not exist for the projection. Please generate them before clustering.\")\n",
    "            return\n",
    "\n",
    "        embeddings = [data['embedding'] for data in embeddings_data]\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        kmeans.fit(embeddings)\n",
    "        \n",
    "        clustering_results = [{'nodeId': data['nodeId'], 'clusterId': label} for data, label in zip(embeddings_data, kmeans.labels_)]\n",
    "        return clustering_results\n",
    "\n",
    "    def run_baseline_clustering(self, projection_name, n_clusters=8, relationship_property='score'):\n",
    "        self.create_weighted_named_projection('Publicacao', 'SIMILAR', relationship_property, projection_name)\n",
    "        return self.apply_clustering(projection_name, n_clusters=n_clusters)\n",
    "\n",
    "    def drop_named_projection(self, projection_name):\n",
    "        drop_projection_query = f\"CALL gds.graph.drop('{projection_name}', false)\"\n",
    "        self.execute_write(drop_projection_query)\n",
    "\n",
    "    # Additional utility methods\n",
    "    def execute_write(self, query, parameters=None, db=None):\n",
    "        with self._driver.session(database=db) as session:\n",
    "            return session.execute_write(self._execute_query, query, parameters)\n",
    "\n",
    "    def execute_read(self, query, parameters=None, db=None):\n",
    "        with self._driver.session(database=db) as session:\n",
    "            return session.execute_read(self._execute_query, query, parameters)\n",
    "\n",
    "    @staticmethod\n",
    "    def _execute_query(tx, query, parameters=None):\n",
    "        result = tx.run(query, parameters or {})\n",
    "        return [record.data() for record in result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # EXECUÇÃO SÍNCRONA NA GPU, SOMENTE PARA DEBUG:\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class ThresholdFinder:\n",
    "    def __init__(self, baseline_clusterer, gnn_model, thresholds, epochs, early_stopping_patience):\n",
    "        self.baseline_clusterer = baseline_clusterer\n",
    "        self.gnn_model = gnn_model\n",
    "        self.thresholds = thresholds\n",
    "        self.epochs = epochs\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.best_threshold = None\n",
    "        self.best_metric = -float('inf')\n",
    "        self.fig, self.ax = plt.subplots()  # Inicializar fig e ax aqui\n",
    "        self.line, = self.ax.plot([], [], 'b-', label='Training Loss')  # Inicializar line aqui\n",
    "\n",
    "    @staticmethod\n",
    "    def contrastive_loss(embeddings, edge_index, edge_weight=None, margin=0.1):\n",
    "        # Obtem os embeddings dos nós de partida e de chegada das arestas\n",
    "        start_embeddings = embeddings[edge_index[0]]\n",
    "        end_embeddings = embeddings[edge_index[1]]\n",
    "        \n",
    "        # Calcula a similaridade de cosseno entre eles\n",
    "        cosine_similarity = torch.nn.functional.cosine_similarity(start_embeddings, end_embeddings)\n",
    "        \n",
    "        # Se edge_weight for fornecido, use-o para ponderar a similaridade\n",
    "        if edge_weight is not None:\n",
    "            cosine_similarity *= edge_weight\n",
    "        \n",
    "        # Perda para pares positivos (queremos maximizar a similaridade de cosseno, ou seja, minimizar o negativo dela)\n",
    "        positive_loss = -cosine_similarity\n",
    "\n",
    "        # Para pares negativos (não conectados), você pode amostrar aleatoriamente ou usar uma abordagem baseada em todos os pares\n",
    "        # Aqui, simplesmente subtrai a similaridade do margin para criar um contraste\n",
    "        negative_loss = torch.relu(margin - cosine_similarity)\n",
    "\n",
    "        # Combina perdas positivas e negativas\n",
    "        loss = positive_loss.mean() + negative_loss.mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def assign_clusters(self, projection_name):\n",
    "        # Este método agora deve retornar os rótulos das comunidades e o Silhouette Score\n",
    "        clustering_results = self.baseline_clusterer.run_baseline_clustering(projection_name, n_clusters=8)\n",
    "        if clustering_results:\n",
    "            labels = np.array([result['clusterId'] for result in clustering_results])\n",
    "            embeddings = np.array([result['embedding'] for result in clustering_results])\n",
    "            score = silhouette_score(embeddings, labels)\n",
    "            return labels, score\n",
    "        else:\n",
    "            return None, -1\n",
    "\n",
    "    def find_best_threshold(self):\n",
    "        for threshold in self.thresholds:\n",
    "            projection_name = f\"threshold_{threshold}\"\n",
    "            # Aqui você precisa garantir que as operações necessárias para criar os relacionamentos no Neo4j foram realizadas\n",
    "            # Isso é, os relacionamentos de similaridade foram criados e a projeção do grafo está pronta para ser usada\n",
    "            labels, score = self.assign_clusters(projection_name)\n",
    "            \n",
    "            if score > self.best_metric:\n",
    "                self.best_metric = score\n",
    "                self.best_threshold = threshold\n",
    "\n",
    "            # A projeção deve ser apagada para liberar recursos se não for mais necessária\n",
    "            self.baseline_clusterer.drop_named_projection(projection_name)\n",
    "\n",
    "        return self.best_threshold, self.best_metric\n",
    "\n",
    "    def train_and_evaluate(self, data_loader):\n",
    "        optimizer = torch.optim.Adam(self.gnn_model.parameters())\n",
    "        best_metric = -float('inf')\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        self.setup_plot()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            for batch_data in data_loader:\n",
    "                self.gnn_model.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Verifica se há arestas no batch_data\n",
    "                if hasattr(batch_data, 'edge_index') and batch_data.edge_index.size(1) > 0:\n",
    "                    edge_index = batch_data.edge_index\n",
    "                    edge_weight = batch_data.edge_attr if hasattr(batch_data, 'edge_attr') else None\n",
    "                    embeddings = self.gnn_model(batch_data.x, edge_index, edge_weight)\n",
    "                    loss = self.contrastive_loss(embeddings, edge_index, edge_weight)\n",
    "                else:\n",
    "                    # Se não houver arestas, pula o cálculo da perda\n",
    "                    continue  # Ou você pode definir um comportamento diferente para o caso de não haver arestas\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Calcula e imprime a perda média\n",
    "            avg_loss = total_loss / len(data_loader)\n",
    "            logging.info(f'Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss}')\n",
    "            self.plot_loss(epoch, avg_loss)\n",
    "\n",
    "            # Avaliação e cálculo do Silhouette Score\n",
    "            # ... [restante do código para avaliação] ...\n",
    "\n",
    "        plt.ioff()\n",
    "        return best_metric, best_loss\n",
    "\n",
    "    def setup_plot(self):\n",
    "        # Configura o gráfico de plotagem.\n",
    "        self.fig, self.ax = plt.subplots(figsize=(19, 6))\n",
    "        self.ax.set_xlabel('Epoch')\n",
    "        self.ax.set_ylabel('Loss')\n",
    "        self.ax.set_title('Training Loss Over Time')\n",
    "        self.line, = self.ax.plot([], [], 'b-', label='Training Loss')\n",
    "        self.ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss(self, epoch, loss_value):\n",
    "        # Atualiza os dados da linha com a nova perda.\n",
    "        xdata = list(self.line.get_xdata()) + [epoch]\n",
    "        ydata = list(self.line.get_ydata()) + [loss_value]\n",
    "        self.line.set_xdata(xdata)\n",
    "        self.line.set_ydata(ydata)\n",
    "\n",
    "        # Re-calcule os limites com base nos novos dados.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view(True, True, True)\n",
    "\n",
    "        # Atualize o gráfico com os novos dados.\n",
    "        clear_output(wait=True)\n",
    "        display(self.fig)\n",
    "\n",
    "'''\n",
    "Nesta classe o pinning de memória é ativado somente se o tensor data não estiver na GPU (not data.is_cuda). O DataLoader não tentará fixar tensores que já estão na GPU. Além disso, verifique se a criação dos tensores no método _create_tensors_from_nodes_and_edges só transfere os dados para a GPU após a criação do DataLoader. Se estiver transferindo antes, isso pode causar problemas com o pinning de memória. O método to(self.device) só deve ser chamado depois de os tensores serem fixados na memória e carregados pelo DataLoader.\n",
    "\n",
    "'''\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "import logging\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class DataLoaderExtension:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def _create_tensors_from_nodes_and_edges(self, nodes, edges):\n",
    "        \"\"\"Transform nodes and edges into PyTorch tensors.\"\"\"\n",
    "        try:\n",
    "            # Garanta que 'embedding' é uma lista ou um tensor\n",
    "            x = torch.tensor([node['embedding'] for node in nodes], dtype=torch.float)\n",
    "            \n",
    "            # Garanta que os índices das arestas sejam uma lista de listas ou uma lista de tuplas\n",
    "            edge_index = torch.tensor([[edge['source'], edge['target']] for edge in edges], dtype=torch.long)\n",
    "            \n",
    "            # Certifique-se de que edge_index é uma matriz de 2 x N\n",
    "            if edge_index.numel() == 0:\n",
    "                edge_index = edge_index.view(2, -1)\n",
    "            else:\n",
    "                edge_index = edge_index.t().contiguous()\n",
    "\n",
    "            # Atributos das arestas devem ser uma lista simples se fornecidos na consulta\n",
    "            edge_weights = torch.tensor([edge['weight'] for edge in edges], dtype=torch.float)\n",
    "\n",
    "            # Certifique-se de que os índices estão dentro dos limites antes de transferir para a GPU\n",
    "            if edge_index.numel() > 0:  # Só verifica se edge_index não está vazio\n",
    "                max_index = edge_index.max()\n",
    "                if max_index >= x.size(0):\n",
    "                    raise IndexError(\"Índice de aresta fora dos limites: \"\n",
    "                                     f\"máximo índice {max_index}, número de nós {x.size(0)}.\")\n",
    "\n",
    "            return x.to(self.device), edge_index.to(self.device), edge_weights.to(self.device)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro ao criar tensores a partir de nós e arestas: {e}\")\n",
    "            raise\n",
    "\n",
    "    def fetch_graph_data(self):\n",
    "        \"\"\"Fetch graph data from Neo4j and create a PyTorch Geometric Data object.\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            nodes = session.run(\"MATCH (n:Publicacao) RETURN id(n) AS id, n.embedding AS embedding\").data()\n",
    "            edges = session.run(\"\"\"\n",
    "                MATCH (n1:Publicacao)-[r:SIMILAR]->(n2:Publicacao)\n",
    "                RETURN id(n1) AS source, id(n2) AS target, r.weight AS weight\n",
    "            \"\"\").data()\n",
    "\n",
    "            # Validação adicional para garantir que os índices das arestas estão corretos\n",
    "            nodes_ids = {node['id']: i for i, node in enumerate(nodes)}\n",
    "            edges = [\n",
    "                (nodes_ids[edge['source']], nodes_ids[edge['target']], edge['weight'])\n",
    "                for edge in edges\n",
    "                if edge['source'] in nodes_ids and edge['target'] in nodes_ids\n",
    "            ]\n",
    "\n",
    "            x, edge_index, edge_attr = self._create_tensors_from_nodes_and_edges(nodes, edges)\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            return data\n",
    "    \n",
    "    def create_data_loader(self, batch_size=32):\n",
    "        \"\"\"Creates a DataLoader with the provided batch size.\"\"\"\n",
    "        data = self.fetch_graph_data()\n",
    "        data_list = [data]  # DataLoader expects a list of Data objects\n",
    "        data_loader = DataLoader(data_list, batch_size=batch_size, shuffle=True, pin_memory=(self.device.type == 'cpu'))\n",
    "        return data_loader\n",
    "    \n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN_weight(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN_weight, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index=None, edge_weight=None):\n",
    "        # Ajuste para aceitar tensores diretamente\n",
    "        if edge_index is not None and edge_weight is not None:\n",
    "            x = torch.relu(self.conv1(x, edge_index, edge_weight))\n",
    "            x = self.conv2(x, edge_index, edge_weight)\n",
    "        else:\n",
    "            # Se edge_index é None, significa que não há arestas. Nesse caso, podemos processar apenas os recursos dos nós.\n",
    "            x = torch.relu(self.conv1(x))\n",
    "            x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Certifique-se de que todas as classes relevantes estejam importadas:\n",
    "# DataLoaderExtension, ThresholdFinder, BaselineClusterer, GCN_weight\n",
    "\n",
    "# Configuração inicial e parâmetros do Neo4j\n",
    "# neo4j_uri = 'bolt://localhost:7687'\n",
    "# neo4j_user = 'neo4j'\n",
    "# neo4j_password = 'password'\n",
    "\n",
    "# Parâmetros do modelo\n",
    "input_dim = 768  # O tamanho dos embeddings de entrada\n",
    "hidden_dim = 64  # Tamanho da camada oculta\n",
    "output_dim = 8   # Número esperado de comunidades/clusters\n",
    "\n",
    "# Configurar ambiente de logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Identificar se a GPU está disponível e configurar o dispositivo de treinamento\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f\"Treinamento será realizado no dispositivo: {device}\")\n",
    "\n",
    "# Instanciar DataLoaderExtension com as credenciais do Neo4j\n",
    "data_loader_extension = DataLoaderExtension(neo4j_uri, neo4j_user, neo4j_password)\n",
    "\n",
    "# Criar DataLoader para PyTorch Geometric\n",
    "data_loader = data_loader_extension.create_data_loader(batch_size=32)\n",
    "\n",
    "# Instanciar BaselineClusterer com as credenciais do Neo4j\n",
    "baseline_clusterer = BaselineClusterer(neo4j_uri, neo4j_user, neo4j_password)\n",
    "\n",
    "# Definir o modelo GNN a ser treinado\n",
    "model = GCN_weight(input_dim, hidden_dim, output_dim)\n",
    "model.to(device)  # Mover o modelo para o dispositivo correto (CPU ou GPU)\n",
    "\n",
    "# Definir os limites para os thresholds a serem testados\n",
    "thresholds = np.arange(1.0, 0.7, -0.05)\n",
    "\n",
    "# Configurar o número de épocas e a paciência para parada antecipada\n",
    "epochs = 100\n",
    "early_stopping_patience = 10\n",
    "\n",
    "# Instanciar a classe ThresholdFinder\n",
    "threshold_finder = ThresholdFinder(\n",
    "    baseline_clusterer=baseline_clusterer,\n",
    "    gnn_model=model,\n",
    "    thresholds=thresholds,\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "best_metric, best_loss = threshold_finder.train_and_evaluate(data_loader)\n",
    "\n",
    "# Após o treinamento e avaliação, encontre o melhor threshold\n",
    "best_threshold, best_metric = threshold_finder.find_best_threshold()\n",
    "logging.info(f\"Melhor threshold encontrado: {best_threshold} com Silhouette Score: {best_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class TrainingPipeline:\n",
    "    def __init__(self, neo4j_uri, neo4j_user, neo4j_password, model, optimizer=None, model_name=\"default_model\"):\n",
    "        self.neo4j_service = CosineSimilarityRelationship(neo4j_uri, neo4j_user, neo4j_password, model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optimizer if optimizer else torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "        self.loss_history = []\n",
    "        self.fig, self.ax = plt.subplots(figsize=(19, 6))\n",
    "        self.ax.set_xlabel('Epoch')\n",
    "        self.ax.set_ylabel('Loss')\n",
    "        self.line, = self.ax.plot([], [], 'b-', label='Training Loss')\n",
    "        self.ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Restante dos métodos...\n",
    "\n",
    "    def train(self, data, epochs, early_stopping_patience=10, loss_fn=None):\n",
    "        logging.info(\"Iniciando o treinamento...\")\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        self.setup_plot()  # Configura o gráfico de plotagem.\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = loss_fn(output, data) if loss_fn else self.compute_loss(output, data)\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                logging.error(f'Loss inválida detectada na época {epoch+1}: {loss}')\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize(self.device)\n",
    "\n",
    "            loss_value = loss.item()\n",
    "            self.loss_history.append(loss_value)\n",
    "            logging.info(f'Época {epoch+1}/{epochs}, Perda: {loss_value}')\n",
    "\n",
    "            if loss_value < best_loss:\n",
    "                best_loss = loss_value\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                logging.info(f\"Parada antecipada acionada na época {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "            self.plot_loss(epoch)\n",
    "\n",
    "        plt.ioff()\n",
    "\n",
    "    def plot_loss(self, epoch):\n",
    "        self.line.set_xdata(np.append(self.line.get_xdata(), epoch))\n",
    "        self.line.set_ydata(np.append(self.line.get_ydata(), self.loss_history[-1]))\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view(True, True, True)\n",
    "        clear_output(wait=True)\n",
    "        display(self.fig)\n",
    "\n",
    "\n",
    "# Inicialize um modelo GML\n",
    "gnn_model = GCN_weight(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Crie um otimizador para o modelo, se necessário\n",
    "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01)\n",
    "\n",
    "# Crie uma função de perda específica para o seu modelo, se necessário\n",
    "def custom_loss_fn(output, data):\n",
    "    # Implemente sua função de perda personalizada\n",
    "    return torch.nn.functional.mse_loss(output, data.y)\n",
    "\n",
    "# Crie uma instância do pipeline de treinamento\n",
    "training_pipeline = TrainingPipeline(\n",
    "    neo4j_uri, neo4j_user, neo4j_password, \n",
    "    gnn_model, \n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "# Carregue os dados do Neo4j\n",
    "data = training_pipeline.load_data_from_neo4j()\n",
    "\n",
    "# Treine o modelo\n",
    "training_pipeline.train(data, epochs=100, early_stopping_patience=10, loss_fn=custom_loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from collections import defaultdict\n",
    "\n",
    "# class ModelController:\n",
    "#     def __init__(self, neo4j_uri, neo4j_user, neo4j_password, models, loss_functions):\n",
    "#         \"\"\"\n",
    "#         Inicializa o controlador de modelos.\n",
    "        \n",
    "#         :param neo4j_uri: URI para conexão com o Neo4j.\n",
    "#         :param neo4j_user: Usuário para conexão com o Neo4j.\n",
    "#         :param neo4j_password: Senha para conexão com o Neo4j.\n",
    "#         :param models: Um dicionário de modelos GNN onde a chave é o nome do modelo e o valor é a instância do modelo.\n",
    "#         :param loss_functions: Um dicionário de funções de perda onde a chave é o nome do modelo e o valor é a função de perda.\n",
    "#         \"\"\"\n",
    "#         self.neo4j_uri = neo4j_uri\n",
    "#         self.neo4j_user = neo4j_user\n",
    "#         self.neo4j_password = neo4j_password\n",
    "#         self.models = models\n",
    "#         self.loss_functions = loss_functions\n",
    "#         self.results = defaultdict(dict)\n",
    "\n",
    "#     def run(self, epochs, early_stopping_patience):\n",
    "#         \"\"\"\n",
    "#         Executa o treinamento para cada modelo e função de perda fornecidos.\n",
    "        \n",
    "#         :param epochs: Número de épocas para treinar cada modelo.\n",
    "#         :param early_stopping_patience: Paciência para a parada antecipada.\n",
    "#         \"\"\"\n",
    "#         for model_name, model in self.models.items():\n",
    "#             print(f\"Iniciando treinamento para o modelo: {model_name}\")\n",
    "            \n",
    "#             # Prepara o otimizador para o modelo atual\n",
    "#             optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "            \n",
    "#             # Prepara o pipeline de treinamento para o modelo atual\n",
    "#             training_pipeline = TrainingPipeline(\n",
    "#                 self.neo4j_uri, self.neo4j_user, self.neo4j_password, \n",
    "#                 model, optimizer\n",
    "#             )\n",
    "            \n",
    "#             # Carrega dados do Neo4j\n",
    "#             data = training_pipeline.load_data_from_neo4j()\n",
    "            \n",
    "#             # Pega a função de perda correspondente ao modelo atual\n",
    "#             loss_fn = self.loss_functions[model_name]\n",
    "            \n",
    "#             # Treina o modelo e coleta os resultados\n",
    "#             training_pipeline.train(data, epochs, early_stopping_patience, loss_fn)\n",
    "            \n",
    "#             # Armazena os resultados para comparação posterior\n",
    "#             self.results[model_name]['loss_history'] = training_pipeline.loss_history\n",
    "            \n",
    "#             # Aqui você pode adicionar mais métricas se desejar\n",
    "#             # Por exemplo: self.results[model_name]['accuracy'] = compute_accuracy(...)\n",
    "            \n",
    "#             print(f\"Treinamento concluído para o modelo: {model_name}\")\n",
    "        \n",
    "#         # Retornar resultados para análise posterior\n",
    "#         return self.results\n",
    "\n",
    "# # Exemplo de como usar a classe ModelController\n",
    "# models = {\n",
    "#     'ModeloA': GNNModelA(input_dim, hidden_dim, output_dim),\n",
    "#     'ModeloB': GNNModelB(input_dim, hidden_dim, output_dim)\n",
    "# }\n",
    "\n",
    "# loss_functions = {\n",
    "#     'ModeloA': custom_loss_fn_model_a,\n",
    "#     'ModeloB': custom_loss_fn_model_b\n",
    "# }\n",
    "\n",
    "# controller = ModelController(neo4j_uri, neo4j_user, neo4j_password, models, loss_functions)\n",
    "# results = controller.run(epochs=100, early_stopping_patience=10)\n",
    "\n",
    "# # Após a execução, 'results' contém o histórico de perda para cada modelo,\n",
    "# # que você pode usar para comparar a performance entre eles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from torch_geometric.data import DataLoader\n",
    "\n",
    "# class ThresholdFinder:\n",
    "#     def __init__(self, neo4j_service, gnn_model, data_loader_func, thresholds, epochs, early_stopping_patience):\n",
    "#         self.neo4j_service = neo4j_service\n",
    "#         self.gnn_model = gnn_model\n",
    "#         self.data_loader_func = data_loader_func\n",
    "#         self.thresholds = thresholds\n",
    "#         self.epochs = epochs\n",
    "#         self.early_stopping_patience = early_stopping_patience\n",
    "#         self.best_threshold = None\n",
    "#         self.best_metric = -float('inf')\n",
    "\n",
    "#     def contrastive_loss(embeddings, edge_index, edge_weight=None, margin=0.1):\n",
    "#         # Obtem os embeddings dos nós de partida e de chegada das arestas\n",
    "#         start_embeddings = embeddings[edge_index[0]]\n",
    "#         end_embeddings = embeddings[edge_index[1]]\n",
    "        \n",
    "#         # Calcula a similaridade de cosseno entre eles\n",
    "#         cosine_similarity = torch.nn.functional.cosine_similarity(start_embeddings, end_embeddings)\n",
    "        \n",
    "#         # Se edge_weight for fornecido, use-o para ponderar a similaridade\n",
    "#         if edge_weight is not None:\n",
    "#             cosine_similarity *= edge_weight\n",
    "        \n",
    "#         # Perda para pares positivos (queremos maximizar a similaridade de cosseno, ou seja, minimizar o negativo dela)\n",
    "#         positive_loss = -cosine_similarity\n",
    "\n",
    "#         # Para pares negativos (não conectados), você pode amostrar aleatoriamente ou usar uma abordagem baseada em todos os pares\n",
    "#         # Aqui, simplesmente subtrai a similaridade do margin para criar um contraste\n",
    "#         negative_loss = torch.relu(margin - cosine_similarity)\n",
    "\n",
    "#         # Combina perdas positivas e negativas\n",
    "#         loss = positive_loss.mean() + negative_loss.mean()\n",
    "        \n",
    "#         return loss\n",
    "\n",
    "#     def find_best_threshold(self):\n",
    "#         for threshold in self.thresholds:\n",
    "#             # Crie relações de similaridade usando o threshold atual\n",
    "#             self.neo4j_service.create_similarity_embeedings_relationships(threshold)\n",
    "#             # Carregue os dados e treine o modelo\n",
    "#             metric, _ = self.train_and_evaluate(self.data_loader_func())\n",
    "\n",
    "#             if metric > self.best_metric:\n",
    "#                 self.best_metric = metric\n",
    "#                 self.best_threshold = threshold\n",
    "\n",
    "#         return self.best_threshold, self.best_metric\n",
    "\n",
    "#     def train_and_evaluate(self, data_loader):\n",
    "#         optimizer = torch.optim.Adam(self.gnn_model.parameters())\n",
    "#         best_metric = -float('inf')\n",
    "#         best_loss = float('inf')\n",
    "#         patience_counter = 0\n",
    "\n",
    "#         for epoch in range(self.epochs):\n",
    "#             total_loss = 0\n",
    "#             for data in data_loader:\n",
    "#                 self.gnn_model.train()\n",
    "#                 optimizer.zero_grad()\n",
    "#                 embeddings = self.gnn_model(data.x, data.edge_index)\n",
    "#                 loss = self.contrastive_loss(embeddings, data.edge_index, data.edge_attr)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 total_loss += loss.item()\n",
    "\n",
    "#             avg_loss = total_loss / len(data_loader)\n",
    "#             print(f'Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss}')\n",
    "\n",
    "#             # Avaliação e cálculo do Silhouette Score aqui:\n",
    "#             self.gnn_model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 embeddings = self.gnn_model(data.x, data.edge_index)\n",
    "#                 # Substitua 'assign_clusters' pela sua própria função que atribui os nós aos clusters\n",
    "#                 labels = assign_clusters(embeddings)  \n",
    "#                 if len(np.unique(labels)) > 1:  # O Silhouette Score requer mais de um cluster\n",
    "#                     metric = silhouette_score(embeddings.cpu().numpy(), labels.cpu().numpy())\n",
    "#                     print(f'Silhouette Score: {metric}')\n",
    "#                     if metric > best_metric:\n",
    "#                         best_metric = metric\n",
    "#                         patience_counter = 0\n",
    "#                     else:\n",
    "#                         patience_counter += 1\n",
    "#                         if patience_counter >= self.early_stopping_patience:\n",
    "#                             print(f\"Parada antecipada acionada na época {epoch+1}.\")\n",
    "#                             break\n",
    "#                 else:\n",
    "#                     print(\"Não foi possível calcular o Silhouette Score com um único cluster.\")\n",
    "\n",
    "#         return best_metric, best_loss\n",
    "\n",
    "# # Uso da classe ThresholdFinder:\n",
    "# # ...\n",
    "# # Você precisará definir 'neo4j_service', 'gnn_model' e 'data_loader_func' de acordo com a sua aplicação.\n",
    "# # thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "# # finder = ThresholdFinder(neo4j_service, gnn_model, data_loader_func, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Chamada:\n",
    "# neo4j_service = ...  # Your Neo4j service instance\n",
    "# gnn_model = ...  # Your GNN model instance\n",
    "# data_loader_func = ...  # Function to create a DataLoader instance\n",
    "# thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "# epochs = 100\n",
    "# early_stopping_patience = 10\n",
    "\n",
    "# finder = ThresholdFinder(neo4j_service, gnn_model, data_loader_func, metrics_func, thresholds, epochs, early_stopping_patience)\n",
    "# best_threshold, best_metric = finder.find_best_threshold()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizando o modelo para obter o melhor threshold\n",
    "\n",
    "Com o que já fizemos vamos implementar uma classe robusta que determina o melhor threshold de similaridade, com os seguintes passos:\n",
    "\n",
    "    Definir um conjunto de valores de threshold para testar.\n",
    "    Para cada threshold, criar relações SIMILAR no Neo4j.\n",
    "    Carregar os dados e treinar o modelo de machine learning em grafos para cada threshold.\n",
    "    Avaliar a qualidade dos clusters formados utilizando métricas adequadas.\n",
    "    Comparar as métricas de todos os thresholds e selecionar o melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes para implementar o modelo de GML\n",
    "\n",
    "Para continuar com a implementação, usamos as classes para:\n",
    "\n",
    "    Extrair os vetores de embeddings dos nós do Neo4j.\n",
    "    Construir um grafo de similaridade com base na distância do cosseno entre os vetores de embeddings.\n",
    "    Aplicar algoritmos de clusterização não supervisionados como o Louvain.\n",
    "    Avaliar e ajustar iterativamente o threshold de similaridade e outros parâmetros do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class GraphEmbeddingService:\n",
    "    def __init__(self, cosine_similarity_service, max_workers=10):\n",
    "        \"\"\"\n",
    "        Initialize the service with an instance of CosineSimilarityRelationship and set up logging.\n",
    "        \"\"\"\n",
    "        self.cosine_similarity_service = cosine_similarity_service\n",
    "        self.max_workers = max_workers\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "    def fetch_embeddings(self, label):\n",
    "        \"\"\"\n",
    "        Use the CosineSimilarityRelationship service to fetch embeddings with exception handling.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            embeddings = self.cosine_similarity_service.get_all_embeddings(label)\n",
    "            logging.info(f\"Fetched {len(embeddings)} embeddings for label {label}.\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching embeddings for label {label}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _calculate_and_persist_similarity(self, source, target, source_label, target_label, threshold):\n",
    "        \"\"\"\n",
    "        A helper function that is meant to run in a thread, calculating and persisting similarity.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            similarity = self.cosine_similarity_service.cosine_similarity(\n",
    "                np.array(source[\"embedding\"]), np.array(target[\"embedding\"]))\n",
    "            normalized_similarity = self.cosine_similarity_service.normalize_similarity(similarity)\n",
    "            if similarity > threshold:\n",
    "                self.cosine_similarity_service.graph.run(\n",
    "                    \"\"\"\n",
    "                    MATCH (source:{source_label}) WHERE id(source) = $source_id\n",
    "                    MATCH (target:{target_label}) WHERE id(target) = $target_id\n",
    "                    MERGE (source)-[:SIMILAR {score: $similarity, weight: $normalized_similarity}]->(target)\n",
    "                    \"\"\",\n",
    "                    source_id=source[\"id\"],\n",
    "                    target_id=target[\"id\"],\n",
    "                    similarity=float(similarity),\n",
    "                    normalized_similarity=float(normalized_similarity),\n",
    "                    source_label=source_label,\n",
    "                    target_label=target_label\n",
    "                )\n",
    "                return 1\n",
    "            return 0\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in similarity calculation or persistence: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def calculate_similarities(self, source_label, target_label, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Calculate and persist similarities between two types of nodes using multithreading.\n",
    "        \"\"\"\n",
    "        source_embeddings = self.fetch_embeddings(source_label)\n",
    "        target_embeddings = self.fetch_embeddings(target_label)\n",
    "\n",
    "        relationships_created_count = 0\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_similarity = {\n",
    "                executor.submit(self._calculate_and_persist_similarity, source, target, source_label, target_label, threshold):\n",
    "                (source, target) for source in source_embeddings for target in target_embeddings\n",
    "            }\n",
    "\n",
    "            for future in tqdm(as_completed(future_to_similarity), total=len(future_to_similarity), desc=\"Calculando Similaridades...\"):\n",
    "                relationships_created_count += future.result()\n",
    "\n",
    "        logging.info(f\"Total {relationships_created_count} relacionamentos estabelecidos entre {source_label} e {target_label}.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estabelecimento de um grafo de similaridade de base com 0.6 de threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "# Usage example:\n",
    "cosine_similarity_service = CosineSimilarityRelationship(uri, user, password)\n",
    "graph_embedding_service = GraphEmbeddingService(cosine_similarity_service)\n",
    "\n",
    "# Calculate similarities between Publicacao and Subarea/Especialidade nodes with logging and exception handling.\n",
    "threshold=0.6\n",
    "graph_embedding_service.calculate_similarities(\"Publicacao\", \"Subárea\", threshold)\n",
    "graph_embedding_service.calculate_similarities(\"Publicacao\", \"Especialidade\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class TrainingCoordinator:\n",
    "    def __init__(self, uri, user, password, input_dim, hidden_dim, output_dim):\n",
    "        self.data_loader_extension = DataLoaderExtension(uri, user, password)\n",
    "        self.model = GNN(input_dim, hidden_dim, output_dim).to(self.data_loader_extension.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "        self.loss_history = []\n",
    "\n",
    "    def train(self, epochs, early_stopping_patience=10):\n",
    "        data_loader = self.data_loader_extension.create_data_loader(batch_size=32)\n",
    "        early_stopping_counter = 0\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in data_loader:\n",
    "                batch = batch.to(self.data_loader_extension.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(batch)\n",
    "                loss = F.mse_loss(output, batch.y)  # Note: Implement the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe de treinamento para aprendizado supervisionado\n",
    "\n",
    "Neste cenário, a função de perda está definida como F.mse_loss. Para iniciar o treinamento basta que todas as dependências estejam instaladas e configuradas corretamente e que o Neo4j esteja acessível nas credenciais fornecidas. Além disso, ajuste as dimensões do modelo.\n",
    "\n",
    "Já para um cenário de aprendizado não supervisionado, implementamos uma função de perda adequada que reflete o objetivo de clusterização sem rótulos, como a função de perda baseada em modularidade, ou função de perda contrastiva, por exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definindo um modelo simples de GNN com o PyTorch Geometric\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Classe de treinamento com plotagem do treinamento e early stopping\n",
    "class TrainingCoordinator:\n",
    "    def __init__(self, uri, user, password, input_dim, hidden_dim, output_dim):\n",
    "        self.data_loader_extension = DataLoaderExtension(uri, user, password)\n",
    "        self.model = GNN(input_dim, hidden_dim, output_dim).to(self.data_loader_extension.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "        self.loss_history = []\n",
    "\n",
    "    def train(self, epochs, early_stopping_patience=10):\n",
    "        data_loader = self.data_loader_extension.create_data_loader(batch_size=32)\n",
    "        early_stopping_counter = 0\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in data_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(batch)\n",
    "                loss = F.mse_loss(output, batch.y)  # Somente para quando houver rótulos da verdade\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(data_loader)\n",
    "            self.loss_history.append(avg_loss)\n",
    "\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter == early_stopping_patience:\n",
    "                    print(f\"Early stopping on epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "            self.plot_loss(epoch)\n",
    "\n",
    "    def plot_loss(self, epoch):\n",
    "        plt.plot(self.loss_history, label='Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training Loss after epoch {epoch+1}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uri = 'bolt://localhost:7687'  # Substitua pelo seu URI do Neo4j\n",
    "# user = 'neo4j'\n",
    "# password = 'password'\n",
    "# input_dim = 768  # Deve corresponder à dimensão do embedding\n",
    "# hidden_dim = 64\n",
    "# output_dim = 8  # Deve corresponder ao número de comunidades/grandes áreas\n",
    "\n",
    "# coordinator = TrainingCoordinator(uri, user, password, input_dim, hidden_dim, output_dim)\n",
    "# coordinator.train(epochs=100, early_stopping_patience=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes para o aprendizado não supervisionado, com pseudorótulos\n",
    "\n",
    "Para implementar a função de perda alinhada com o aprendizado não supervisionado demandado pelo problema de clusterizar os títulos de publicações científicas nas Grandes Áreas do CNPq, adotamos abordagem baseada na modularidade. A modularidade é uma métrica que avalia a força da divisão de uma rede em módulos (ou comunidades). Em teoria dos grafos, uma alta modularidade significa que as conexões dentro de cada um dos grupos são densas, mas entre os grupos diferentes são esparsas.\n",
    "\n",
    "No entanto, calcular a modularidade diretamente durante o treinamento do modelo pode ser computacionalmente inviável, especialmente para grandes grafos. Uma alternativa é usar uma proxy para a modularidade, como uma função de perda baseada em similaridade que incentiva nós dentro do mesmo cluster a terem embeddings semelhantes, enquanto nós de diferentes clusters devem ter embeddings distintos.\n",
    "\n",
    "Uma abordagem possível é utilizar uma função de perda contrastiva, como a InfoNCE, que é comumente usada em aprendizado auto-supervisionado. Neste cenário, nós que estão conectados por uma aresta (indicando alta similaridade semântica) devem ter embeddings mais próximos uns dos outros do que de nós que não estão conectados. \n",
    "\n",
    "Assim, a implementação da função de perda considera a similaridade entre nós conectados diretamente no grafo e penaliza a similaridade entre nós não conectados. A função torch.eye é usada para criar uma máscara que evita a seleção da auto-similaridade e das similaridades positivas no cálculo dos scores negativos.\n",
    "\n",
    "A função de perda resultante é uma estimação da modularidade, onde a similaridade dos nós dentro do mesmo cluster é maximizada (pontuação positiva alta), enquanto a similaridade entre nós de diferentes clusters é minimizada (pontuação negativa baixa).\n",
    "\n",
    "É importante notar que esta é uma simplificação e não calcula a modularidade diretamente. Para cenários de grafos grandes, pode ser necessário investigar formas de calcular essa perda de maneira mais eficiente ou explorar outras funções de perda que possam ser calculadas de forma mais escalável, o que foge ao escopo deste trabalho de pesquisa que visa aplicar o problema a grupos pequenos de pesquisadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader, Data\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class DataLoaderExtension:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def fetch_graph_data(self):\n",
    "        with self.driver.session() as session:\n",
    "            # Error handling has been added here.\n",
    "            try:\n",
    "                nodes_query = \"MATCH (n:Publicacao) RETURN id(n) AS id, n.embedding AS embedding\"\n",
    "                nodes = session.run(nodes_query).data()\n",
    "                \n",
    "                edges_query = \"\"\"\n",
    "                MATCH (p1:Publicacao)-[r:SIMILAR]->(p2:Publicacao)\n",
    "                RETURN id(p1) AS source, id(p2) AS target, r.weight AS weight\n",
    "                \"\"\"\n",
    "                edges = session.run(edges_query).data()\n",
    "\n",
    "                x = torch.tensor([node['embedding'] for node in nodes], dtype=torch.float)\n",
    "                edge_index = torch.tensor([(edge['source'], edge['target']) for edge in edges], dtype=torch.long).t().contiguous()\n",
    "                edge_weight = torch.tensor([edge['weight'] for edge in edges], dtype=torch.float)\n",
    "\n",
    "                data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight).to(self.device)\n",
    "                return data\n",
    "            except Exception as e:\n",
    "                logging.error(\"Failed to fetch graph data: {}\".format(e))\n",
    "                raise\n",
    "\n",
    "    def create_data_loader(self, batch_size=32):\n",
    "        data = self.fetch_graph_data()\n",
    "        return DataLoader([data], batch_size=batch_size, shuffle=True, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingCoordinator:\n",
    "    def __init__(self, uri, user, password, input_dim, hidden_dim, output_dim):\n",
    "        self.data_loader_extension = DataLoaderExtension(uri, user, password)\n",
    "        self.model = GNN(input_dim, hidden_dim, output_dim).to(self.data_loader_extension.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "        self.loss_history = []\n",
    "\n",
    "    def train(self, epochs, early_stopping_patience=10):\n",
    "        # Early stopping initialization\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for data in self.data_loader_extension.create_data_loader():\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.compute_loss(output, data)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                self.loss_history.append(loss.item())\n",
    "\n",
    "            avg_loss = total_loss / len(self.loss_history)\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "\n",
    "            self.plot_loss()\n",
    "\n",
    "    def compute_loss(self, output, data):\n",
    "        # A função de perda espera que 'output' seja os embeddings dos nós após passar pelo GNN,\n",
    "        # e 'data.edge_index' seja os índices das arestas conectando os nós.\n",
    "        edge_index = data.edge_index\n",
    "        embeddings = output\n",
    "\n",
    "        # Selecionar os embeddings dos nós conectados por arestas\n",
    "        source_embeddings = embeddings[edge_index[0]]\n",
    "        target_embeddings = embeddings[edge_index[1]]\n",
    "\n",
    "        # Calcular a similaridade (ex.: produto escalar) entre os embeddings conectados\n",
    "        positive_score = torch.sum(source_embeddings * target_embeddings, dim=1)\n",
    "\n",
    "        # Calcular os scores negativos comparando cada nó com todos os outros\n",
    "        negative_score = torch.matmul(embeddings, embeddings.t())\n",
    "\n",
    "        # Máscara para remover os scores positivos e auto-similaridade da comparação negativa\n",
    "        negative_mask = ~torch.eye(negative_score.size(0), dtype=torch.bool)\n",
    "        negative_score = negative_score[negative_mask].view(negative_score.size(0), -1)\n",
    "\n",
    "        # Calcular a perda contrastiva\n",
    "        loss = -torch.mean(torch.log(\n",
    "            torch.exp(positive_score) /\n",
    "            torch.sum(torch.exp(negative_score), dim=1)\n",
    "        ))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.loss_history, label='Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Usage:\n",
    "# uri, user, password would be your database credentials\n",
    "# input_dim, hidden_dim, output_dim would be dimensions for your model architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de uso de um Modelo GCN\n",
    "\n",
    "Para criar uma classe que treina um modelo de Graph Machine Learning (GML) não-supervisionado, acompanha o treinamento com plotagem gráfica e implementa early stopping, precisamos definir um modelo de GNN (Graph Neural Network) adequado e um loop de treinamento.\n",
    "\n",
    "A função de perda no cenário não-supervisionado para Graph Machine Learning utilizada considera abordagem baseada na maximização da modularidade, que é uma medida da estrutura de comunidades em redes. A modularidade compara a densidade de arestas dentro das comunidades com a densidade de arestas esperada em um grafo aleatório. Neste cenário do GML, a função de perda de modularidade para a classe GMLTrainer é baseada em modularidade, foi adaptada para trabalhar com embeddings, onde o objetivo é maximizar a similaridade de embeddings dentro do mesmo cluster e minimizar a similaridade entre clusters diferentes.\n",
    "\n",
    "Neste cenário, o output é a saída do modelo GNN, que assume ser um conjunto de logits para cada nó indicando a pertinência a possíveis clusters. A função de perda calcula a probabilidade de cada nó pertencer a cada cluster usando a função softmax. Em seguida, calcula-se uma matriz de similaridade entre todos os pares de nós e a modularidade Q do agrupamento. Por fim, a função de perda retorna o negativo da modularidade, pois o PyTorch minimiza a função de perda por padrão, e queremos maximizar a modularidade.\n",
    "\n",
    "Esta implementação é utilizada como mínimo produto viável para validar a abordagem em um estudo de caso, e pode ter limitações de desempenho para grafos grandes devido ao custo computacional de calcular a matriz de similaridade completa e a conversão de uma representação esparsa para densa da matriz de adjacência. Em casos de grafos de grande escala, considerações adicionais, como o uso de aproximações ou técnicas de otimização, podem ser necessárias.\n",
    "\n",
    "Usamos o framework do PyTorch e do PyTorch Geometric para construir a classe do modelo e treiná-lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing, GCNConv\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "class GMLModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GMLModel, self).__init__()\n",
    "        # Define a GCN model (uma simples Graph Convolutional Network).\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Passa os dados através da rede.\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GMLTrainer:\n",
    "    def __init__(self, model, dataset, learning_rate=0.01, weight_decay=5e-4):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def train(self, epochs, early_stopping_patience=10):\n",
    "        self.model.train()\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for data in DataLoader(self.dataset, batch_size=32, shuffle=True):\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.compute_loss(output, data)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(self.dataset)\n",
    "            self.loss_history.append(avg_loss)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(f'Stopping early at epoch {epoch+1}')\n",
    "                    break\n",
    "            \n",
    "            # Plotagem do treinamento\n",
    "            self.plot_loss()\n",
    "            \n",
    "    def compute_loss(self, output, data):\n",
    "        # Nesta implementação, assumimos que a saída do modelo ('output') é um vetor de logits\n",
    "        # para cada nó, que indica a probabilidade de pertencer a cada cluster.\n",
    "\n",
    "        # Aplicar softmax para obter distribuições de probabilidade sobre os clusters.\n",
    "        probas = torch.nn.functional.softmax(output, dim=1)\n",
    "        \n",
    "        # Calcular a similaridade entre todos os pares de nós (isso pode ser muito custoso computacionalmente para grafos grandes).\n",
    "        similarity_matrix = torch.matmul(probas, probas.t())\n",
    "\n",
    "        # Construir a matriz de adjacências (esperamos que 'data' tenha uma matriz de adjacências binárias).\n",
    "        A = torch.sparse.FloatTensor(data.edge_index, torch.ones(data.edge_index.shape[1]), \n",
    "                                     torch.Size([data.num_nodes, data.num_nodes])).to_dense()\n",
    "        \n",
    "        # Calcular o grau de cada nó.\n",
    "        degrees = A.sum(dim=1)\n",
    "        \n",
    "        # Calcular a modularidade.\n",
    "        M = A - torch.ger(degrees, degrees) / degrees.sum()\n",
    "        Q = (M * similarity_matrix).sum()\n",
    "        \n",
    "        # Como queremos maximizar a modularidade, podemos minimizar o negativo da modularidade.\n",
    "        loss = -Q\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.loss_history, label='Training Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exemplo:\n",
    "# input_dim = 768  # Dimensão do embedding.\n",
    "# hidden_dim = 128  # Dimensão oculta.\n",
    "# output_dim = 8  # Número de clusters.\n",
    "\n",
    "# # Supondo que 'dataset' é uma lista de objetos Data do PyTorch Geometric com os embeddings carregados.\n",
    "# model = GMLModel(input_dim, hidden_dim, output_dim)\n",
    "# trainer = GMLTrainer(model, dataset)\n",
    "# trainer.train(epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrutura simplificada de um modelo de GNN genérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "\n",
    "class GNNModel(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        # Inicializa a classe base MessagePassing com a estratégia de agregação (mean, sum, max, etc.)\n",
    "        super(GNNModel, self).__init__(aggr='mean')  # 'mean' aggregation.\n",
    "        # Camadas lineares para transformações de características\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_update = torch.nn.Linear(in_channels + out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Adiciona conexões de loop para cada nó (i.e., self-loops)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Calcula a normalização dos graus dos nós\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Inicia a passagem de mensagens\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # Normaliza as características dos nós antes da agregação\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # Atualiza os nós após a agregação\n",
    "        new_features = torch.cat([x, aggr_out], dim=1)\n",
    "        return self.lin_update(new_features)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Reinicializa os parâmetros do modelo\n",
    "        self.lin.reset_parameters()\n",
    "        self.lin_update.reset_parameters()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyGDataLoader:\n",
    "    def __init__(self, graph_data):\n",
    "        self.graph_data = graph_data\n",
    "\n",
    "    def get_data_loader(self):\n",
    "        data_list = [Data(x=torch.tensor(node['embedding'], dtype=torch.float), \n",
    "                          edge_index=torch.tensor(list(zip(*edges)), dtype=torch.long))\n",
    "                     for node, edges in self.graph_data]\n",
    "        return DataLoader(data_list, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exemplo:\n",
    "# # Suponha que você tenha 16 características de entrada e queira obter 32 características de saída\n",
    "# gnn_model = GNNModel(in_channels=16, out_channels=32)\n",
    "\n",
    "# # Suponha que 'x' é um tensor de características dos nós com shape [num_nodes, in_channels]\n",
    "# # e 'edge_index' é um tensor que define as arestas do grafo com shape [2, num_edges]\n",
    "# # x = ...\n",
    "# # edge_index = ...\n",
    "\n",
    "# try:\n",
    "#     # Passa os dados através do modelo\n",
    "#     out = gnn_model(x, edge_index)\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred while forwarding through the GNN model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering de referência"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de avançar com técnicas de aprendizado de máquina em grafos, é prudente estabelecer uma linha de base de desempenho utilizando métodos de agrupamento tradicionais. O estabelecimento de uma linha de base é fundamental para posteriormente comparar o desempenho de abordagens mais complexas e computacionalmente exigentes, como redes neurais de grafos. Tendo isso em mente, apresento uma classe BaselineClusterer, destinada a gerar e gerir projeções nomeadas no Neo4j, extrair características dessas projeções e aplicar métodos de clustering tradicionais, como K-means, sobre os embeddings dos nós. A partir dos embeddings já gerados e disponíveis como propriedades nos nós, a classe BaselineClusterer focará em:\n",
    "\n",
    "    Criar projeções nomeadas dos grafos com base nos embeddings.\n",
    "    Executar algoritmos de clustering sobre esses embeddings.\n",
    "    Extrair métricas relevantes para avaliar o desempenho do clustering.\n",
    "    Limpar as projeções nomeadas após a extração de informações.\n",
    "\n",
    "O seguinte código em Python implementa a classe BaselineClusterer, utilizando as bibliotecas neo4j, numpy e sklearn para efetuar o clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "class BaselineClusterer(Neo4jService):\n",
    "    def __init__(self, uri, user, password):\n",
    "        super().__init__(uri, user, password)\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_projection_exists(tx, projection_name):\n",
    "        # Execute the query to check if the projection exists within the transaction scope\n",
    "        result = tx.run(f\"\"\"\n",
    "        CALL gds.graph.exists('{projection_name}')\n",
    "        YIELD exists\n",
    "        RETURN exists\n",
    "        \"\"\")\n",
    "        return result.single()[0]\n",
    "    \n",
    "    def check_and_drop_projection(self, projection_name, db='neo4j'):\n",
    "        exists_query = \"CALL gds.graph.exists($projection_name) YIELD exists\"\n",
    "        drop_query = \"CALL gds.graph.drop($projection_name)\"\n",
    "        with self._driver.session(database=db) as session:\n",
    "            exists = session.run(exists_query, projection_name=projection_name).single()[0]\n",
    "            if exists:\n",
    "                session.run(drop_query, projection_name=projection_name)\n",
    "    \n",
    "    def create_weighted_named_projection(self, label, relationship_type, relationship_property, projection_name):\n",
    "        self.check_and_drop_projection(projection_name)\n",
    "        create_projection_query = f\"\"\"\n",
    "        CALL gds.graph.project(\n",
    "            '{projection_name}',\n",
    "            {{\n",
    "                {label}: {{}}\n",
    "            }},\n",
    "            {{\n",
    "                {relationship_type}: {{\n",
    "                    type: '{relationship_type}',\n",
    "                    properties: '{relationship_property}',\n",
    "                    orientation: 'UNDIRECTED'\n",
    "                }}\n",
    "            }}\n",
    "        )\n",
    "        YIELD graphName\n",
    "        RETURN graphName\n",
    "        \"\"\"\n",
    "        self.execute_write(create_projection_query)\n",
    "\n",
    "    def create_named_projection(self, label, relationship_type, projection_name, relationship_properties=None):\n",
    "        self.check_and_drop_projection(projection_name)\n",
    "        \n",
    "        # If relationship_properties are provided, include them in the projection\n",
    "        relationship_projection = (\n",
    "            f\"'{relationship_type}': {{'properties': {relationship_properties}}}\"\n",
    "            if relationship_properties else\n",
    "            f\"'{relationship_type}'\"\n",
    "        )\n",
    "        \n",
    "        create_projection_query = f\"\"\"\n",
    "        CALL gds.graph.project(\n",
    "            '{projection_name}',\n",
    "            ['{label}'],\n",
    "            {{{relationship_projection}}}\n",
    "        )\n",
    "        YIELD graphName\n",
    "        RETURN graphName\n",
    "        \"\"\"\n",
    "        self.execute_write(create_projection_query)\n",
    "\n",
    "    def generate_embeddings(self, projection_name, algorithm='fastRP', embedding_size=128):\n",
    "        embeddings_query = f\"\"\"\n",
    "        CALL gds.{algorithm}.write(\n",
    "            '{projection_name}',\n",
    "            {{\n",
    "                writeProperty: 'embedding',\n",
    "                embeddingDimension: {embedding_size}\n",
    "            }}\n",
    "        )\n",
    "        YIELD nodePropertiesWritten\n",
    "        RETURN nodePropertiesWritten\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.execute_write(embeddings_query)\n",
    "            if result and result[0]['nodePropertiesWritten'] > 0:\n",
    "                return result\n",
    "            else:\n",
    "                raise ValueError(\"  No embeddings were written to the nodes.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to generate embeddings: {e}\")\n",
    "            return None\n",
    "\n",
    "    def validate_embeddings_written(self, projection_name):\n",
    "        validate_query = f\"\"\"\n",
    "        MATCH (n) WHERE exists(n.embedding)\n",
    "        RETURN count(n) as embeddedNodeCount\n",
    "        \"\"\"\n",
    "        result = self.execute_read(validate_query)\n",
    "        if result:\n",
    "            print(f\"Number of nodes with 'embedding' property: {result[0]['embeddedNodeCount']}\")\n",
    "        else:\n",
    "            print(\"  No nodes with 'embedding' property found.\")\n",
    "        return result\n",
    "\n",
    "    def extract_embeddings(self, projection_name):\n",
    "        get_embeddings_query = f\"\"\"\n",
    "        CALL gds.graph.nodeProperties.stream(\n",
    "            '{projection_name}',\n",
    "            ['embedding']\n",
    "        )\n",
    "        YIELD nodeId, nodeProperty, propertyValue\n",
    "        WHERE nodeProperty = 'embedding'\n",
    "        RETURN nodeId, propertyValue AS embedding\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self.execute_read(get_embeddings_query)\n",
    "            if not results:\n",
    "                print(f\"  No embeddings were found for projection '{projection_name}'. Ensure that the 'embedding' property exists on the nodes.\")\n",
    "                return None\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to extract embeddings: {e}\")\n",
    "            return None\n",
    "\n",
    "    def check_graph_projection_exists(self, projection_name):\n",
    "        check_query = f\"\"\"\n",
    "        CALL gds.graph.exists('{projection_name}')\n",
    "        YIELD exists\n",
    "        RETURN exists\n",
    "        \"\"\"\n",
    "        result = self.execute_read(check_query)\n",
    "        exists = result[0]['exists'] if result else False\n",
    "        if not exists:\n",
    "            print(f\"  Graph projection '{projection_name}' does not exist.\")\n",
    "        return exists\n",
    "\n",
    "    def test_embeddings_generation(self, projection_name):\n",
    "        # Generate embeddings\n",
    "        result = self.generate_embeddings(projection_name)\n",
    "        if not result:\n",
    "            print(\"  Embeddings generation failed.\")\n",
    "            return False\n",
    "\n",
    "        # Validate embeddings were written\n",
    "        validation_result = self.validate_embeddings_written(projection_name)\n",
    "        if not validation_result or validation_result[0]['embeddedNodeCount'] <= 0:\n",
    "            print(\"Embeddings were not written correctly.\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def apply_clustering(self, projection_name, n_clusters=8):\n",
    "        try:\n",
    "            embeddings_data = self.extract_embeddings(projection_name)\n",
    "            if not embeddings_data:\n",
    "                print( \"  Embeddings do not exist for the projection. Please generate them before clustering.\")\n",
    "                return\n",
    "\n",
    "            embeddings = [data['embedding'] for data in embeddings_data]\n",
    "            if embeddings:\n",
    "                print(f\"Embeddings retrieved successfully: {len(embeddings)} nodes\")\n",
    "            else:\n",
    "                raise ValueError(\"  Embeddings list is empty.\")\n",
    "\n",
    "            kmeans = KMeans(n_clusters=n_clusters)\n",
    "            kmeans.fit(embeddings)\n",
    "            \n",
    "            clustering_results = [{'nodeId': data['nodeId'], 'clusterId': label} for data, label in zip(embeddings_data, kmeans.labels_)]\n",
    "            return clustering_results\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to apply clustering: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_clustering_results(self):\n",
    "        # Supondo que 'clustering_query' é sua consulta de clustering\n",
    "        clustering_query = \"\"\"\n",
    "        CALL gds.louvain.stream({\n",
    "        nodeProjection: 'Publication',\n",
    "        relationshipProjection: {\n",
    "            RELATIONSHIP_TYPE: {\n",
    "            type: 'SIMILAR',\n",
    "            properties: {}\n",
    "            }\n",
    "        },\n",
    "        relationshipWeights: 'weight',\n",
    "        includeIntermediateCommunities: false\n",
    "        })\n",
    "        YIELD nodeId, communityId\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self.execute_read(clustering_query)\n",
    "            if results:\n",
    "                return [result for result in results if 'clusterId' in result and 'nodeId' in result]\n",
    "            else:\n",
    "                print(\"  No clustering results found.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"  Error while getting clustering results: {e}\")\n",
    "            return None\n",
    "\n",
    "    def run_baseline_clustering(self, projection_name, n_clusters=8, relationship_property='score'):\n",
    "        self.create_weighted_named_projection('Publicacao', 'SIMILAR', relationship_property, projection_name)\n",
    "        return self.apply_clustering(projection_name, n_clusters=n_clusters)\n",
    "\n",
    "    def drop_named_projection(self, projection_name):\n",
    "        drop_projection_query = f\"\"\"\n",
    "        CALL gds.graph.drop('{projection_name}', false)\n",
    "        \"\"\"\n",
    "        self.execute_write(drop_projection_query)\n",
    "\n",
    "    # Additional utility methods\n",
    "    # Anteriormente: session.write_transaction(...)\n",
    "    # Atualizado para: session.execute_write(...)\n",
    "    def execute_write(self, query, parameters=None, db=None):\n",
    "        with self._driver.session(database=db) as session:\n",
    "            return session.execute_write(self._execute_query, query, parameters)\n",
    "\n",
    "    # Anteriormente: session.read_transaction(...)\n",
    "    # Atualizado para: session.execute_read(...)\n",
    "    def execute_read(self, query, parameters=None, db=None):\n",
    "        with self._driver.session(database=db) as session:\n",
    "            return session.execute_read(self._execute_query, query, parameters)\n",
    "\n",
    "    @staticmethod\n",
    "    def _execute_query(tx, query, parameters=None):\n",
    "        result = tx.run(query, parameters or {})\n",
    "        return [record.data() for record in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação da clusterização de linha de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Instantiate the class with Neo4j server details\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "try:\n",
    "    clusterer = BaselineClusterer(uri, user, password)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while initializing the BaselineClusterer: {e}\")\n",
    "    exit()\n",
    "print('Contextado ao neo4j com sucesso...')\n",
    "\n",
    "# Define parameters for the projection and algorithm\n",
    "projection_name = \"projection06\"\n",
    "label = \"Publicacao\"\n",
    "relationship_type = \"SIMILAR\"\n",
    "# relationship_properties = {'score': 'score'}  # Include the property 'score' as a weight\n",
    "relationship_property = \"score\"  # Assuming 'score' is the property to be used as weight\n",
    "n_clusters = 8  # The desired number of clusters\n",
    "\n",
    "# The Fast Random Projection algorithm to generate embeddings\n",
    "algorithm = 'fastRP'\n",
    "embedding_size = 128  # The desired embedding vector size\n",
    "\n",
    "# Ensure the named projection exists before generating embeddings\n",
    "try:\n",
    "    # This will check and drop the projection if it exists\n",
    "    clusterer.check_and_drop_projection(projection_name)\n",
    "    print(f'Projeção {projection_name} criada com sucesso...')\n",
    "except Exception as e:\n",
    "    print(f\"  An error occurred during creating projection: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Após a criação da projeção:\n",
    "if not clusterer.test_embeddings_generation(projection_name):\n",
    "    print(\"  There was a problem with the embeddings generation.\")\n",
    "    exit()\n",
    "\n",
    "if not clusterer.check_graph_projection_exists(projection_name):\n",
    "    print(\"  Graph projection does not exist. Cannot generate embeddings.\")\n",
    "    # Logic to handle non-existent graph projection\n",
    "\n",
    "try:\n",
    "    # Recreate the named projection with weights\n",
    "    clusterer.create_weighted_named_projection(label, relationship_type, relationship_property, projection_name)\n",
    "    print(f'Adicionados pesos de aresta à projeção {projection_name}...')\n",
    "except Exception as e:\n",
    "    print(f\"  An error occurred during adding weights: {e}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    # Generate embeddings\n",
    "    embedding_generation_result = clusterer.generate_embeddings(projection_name, algorithm, embedding_size)\n",
    "    if embedding_generation_result is None or not embedding_generation_result[0]['nodePropertiesWritten'] > 0:\n",
    "        raise Exception(\"  Embeddings generation failed.\")\n",
    "    print('Embeedings gerados com sucesso...')\n",
    "except Exception as e:\n",
    "    print(f\"  An error occurred during embedding generation or clustering: {e}\")\n",
    "    exit()\n",
    "\n",
    "## Erro ocorre a partir daqui:\n",
    "try:\n",
    "    # Now that embeddings are generated, proceed with clustering\n",
    "    clustering_results = clusterer.run_baseline_clustering(projection_name, n_clusters)\n",
    "    if clustering_results:\n",
    "        print(\"Clustering results obtained:\")\n",
    "        for result in clustering_results:\n",
    "            # Aqui garantimos que ambos 'nodeId' e 'clusterId' são chaves no resultado\n",
    "            if 'nodeId' in result and 'clusterId' in result:\n",
    "                print(f\"Node ID: {result['nodeId']}, Cluster ID: {result['clusterId']}\")\n",
    "            else:\n",
    "                print(f\"Invalid clustering result format: {result}\")\n",
    "    else:\n",
    "        print(\"No clustering results were obtained.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during clustering: {e}\")\n",
    "    exit()\n",
    "\n",
    "# (Optional) After finishing with the projection, you might want to drop it to save resources\n",
    "# Uncomment the following code to drop the projection\n",
    "# try:\n",
    "#     clusterer.drop_named_projection(projection_name)\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred while dropping the projection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_named_projection2(self, projection_name):\n",
    "    drop_projection_query = f\"\"\"\n",
    "    CALL gds.graph.drop($projectionName)\n",
    "    \"\"\"\n",
    "    with self._driver.session() as session:\n",
    "        session.run(drop_projection_query, projectionName=projection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_service = Neo4jService(uri, user, password)\n",
    "list_projections = neo4j_service.list_projections()\n",
    "\n",
    "list_projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculos de similaridade semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import optuna\n",
    "import logging\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from neo4j import GraphDatabase\n",
    "from community import community_louvain\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from sklearn.model_selection import KFold\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from py2neo import Graph, ServiceUnavailable, Neo4jError\n",
    "\n",
    "class CosineSimilarityRelationship(Neo4jService):\n",
    "    def __init__(self, uri, user, password, model_name=\"default_model\"):\n",
    "        super().__init__(uri, user, password)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def check_connection(self):\n",
    "        if not hasattr(self, '_driver') or not self._driver:\n",
    "            logging.error(\"No database connection\")\n",
    "            raise Exception(\"No database connection\")\n",
    "\n",
    "    def cosine_similarity(self, a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "    def normalize_similarity(self, similarity):\n",
    "        return (similarity + 1) / 2\n",
    "\n",
    "    def get_memory_usage(self):\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_MB = process.memory_info().rss / (1024 ** 2)\n",
    "        return memory_MB\n",
    "\n",
    "    def get_all_embeddings(self, label):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(f\"MATCH (n:{label}) RETURN id(n) AS id, n.embedding AS embedding\")\n",
    "            return [record for record in result]\n",
    "\n",
    "    def process_similarity_for_nodes(self, source_embeddings, target_embeddings, source_label, target_label, threshold, batch_size):\n",
    "        relationships_created_count = 0\n",
    "        node_pairs_count = 0\n",
    "        \n",
    "        with self._driver.session() as session:\n",
    "            tx = session.begin_transaction()\n",
    "            try:\n",
    "                for source in tqdm(source_embeddings, desc=f\"Calculando similaridade semântica entre: {source_label}/{target_label}\"):\n",
    "                    for target in target_embeddings:\n",
    "                        similarity = self.cosine_similarity(np.array(source[\"embedding\"]), np.array(target[\"embedding\"]))\n",
    "                        normalized_similarity = self.normalize_similarity(similarity)\n",
    "                        node_pairs_count += 1\n",
    "                        if similarity > threshold:\n",
    "                            query = f\"\"\"\n",
    "                            MATCH (source:{source_label}) WHERE id(source) = $source_id\n",
    "                            MATCH (target:{target_label}) WHERE id(target) = $target_id\n",
    "                            MERGE (source)-[:SIMILAR {{score: $similarity, weight: $normalized_similarity}}]->(target)\n",
    "                            \"\"\"\n",
    "                            tx.run(query, source_id=source[\"id\"], target_id=target[\"id\"], similarity=float(similarity), normalized_similarity=float(normalized_similarity))\n",
    "                            relationships_created_count += 1\n",
    "                            if relationships_created_count % batch_size == 0:\n",
    "                                tx.commit()\n",
    "                                # logging.info(f\"Committed {batch_size} relationships for {source_label}/{target_label}.\")\n",
    "                                tx = session.begin_transaction()\n",
    "                # Commit any remaining relationships after the loop\n",
    "                tx.commit()\n",
    "                # logging.info(f\"Committed remaining relationships for {source_label}/{target_label}.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"An error occurred during transaction: {e}\")\n",
    "                tx.rollback()\n",
    "                raise\n",
    "            return node_pairs_count, relationships_created_count\n",
    "\n",
    "    def create_similarity_embeedings_relationships(self, threshold=0.7, batch_size=3000):\n",
    "        start_time = time.time()\n",
    "        self.check_connection()\n",
    "        initial_memory = self.get_memory_usage()\n",
    "\n",
    "        pub_embeddings = self.get_all_embeddings(\"Publicacao\")\n",
    "        esp_embeddings = self.get_all_embeddings(\"Especialidade\")\n",
    "        sub_embeddings = self.get_all_embeddings(\"Subárea\")\n",
    "\n",
    "        total_pub = len(pub_embeddings)\n",
    "        total_esp = len(esp_embeddings)\n",
    "        total_sub = len(sub_embeddings)\n",
    "\n",
    "        logging.info(f\"Nodes: Publicacao {total_pub}, Especialidade {total_esp}, Subárea {total_sub}\")\n",
    "\n",
    "        pub_sub_pairs, pub_sub_rels = self.process_similarity_for_nodes(pub_embeddings, sub_embeddings, \"Publicacao\", \"Subárea\", threshold, batch_size)\n",
    "        pub_esp_pairs, pub_esp_rels = self.process_similarity_for_nodes(pub_embeddings, esp_embeddings, \"Publicacao\", \"Especialidade\", threshold, batch_size)\n",
    "\n",
    "        final_memory = self.get_memory_usage()\n",
    "        end_time = time.time()\n",
    "        memory_difference = final_memory - initial_memory\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        logging.info(f\"RAM Consumption: {np.round(memory_difference,2)} MB\")\n",
    "        logging.info(f\"Processing Time: {np.round(processing_time,2)} seconds\")\n",
    "        logging.info(f\"Current Memory Usage: {np.round(final_memory,2)} MB\")\n",
    "        logging.info(f\"Execution time for similarity calculations and relationship creation: {np.round(processing_time, 2)} seconds\")\n",
    "        logging.info(f\"Similarity threshold: {threshold}\")\n",
    "        logging.info(f\"Total node pairs analyzed: {pub_sub_pairs + pub_esp_pairs}\")\n",
    "        logging.info(f\"Node pairs Publicacao/Subárea: {pub_sub_pairs}\")\n",
    "        logging.info(f\"Node pairs Publicacao/Especialidade: {pub_esp_pairs}\")\n",
    "        logging.info(f\"Total relationships created: {pub_sub_rels + pub_esp_rels}\")\n",
    "        logging.info(f\"Relationships created Publicacao/Subárea: {pub_sub_rels}\")\n",
    "        logging.info(f\"Relationships created Publicacao/Especialidade: {pub_esp_rels}\")\n",
    "\n",
    "    def run_similarity_operations(self, threshold=0.7):\n",
    "        self.create_similarity_embeedings_relationships(threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar relações de similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponha que as credenciais do seu banco de dados sejam as seguintes:\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "# Criação de uma instância da classe Neo4jService\n",
    "neo4j_service = Neo4jService(uri, user, password)\n",
    "\n",
    "# Agora, chamamos o método para deletar todas as relações SIMILAR\n",
    "# Você pode especificar um tamanho de lote diferente se necessário\n",
    "neo4j_service.delete_similar_relationships(batch_size=10000)\n",
    "\n",
    "# Não se esqueça de fechar a conexão quando terminar\n",
    "neo4j_service.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>FASE 6: Graph Machine Learning (Downstream)</b> \n",
    "\n",
    "    Aplicação: \n",
    "    Clustering por aprendizado não supervisionado através de embeedings da geração dinâmica de projeção de grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes de ML da interação Neo4j -> PyTorch Geometric\n",
    "\n",
    "    GraphDataRetriever\n",
    "    GraphService\n",
    "    DataLoaderExtension\n",
    "    GraphNeuralNetworkModel    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Batch, Data, DataLoader\n",
    "\n",
    "class GraphDataRetriever:\n",
    "    def __init__(self, uri, user, password):\n",
    "        try:\n",
    "            self.graph = Graph(uri, auth=(user, password))\n",
    "        except Neo4jError as e:\n",
    "            print(\"Error connecting to the Neo4j database:\", e)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def retrieve_graph_data(self, threshold):\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "            MATCH (p:Publicacao)-[s:SIMILAR]->(t)\n",
    "            WHERE s.score >= {}\n",
    "            RETURN p, s, t\n",
    "            \"\"\".format(threshold)\n",
    "            results = self.graph.run(query)\n",
    "\n",
    "            node_features, edge_features, edge_index = [], [], []\n",
    "\n",
    "            for record in results:\n",
    "                p, s, t = record['p'], record['s'], record['t']\n",
    "                degree = len(list(self.graph.match((p, None, None))))\n",
    "                node_features.append([degree])\n",
    "                edge_features.append([s['score']])\n",
    "                edge_index.append([p.id, t.id])\n",
    "\n",
    "            node_features = torch.tensor(node_features, dtype=torch.float).to(self.device)\n",
    "            edge_features = torch.tensor(edge_features, dtype=torch.float).to(self.device)\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous().to(self.device)\n",
    "\n",
    "            # Compute global graph features\n",
    "            # Centralidade dos Nós\n",
    "            degree_centrality_query = \"\"\"\n",
    "            MATCH (n:Publicacao)\n",
    "            RETURN n.name AS node, size((n)--()) AS degree\n",
    "            \"\"\"\n",
    "            degree_centrality = self.graph.run(degree_centrality_query).data()\n",
    "\n",
    "            # Coeficiente de Aglomeração (Clustering Coefficient)\n",
    "            clustering_coefficient_query = \"\"\"\n",
    "            MATCH (n:Publicacao)\n",
    "            WITH n, apoc.node.degree(n) AS degree, apoc.coll.toSet([neighbor IN apoc.neighbors.nodes(n) | neighbor]) AS neighbors\n",
    "            WHERE degree > 1\n",
    "            RETURN n.name AS node, toFloat(size(apoc.coll.pairsMin(neighbors)[index IN range(0, size(apoc.coll.pairsMin(neighbors))-2) | \n",
    "            CASE WHEN (neighbors[index])-->(neighbors[index+1]) THEN 1 ELSE 0 END]) + size(apoc.coll.pairsMin(neighbors)[index IN range(0, \n",
    "            size(apoc.coll.pairsMin(neighbors))-2) | CASE WHEN (neighbors[index+1])-->(neighbors[index]) THEN 1 ELSE 0 END]))/toFloat((degree*(degree-1))) AS clustering\n",
    "            \"\"\"\n",
    "            clustering_coefficient = self.graph.run(clustering_coefficient_query).data()\n",
    "\n",
    "            # Densidade do Grafo\n",
    "            graph_density_query = \"\"\"\n",
    "            MATCH (n:Publicacao), (m:Publicacao) WHERE id(n) < id(m)\n",
    "            RETURN toFloat(count(distinct(n, m))) / (toFloat(count(n)*count(n)-1)/2) AS graph_density\n",
    "            \"\"\"\n",
    "            graph_density = self.graph.run(graph_density_query).data()[0]['graph_density']\n",
    "\n",
    "            # Transitividade\n",
    "            graph_transitivity_query = \"\"\"\n",
    "            CALL apoc.metrics.clusteringCoefficient()\n",
    "            YIELD clusteringCoefficient\n",
    "            RETURN clusteringCoefficient\n",
    "            \"\"\"\n",
    "            graph_transitivity = self.graph.run(graph_transitivity_query).data()[0]['clusteringCoefficient']\n",
    "\n",
    "            # Número de Componentes Conexas\n",
    "            connected_components_query = \"\"\"\n",
    "            CALL apoc.algo.unionFind('Publicacao')\n",
    "            YIELD partition\n",
    "            RETURN count(distinct partition) AS number_of_components\n",
    "            \"\"\"\n",
    "            num_connected_components = self.graph.run(connected_components_query).data()[0]['number_of_components']\n",
    "\n",
    "            # Assortatividade\n",
    "            assortativity_query = \"\"\"\n",
    "            CALL apoc.metrics.assortativity.degree('Publicacao', 'INCOMING', 'INCOMING')\n",
    "            YIELD assortativityCoefficient\n",
    "            RETURN assortativityCoefficient\n",
    "            \"\"\"\n",
    "            assortativity = self.graph.run(assortativity_query).data()[0]['assortativityCoefficient']\n",
    "\n",
    "            # Integração com o objeto Data\n",
    "            data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)\n",
    "            data.degree_centrality = torch.tensor([node['degree'] for node in degree_centrality], dtype=torch.float)\n",
    "            data.clustering_coefficient = torch.tensor([node['clustering'] for node in clustering_coefficient], dtype=torch.float)\n",
    "            data.graph_density = torch.tensor([graph_density], dtype=torch.float)\n",
    "            data.graph_transitivity = torch.tensor([graph_transitivity], dtype=torch.float)\n",
    "            data.num_connected_components = torch.tensor([num_connected_components], dtype=torch.int)\n",
    "            data.assortativity = torch.tensor([assortativity], dtype=torch.float)\n",
    "\n",
    "            return data\n",
    "\n",
    "        except Neo4jError as e:\n",
    "            print(\"Error retrieving graph data:\", e)\n",
    "            return None\n",
    "\n",
    "    def create_dataset(self, thresholds):\n",
    "        dataset = []\n",
    "        for t in thresholds:\n",
    "            data = self.retrieve_graph_data(t)\n",
    "            dataset.append(data)\n",
    "        return dataset\n",
    "    \n",
    "class GraphService:\n",
    "    def __init__(self, graph_data):\n",
    "        self.graph_data = graph_data\n",
    "\n",
    "    def get_data(self):\n",
    "        # Esta função pega os dados do grafo e os transforma no formato adequado\n",
    "        edge_index = torch.tensor(self.graph_data['edges'], dtype=torch.long)\n",
    "        x = torch.tensor(self.graph_data['nodes'], dtype=torch.float)\n",
    "        y = torch.tensor(self.graph_data['labels'], dtype=torch.long)\n",
    "        return Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "\n",
    "    def split_data(self, data, train_percent=0.7):\n",
    "        train_size = int(train_percent * len(data))\n",
    "        test_size = len(data) - train_size\n",
    "        train_data, test_data = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "        return DataLoader(train_data, batch_size=64, shuffle=True), DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "class DynamicGraphDataset(Dataset):\n",
    "    def __init__(self, data_loader_extension, thresholds):\n",
    "        self.data_loader_extension = data_loader_extension\n",
    "        self.thresholds = thresholds\n",
    "        self.all_graph_data = self.data_loader_extension.create_graph(thresholds)\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length could be the number of thresholds or graphs we have.\n",
    "        return len(self.all_graph_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming thresholds are sorted or otherwise ordered,\n",
    "        # idx will correspond to the position in the thresholds list.\n",
    "        threshold = self.thresholds[idx]\n",
    "        # Fetch the corresponding graph data for this threshold.\n",
    "        graph_data = self.all_graph_data[threshold]\n",
    "        return graph_data\n",
    "\n",
    "class DataLoaderExtension:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.graph = Graph(uri, auth=(user, password))\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def fetch_nodes_and_edges(self):\n",
    "        try:\n",
    "            node_cursor = self.graph.run(\"MATCH (p:Publicacao) RETURN id(p) AS id, p.embedding AS embedding\")\n",
    "            rel_cursor = self.graph.run(\"\"\"\n",
    "            MATCH (p1:Publicacao)-[r:SIMILAR]->(p2)\n",
    "            RETURN id(p1) AS source, id(p2) AS target, r.score AS weight\n",
    "            \"\"\")\n",
    "\n",
    "            nodes = list(node_cursor)\n",
    "            edges = [(record['source'], record['target'], record['weight']) for record in rel_cursor]\n",
    "\n",
    "            return nodes, edges\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in fetch_nodes_and_edges: {e}\")\n",
    "            return [], []\n",
    "\n",
    "    def transform_to_tensors(self, nodes, edges):\n",
    "        try:\n",
    "            x = torch.tensor([node['embedding'] for node in nodes], dtype=torch.float).to(self.device)\n",
    "            edge_index = torch.tensor([edge[:2] for edge in edges], dtype=torch.long).t().contiguous().to(self.device)\n",
    "            edge_weights = torch.tensor([edge[2] for edge in edges], dtype=torch.float).to(self.device)\n",
    "            return x, edge_index, edge_weights\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in transform_to_tensors: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    def load_graph_data(self):\n",
    "        nodes, edges = self.fetch_nodes_and_edges()\n",
    "        if not nodes:\n",
    "            logging.warning(\"No nodes were found in the database.\")\n",
    "            return None\n",
    "\n",
    "        x, edge_index, edge_weights = self.transform_to_tensors(nodes, edges)\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_weights)\n",
    "        return data\n",
    "\n",
    "    def collate_graphs(data_list):\n",
    "        return Batch.from_data_list(data_list)\n",
    "\n",
    "    def create_data_loader(self, batch_size=32):\n",
    "        dataset = [self.load_graph_data()]\n",
    "        # Ensure the dataset contains valid Data objects or is not empty.\n",
    "        if dataset[0] is None:\n",
    "            raise ValueError(\"The graph data could not be loaded. Please check the dataset creation process.\")\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn= self.collate_graphs, pin_memory=True if self.device.type == \"cuda\" else False)\n",
    "\n",
    "    def create_data_loader(self, batch_size=32):\n",
    "        data = self.load_graph_data()\n",
    "        # Ensure the data is not None and is a Data object.\n",
    "        if data is None or not isinstance(data, Data):\n",
    "            raise ValueError(\"The graph data could not be loaded or is not of type Data.\")\n",
    "        dataset = [data]\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=Batch.from_data_list, pin_memory=True if self.device.type == \"cuda\" else False)\n",
    "\n",
    "    def create_data_loader(self, thresholds, batch_size=32):\n",
    "        dataset = DynamicGraphDataset(self, thresholds)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True if self.device.type == \"cuda\" else False)\n",
    "\n",
    "    def create_graph(self, thresholds):\n",
    "        \"\"\"\n",
    "        Função para criar grafos baseados em diferentes limiares de similaridade de cosseno.\n",
    "\n",
    "        Parâmetros:\n",
    "        - uri (str): URI do banco de dados Neo4j.\n",
    "        - user (str): Nome do usuário do banco de dados.\n",
    "        - password (str): Senha do usuário do banco de dados.\n",
    "        - model_name (str): Nome do modelo para gerar embeddings. Padrão é \"default_model\".\n",
    "        - thresholds (list of float): Lista de limiares para determinar similaridade. Padrão é [0.6, 0.7, 0.8, 0.9].\n",
    "        - batch_size (int): Tamanho do lote para inserção em lote no banco de dados. Padrão é 3000.\n",
    "\n",
    "        Retorna:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        cosine_sim_rel = CosineSimilarityRelationship(self.uri, self.user, self.password)\n",
    "        all_graph_data = {}\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            print(f\"/nGerando similaridades com threshold: {threshold}\")\n",
    "\n",
    "            # Utilizar o método 'run_similarity_operations' para criar relações de similaridade\n",
    "            cosine_sim_rel.run_similarity_operations(threshold)\n",
    "            \n",
    "            # Obter nós e arestas após a criação das relações\n",
    "            nodes, edges = self.fetch_nodes_and_edges()\n",
    "\n",
    "            if not nodes:\n",
    "                logging.warning(f\"No nodes were found in the database for threshold: {threshold}.\")\n",
    "                continue\n",
    "\n",
    "            x, edge_index, edge_weights = self.transform_to_tensors(nodes, edges)\n",
    "\n",
    "            # Armazenar os dados do grafo para o threshold utilizado\n",
    "            all_graph_data[threshold] = Data(x=x, edge_index=edge_index, edge_attr=edge_weights)\n",
    "\n",
    "        return all_graph_data\n",
    "    \n",
    "class GraphNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, model_type='GNN',\n",
    "                 uri: str = None, user: str = None, password: str = None):\n",
    "        super(GraphNeuralNetworkModel, self).__init__()\n",
    "        \n",
    "        # Validar a consistência dos tipos de dados das variáveis de entrada\n",
    "        if not isinstance(input_dim, int) or not isinstance(hidden_dim, int) or not isinstance(output_dim, int):\n",
    "            raise ValueError(\"Input, hidden, and output dimensions must be integers.\")\n",
    "        if not isinstance(model_type, str):\n",
    "            raise ValueError(\"Model type must be a string.\")\n",
    "        if uri and not all(isinstance(param, str) for param in [uri, user, password]):\n",
    "            raise ValueError(\"URI, user, and password must be strings if provided.\")\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        \n",
    "        if self.uri and self.user and self.password:\n",
    "            # O GraphDataRetriever seria uma classe ou função separada, definida em outro lugar.\n",
    "            self.graph_data_retriever = GraphDataRetriever(uri, user, password)\n",
    "        else:\n",
    "            self.graph_data_retriever = None\n",
    "        \n",
    "        # Definição das camadas do modelo\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "        \n",
    "        # Parâmetros da função de perda não supervisionada\n",
    "        self.weight_similarity = nn.Parameter(torch.rand(1))\n",
    "        self.weight_edges_penalty = nn.Parameter(torch.rand(1))\n",
    "        self.weight_density_differential = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def unsupervised_loss_function(self, output, threshold):\n",
    "        # Recupera os dados do gráfico dinâmico para o limite atual\n",
    "        data = self.graph_data_retriever.retrieve_graph_data(threshold)\n",
    "        if data is None:\n",
    "            raise ValueError(\"Graph data retrieval failed.\")\n",
    "\n",
    "        # Calcula a matriz de similaridade com base na saída\n",
    "        similarity_matrix = self.compute_similarity_matrix(output)\n",
    "        \n",
    "        # Similaridade Intra-communidade\n",
    "        intra_community_similarity = (similarity_matrix * self.create_adjacency_matrix(data)).sum()\n",
    "        loss_similarity = -self.weight_similarity * intra_community_similarity\n",
    "\n",
    "        # Penalidade por arestas redundantes\n",
    "        similar_edges_penalty = self.compute_similar_edges_penalty(data.edge_index)\n",
    "        excess_edges_penalty = self.weight_edges_penalty * similar_edges_penalty.sum()\n",
    "\n",
    "        # Penalização diferencial de densidade\n",
    "        community_density = data.graph_density\n",
    "        # O diferencial de densidade alvo pode ser um parâmetro definido com base no limite ou outra heurística\n",
    "        target_density_differential = self.compute_target_density_differential(threshold, data)\n",
    "        loss_density_differential = self.weight_density_differential * (community_density - target_density_differential).abs().sum()\n",
    "\n",
    "        # Penalização total\n",
    "        total_loss = loss_similarity + excess_edges_penalty + loss_density_differential\n",
    "        return total_loss\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Obtenção das características dos nós e arestas\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Passagem pelas camadas convolucionais\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)  # Aplica dropout apenas durante o treinamento\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Agrupamento global das características do nó\n",
    "        x = global_mean_pool(x, data.batch)  # Pooling para cada exemplo no batch\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_similarity_matrix(self, output):\n",
    "        # Cálculo da matriz de similaridade com base no output do modelo\n",
    "        similarity_matrix = torch.mm(output, output.t())\n",
    "        return similarity_matrix\n",
    "\n",
    "    def create_adjacency_matrix(self, data):\n",
    "        # Assumindo que `data` contém um tensor edge_index que descreve a conectividade do grafo,\n",
    "        # criaremos uma matriz de adjacência onde cada entrada A_ij indica se existe\n",
    "        # uma aresta entre os nós i e j.\n",
    "        node_count = data.num_nodes\n",
    "        adjacency_matrix = torch.zeros((node_count, node_count), dtype=torch.float32)\n",
    "        # Fill the adjacency matrix using the edge indices\n",
    "        adjacency_matrix[data.edge_index[0], data.edge_index[1]] = 1\n",
    "        return adjacency_matrix\n",
    "\n",
    "    def compute_target_density_differential(self, threshold, data):\n",
    "        # Para simplificar, definimos o diferencial de densidade alvo como uma função do limite.\n",
    "        # Outro cálculo real pode ser inserido baseado no conhecimento específico do domínio.\n",
    "        # Um exemplo simples poderia ser definir o diferencial alvo proporcional ao limite.\n",
    "        target_density_differential = threshold * 0.1\n",
    "        return target_density_differential\n",
    "\n",
    "    def compute_similar_edges_penalty(self, edge_index, similarity_matrix):\n",
    "        # Supondo que similarity_matrix seja uma matriz simétrica com valores de similaridade entre todos os pares de nós.\n",
    "        num_edges = edge_index.size(1)\n",
    "        penalty = 0.0\n",
    "\n",
    "        # Itera sobre as arestas para calcular a penalidade\n",
    "        for i in range(num_edges):\n",
    "            edge = edge_index[:, i]\n",
    "            penalty += similarity_matrix[edge[0], edge[1]]\n",
    "        \n",
    "        # Média da penalidade pelo número de arestas para normalizá-la\n",
    "        return penalty / num_edges\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros a passar como variáveis para iniciar o treinamento:\n",
    "# - num_epochs: the number of epochs to train for\n",
    "# - model: an instance of your graph neural network model\n",
    "# - optimizer: an instance of an optimizer (e.g., Adam)\n",
    "# - data_loader: a PyTorch DataLoader that loads your graph data\n",
    "# - device: the device (CPU or GPU) to perform computation on\n",
    "\n",
    "# Inicialização\n",
    "uri = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "# Inicialização do modelo\n",
    "num_epochs = 32\n",
    "batch_size = 100\n",
    "\n",
    "input_dim = 16      # Example: feature dimension of each node\n",
    "hidden_dim = 64     # Example: dimension of hidden layers\n",
    "output_dim = 2      # Example: dimension of output features or classes\n",
    "model_type = 'GCN'  # Assume we have different model types like 'GNN', 'GCN', 'GAT', etc.\n",
    "\n",
    "# Instanciação do modelo\n",
    "model = GraphNeuralNetworkModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, model_type='GCN')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # O otimizador Adam é frequentemente utilizado em aprendizado não supervisionado\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def collate_graphs(data_list):\n",
    "    return Batch.from_data_list(data_list)\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "# Inicializa o DataLoaderExtension com os parâmetros necessários\n",
    "data_loader_ext = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_graphs, pin_memory=True if device == 'cuda' else False)\n",
    "data_loader = data_loader_ext.create_data_loader(thresholds=[0.6, 0.7, 0.8, 0.9], batch_size=32)\n",
    "\n",
    "# Loop de treinamento\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for data in data_loader:\n",
    "        # Mude para o dispositivo adequado\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Passo adiante\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calcule a penalização não supervisionada usando a função definida\n",
    "        loss = model.unsupervised_loss_function(output, data)\n",
    "        \n",
    "        # Passo para trás\n",
    "        optimizer.zero_grad()   # Clear gradients to prevent accumulation\n",
    "        loss.backward()         # Compute the gradients of the loss w.r.t. the parameters\n",
    "        optimizer.step()        # Update the parameters based on the gradients\n",
    "        \n",
    "        # Acumula a penalização para impressão, registro ou qualquer outro propósito\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Registrando a perda média ao longo da época\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
    "    \n",
    "    # Here you may include any additional logging, validation, or early stopping if required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>FASE 7: Enriquecer o modelo no grafo</b>\n",
    "\n",
    "    Criar nós secundários\n",
    "    Persistir no modelo em Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Outros Modelos de GML<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Definição do modelo de Graph Neural Network usando PyTorch Geometric.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "class GraphEmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, output_channels, dropout=0.5):\n",
    "        super(GraphEmbeddingModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, output_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Primeira camada de convolução\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Segunda camada de convolução\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Terceira camada de convolução\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GraphModel:\n",
    "    \"\"\"\n",
    "    Classe representando o modelo de aprendizado profundo para grafos com PyTorch Geometric.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, model_type='GNN'):\n",
    "        if model_type == 'GNN':\n",
    "            self.model = GNNModel(input_dim, hidden_dim, output_dim)\n",
    "        elif model_type == 'Embedding':\n",
    "            self.model = GraphEmbeddingModel(input_dim, hidden_dim)\n",
    "        else:\n",
    "            raise ValueError(\"Model type not supported.\")\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "\n",
    "    def train(self, train_data, epochs: int = 10):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for data in train_data:  # Assumindo que train_data é um DataLoader\n",
    "                self.optimizer.zero_grad()\n",
    "                out = self.model(data)\n",
    "                loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def train_with_pseudo_labels(self, data_loader):\n",
    "        \"\"\"\n",
    "        Este método treina o modelo em um conjunto de dados sem rótulos verdadeiros,\n",
    "        utilizando pseudo-rótulos gerados pelo próprio modelo para o treinamento.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_data in data_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Realizar a passagem para frente para gerar pseudo-rótulos\n",
    "                logits = self.model(batch_data)\n",
    "                pseudo_labels = logits.argmax(dim=1)\n",
    "                \n",
    "                # Treinar o modelo utilizando os pseudo-rótulos\n",
    "                loss = F.nll_loss(logits, pseudo_labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def predict(self, data):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(data)\n",
    "        return logits\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(data)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct = (pred == data.y).sum().item()\n",
    "            acc = correct / len(data.y)\n",
    "        return acc\n",
    "\n",
    "    def self_learning(self, unlabeled_data):\n",
    "        # Neste método, a ideia é usar o modelo para fazer previsões em dados não rotulados\n",
    "        # E então usar essas previsões como novas labels \"pseudo\" para treinar o modelo novamente\n",
    "        logits = self.predict(unlabeled_data)\n",
    "        pseudo_labels = logits.argmax(dim=1)\n",
    "        unlabeled_data.y = pseudo_labels  # Definir as pseudo labels\n",
    "        self.train(unlabeled_data)  # Treinar novamente usando as pseudo labels\n",
    "\n",
    "class GraphAnalysisController:\n",
    "    def __init__(self, data_loader_extension, input_dim, hidden_dim, output_dim):\n",
    "        self.data_loader_extension = data_loader_extension\n",
    "        self.graph_model = GraphModel(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    def train_model(self, thresholds, num_epochs=100, batch_size=1, beta=1.0):\n",
    "        # Create graphs based on thresholds using DataLoaderExtension\n",
    "        graph_data_dict = self.data_loader_extension.create_graph(thresholds)\n",
    "\n",
    "        for threshold, data in graph_data_dict.items():\n",
    "            print(f\"Training model for threshold: {threshold}\")\n",
    "            train_data_loader = DataLoader([data], batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0\n",
    "                for batch_data in train_data_loader:\n",
    "                    self.graph_model.optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass through the model\n",
    "                    output = self.graph_model.model(batch_data)\n",
    "                    \n",
    "                    # Compute custom loss\n",
    "                    loss = self.custom_loss(output, batch_data, beta)\n",
    "                    \n",
    "                    # Backward pass and optimization\n",
    "                    loss.backward()\n",
    "                    self.graph_model.optimizer.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                average_loss = total_loss / len(train_data_loader)\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Threshold: {threshold}')\n",
    "    \n",
    "    @staticmethod\n",
    "    def custom_loss(output, data, beta=1.0):\n",
    "        # Convert PyG data to a NetworkX graph\n",
    "        G = nx.from_edgelist(data.edge_index.cpu().numpy().T)\n",
    "\n",
    "        # Use Louvain algorithm to obtain node communities\n",
    "        partition = community_louvain.best_partition(G)\n",
    "\n",
    "        # Calculate modularity\n",
    "        modularity_score = community_louvain.modularity(partition, G)\n",
    "        \n",
    "        # Calculate the mean distance (which needs to be defined appropriately)\n",
    "        mean_distance = torch.mean(output, dim=0)\n",
    "\n",
    "        # Combine modularity and mean distance into the loss\n",
    "        loss = -modularity_score + beta * mean_distance\n",
    "        return loss\n",
    "\n",
    "    def save_model(self, path):\n",
    "        # Save the trained model for later use\n",
    "        torch.save(self.graph_model.model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # def samples_unsupervised_loss_function(self, output, data):\n",
    "    #     \"\"\"\n",
    "    #     Compute the unsupervised loss between the output of the model and the input data.\n",
    "    #     This function assumes a generative modeling task where the model output is\n",
    "    #     expected to approximate the data distribution.\n",
    "\n",
    "    #     Parameters:\n",
    "    #     - output: tensor, the output from the model, which can be probabilities, embeddings, etc.\n",
    "    #     - data: tensor, the original data input to the model.\n",
    "        \n",
    "    #     Returns:\n",
    "    #     - A scalar tensor representing the unsupervised loss.\n",
    "    #     \"\"\"        \n",
    "    #     # Example 1: Reconstruction loss for autoencoders\n",
    "    #     # In the context of autoencoders, a common approach is to minimize the reconstruction\n",
    "    #     # error, which measures the difference between the input and its reconstruction.\n",
    "    #     # For continuous data, mean squared error (MSE) is often employed.\n",
    "    #     reconstruction_loss = torch.mean((output - data) ** 2)\n",
    "        \n",
    "    #     # Example 2: Negative log likelihood for probabilistic models\n",
    "    #     # For models such as Variational Autoencoders (VAEs), the negative log-likelihood\n",
    "    #     # of the data given the model is minimized, which is equivalent to maximizing\n",
    "    #     # the probability that the model generates the data.\n",
    "    #     # Assuming 'output' represents log probabilities, the negative log likelihood can\n",
    "    #     # be computed as follows:\n",
    "    #     negative_log_likelihood = -torch.mean(torch.sum(output * data, dim=1))\n",
    "        \n",
    "    #     # Example 3: Contrastive Loss for representation learning\n",
    "    #     # In certain unsupervised learning tasks, such as representation learning, contrastive\n",
    "    #     # loss may be used to encourage the model to learn similar representations for similar\n",
    "    #     # data points and dissimilar representations for dissimilar data points.\n",
    "    #     # This requires additional processing to define pairs or sets of data points and is not\n",
    "    #     # directly applicable solely with 'output' and 'data'.\n",
    "        \n",
    "    #     # Selecting the appropriate loss based on the task at hand\n",
    "    #     # The selection of the loss is a crucial step and must align with the overall objectives\n",
    "    #     # of the unsupervised learning task.\n",
    "        \n",
    "    #     similarity_matrix = self.compute_similarity_matrix(output)\n",
    "    #     intra_community_similarity = (similarity_matrix * data.adjacency_matrix).sum()\n",
    "    #     loss_similarity = -self.weight_similarity * intra_community_similarity\n",
    "\n",
    "    #     similar_edges_penalty = self.compute_similar_edges_penalty(data.edge_index)\n",
    "    #     excess_edges_penalty = self.weight_edges_penalty * similar_edges_penalty.sum()\n",
    "\n",
    "    #     community_density = data.graph_density\n",
    "    #     target_density_differential = ...  # Valor alvo para a densidade diferencial entre comunidades\n",
    "    #     loss_density_differential = self.weight_density_differential * (community_density - target_density_differential).abs().sum()\n",
    "\n",
    "    #     # Combina os termos da perda\n",
    "    #     total_loss = loss_similarity + excess_edges_penalty + loss_density_differential\n",
    "    #     # ... incorporar outros termos de perda, se necessário\n",
    "\n",
    "    #     return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe para o Pipeline de Treinamento com NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "\n",
    "class GraphTrainingPipeline:\n",
    "    def __init__(self, data_source_path, uri, username, password, input_dim, hidden_dim, output_dim, model_type='GNN', batch_size=32, n_splits=5, beta=1.0):\n",
    "        self.data_loader_ext = DataLoaderExtension(uri, username, password)\n",
    "        self.data_loader = self.data_loader_ext.create_data_loader(batch_size=batch_size)\n",
    "        self.controller = GraphAnalysisController(data_source_path)\n",
    "        self.model_type = model_type\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.beta = beta\n",
    "        self.n_splits = n_splits\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "\n",
    "    def custom_loss(self, output, data):\n",
    "        # Converter os dados PyG para um grafo NetworkX\n",
    "        G = nx.from_edgelist(data.edge_index.cpu().numpy().T)\n",
    "        # Usando o Algoritmo de Louvain para obter a comunidade dos nós\n",
    "        partition = community_louvain.best_partition(G)\n",
    "        # Calcular a modularidade\n",
    "        modularity_score = community_louvain.modularity(partition, G)\n",
    "        # Calcular a distância média\n",
    "        mean_distance = torch.mean(output, dim=0)\n",
    "        # Combinar a modularidade e a distância média na função de perda\n",
    "        loss = -modularity_score + self.beta * mean_distance\n",
    "        return loss\n",
    "\n",
    "    def train_and_evaluate(self, epochs=10):\n",
    "        kf = KFold(n_splits=self.n_splits)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(self.data_loader.dataset)):\n",
    "            print(f\"Fold {fold + 1}/{self.n_splits}\")\n",
    "            train_subset = torch.utils.data.Subset(self.data_loader.dataset, train_idx)\n",
    "            val_subset = torch.utils.data.Subset(self.data_loader.dataset, val_idx)\n",
    "\n",
    "            train_loader = DataLoader(train_subset, batch_size=len(train_subset))\n",
    "            val_loader = DataLoader(val_subset, batch_size=len(val_subset))\n",
    "\n",
    "            # Initialize the GraphModel with the specified model type\n",
    "            self.controller.graph_model = GraphModel(self.input_dim, self.hidden_dim, self.output_dim, model_type=self.model_type)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                # Training loop\n",
    "                self.controller.train_model(train_loader, self.custom_loss, epochs=1)\n",
    "\n",
    "                # Validation loop\n",
    "                accuracy = self.controller.evaluate_model(val_loader)\n",
    "                self.accuracies.append(accuracy)\n",
    "                # Loss can be logged during the training process within `train_model` method\n",
    "\n",
    "                # Real-time metrics plot\n",
    "                self.plot_metrics()\n",
    "                clear_output(wait=True)\n",
    "\n",
    "    def unsupervised_loss(self, output, data):\n",
    "        # An example loss function that might use graph structure alone\n",
    "        # for reconstruction loss or similar unsupervised loss.\n",
    "        pass  # to be implemented\n",
    "\n",
    "    def train_unsupervised(self, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            for data in self.data_loader:\n",
    "                self.controller.optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass through the model. For unsupervised learning,\n",
    "                # we don't use labels, just the input features and structure.\n",
    "                output = self.controller.graph_model(data)\n",
    "                \n",
    "                # Calculate unsupervised loss\n",
    "                loss = self.unsupervised_loss(output, data)\n",
    "                \n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                self.controller.optimizer.step()\n",
    "\n",
    "                # Logging the losses for visualization\n",
    "                self.losses.append(loss.item())\n",
    "\n",
    "            # Real-time metrics plot after each epoch\n",
    "            self.plot_losses()\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.figure(figsize=(15, 9))\n",
    "        plt.plot(self.losses, '-o', label='Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(15, 9))\n",
    "\n",
    "        # Plotting losses\n",
    "        ax[0].plot(self.losses, '-o', label='Training Loss')\n",
    "        ax[0].set_title('Training Loss')\n",
    "        ax[0].set_xlabel('Epochs')\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].legend()\n",
    "\n",
    "        # Plotting accuracies\n",
    "        ax[1].plot(self.accuracies, '-o', label='Validation Accuracy')\n",
    "        ax[1].set_title('Validation Accuracy')\n",
    "        ax[1].set_xlabel('Epochs')\n",
    "        ax[1].set_ylabel('Accuracy')\n",
    "        ax[1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.controller.save_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pathcsv = '../../../../csv/'\n",
    "print(os.listdir(pathcsv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerenciamento de projeções no grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class GraphProjectionManager(Neo4jService):\n",
    "    def __init__(self, uri, user, password):\n",
    "        super().__init__(uri, user, password)\n",
    "\n",
    "    def create_named_projection(self, projection_name, node_query, relationship_query, node_properties, relationship_properties):\n",
    "        \"\"\"\n",
    "        Creates a named graph projection in Neo4j GDS using the specified node and relationship queries.\n",
    "        \"\"\"\n",
    "        create_projection_query = f\"\"\"\n",
    "        CALL gds.graph.project.cypher(\n",
    "            '{projection_name}',\n",
    "            '{node_query}',\n",
    "            '{relationship_query}',\n",
    "            {{nodeProperties: {node_properties}, relationshipProperties: {relationship_properties}}}\n",
    "        )\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self._driver.session() as session:\n",
    "                session.run(create_projection_query)\n",
    "                self.logger.info(f\"Projection {projection_name} created successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating projection {projection_name}: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def extract_projection_data(self, projection_name):\n",
    "        \"\"\"\n",
    "        Extracts and returns data from a named graph projection.\n",
    "        Utilizes APOC export procedures for efficient data extraction.\n",
    "        \"\"\"\n",
    "        extract_query = f\"\"\"\n",
    "        CALL gds.beta.graph.export.csv('{projection_name}', {{\n",
    "            exportName: '{projection_name}_export'\n",
    "        }})\n",
    "        YIELD fileNames, nodeCount, relationshipCount\n",
    "        RETURN fileNames, nodeCount, relationshipCount\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self._driver.session() as session:\n",
    "                result = session.run(extract_query)\n",
    "                data = result.single()\n",
    "                self.logger.info(f\"Data extracted from projection {projection_name}: {data['fileNames']}\")\n",
    "                return data['fileNames'], data['nodeCount'], data['relationshipCount']\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting data from projection {projection_name}: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def drop_named_projection(self, projection_name):\n",
    "        \"\"\"\n",
    "        Drops a named graph projection to free up resources.\n",
    "        \"\"\"\n",
    "        drop_projection_query = f\"CALL gds.graph.drop('{projection_name}')\"\n",
    "        try:\n",
    "            with self._driver.session() as session:\n",
    "                session.run(drop_projection_query)\n",
    "                self.logger.info(f\"Projection {projection_name} dropped successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error dropping projection {projection_name}: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "# Usage example:\n",
    "\n",
    "# Initialize the projection manager with Neo4j credentials\n",
    "projection_manager = GraphProjectionManager(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Create a named projection\n",
    "projection_manager.create_named_projection(\n",
    "    projection_name=\"my_named_projection\",\n",
    "    node_query=\"MATCH (n) RETURN id(n) AS id\",\n",
    "    relationship_query=\"MATCH (n)-[r]->(m) RETURN id(n) AS source, id(m) AS target\",\n",
    "    node_properties=['property1', 'property2'],\n",
    "    relationship_properties=['weight']\n",
    ")\n",
    "\n",
    "# Extract data from the named projection\n",
    "file_names, node_count, relationship_count = projection_manager.extract_projection_data(\"my_named_projection\")\n",
    "\n",
    "# Drop the named projection after use\n",
    "projection_manager.drop_named_projection(\"my_named_projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Penalização Customizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função de perda customizada avaliada pela variação da modularidade do grafo (usando o Algoritmo de Louvain) e a distância média entre os nós. \n",
    "\n",
    "Para isso, precisamos ter um pacote que implemente o Algoritmo de Louvain, ou implementar um novo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community as community_louvain\n",
    "import networkx as nx\n",
    "\n",
    "def custom_loss(output, data, beta=1.0):\n",
    "    # Converter os dados PyG para um grafo NetworkX\n",
    "    G = nx.from_edgelist(data.edge_index.cpu().numpy().T)\n",
    "\n",
    "    # Usando o Algoritmo de Louvain para obter a comunidade dos nós\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # Calcular a modularidade\n",
    "    modularity_score = community_louvain.modularity(partition, G)\n",
    "    \n",
    "    # Calcular a distância média (por enquanto, vamos considerar uma distância fictícia, isso deve ser adaptado)\n",
    "    mean_distance = torch.mean(output, dim=0)\n",
    "\n",
    "    # Combinar a modularidade e a distância média na função de perda\n",
    "    loss = -modularity_score + beta * mean_distance\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de treinamento com PyTorch Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integramos essa função de perda no pipeline de treinamento do PyTorch Geometric. Como é um problema não supervisionado, a abordagem de treinamento é um pouco diferente da mais usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(graph_model, data_loader, epochs=10, beta=1.0):\n",
    "    optimizer = torch.optim.Adam(graph_model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data in data_loader:  # Assumindo que data_loader é um DataLoader do PyG\n",
    "            optimizer.zero_grad()\n",
    "            output = graph_model(data.x, data.edge_index)\n",
    "            loss = custom_loss(output, data, beta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para acompanhar aprendizagem em Tempo Real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para acompanhar em tempo real integramos o TensorBoard, uma ferramenta do TensorFlow que permite visualizar métricas de treinamento em tempo real, também é compatível com o PyTorch.\n",
    "\n",
    "Aqui, mostramos um breve código para integrar o TensorBoard. Para usar esta funcionalidade, instale o tensorboard e o torch.utils.tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Inicializar o TensorBoard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Dentro da sua função de treinamento...\n",
    "def train_model(graph_model, data_loader, epochs=10, beta=1.0):\n",
    "    optimizer = torch.optim.Adam(graph_model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = graph_model(data.x, data.edge_index)\n",
    "            loss = custom_loss(output, data, beta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Registrar a perda no TensorBoard\n",
    "            writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "            \n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Finalmente, para visualizar o TensorBoard\n",
    "# Execute no terminal:\n",
    "# tensorboard --logdir=runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para acompanhar treinamento pelo Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline que usa as classes \"Neo4jService\", \"DataLoaderExtension\", \"GraphService\", \n",
    "# \"GraphEmbeddingModel\", \"GNNModel\" e \"GraphModel\" já foram definidas anteriormente:\n",
    "\n",
    "# Inicialização\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "# Instanciando o serviço Neo4j e o DataLoaderExtension\n",
    "data_loader_ext = DataLoaderExtension(uri, user, password)\n",
    "\n",
    "# Obtendo dados de treinamento dinâmicos para diferentes limiares (thresholds)\n",
    "thresholds = [0.6, 0.7, 0.8, 0.9]\n",
    "all_graph_data = data_loader_ext.create_graph(thresholds)\n",
    "\n",
    "# Inicialização do modelo GNN\n",
    "input_dim = len(all_graph_data[thresholds[0]].x[0])\n",
    "hidden_dim = 128\n",
    "output_dim = 8  # Supondo 8 comunidades para clusterização\n",
    "\n",
    "graph_model = GraphModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Uma função de perda simples é a Negative Log Likelihood (nll_loss), por exemplo.\n",
    "# loss_function = F.nll_loss\n",
    "                                                                                                                                                                 \n",
    "# Mas, aqui usamos a função de perda customizada (baseada em similaridade e distância média). \n",
    "# loss_function = custom_loss(output, data, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizagem supervisionada \n",
    "    \n",
    "    (não aplicável inicialmente por falta de rótulos marcados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de treinamento para aprendizado supervisionado\n",
    "# EPOCHS = 10\n",
    "# best_threshold = thresholds[0]\n",
    "# lowest_loss = float('inf')\n",
    "\n",
    "# for threshold, graph_data in all_graph_data.items():\n",
    "#     data_loader = DataLoader([graph_data], batch_size=32, shuffle=True)\n",
    "    \n",
    "#     total_loss = 0.0\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         for data in data_loader:\n",
    "#             graph_model.optimizer.zero_grad()\n",
    "#             out = graph_model.model(data)\n",
    "#             # Uma vez que é um problema não supervisionado, a função de perda precisa ser alterada. \n",
    "#             # Usamos a nll_loss apenas como um exemplo.\n",
    "#             loss = loss_function(out, data.y)  \n",
    "#             loss.backward()\n",
    "#             graph_model.optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "            \n",
    "#     avg_loss = total_loss / EPOCHS\n",
    "#     print(f\"Threshold: {threshold}, Avg Loss: {avg_loss}\")\n",
    "    \n",
    "#     # Atualize o melhor limiar com base na menor perda\n",
    "#     if avg_loss < lowest_loss:\n",
    "#         best_threshold = threshold\n",
    "#         lowest_loss = avg_loss\n",
    "\n",
    "# print(f\"Best threshold for clustering: {best_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizagem não-supervisionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import community as community_louvain\n",
    "import networkx as nx\n",
    "\n",
    "# Funçao de penalização personalizada para aprimorar a clusterização\n",
    "def compute_unsupervised_loss(output, data, beta=1.0):\n",
    "    # Converter os dados PyG para um grafo NetworkX\n",
    "    G = nx.from_edgelist(data.edge_index.cpu().numpy().T)\n",
    "\n",
    "    # Usando o Algoritmo de Louvain para obter a comunidade dos nós\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # Calcular a modularidade\n",
    "    modularity_score = community_louvain.modularity(partition, G)\n",
    "    \n",
    "    # Calcular a distância média (por enquanto, vamos considerar uma distância fictícia, isso deve ser adaptado)\n",
    "    mean_distance = torch.mean(output, dim=0)\n",
    "\n",
    "    # Combinar a modularidade e a distância média na função de perda\n",
    "    loss = -modularity_score + beta * mean_distance\n",
    "    return loss\n",
    "\n",
    "# Treinamento para aprendizado não-supervisionado em grafos\n",
    "EPOCHS = 10\n",
    "best_threshold = thresholds[0]\n",
    "lowest_loss = float('inf')\n",
    "\n",
    "for threshold, graph_data in all_graph_data.items():\n",
    "    data_loader = DataLoader([graph_data], batch_size=32, shuffle=True)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        for data in data_loader:\n",
    "            graph_model.optimizer.zero_grad()\n",
    "            out = graph_model.model(data)\n",
    "            \n",
    "            # Aqui você definirá sua função de perda específica para o problema não-supervisionado.\n",
    "            # Suponha que 'compute_unsupervised_loss' é uma função que você tenha definido:\n",
    "            loss = compute_unsupervised_loss(out, data)  \n",
    "            \n",
    "            loss.backward()\n",
    "            graph_model.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    avg_loss = total_loss / EPOCHS\n",
    "    print(f\"Threshold: {threshold}, Avg Loss: {avg_loss}\")\n",
    "    \n",
    "    # Atualize o melhor limiar com base na menor perda\n",
    "    if avg_loss < lowest_loss:\n",
    "        best_threshold = threshold\n",
    "        lowest_loss = avg_loss\n",
    "\n",
    "print(f\"Best threshold for clustering: {best_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GraphModel(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Classe representando um modelo de aprendizado profundo não supervisionado para grafos.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, model_type='GNN'):\n",
    "#         super(GraphModel, self).__init__()\n",
    "#         self.model_type = model_type\n",
    "#         if model_type == 'GNN':\n",
    "#             self.model = GNNModel(input_dim, hidden_dim, output_dim)\n",
    "#         elif model_type == 'Embedding':\n",
    "#             self.model = GraphEmbeddingModel(input_dim, hidden_dim)\n",
    "#         else:\n",
    "#             raise ValueError(\"Model type not supported.\")\n",
    "#         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "\n",
    "#     def unsupervised_loss_function(self, output, data):\n",
    "#         # output: representação de características aprendidas para cada nó\n",
    "#         # data: os dados do grafo contendo informação sobre os nós e arestas\n",
    "        \n",
    "#         # Perda de Agrupamento (por exemplo, a distância média inversa entre nós e centros de cluster)\n",
    "#         loss_cluster = ... # Implementar o cálculo da perda de agrupamento aqui\n",
    "\n",
    "#         # Perda de Regularização de Arestas SIMILAR\n",
    "#         # Calcular a contagem de arestas SIMILAR saindo de cada nó Publicacao\n",
    "#         similar_edges = ... # Implementar a lógica para identificar arestas SIMILAR\n",
    "#         excess_edges = torch.clamp(similar_edges - 2, min=0)\n",
    "#         loss_similar = torch.sum(excess_edges)\n",
    "\n",
    "#         # Perda de Densidade de Comunidade\n",
    "#         # Calcular densidades das comunidades e identificar a diferença entre a mais densa e as outras\n",
    "#         community_densities = ... # Implementar o cálculo de densidades das comunidades\n",
    "#         max_density = community_densities.max()\n",
    "#         density_differences = max_density - community_densities\n",
    "#         # Penalizar se a comunidade mais densa não é suficientemente mais densa\n",
    "#         loss_density = torch.sum(torch.clamp(density_differences - density_threshold, min=0))\n",
    "\n",
    "#         # Combinar termos de perda\n",
    "#         lambda_similar = ... # Definir com base na importância de restringir arestas SIMILAR\n",
    "#         lambda_density = ... # Definir com base na importância da diferença de densidade da comunidade\n",
    "#         total_loss = loss_cluster + lambda_similar * loss_similar + lambda_density * loss_density\n",
    "        \n",
    "#         return total_loss\n",
    "\n",
    "#     def train(self, data_loader, epochs: int = 10):\n",
    "#         \"\"\"\n",
    "#         Método para treinar o modelo usando dados não rotulados.\n",
    "#         \"\"\"\n",
    "#         self.model.train()\n",
    "#         for epoch in range(epochs):\n",
    "#             total_loss = 0\n",
    "#             for data in data_loader:  # Assumindo que data_loader é um DataLoader\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 out = self.model(data)\n",
    "#                 loss = self.unsupervised_loss_function(out, data)\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "#                 total_loss += loss.item()\n",
    "#             print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(data_loader)}\")\n",
    "\n",
    "#     def predict(self, data):\n",
    "#         \"\"\"\n",
    "#         Método para aplicar o modelo treinado e obter saídas, como representações de características.\n",
    "#         \"\"\"\n",
    "#         self.model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             return self.model(data)\n",
    "\n",
    "#     def evaluate(self, data_loader):\n",
    "#         \"\"\"\n",
    "#         Método para avaliar o modelo usando métricas não supervisionadas.\n",
    "#         \"\"\"\n",
    "#         self.model.eval()\n",
    "#         # A avaliação deve ser implementada de acordo com a tarefa específica.\n",
    "#         # Por exemplo, pode-se calcular a silhueta ou outras métricas de clustering.\n",
    "#         raise NotImplementedError(\"Evaluation metrics for unsupervised learning need to be implemented.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
