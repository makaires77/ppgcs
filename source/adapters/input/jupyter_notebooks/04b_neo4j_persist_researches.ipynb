{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <center><center><img src=\"https://raw.githubusercontent.com/makaires77/fioce/master/assets/logo_fioce.png\" \n",
    "style=\"height:150px\" alt=\"Logo_Fiocruz\"></center></center> -->\n",
    "\n",
    "<center><center><img src=\"https://user-images.githubusercontent.com/61051085/81343928-3ce9d500-908c-11ea-9850-0210b4e94ba0.jpg\" \n",
    "style=\"height:150px\" alt=\"Logo_Unifor\"></center></center>\n",
    "\n",
    "## <center>Gerar o Modelo de Grafo no Neo4j<br /></center>\n",
    "\n",
    "    Antonio Marcos Aires Barbosa (Fiocruz Ceará)\n",
    "\n",
    "**Introdução**\n",
    "\n",
    "Nesta etapa do trabalho populamos o banco de dados de grafo com os dados dos currículos previamente extraídos do Lattes dos servidores da unidade Fiocruz Ceará."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanciar a classe e estabelecer conexão com Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mak/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Caminho base do repositório: /home/mak/gml_classifier-1\n",
      "Arquivos de entrada de dados: ['normalized_dict_list.json', 'dict_list_fioce.json']\n",
      "                              34 currículos carregados arquivo: 'dict_list_fioce.json'\n",
      "\n",
      "GPU disponível para execução de código.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Base do diretório do repositório\n",
    "base_repo_dir = None\n",
    "if 'win' in sys.platform:\n",
    "    base_repo_dir = 'C:\\\\Users\\\\marcos.aires\\\\gml_classifier-1'  # Caminho base no Windows\n",
    "else:\n",
    "    base_repo_dir = '/home/mak/gml_classifier-1'  # Caminho base em Linux\n",
    "\n",
    "# Construir os caminhos usando os.path.join\n",
    "folder_utils = os.path.join(base_repo_dir, 'utils')\n",
    "folder_domain = os.path.join(base_repo_dir, 'source', 'domain')\n",
    "folder_data_input = os.path.join(base_repo_dir, 'data', 'input')\n",
    "folder_data_output = os.path.join(base_repo_dir, 'data', 'output')\n",
    "\n",
    "# Adicionar pastas locais ao sys.path para permitir importação de pacotes\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "from bert_embeedings_to_neo4j import BertEmbeddingsToNeo4j, Neo4jConnector\n",
    "from experiment_monitor import ExperimentMonitor\n",
    "from articles_counter import ArticlesCounter\n",
    "from json_fle_manager import JSONFileManager as jfm\n",
    "from experiment_profiler import TimeProfiler\n",
    "from lda_extractor import LDAExtractor\n",
    "\n",
    "# Para o caso de folder_data_prod, que parece ser exclusivo para ambientes Unix\n",
    "folder_data_prod = os.path.join(base_repo_dir, 'data') if not 'win' in sys.platform else None\n",
    "\n",
    "print(f\" Caminho base do repositório: {base_repo_dir}\")\n",
    "print(f\"Arquivos de entrada de dados: {jfm.list_json_files(folder_data_input)}\")\n",
    "\n",
    "# Definir arquivo dados brutos a processar e gerar dataset\n",
    "profiler = TimeProfiler()\n",
    "monitor = ExperimentMonitor(base_repo_dir, profiler)\n",
    "filename = 'dict_list_fioce.json'\n",
    "dict_list = monitor.load_from_json(folder_data_input,filename)\n",
    "print(f\"{' '*30}{len(dict_list)} currículos carregados arquivo: '{filename}'\")\n",
    "\n",
    "if monitor.is_gpu_available():\n",
    "    print(\"\\nGPU disponível para execução de código.\")\n",
    "else:\n",
    "    print(\"\\nNão foi detectada nenhuma GPU configurada corretamente no ambiente.\")\n",
    "\n",
    "atualizador = ArticlesCounter(dict_list)\n",
    "dtf_atualizado = atualizador.extrair_data_atualizacao(dict_list)\n",
    "# dtf_atualizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperar os dados dos artigos persistidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "neo4j_conn = Neo4jConnector(uri, username, password)\n",
    "bert_to_neo4j = BertEmbeddingsToNeo4j(uri, username, password)\n",
    "\n",
    "articles = neo4j_conn.extract_article_data()\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in articles:\n",
    "#     print(i.get('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'extract_article_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbert_to_neo4j\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_and_update_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m neo4j_conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      3\u001b[0m bert_to_neo4j\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/gml_classifier-1/source/domain/bert_embeedings_to_neo4j.py:449\u001b[0m, in \u001b[0;36mBertEmbeddingsToNeo4j.process_and_update_embeddings\u001b[0;34m(self, neo4j_connector)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_and_update_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, neo4j_connector):\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Extrair dados dos artigos\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     articles \u001b[38;5;241m=\u001b[39m \u001b[43mneo4j_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_article_data\u001b[49m()\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;66;03m# Processar cada artigo para gerar e atualizar embeddings\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'extract_article_data'"
     ]
    }
   ],
   "source": [
    "bert_to_neo4j.process_and_update_embeddings(articles)\n",
    "neo4j_conn.close()\n",
    "bert_to_neo4j.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ler dicionários e criar nós de Pessoas e Artigos no Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lê lista de dicionários de um arquivo JSON local\n",
    "folder='/home/mak/gml_classifier-1/data/input/'\n",
    "json_file='dict_list_fioce.json'\n",
    "with open(folder + json_file, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "    \n",
    "curriculum_dict_list = bert_to_neo4j.load_data_from_json(folder + json_file)\n",
    "\n",
    "# Persiste pesquisadores no banco de dados\n",
    "results = bert_to_neo4j.process_researchers(curriculum_dict_list)\n",
    "bert_to_neo4j.close()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar relacionamentos de equipes (nós do tipo Team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_to_neo4j.process_relationships_from_file()  # Usando valores padrão para ingerir dados de pessoal\n",
    "# bert_to_neo4j.process_relationships_from_file('/caminho/personalizado', 'arquivo_alternativo.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atribuir nomes persistidos a Equipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_connector = Neo4jConnector(uri, username, password)\n",
    "\n",
    "# Cria uma instância da DataEntryInterface\n",
    "data_entry_interface = DataEntryInterface(neo4j_connector)\n",
    "\n",
    "# Inicia o processo de associação de equipes usando o arquivo CSV\n",
    "data_entry_interface.start_process()\n",
    "\n",
    "# Exibe o widget para associações adicionais\n",
    "# Isso será útil para associar pessoas que não foram incluídas no processo CSV\n",
    "data_entry_interface.display_unassociated_people_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrigir alguma associação indevida ou que mudou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entry_interface = DataEntryInterface(neo4j_connector)\n",
    "data_entry_interface.display_and_edit_existing_associations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso exista ainda pessoa não associada aparecerá aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_embeedings_to_neo4j import Neo4jConnector\n",
    "\n",
    "data_interface = DataEntryInterface(neo4j_connector)\n",
    "data_interface.display_unassociated_people_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar associações Pessoa - Equipe em arquivo *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_connector.save_associations_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso exista um arquivo de associação pode ser carregado aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file_path = \"/home/mak/gml_classifier-1/data/input/relations_person_team.csv\"\n",
    "# neo4j_connector.associate_people_to_teams_from_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar uma Organização com base em uma lista de Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de nomes de temas a serem conectados\n",
    "team_names = ['Biotecnologia', 'Saúde da Família', 'Saúde e Ambiente', 'Saúde Digital']\n",
    "\n",
    "# Chamada do método para criar a organização e conectar os temas\n",
    "bert_to_neo4j.create_organization_and_connect_teams(\"Fiocruz Ceará\", \"Fiocruz_CE\", team_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Gerar os embeedings de artigos</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache() # Limpa os caches da memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Adicionar o caminho até das pastas locais ao sys.path\n",
    "from bert_embeedings_to_neo4j import Ber\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "# Exemplo de uso\n",
    "neo4j_conn = Neo4jConnector(uri, username, password)\n",
    "bert_embedder = BertEmbeddingsToNeo4j(uri, username, password)\n",
    "\n",
    "bert_embedder.execute_all_embeddings_update(neo4j_conn)\n",
    "\n",
    "neo4j_conn.close()\n",
    "bert_embedder.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar relacionamentos CNPq com dados preenchidos no Lattes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_to_neo4j.create_cnpq_relationships()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deletar todos os nós de um tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_to_neo4j.delete_all_nodes_of_type(\"Article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_to_neo4j.delete_all_nodes_of_type(\"Organization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_to_neo4j.delete_all_nodes_of_type(\"Team\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_to_neo4j.delete_all_nodes_of_type(\"Person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar um modelo LDA de separação de tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o diretório base para a localização dos arquivos e parâmetros do modelo\n",
    "num_topics = 8\n",
    "passes = 15\n",
    "random_state = 100\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "\n",
    "# Criar instância do LDAExtractor\n",
    "lda_extractor = LDAExtractor(base_repo_dir, num_topics, passes, random_state, model_name)\n",
    "\n",
    "# Nome do arquivo JSON com os dados dos documentos\n",
    "json_data_filename = 'output_py_gpu_singlethread.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizar títulos extraídos\n",
    "# lda_extractor.extract_text_from_json(json_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamar o método para treinar o modelo LDA\n",
    "lda_model, dictionary, corpus = lda_extractor.train_lda_model(json_data_filename)\n",
    "\n",
    "# O modelo LDA, dicionário e corpus agora estão disponíveis para uso posterior\n",
    "# Exemplo: lda_model.show_topics(), dictionary.token2id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Supondo que 'lda_model' seja o seu modelo LDA treinado\n",
    "filepath = os.path.join(folder_data_output,'lda_model.pkl')\n",
    "with open(filepath, 'wb') as file:\n",
    "    pickle.dump(lda_model, file)\n",
    "\n",
    "with open(filepath, 'rb') as file:\n",
    "    loaded_lda_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para usar um modelo LDA existente\n",
    "lda_model_filepath = os.path.join(folder_data_output,'lda_model.pkl')\n",
    "dic_gensim_filepath = os.path.join(folder_data_output,'dictionary.gensim')\n",
    "\n",
    "# Carregar o modelo e o dicionário usando o método load_existing_model\n",
    "lda_extractor.load_existing_model(lda_model_filepath, dic_gensim_filepath)\n",
    "\n",
    "# Classificar um título\n",
    "titulo = \"SARS-CoV-2 genomic surveillance in Rondônia, Brazilian Western Amazon\"\n",
    "\n",
    "topico, probabilidade = lda_extractor.classify_title_to_topic(titulo)\n",
    "print(f\"Título pertence ao tópico {topico} com probabilidade {probabilidade:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para usar um modelo LDA existente\n",
    "lda_model_filepath = os.path.join(folder_data_output,'lda_model.pkl')\n",
    "dic_gensim_filepath = os.path.join(folder_data_output,'dictionary.gensim')\n",
    "\n",
    "# Carregar o modelo e o dicionário usando o método load_existing_model\n",
    "lda_extractor.load_existing_model(lda_model_filepath, dic_gensim_filepath)\n",
    "\n",
    "# Agora você pode chamar plot_wordcloud sem argumentos\n",
    "# lda_extractor.plot_wordcloud()\n",
    "\n",
    "# lda_extractor.plot_wordcloud_plotly(topic_id=0)  # Visualizar a nuvem de palavras do primeiro tópico"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
