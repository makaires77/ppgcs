{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><center><img src=\"https://raw.githubusercontent.com/makaires77/fioce/master/assets/logo_fioce.png\" \n",
    "style=\"height:150px\" alt=\"Logo_Unifor\"></center></center>\n",
    "\n",
    "## <center>Análise exploratória para avaliar utilização dos dados dos currículos Lattes para propor Modelo de Grafo em PDI. </center>\n",
    "\n",
    "    Antonio Marcos Aires Barbosa – Fiocruz Ceará\n",
    "\n",
    "**Introdução**\n",
    "\n",
    "A análise de Grafos permite obter insights como produtos de análises em contextos da realidade com base em modelos capazes de lidar dados heterogêneos e relações complexas.\n",
    "\n",
    "\n",
    "Neste trabalho propomos uma análise dos dados de pesquisa acadêmica tendo como fonte de dados os currículo Lattes de servidores da unidade Fiocruz Ceará.\n",
    "\n",
    "**Objetivo geral:**\n",
    "\n",
    "    Explorar dados dos currículos de servidores da Fiocruz Ceará.\n",
    "\n",
    "**Objetivos Específicos**\n",
    "\n",
    "    1. Extrair dados dos currículos;\n",
    "    2. Propor modelo de grafo para análises futuras;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 0: Preparar e Testar Ambiente</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    from string import Formatter\n",
    "    \n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    \n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    \n",
    "    return f.format(fmt, **values)\n",
    "\n",
    "def tempo(start, end):\n",
    "    from datetime import timedelta\n",
    "        \n",
    "    t=end-start\n",
    "\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "# https://sh-tsang.medium.com/tutorial-cuda-cudnn-anaconda-jupyter-pytorch-installation-in-windows-10-96b2a2f0ac57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_amb():\n",
    "    ## Visualizar versões dos principais componentes\n",
    "    import os\n",
    "    import pip\n",
    "    import sys\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    # !pip3 install shutup\n",
    "    # import shutup; shutup.please()\n",
    "    \n",
    "    pyVer      = sys.version\n",
    "    pipVer     = pip.__version__\n",
    "    \n",
    "    print('\\nVERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT')\n",
    "    print('Interpretador em uso:', sys.executable)\n",
    "    print('    Ambiente ativado:',os.environ['CONDA_DEFAULT_ENV'])\n",
    "    print('     Python: '+pyVer, '\\n        Pip:', pipVer,'\\n'\n",
    "         )\n",
    "\n",
    "    !nvcc -V\n",
    "\n",
    "def try_gpu():\n",
    "    print('\\nVERSÕES DO PYTORCH E GPU DISPONÍVEIS')\n",
    "    try:\n",
    "        import torch\n",
    "        print('    PyTorch:',torch.__version__)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Dispositivo:',device)\n",
    "        print('Disponível :',device,torch.cuda.is_available(),' | Inicializado:',torch.cuda.is_initialized(),'| Capacidade:',torch.cuda.get_device_capability(device=None))\n",
    "        print('Nome GPU   :',torch.cuda.get_device_name(0),'         | Quantidade:',torch.cuda.device_count(),'\\n')\n",
    "    except Exception as e:\n",
    "        print('Erro ao configurar a GPU:',e,'\\n')\n",
    "\n",
    "def try_folders(drives,pastas,pastasraiz):\n",
    "    import os\n",
    "    for drive in drives:\n",
    "        for i in pastas:\n",
    "            for j in pastasraiz:\n",
    "                try:\n",
    "                    caminho_testado = drive+i+j\n",
    "                    if os.path.isfile(caminho_testado+'/chromedriver/chromedriver.exe'):\n",
    "                        print(f\"Listando arqivos em: {caminho_testado}\")\n",
    "                        print(os.listdir(caminho_testado))\n",
    "                        caminho = caminho_testado+'/'\n",
    "                except:\n",
    "                    caminho=''\n",
    "                    print('Não foi possível encontrar uma pasta de trabalho')\n",
    "    return caminho\n",
    "\n",
    "def try_browser(raiz):\n",
    "    print('\\nVERSÕES DO BROWSER E DO CHROMEDRIVER INSTALADAS')\n",
    "    from selenium import webdriver\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    try:\n",
    "        # Caminho para o seu chromedriver\n",
    "        driver_path=raiz+'chromedriver/chromedriver.exe'\n",
    "        print(driver_path)\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        str1 = driver.capabilities['browserVersion']\n",
    "        str2 = driver.capabilities['chrome']['chromedriverVersion'].split(' ')[0]\n",
    "        print(f'     Versão do browser: {str1}')\n",
    "        print(f'Versão do chromedriver: {str2}')\n",
    "        driver.quit()\n",
    "\n",
    "        if str1[0:3] != str2[0:3]: \n",
    "            print(\"Versões incompatíveis, atualizar chromedriver!\")\n",
    "            print('  Baixar versão atualizada do Chromedriver em:')\n",
    "            print('  https://googlechromelabs.github.io/chrome-for-testing/#stable')\n",
    "            print('     Ex. Versão 116 PARA WINDOWS:')\n",
    "            print('\t    https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/win64/chromedriver-win64.zip')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def try_chromedriver(caminho):\n",
    "    try:\n",
    "        import os\n",
    "        os.listdir(caminho)\n",
    "    except Exception as e:\n",
    "        raiz=caminho\n",
    "\n",
    "    finally:\n",
    "        print(raiz)\n",
    "    return raiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definir_sistema(pastaraiz):\n",
    "    import os\n",
    "    import sys\n",
    "    sistema_operacional =sys.platform\n",
    "\n",
    "    try:\n",
    "        if 'linux' in sistema_operacional:\n",
    "            print('Sistema operacional Linux')\n",
    "            try:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'mak/'\n",
    "                os.listdir(drive+usuario)\n",
    "            except:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'marcos/'\n",
    "        elif 'win32' in sistema_operacional:\n",
    "            print('Sistema operacional Windows')\n",
    "            drive   = 'C'\n",
    "            print(f'Drive em uso {drive.upper()}')\n",
    "            # drive = 'E'\n",
    "            # drive = input('Indique qual a letra da unidade onde deseja armazenar os arquivos (Ex.: C, E...)')\n",
    "            usuario = 'Users/marco/'\n",
    "            if os.path.isdir(drive+':/'+usuario) is False:\n",
    "                usuario = 'Users/marcos.aires/'\n",
    "        else:\n",
    "            print('SO não reconhecido')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Erro ao preparar as pastas locais!')\n",
    "        print(e)\n",
    "\n",
    "    caminho = drive+':/'+usuario+pastaraiz+'/'\n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "\n",
    "    return caminho, drive, usuario\n",
    "\n",
    "def preparar_pastas(caminho):\n",
    "    import os\n",
    "\n",
    "    # caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    # caminho = drive+':/'+usuario+pastaraiz\n",
    "    # caminho = drive+':/'+pastaraiz\n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'xml_zip/'\n",
    "    pathcsv  = caminho+'csv/'\n",
    "    pathjson = caminho+'json/'\n",
    "    pathfig  = caminho+'fig/'\n",
    "    pathaux  = caminho\n",
    "    pathout  = caminho+'output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XML', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    \n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sistema operacional Windows\n",
      "Drive em uso C\n",
      "Pasta armazenagem local C:/Users/marcos.aires/fioce/\n",
      "\n",
      "\n",
      "VERSÕES DO PYTORCH E GPU DISPONÍVEIS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PyTorch: 1.12.1+cpu\n",
      "Dispositivo: cpu\n",
      "Erro ao configurar a GPU: Torch not compiled with CUDA enabled \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pastaraiz = 'fioce'\n",
    "caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "# try_amb()\n",
    "try_gpu()\n",
    "# try_browser(caminho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listando arqivos em: C:/Users/marcos.aires/fioce\n",
      "['.git', '.gitignore', 'assets', 'chromedriver', 'csv', 'doc', 'fig', 'json', 'output', 'scripts', 'source', 'utils', 'xls_zip', 'xml_zip']\n",
      "Pasta para os xml já existe!\n",
      "Pasta para os CSV já existe!\n",
      "Pasta para os JSON já existe!\n",
      "Pasta para figuras já existe!\n",
      "Pasta para saídas já existe!\n",
      "\n",
      "Caminho da pasta raiz C:/Users/marcos.aires/fioce/\n",
      "Caminho arquivos  XML C:/Users/marcos.aires/fioce/xml_zip/\n",
      "Caminho arquivos JSON C:/Users/marcos.aires/fioce/json/\n",
      "Caminho arquivos  CSV C:/Users/marcos.aires/fioce/csv/\n",
      "Caminho para  figuras C:/Users/marcos.aires/fioce/fig/\n",
      "Pasta arquivos saídas C:/Users/marcos.aires/fioce/output/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:/Users/marcos.aires/fioce/xml_zip/',\n",
       " 'C:/Users/marcos.aires/fioce/csv/',\n",
       " 'C:/Users/marcos.aires/fioce/json/',\n",
       " 'C:/Users/marcos.aires/fioce/fig/',\n",
       " 'C:/Users/marcos.aires/fioce/',\n",
       " 'C:/Users/marcos.aires/fioce/output/')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drives=['C:/Users/','E:/','./home/']\n",
    "pastas=['marcos.aires/', 'marco/']\n",
    "pastasraiz=['kgfioce','fioce']\n",
    "caminho=try_folders(drives,pastas,pastasraiz)\n",
    "\n",
    "preparar_pastas(caminho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 1: Implementar funções de trabalho</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções básicas importar, conectar e gerar driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ambiente beakerx\n",
    "# !pip install py2neo\n",
    "# !pip install py2neo --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, sqlite3, asyncio\n",
    "import os, re, time, traceback, json\n",
    "import warnings, csv, sys, string\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from string import Formatter\n",
    "from neo4j import GraphDatabase\n",
    "from datetime import datetime, timedelta\n",
    "from flask import render_template_string\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "from collections import deque, defaultdict, Counter\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "delay = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_driver(caminho):\n",
    "    '''\n",
    "    Função 1: Conecta ao servidor do CNPq para busca de currículo\n",
    "    '''\n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    \n",
    "\n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    # options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    # driver   = webdriver.Chrome(options=options)\n",
    "    driver_path=caminho+'chromedriver/chromedriver.exe'\n",
    "    # print(driver_path)\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service)    \n",
    "    url_buscaespecialista = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    driver.get(url_buscaespecialista) # acessa a url de busca do CNPQ   \n",
    "    \n",
    "    driver.set_window_position(-20, -10)\n",
    "    driver.set_window_size(170, 1896)\n",
    "    driver.mouse = webdriver.ActionChains(driver)\n",
    "    # url        = browser.command_executor._url #\"http://127.0.0.1:60622/hub\"\n",
    "    # session_id = browser.session_id            #'4e167f26-dc1d-4f51-a207-f761eaf73c31'\n",
    "\n",
    "    # return browser, url, session_id\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções acessórias em tratar HTML e chegar ao currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_name(driver, delay, NOME):\n",
    "    '''\n",
    "    Função 2: move cursor para o campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    '''\n",
    "    Função auxiliar para paginar resultados na página de busca\n",
    "    '''\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def achar_busca(driver, delay):\n",
    "    '''\n",
    "    Função auxiliar para clicar no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = driver.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(driver, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "               #expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "               #logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função principal de extração de dados do CVLattes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def extract_data_from_cvuri(element):\n",
    "    # Obter o valor do atributo cvuri\n",
    "    cvuri = element.get_attribute('cvuri')\n",
    "    \n",
    "    # Fazer o parsing da URL para extrair os parâmetros\n",
    "    parsed_url = urlparse(cvuri)\n",
    "    params = parse_qs(parsed_url.query)\n",
    "    \n",
    "    # Converter a lista de valores para valores únicos, já que parse_qs retorna listas\n",
    "    data_dict = {k: v[0] for k, v in params.items()}\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "def open_curriculum(driver, elm_vinculo):\n",
    "    \"\"\"\n",
    "    Função principal para extrair dados de cada página de currículo.\n",
    "    \n",
    "    Parameters:\n",
    "        - driver (webdriver object): The Selenium webdriver object.\n",
    "        - elm_vinculo (WebElement): O objeto achado pelas funções anteriores.    \n",
    "    Returns:\n",
    "        Dicionário com dados extraídos do tooltip\n",
    "    \"\"\"       \n",
    "    link_nome = achar_busca(driver, delay)\n",
    "    window_before = driver.current_window_handle\n",
    "\n",
    "    limite = 5\n",
    "    if str(elm_vinculo) == 'nan':\n",
    "        print('Vínculo não encontrado, passando para o próximo nome...')\n",
    "        raise Exception\n",
    "    try:\n",
    "        print('Vínculo encontrado no currículo de nome:', elm_vinculo.text)\n",
    "    except AttributeError:\n",
    "        print('Erro ao tentar acessar o texto do vínculo. O elemento pode não ter sido localizado corretamente.')\n",
    "    \n",
    "    # Clicar no botão \"Abrir Currículo\" e mudar de aba\n",
    "    try:\n",
    "        link_nome = achar_busca(driver, delay)\n",
    "    except Exception as e:\n",
    "        print('Erro')\n",
    "        print(e)\n",
    "\n",
    "    if link_nome.text == None:\n",
    "        xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "        print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            wait_ms=200,\n",
    "            limit=limite,\n",
    "            on_exhaust=(f'Problema ao acessar ao servidor do CNPq. {limite} tentativas sem sucesso.'))\n",
    "    try:\n",
    "        ActionChains(driver).click(link_nome).perform()\n",
    "    except:\n",
    "        print(f'Currículo não encontrado.')\n",
    "\n",
    "    retry(WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "        wait_ms=200,\n",
    "        limit=limite,\n",
    "        on_exhaust=(f'Problema ao acessar ao servidor do CNPq. {limite} tentativas sem sucesso.'))\n",
    "\n",
    "    # Clicar no botão para abrir o currículo\n",
    "    btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    ActionChains(driver).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "    # Gerenciar janelas abertas no navegador\n",
    "    WebDriverWait(driver, delay).until(EC.number_of_windows_to_be(2))\n",
    "    window_after = driver.window_handles\n",
    "    new_window = [x for x in window_after if x != window_before][0]\n",
    "    driver.switch_to.window(new_window)\n",
    "\n",
    "    # Definir soup fora do loop para que esteja acessível em todo o escopo\n",
    "    soup = None\n",
    "    tooltips_data = []\n",
    "\n",
    "    # Extração dos dados em tooltips em <div id=\"artigos-completos\">    \n",
    "    # Esperar para garantir que todos os elementos da seção \"#artigos-completos\" foram carregados\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_all_elements_located((\n",
    "            By.CSS_SELECTOR, \"#artigos-completos img.ajaxJCR\"))\n",
    "    )\n",
    "\n",
    "    tooltip_data_list = []\n",
    "\n",
    "    # Localizar a div principal pelas classes de div\n",
    "    layout_cells = driver.find_elements(By.CSS_SELECTOR, '#artigos-completos .layout-cell-11 .layout-cell-pad-5')\n",
    "    print(len(layout_cells), 'células principais de dados encontradas')\n",
    "\n",
    "    for cell in layout_cells:\n",
    "        cvuri_dict = {}  # Defina um valor padrão vazio para cvuri_dict\n",
    "        \n",
    "        # Extrair ISSN da classe \".citado\"\n",
    "        try:\n",
    "            elem_citado = cell.find_element(By.CSS_SELECTOR, '.citado')\n",
    "            cvuri_dict = extract_data_from_cvuri(elem_citado)\n",
    "            print(f'Dados artigo: {cvuri_dict}')\n",
    "        except NoSuchElementException:\n",
    "            print('Nenhum dado encontrado na classe \"citado\".')\n",
    "        \n",
    "        # Extrair o DOI\n",
    "        try:\n",
    "            doi_elem = cell.find_element(By.CSS_SELECTOR, \"a.icone-producao.icone-doi\")\n",
    "            doi_link = doi_elem.get_attribute(\"href\")\n",
    "            issn_match = re.search(r\"issn=(\\d+)\", doi_link)\n",
    "            issn_from_doi = issn_match.group(1) if issn_match else None\n",
    "        except NoSuchElementException:\n",
    "            doi_link = None\n",
    "            issn_from_doi = None\n",
    "\n",
    "        # Extrair dados do JCR dos tooltips\n",
    "        try:\n",
    "            tooltip_elem = cell.find_element(By.CSS_SELECTOR, \"img.ajaxJCR\")\n",
    "            \n",
    "            # Traga o tooltip à vista para extração\n",
    "            ActionChains(driver).move_to_element(tooltip_elem).perform()\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "            original_title = tooltip_elem.get_attribute(\"original-title\")\n",
    "            original_title = original_title.split('<br />')[1].strip()\n",
    "            match = re.search(r\"Fator de impacto \\(JCR \\d{4}\\): (\\d+\\.\\d+)\", original_title)\n",
    "            impact_factor = match.group(1) if match else None\n",
    "            data_issn = issn_from_doi or tooltip_elem.get_attribute(\"data-issn\")\n",
    "            if not data_issn:\n",
    "                data_issn\n",
    "        except NoSuchElementException:\n",
    "            original_title = None\n",
    "            impact_factor = None\n",
    "            data_issn = issn_from_doi\n",
    "\n",
    "        # Compile os dados\n",
    "        tooltip_data = {\n",
    "            \"doi\": doi_link,\n",
    "            \"data-issn\": data_issn,\n",
    "            \"original_title\": original_title,\n",
    "            \"impact-factor\": impact_factor,\n",
    "        }\n",
    "        tooltip_data.update(cvuri_dict)\n",
    "\n",
    "        tooltip_data_list.append(tooltip_data)\n",
    "\n",
    "    ## Extraçao com tentativas para casos de muita instabilidade no servidor CNPq\n",
    "    # index = 0\n",
    "    # max_attempts = 3\n",
    "\n",
    "    # while index < tooltip_count:\n",
    "    #     attempts = 0\n",
    "    #     while index < tooltip_count:\n",
    "    #         attempts = 0\n",
    "    #         while attempts < max_attempts:\n",
    "    #             try:\n",
    "    #                 # Refetch os elementos para evitar StaleElementReferenceException\n",
    "    #                 elems = driver.find_elements(By.CSS_SELECTOR, \"#artigos-completos img.ajaxJCR\")\n",
    "    #                 elem = elems[index]\n",
    "                    \n",
    "    #                 ActionChains(driver).move_to_element(elem).perform()\n",
    "    #                 time.sleep(0.3)\n",
    "\n",
    "    #                 data_issn = elem.get_attribute(\"data-issn\")\n",
    "    #                 original_title = elem.get_attribute(\"original-title\")\n",
    "\n",
    "    #                 if original_title:\n",
    "    #                     match = re.search(r\"Fator de impacto \\(JCR \\d{4}\\): (\\d+\\.\\d+)\", original_title)\n",
    "    #                     impact_factor = match.group(1) if match else None\n",
    "                    \n",
    "    #                 # Se não tivermos um original-title, vamos buscar pelo DOI\n",
    "    #                 if not original_title:\n",
    "    #                     print('JCR não disponível, buscando ISSN pelo DOI...')\n",
    "    #                     try:\n",
    "    #                         # Localizar o elemento pai do elemento atual\n",
    "    #                         parent_elem = elem.find_element_by_xpath('..')\n",
    "                            \n",
    "    #                         # Tentando localizar o elemento que tem o DOI a partir do elemento pai\n",
    "    #                         # doi_elem = parent_elem.find_element(By.XPATH, \".//a[@class='icone-producao icone-doi']\")\n",
    "    #                         # doi_elem = elem.find_element(\n",
    "    #                         #     By.XPATH, \"preceding-sibling::a[@class='icone-producao icone-doi']\")\n",
    "    #                         doi_elem = parent_elem.find_element(\n",
    "    #                             By.XPATH, \"preceding-sibling::a[@class='icone-producao icone-doi']\")\n",
    "\n",
    "    #                         doi_link = doi_elem.get_attribute(\"href\")\n",
    "    #                         print(doi_link)\n",
    "                            \n",
    "    #                         # Usando regex para extrair o DOI e o ISSN\n",
    "    #                         issn_match = re.search(r\"issn=(\\d+)\", doi_link)\n",
    "    #                         issn_from_doi = issn_match.group(1) if issn_match else None\n",
    "                            \n",
    "    #                         # Se obtivemos o ISSN a partir do DOI, usamos ele\n",
    "    #                         if issn_from_doi:\n",
    "    #                             data_issn = issn_from_doi\n",
    "\n",
    "    #                     except NoSuchElementException:\n",
    "    #                         # Se não conseguimos encontrar o DOI, continuamos a processar\n",
    "    #                         pass\n",
    "    #                     impact_factor = None\n",
    "\n",
    "    #                 tooltip_data = {\n",
    "    #                     \"data-issn\": data_issn,\n",
    "    #                     \"original_title\": original_title,\n",
    "    #                     \"impact-factor\": impact_factor\n",
    "    #                 }\n",
    "\n",
    "    #                 tooltip_data_list.append(tooltip_data)\n",
    "    #                 print(f\"{index+1:2}. Tooltip extraído com sucesso: {data_issn}\")  # Log do tooltip extraído\n",
    "    #                 index += 1  # Incrementar o índice para mover-se para o próximo elemento\n",
    "    #                 break  # Break out of the attempts loop once successful\n",
    "\n",
    "    #             except IndexError:\n",
    "    #                 # print('Elemento faltando na extração, passando ao próximo...')\n",
    "    #                 index += 1\n",
    "    #                 break\n",
    "\n",
    "    #             except StaleElementReferenceException:\n",
    "    #                 # Increment the attempts count and try again\n",
    "    #                 attempts += 1\n",
    "\n",
    "    #         if attempts == max_attempts:\n",
    "    #             print(f\"Não foi possível processar o elemento de índice {index} após {max_attempts} tentativas. Passando ao próximo...\")\n",
    "    #             index += 1\n",
    "\n",
    "    print(f'Dados de {len(tooltip_data_list)} artigos completos extraídos')\n",
    "\n",
    "    # After adding the tooltip data, get the updated HTML content\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Use BeautifulSoup to parse\n",
    "    if page_source is not None:\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        soup.attrs['tooltips'] = tooltip_data_list\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def handle_stale_file_error(driver, max_retries=5, retry_interval=10):\n",
    "    \"\"\"\n",
    "    Detects and handles the \"Stale file handle\" error message in the webpage content.\n",
    "    \n",
    "    Parameters:\n",
    "        - driver (webdriver object): The Selenium webdriver object.\n",
    "        - max_retries (int): Maximum number of retries if the error is detected.\n",
    "        - retry_interval (int): Time interval (in seconds) between retries.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the error was resolved within the retry limit, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            error_div = driver.find_element(By.CSS_SELECTOR, 'resultado')\n",
    "            linha1 = error_div.fidChild('li')\n",
    "            if 'Stale file handle' in linha1.text:\n",
    "                time.sleep(retry_interval)\n",
    "            else:\n",
    "                return True\n",
    "        except NoSuchElementException:\n",
    "            # If the error div is not found, it's assumed the error is resolved.\n",
    "            return True\n",
    "        \n",
    "    # If the loop completes without breaking, it means the error wasn't resolved in the given retries.\n",
    "    return False\n",
    "\n",
    "def find_terms(NOME, instituicao, unidade, termo, driver, delay, limite):\n",
    "    \"\"\"\n",
    "    Função para manipular o HTML até abir a página HTML de cada currículo   \n",
    "\n",
    "    Parameters:\n",
    "        - NOME: É o nome completo de cada pesquisador\n",
    "        - Instituição, unidade e termo: Strings a buscar no currículo para reduzir duplicidades\n",
    "        - driver (webdriver object): The Selenium webdriver object.\n",
    "        - limite (int): Número máximo de tentativas em casos de erro.\n",
    "        - delay (int): tempo em milisegundos a esperar nas operações de espera.\n",
    "    \n",
    "    Returns:\n",
    "        elm_vinculo, np.NaN, np.NaN, np.NaN, driver.\n",
    "    \"\"\"\n",
    "\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    \n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(driver, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, driver\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(driver)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(driver, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:', e)\n",
    "                \n",
    "                # Call the handle_stale_file_error function\n",
    "                if handle_stale_file_error(driver):\n",
    "                    # If the function returns True, it means the error was resolved.\n",
    "                    # try to get the nome_vinculo again:\n",
    "                    try:\n",
    "                        elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                        nome_vinculo = elm_vinculo.text\n",
    "                    except Exception as e2:\n",
    "                        print('Erro ao encontrar o primeiro resultado da lista de nomes após tratamento do erro:', e2)\n",
    "                        return np.NaN, NOME, np.NaN, e2, driver\n",
    "                else:\n",
    "                    # If the function returns False, it means the error was not resolved within the given retries.\n",
    "                    return np.NaN, NOME, np.NaN, e, driver\n",
    "\n",
    "                print('Não foi possível extrair por falha no servidor do CNPq:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, driver\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, driver\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(driver)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(driver)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(driver, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', driver\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, driver\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # driver.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = driver.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função find_terms()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, driver\n",
    "    \n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "def is_abreviation(substring):\n",
    "    pattern = r'(?: [a-zA-Z]\\.)|(?: [a-zA-Z] \\.)'\n",
    "    if re.search(pattern, substring):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_surname(substring):\n",
    "    return not is_abreviation(substring) and substring.endswith(', ')\n",
    "\n",
    "def compose_full_name(surname, parts, marker):\n",
    "    full_name = \"\"\n",
    "    for part in parts:\n",
    "        full_name += part + marker\n",
    "    full_name += ' '+surname\n",
    "    return full_name.strip()\n",
    "\n",
    "def split_authors(string, verbose=False):\n",
    "    authors_names = []\n",
    "    \n",
    "    m1a = \"; \"\n",
    "    m1b = \", \"\n",
    "    \n",
    "    if m1a in string and (string.count(m1a) <= string.count(m1b) or m1b not in string):\n",
    "        marker = m1a\n",
    "        authors_names = string.split(marker)\n",
    "        return [x.strip() for x in authors_names]\n",
    "    else:\n",
    "        marker = m1b\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Selected marker: \"{marker}\"')\n",
    "\n",
    "    parts_list = string.split(marker)\n",
    "    name    = \"\"\n",
    "    surname = \"\"\n",
    "    \n",
    "    for part in parts_list:\n",
    "        if is_abreviation(part):\n",
    "            classification = 'Abrev'\n",
    "            name += part + marker\n",
    "        else:\n",
    "            classification = 'Name'\n",
    "            if is_surname(part) or surname == \"\":\n",
    "                classification = 'SOBRENOME'\n",
    "                if surname:\n",
    "                    full_name = compose_full_name(surname.strip(), name.split(marker), ', ')\n",
    "                    if full_name not in authors_names:\n",
    "                        authors_names.append(full_name)\n",
    "                surname = part\n",
    "                name = part + marker\n",
    "            else:\n",
    "                name = part + marker\n",
    "        if verbose:\n",
    "            # print(f'Quantidade de m1a: {string.count(m1a)} \\nQuantidade de m1b: {string.count(m1b)}')\n",
    "            print(f'Autor: {part:40} | Forma: {classification}')\n",
    "\n",
    "        authors_names.append(part.strip(marker).strip())\n",
    "     \n",
    "    return [x.strip() for x in authors_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extrair_indices(layout_celula):\n",
    "    indice = layout_celula.select_one('div.layout-cell-pad-5.text-align-right')\n",
    "    valor = layout_celula.select_one('div.layout-cell.layout-cell-9 div.layout-cell-pad-5')\n",
    "    \n",
    "    if indice and valor:\n",
    "        valores_extraidos = valor.text.split('<br class=\"clear\">') if '<br class=\"clear\">' in valor.text else valor.text.split('\\n\\t\\t\\n\\t')\n",
    "        return indice.text, valores_extraidos\n",
    "    return None, None\n",
    "\n",
    "def extrair_secao(title_wrapper):\n",
    "    tags_relevantes = ['h1', 'ul', 'b']\n",
    "    tags_encontradas = []\n",
    "    for tag in tags_relevantes:\n",
    "        elemento_encontrado = title_wrapper.select_one(tag)\n",
    "        if elemento_encontrado:\n",
    "            tags_encontradas.append((tag, elemento_encontrado))\n",
    "    tags_ordenadas = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def encontrar_subchave(title_wrapper):\n",
    "    tags_relevantes  = ['ul', 'a', 'b']\n",
    "    tags_encontradas = [(tag, title_wrapper.find(tag)) for tag in tags_relevantes]\n",
    "    tags_ordenadas   = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def extrair_titulo(title_wrapper):\n",
    "    inst_back_texts = {}\n",
    "    inst_back_elements = title_wrapper.find_all('div', class_='inst_back')\n",
    "    for index, inst_back in enumerate(inst_back_elements):\n",
    "        b_tag = inst_back.find('b')\n",
    "        if b_tag:\n",
    "            inst_back_texts[index] = b_tag.text.strip()\n",
    "    return inst_back_texts if inst_back_texts else None\n",
    "\n",
    "def extrair_wraper(soup, json_data):\n",
    "    title_wrappers = soup.select('div.layout-cell-pad-main div.title-wrapper')\n",
    "    for title_wrapper in title_wrappers:\n",
    "        section_name = extrair_secao(title_wrapper)\n",
    "        if section_name:\n",
    "            section_name = section_name.text.strip()\n",
    "            \n",
    "            titles = extrair_titulo(title_wrapper)\n",
    "            json_data[\"Properties\"][section_name] = {}\n",
    "            \n",
    "            if titles:\n",
    "                for index, title in titles.items():\n",
    "                    json_data[\"Properties\"][section_name][title] = {}\n",
    "            \n",
    "            layout_cells = title_wrapper.select('div.layout-cell.layout-cell-12.data-cell')\n",
    "            for layout_celula in layout_cells:\n",
    "                indice, valores_extraidos = extrair_indices(layout_celula)\n",
    "                if indice and valores_extraidos:\n",
    "                    if titles and indice in titles.values():\n",
    "                        if len(titles) > 1:\n",
    "                            for title in titles.values():\n",
    "                                if title.strip() in indice:\n",
    "                                    json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                        else:\n",
    "                            title = list(titles.values())[0]\n",
    "                            json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                    else:\n",
    "                        json_data[\"Properties\"][section_name][indice] = valores_extraidos\n",
    "    return json_data\n",
    "\n",
    "def imprimir_informacoes(dados_json, nome_no, indent=0):\n",
    "    indentation = '    ' * indent  # Calculating the current indentation level\n",
    "\n",
    "    if dados_json and nome_no and dados_json.get(nome_no):\n",
    "        if indent == 0:  # Logging node-level information only at the root\n",
    "            logging.info(f\"{indentation}Node: {nome_no}\")\n",
    "            logging.info(f\"{indentation}Total keys extracted: {len(dados_json[nome_no].keys())}\")\n",
    "        \n",
    "        for key in dados_json[nome_no].keys():\n",
    "            logging.info(f\"{indentation}{key.strip() if key else ''}\")\n",
    "\n",
    "            if isinstance(dados_json[nome_no][key], dict):  # Check for nested dictionaries\n",
    "                # Recursive call to handle nested dictionaries\n",
    "                imprimir_informacoes(dados_json[nome_no], key, indent + 1)\n",
    "            else:\n",
    "                logging.info(f\"{indentation}    Values: {dados_json[nome_no][key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funçoes para montar dicionários para persistir em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jPersister:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(input_data):\n",
    "        if input_data is None:\n",
    "            return None\n",
    "        \n",
    "        if isinstance(input_data, dict):\n",
    "            for key, value in input_data.items():\n",
    "                if isinstance(value, dict):  # Se um valor ainda é um dicionário, converte em string JSON\n",
    "                    input_data[key] = json.dumps(Neo4jPersister.convert_to_primitives(value), ensure_ascii=False)\n",
    "                else:\n",
    "                    input_data[key] = Neo4jPersister.convert_to_primitives(value)\n",
    "            return input_data\n",
    "        \n",
    "        elif isinstance(input_data, list):\n",
    "            return [Neo4jPersister.convert_to_primitives(item) for item in input_data]\n",
    "        \n",
    "        elif isinstance(input_data, str):\n",
    "            if 'http://' in input_data or 'https://' in input_data:\n",
    "                parts = input_data.split(\" \")\n",
    "                new_parts = [urllib.parse.quote(part) if part.startswith(('http://', 'https://')) else part for part in parts]\n",
    "                return \" \".join(new_parts)\n",
    "            return input_data\n",
    "        \n",
    "        elif isinstance(input_data, (int, float, bool)):\n",
    "            return input_data\n",
    "        \n",
    "        else:\n",
    "            return str(input_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return Neo4jPersister.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise\n",
    "\n",
    "    def persist_data(self, data_dict, label):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"CREATE (node:{label}) SET node = $props\"\n",
    "            session.run(query, props=data_dict_primitives)\n",
    "\n",
    "    def update_data(self, node_id, data_dict):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MATCH (node) WHERE id(node) = {node_id} SET node += $props\"\n",
    "            session.run(query, props=data_dict_primitives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tit1_soup(soup, data_dict=None, verbose=False):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "    \n",
    "    # Títulos contendo subseções\n",
    "    tit1 = ['Identificação', 'Endereço', 'Formação acadêmica/titulação', 'Pós-doutorado', 'Formação Complementar',\n",
    "            'Linhas de pesquisa', 'Projetos de pesquisa', 'Projetos de extensão',\n",
    "            'Projetos de desenvolvimento', 'Revisor de periódico', 'Revisor de projeto de fomento', 'Áreas de atuação',\n",
    "            'Idiomas', 'Inovação']\n",
    "\n",
    "    tit2 = ['Atuação Profissional'] # dados com subseções\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        # Encontre o título do bloco\n",
    "        titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        \n",
    "        # Verifique se o título está na lista 'tit1'\n",
    "        if titulo in tit1:\n",
    "            if verbose:\n",
    "                print(titulo)\n",
    "            \n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "            for data_cell in data_cells:\n",
    "                divs_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "                divs_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "                keys = []\n",
    "                vals = []\n",
    "\n",
    "                for i, j in zip(divs_layout_cell_3, divs_layout_cell_9):\n",
    "                    if divs_layout_cell_3 and divs_layout_cell_9:\n",
    "                        key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                        key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                        keys.append(key_text)\n",
    "                        val = j.find('div', class_='layout-cell-pad-5')\n",
    "                        val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                        vals.append(val_text)\n",
    "                        if verbose:\n",
    "                            print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "                agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                data_dict[titulo] = Neo4jPersister.convert_to_primitives(agg_dict)\n",
    "\n",
    "        \n",
    "        if titulo in tit2:\n",
    "            if verbose:\n",
    "                print(titulo)\n",
    "            \n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "            for data_cell in data_cells:\n",
    "                sections = data_cell.find_all(\"div\", class_=\"inst_back\")               \n",
    "                if verbose:\n",
    "                    print(len(sections), 'seções')\n",
    "\n",
    "                for section in sections:\n",
    "                    section_name = section.find('b').get_text().strip()\n",
    "                    data_dict[titulo][section_name] = []\n",
    "                    if verbose:\n",
    "                        print(section_name)\n",
    "\n",
    "                    sibling = section.find_next_sibling()\n",
    "                    current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                    while sibling:\n",
    "                        classes = sibling.get('class', [])\n",
    "\n",
    "                        if 'layout-cell-3' in classes:  # Data key\n",
    "                            key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "                            if sibling and 'layout-cell-9' in sibling.get('class', []):  # Check if value is present\n",
    "                                val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                current_data[key] = val\n",
    "                                if verbose:\n",
    "                                    print(len(current_data.values()), key, val)\n",
    "\n",
    "                        elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                            next_sibling = sibling.find_next_sibling()\n",
    "                            if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                sibling = None\n",
    "                            else:\n",
    "                                if current_data:\n",
    "                                    data_dict[titulo][section_name].append(current_data)  # Armazenamos os dados em uma lista\n",
    "\n",
    "                        if sibling:\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tit2_soup(soup, data_dict=None, verbose=False):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')    \n",
    "    \n",
    "    tit2 = ['Produções', 'Bancas', 'Orientações']\n",
    "\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        # Encontre o título do bloco\n",
    "        titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        \n",
    "        # Verifique se o título está na lista 'tit2'\n",
    "        if titulo in tit2:\n",
    "            if verbose:\n",
    "                print(f'Título: {titulo}')\n",
    "\n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "\n",
    "            for data_cell in data_cells:\n",
    "                sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                if verbose:\n",
    "                    print(len(sections), 'seções')\n",
    "                for section in sections:\n",
    "                    section_name = section.find('b').get_text().strip()\n",
    "                    data_dict[titulo][section_name] = {}\n",
    "                    if verbose:\n",
    "                        print(f'Seção: {section_name}')\n",
    "\n",
    "                    sibling = section.find_next_sibling()\n",
    "                    current_subsection = None\n",
    "                    current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                    if section_name == 'Produção bibliográfica':\n",
    "                        subsections = section.find_next_siblings('div', class_='cita-artigos')\n",
    "                        if verbose:\n",
    "                            print(len(subsections), 'subseções')                       \n",
    "                        for subsection in subsections:                            \n",
    "                            if subsection:\n",
    "                                subsection_name = subsection.find('b').get_text().strip()\n",
    "                                if verbose:\n",
    "                                    print(f'    Subseção: {subsection_name}') # nomes de subseção como ocorrências \n",
    "                                    print(f'    {len(subsection)} divs na subseção {subsection_name}')                                \n",
    "                                if subsection_name == 'Citações':\n",
    "                                    current_subsection = subsection_name\n",
    "                                    data_dict[titulo][section_name]['Citações'] = {}\n",
    "                                    sub_section_list = []\n",
    "                                        \n",
    "                                    ## Extrair quantidade de citações e fator H das divs de subseção com classe lyout-cell-12\n",
    "                                    next_siblings = subsection.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "\n",
    "                                    for sibling in next_siblings:\n",
    "                                        citation_counts = sibling.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que contém os Valores de Citações\n",
    "                                        if citation_counts:\n",
    "                                            for i in citation_counts:\n",
    "                                                database = i.get_text()\n",
    "                                                total_trab = i.find_next_sibling(\"div\", class_=\"trab\").get_text().split(\"Total de trabalhos:\")[1]\n",
    "                                                total_cite = i.find_next_sibling(\"div\", class_=\"cita\").get_text().split(\"Total de citações:\")[1]\n",
    "                                                fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "                                                num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "                                                data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\").get_text().split(\"Data:\")[1].strip()\n",
    "\n",
    "                                                # Converta os valores para tipos de dados adequados\n",
    "                                                total_trab = int(total_trab)\n",
    "                                                total_cite = int(total_cite)\n",
    "\n",
    "                                                citation_numbers = {\n",
    "                                                    \"Database\": database,\n",
    "                                                    \"Total de trabalhos\": total_trab,\n",
    "                                                    \"Total de citações\": total_cite,\n",
    "                                                    \"Índice_H\": num_fator_h,\n",
    "                                                    \"Data\": data_wos\n",
    "                                                }\n",
    "\n",
    "                                                # Verifique se a subseção atual já existe no dicionário\n",
    "                                                if 'Citações' not in data_dict[titulo][section_name]:\n",
    "                                                    data_dict[titulo][section_name]['Citações'] = {}  # Inicialize como uma lista vazia\n",
    "\n",
    "                                                data_dict[titulo][section_name]['Citações'] = citation_numbers\n",
    "\n",
    "                                                if verbose:\n",
    "                                                    print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')\n",
    "                            \n",
    "                        ## Encontrar a div irmã de div subseção com classe layout-cell-12 com artigos\n",
    "                        vals_jcr = []\n",
    "                        div_artigo_geral = data_cell.findChild(\"div\", id=\"artigos-completos\")\n",
    "                        if verbose:\n",
    "                            print(f'Encontrada {len(div_artigo_geral)} div geral de artigos')  \n",
    "                        divs_artigos = div_artigo_geral.find_all('div', class_='artigo-completo')\n",
    "                        if verbose:\n",
    "                            print(len(divs_artigos), 'divs de artigos')\n",
    "                        \n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                        if divs_artigos:                              \n",
    "                            for div_artigo in divs_artigos:\n",
    "                                data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = {}                                   \n",
    "                                    ## Extrair filhos da classes de artigos completos que estão à frente\n",
    "                                sibling = div_artigo.findChild()\n",
    "\n",
    "                                while sibling:\n",
    "                                    classes = sibling.get('class', [])\n",
    "\n",
    "                                    if 'layout-cell-1' in classes:  # Data key\n",
    "                                        key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                        sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                        if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                            val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                            info_dict = {\n",
    "                                                'data-issn': 'NULL',\n",
    "                                                'impact-factor': 'NULL',  \n",
    "                                                'jcr-year': 'NULL',\n",
    "                                            }\n",
    "                                            # Remova as tags span da div\n",
    "                                            for span in sibling.find_all('span'):\n",
    "                                                span.extract()\n",
    "                                            \n",
    "                                            val_text = sibling.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "\n",
    "                                            current_data[key] = val_text\n",
    "                                            if verbose:\n",
    "                                                print(len(current_data.values()), key, val)\n",
    "\n",
    "                                            sup_element = sibling.find('sup')\n",
    "                                            raw_jcr_data = sup_element.get_text()\n",
    "                                            # print('sup_element:',sup_element)\n",
    "                                            img_element = sup_element.find('img')\n",
    "                                            # print('img_element:',img_element)\n",
    "\n",
    "                                            if sup_element:\n",
    "                                                if img_element:\n",
    "                                                    original_title = img_element.get('original-title')\n",
    "                                                    if original_title:\n",
    "                                                        info_list = original_title.split('<br />') if original_title.split('<br />') else original_title\n",
    "                                                        \n",
    "                                                        if info_list != 'NULL':\n",
    "                                                            info_dict = {\n",
    "                                                                'data-issn': img_element.get('data-issn'),\n",
    "                                                                'impact-factor': info_list[1].split(': ')[1],\n",
    "                                                                'jcr-year': info_list[1].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')',''),\n",
    "                                                            }\n",
    "                                                    else:\n",
    "                                                        info_dict = {\n",
    "                                                            'data-issn': img_element.get('data-issn'),\n",
    "                                                            'impact-factor': 'NULL',\n",
    "                                                            'jcr-year': 'NULL',\n",
    "                                                        }\n",
    "                                            else:\n",
    "                                                info_dict = {\n",
    "                                                    'data-issn': 'NULL',\n",
    "                                                    'impact-factor': 'NULL',\n",
    "                                                    'jcr-year': 'NULL',\n",
    "                                                }                                                                \n",
    "                                                \n",
    "                                            vals_jcr.append(info_dict)\n",
    "                                            if verbose:\n",
    "                                                print(f'         {info_dict}')\n",
    "\n",
    "                                        if 'JCR' not in data_dict:\n",
    "                                            data_dict['JCR'] = []\n",
    "                                        \n",
    "                                        if verbose:\n",
    "                                            print(len(vals_jcr))\n",
    "                                        data_dict['JCR'] = vals_jcr\n",
    "\n",
    "                                    elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                        next_sibling = sibling.find_next_sibling()\n",
    "                                        if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                            sibling = None\n",
    "                                        else:\n",
    "                                            if current_data:\n",
    "                                                converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                                data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = converted_data\n",
    "\n",
    "                                    if sibling:\n",
    "                                        sibling = sibling.find_next_sibling()\n",
    "                    else:\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "\n",
    "                            if 'cita-artigos' in classes:  # Subsection start\n",
    "                                subsection_name = sibling.find('b').get_text().strip()\n",
    "                                current_subsection = subsection_name\n",
    "                                if verbose:\n",
    "                                    print(f'    Subseção: {subsection_name}')\n",
    "                                data_dict[titulo][section_name][current_subsection] = {}\n",
    "                                current_data = {}  # Inicializamos o dicionário de dados da subseção atual\n",
    "\n",
    "                            elif 'layout-cell-1' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Subsection or section end\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_subsection:\n",
    "                                        data_dict[titulo][section_name][current_subsection] = Neo4jPersister.convert_to_primitives(current_data)  # Armazenamos os dados da subseção atual\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "    \n",
    "    # Verifique se os dados dos tooltips estão presentes no objeto soup\n",
    "    if 'tooltips' in soup.attrs:\n",
    "        tooltips_data = soup.attrs['tooltips']\n",
    "        agg = []\n",
    "        \n",
    "        for tooltip in tooltips_data:\n",
    "            agg_data = {}\n",
    "            \n",
    "            # Extração do ano JCR a partir do \"original_title\"\n",
    "            if tooltip.get(\"original_title\"):\n",
    "                jcr_year = tooltip[\"original_title\"].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')','')\n",
    "                agg_data[\"jcr-ano\"] = jcr_year\n",
    "            \n",
    "            # Adicionar todas as chaves e valores do tooltip ao dicionário agg_data\n",
    "            for key, value in tooltip.items():\n",
    "                agg_data[key] = value\n",
    "            \n",
    "            agg.append(agg_data)\n",
    "        \n",
    "        data_dict['JCR2'] = agg\n",
    "    else:\n",
    "        print('Não foram achados os dados de tooltip')\n",
    "        print(soup.attrs)  \n",
    "           \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tit3_soup(soup, data_dict=None, verbose=False):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "    \n",
    "    # Títulos da seção 'Eventos'\n",
    "    tit3 = ['Eventos']\n",
    "\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        # Encontre o título do bloco\n",
    "        titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        # Verifique se o título está na lista 'tit3'\n",
    "        if titulo in tit3:\n",
    "            if verbose:\n",
    "                print(f'Título: {titulo}')\n",
    "            \n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "            for data_cell in data_cells:\n",
    "                sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                if verbose:\n",
    "                    print(len(sections), 'seções')\n",
    "\n",
    "                for section in sections:\n",
    "                    section_name = section.find('b').get_text().strip()\n",
    "                    data_dict[titulo][section_name] = []\n",
    "                    if verbose:\n",
    "                        print(section_name)\n",
    "\n",
    "                    sibling = section.find_next_sibling()\n",
    "                    current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                    while sibling:\n",
    "                        classes = sibling.get('class', [])\n",
    "\n",
    "                        if 'layout-cell-1' in classes:  # Data key\n",
    "                            key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "                            if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                current_data[key] = val\n",
    "                                if verbose:\n",
    "                                    print(len(current_data.values()), key, val)\n",
    "\n",
    "                        elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                            next_sibling = sibling.find_next_sibling()\n",
    "                            if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                sibling = None\n",
    "                            else:\n",
    "                                if current_data:\n",
    "                                    converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                    data_dict[titulo][section_name] = converted_data\n",
    "\n",
    "                        if sibling:\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função montar data_dict com todos dados extraídos do DOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data_dicts(soup):\n",
    "    \"\"\"\n",
    "    Aggregates data from various dictionary sources into a consolidated nested dictionary, \n",
    "    ensuring that all nested lists within the dictionaries are transformed into nested dictionaries.\n",
    "    \n",
    "    Parameters:\n",
    "    - soup: BeautifulSoup object, representing the parsed HTML content.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: An aggregated dictionary containing the consolidated data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def convert_list_to_dict(lst):\n",
    "        \"\"\"\n",
    "        Converts a list into a dictionary with indices as keys.\n",
    "        \n",
    "        Parameters:\n",
    "        - lst: list, input list to be transformed.\n",
    "        \n",
    "        Returns:\n",
    "        - dict: Transformed dictionary.\n",
    "        \"\"\"\n",
    "        return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "    def merge_dict(d1, d2):\n",
    "        \"\"\"\n",
    "        Recursively merges two dictionaries, transforming nested lists into dictionaries.\n",
    "        \n",
    "        Parameters:\n",
    "        - d1: dict, the primary dictionary into which data is merged.\n",
    "        - d2: dict or list, the secondary dictionary or list from which data is sourced.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        # If d2 is a list, convert it to a dictionary first\n",
    "        if isinstance(d2, list):\n",
    "            d2 = convert_list_to_dict(d2)\n",
    "        \n",
    "        for key, value in d2.items():\n",
    "            if isinstance(value, list):\n",
    "                d2[key] = convert_list_to_dict(value)\n",
    "            if key in d1 and isinstance(d1[key], dict) and isinstance(value, dict):\n",
    "                merge_dict(d1[key], value)\n",
    "            else:\n",
    "                d1[key] = value\n",
    "\n",
    "\n",
    "    # Extract necessary information from soup\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "    name = info_list[0]\n",
    "\n",
    "    # Initialization of the aggregated_data dictionary\n",
    "    aggregated_data = {\"labels\": \"Person\", \"name\": name, \"InfPes\": info_list, \"Resumo\": [elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()]}\n",
    "\n",
    "    # Data extraction and merging\n",
    "    for data_extraction_func in [extract_tit1_soup, extract_tit2_soup, extract_tit3_soup]:\n",
    "        extracted_data = data_extraction_func(soup, verbose=False)\n",
    "        for title, data in extracted_data.items():\n",
    "            if title not in aggregated_data:\n",
    "                aggregated_data[title] = {}\n",
    "            merge_dict(aggregated_data[title], data)\n",
    "\n",
    "    return aggregated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funçoes de plotagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_dict(lst):\n",
    "    \"\"\"\n",
    "    Converts a list into a dictionary with indices as keys.\n",
    "    \n",
    "    Parameters:\n",
    "    - lst: list, input list to be transformed.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Transformed dictionary.\n",
    "    \"\"\"\n",
    "    return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "def dict_jcr_list(data_dict):\n",
    "    # Extract JCR entries from the data dictionary\n",
    "    jcr_entries = data_dict.get('JCR', {})\n",
    "\n",
    "    # Initialize an empty list to store JCR properties\n",
    "    jcr_properties_list = []\n",
    "\n",
    "    # If jcr_entries is a dictionary\n",
    "    if isinstance(jcr_entries, dict):\n",
    "        for key, value in jcr_entries.items():\n",
    "            jcr_properties_list.append(value)\n",
    "\n",
    "    # If jcr_entries is a list\n",
    "    elif isinstance(jcr_entries, list):\n",
    "        jcr_properties_list.extend(jcr_entries)\n",
    "\n",
    "    # If jcr_entries is a string\n",
    "    elif isinstance(jcr_entries, str):\n",
    "        jcr_properties_list.append(jcr_entries)\n",
    "\n",
    "    # If jcr_entries is of any other unexpected type\n",
    "    else:\n",
    "        print(f\"Unexpected data type {type(jcr_entries)} for JCR entries. Expected dictionary, list, or string.\")\n",
    "\n",
    "    return jcr_properties_list\n",
    "\n",
    "\n",
    "def dict_doi_list(data_dict):\n",
    "    # Extract JCR entries from the data dictionary\n",
    "    jcr_entries = data_dict.get('JCR2', {})\n",
    "\n",
    "    # Initialize an empty list to store JCR properties\n",
    "    jcr_properties_list = []\n",
    "\n",
    "    # If jcr_entries is a dictionary\n",
    "    if isinstance(jcr_entries, dict):\n",
    "        for key, value in jcr_entries.items():\n",
    "            jcr_properties_list.append(value)\n",
    "\n",
    "    # If jcr_entries is a list\n",
    "    elif isinstance(jcr_entries, list):\n",
    "        jcr_properties_list.extend(jcr_entries)\n",
    "\n",
    "    # If jcr_entries is a string\n",
    "    elif isinstance(jcr_entries, str):\n",
    "        jcr_properties_list.append(jcr_entries)\n",
    "\n",
    "    # If jcr_entries is of any other unexpected type\n",
    "    else:\n",
    "        print(f\"Unexpected data type {type(jcr_entries)} for JCR entries. Expected dictionary, list, or string.\")\n",
    "\n",
    "    return jcr_properties_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Função para escolher a string com o maior comprimento, considerando NaNs\n",
    "def max_len_string(x):\n",
    "    valid_strings = x.dropna()\n",
    "    if not valid_strings.empty:\n",
    "        return valid_strings.loc[valid_strings.str.len().idxmax()]\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "# Função para escolher a string com o menor comprimento, considerando NaNs\n",
    "def min_len_string(x):\n",
    "    valid_strings = x.dropna()\n",
    "    if not valid_strings.empty:\n",
    "        return valid_strings.loc[valid_strings.str.len().idxmin()]\n",
    "    else:\n",
    "        return np.nan  \n",
    "\n",
    "def break_text_words_into_lines(text, max_lines=3):\n",
    "    if not isinstance(text, str):  # Handle non-string values\n",
    "        return text\n",
    "\n",
    "    words = text.split()\n",
    "    lines = ['']\n",
    "    line_index = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if len(lines[line_index]) + len(word) + 1 > len(text) // max_lines and line_index < max_lines - 1:\n",
    "            lines.append('')\n",
    "            line_index += 1\n",
    "        lines[line_index] += (word + ' ')\n",
    "    \n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "def break_text(text):\n",
    "    if not isinstance(text, str):  # Handle non-string values\n",
    "        return text\n",
    "\n",
    "    # Split at the ' (' character\n",
    "    parts = text.split(' (', 1)\n",
    "    \n",
    "    # If the text was split, then join the parts with a newline in between\n",
    "    if len(parts) > 1:\n",
    "        return parts[0] + \"\\n(\" + parts[1]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def compute_color(value, min_val, max_val):\n",
    "    if pd.isna(value):\n",
    "        return '#808080'  # return a shade of grey for NaN values\n",
    "\n",
    "    position = (value - min_val) / (max_val - min_val)\n",
    "    colors = np.array([[1, 1, 0], [0, 0.5, 0]])  # Yellow to Dark Green\n",
    "    color = colors[0] + position * (colors[1] - colors[0])\n",
    "    return mcolors.to_hex(color)\n",
    "\n",
    "def plot_vertbar(jcr_properties_list):\n",
    "    data = pd.DataFrame(jcr_properties_list)\n",
    "    # Trocar 'Ieee' por 'IEEE' nos nomes de periódicos\n",
    "    data['journal'] = data['journal'].str.replace('Ieee', 'IEEE', case=False)\n",
    "\n",
    "    # Trocar os valores do NULL para tratamento adequado na plotagem\n",
    "    data.replace(['NULL', None], np.nan, inplace=True)\n",
    "    data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "\n",
    "    grouped = data.groupby('data-issn', dropna=False).agg(\n",
    "        count=('issn', 'size'),\n",
    "        impact_factor_max=('impact-factor', 'max'),\n",
    "        journal=('journal', max_len_string),\n",
    "        jcr_year = ('jcr-ano', min_len_string)\n",
    "    ).reset_index()\n",
    "\n",
    "    grouped.rename(columns={\"count\": \"issn_count\", \"impact_factor_max\": \"impact-factor\"}, inplace=True)\n",
    "\n",
    "    count_nan_issn  = data['issn'].isna().sum()\n",
    "    null_row        = pd.DataFrame({'issn': ['MISSING'], 'issn_count': [count_nan_issn], 'impact-factor': [np.nan], 'journal': [np.nan]})\n",
    "    \n",
    "    data_all_counts = pd.concat([grouped, null_row], ignore_index=True)\n",
    "    data_all_counts = data_all_counts.sort_values(by=['impact-factor'], ascending=True)\n",
    "\n",
    "    min_impact = data_all_counts['impact-factor'].min()\n",
    "    max_impact = data_all_counts['impact-factor'].max()\n",
    "    data_all_counts['color'] = data_all_counts['impact-factor'].apply(lambda x: compute_color(x, min_impact, max_impact))\n",
    "\n",
    "    fig = px.bar(data_all_counts, x='data-issn', y='issn_count', color='color',\n",
    "                 title='Frequência de publicação acumulada no período completo versus fator de impacto de cada periódico',\n",
    "                 hover_data=['impact-factor', 'issn_count', 'jcr_year'],\n",
    "                 text=data_all_counts['impact-factor'].apply(lambda x: round(x, 2) if not pd.isna(x) else x),\n",
    "                 color_discrete_map='identity'\n",
    "                )\n",
    "    fig.update_xaxes(tickangle=315,\n",
    "                     tickvals=data_all_counts['issn'].tolist(), \n",
    "                     ticktext=data_all_counts['journal'].apply(break_text).tolist(), \n",
    "                    #  categoryorder='total ascending',\n",
    "                     )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        yaxis=dict(autorange=True),\n",
    "        yaxis_title=\"Frequência de publicação no periódico\",\n",
    "        xaxis_title=\"Periódicos\",\n",
    "        height=800,  # Adjust this value as needed\n",
    "        margin=dict(l=50, r=50, b=150, t=50)  # Increase the bottom margin (b) as needed\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(texttemplate='%{text}', textposition='outside', textfont_size=10)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def plot_scatter(jcr_properties_list):\n",
    "\n",
    "    data = pd.DataFrame(jcr_properties_list)\n",
    "    # Substituir 'NULL' e None por np.nan em todo o dataframe\n",
    "    data.replace(['NULL', None], np.nan, inplace=True)\n",
    "    data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "\n",
    "    # Agrupar pelo 'data-issn', contar ocorrências, maior valor de 'impact-factor' e pegar o 'journal'\n",
    "    grouped = data.groupby('issn', dropna=False).agg(\n",
    "        count=('issn', 'size'),\n",
    "        impact_factor_max=('impact-factor', 'max'),\n",
    "        journal=('journal', max_len_string)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Renomear as colunas para corresponder às suas descrições\n",
    "    grouped.rename(columns={\"count\": \"issn_count\", \"impact_factor_max\": \"impact-factor\"}, inplace=True)\n",
    "    data_all_counts = grouped.sort_values(by=['impact-factor'], ascending=False)\n",
    "\n",
    "    # Criar o gráfico de dispersão com Plotly Express\n",
    "    fig = px.scatter(data_all_counts, x='issn', y='impact-factor', color='issn_count',\n",
    "                     size='issn_count',\n",
    "                     title='Gráfico de Dispersão de Fator de Impacto versus ISSN no período completo',\n",
    "                     labels={'issn': 'ISSN', 'impact-factor': 'Fator de Impacto', 'issn_count': 'Quantidade'},\n",
    "                     hover_data=['journal'],\n",
    "                     color_continuous_scale=\"YlOrRd\")\n",
    "\n",
    "    # Personalizar layout\n",
    "    fig.update_traces(marker=dict(size=data_all_counts['issn_count'] * 2))\n",
    "    fig.update_xaxes(tickangle=315, tickvals=data_all_counts['issn'].tolist(), categoryorder='total ascending')\n",
    "    fig.update_traces(marker=dict(opacity=1))\n",
    "    max_impact = int(data_all_counts['impact-factor'].max()) + 1  # máximo impact-factor arredondado para cima\n",
    "    fig.update_layout(yaxis=dict(autorange=True, tickvals=list(range(max_impact))))\n",
    "\n",
    "    # Exibir o gráfico\n",
    "    fig.show()\n",
    "\n",
    "    return data_all_counts\n",
    "\n",
    "def dataframe_to_list_of_dicts(df):\n",
    "    records = df.to_dict(orient='records')\n",
    "    formatted_records = []\n",
    "    \n",
    "    for record in records:\n",
    "        formatted_record = {}\n",
    "        for key, value in record.items():\n",
    "            if pd.isna(value):\n",
    "                formatted_record[key] = 'NULL'\n",
    "            else:\n",
    "                formatted_record[key] = str(value)\n",
    "        formatted_records.append(formatted_record)\n",
    "    \n",
    "    return formatted_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função integração com CrossRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install crossrefapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from crossref.restful import Works\n",
    "\n",
    "def get_issn(doi):\n",
    "    works = Works()\n",
    "    all_data = works.doi(doi.replace('http://dx.doi.org/','').strip())\n",
    "    issn=all_data['ISSN'][0]\n",
    "    journal=all_data['container-title'][0]\n",
    "    return issn, journal\n",
    "\n",
    "def crossref_complement(data):\n",
    "    ## Substituir NULL por None\n",
    "    data.replace(['NULL', None], np.nan, inplace=True)\n",
    "    data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "    data = data.sort_values(by=['impact-factor','issn','doi'], ascending=False)\n",
    "\n",
    "    issn_crossref=[]\n",
    "    journal_crossref=[]\n",
    "    for i in data['doi']:\n",
    "        try:\n",
    "            issn, journal = get_issn(i)\n",
    "            issn_crossref.append(issn)\n",
    "            journal_crossref.append(journal.replace('amp;',''))\n",
    "        except:\n",
    "            issn_crossref.append(np.nan)\n",
    "            journal_crossref.append(np.nan)\n",
    "\n",
    "    data['issn'] = issn_crossref\n",
    "    data['journal'] = journal_crossref\n",
    "\n",
    "    count_jci = data['impact-factor'].notna().sum()\n",
    "    count_doi = data['doi'].notna().sum()\n",
    "    count_jcrissn = data['issn'].notna().sum()\n",
    "    print(f'{count_doi}  DOI recuperados, {len(data.index)-count_doi}  DOI ausentes')\n",
    "    print(f'{count_jcrissn} ISSN recuperados, {len(data.index)-count_jcrissn} ISSN ausentes')\n",
    "    print(f'{count_jci}  JCI recuperados, {len(data.index)-count_jci}  JCI ausentes')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções buscar em CSVs dados de ISSN do JCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir conteúdo em linhas, pegar linha de rótulos até a penúltima linha (ignorar informações de direitos autorais)\n",
    "def org_lines(file_path):\n",
    "    from io import StringIO\n",
    "\n",
    "    # Acessar para ler rótulos de colunas que estão desencontrados dos dados\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        raw_content = file.read()\n",
    "\n",
    "    # Guardar rótulos de colunas\n",
    "    lines = raw_content.splitlines()[1:3]\n",
    "    csv_content = \"\\n\".join(lines)\n",
    "    columns_labels = pd.read_csv(StringIO(csv_content))\n",
    "    new_columns = list(columns_labels.columns)\n",
    "\n",
    "    # Ler novamente somente os dados e acoplar as colunas adequadamente\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        raw_content = file.read()\n",
    "\n",
    "    lines = raw_content.splitlines()[3:-2]\n",
    "    csv_content = \"\\n\".join(lines)\n",
    "    data_local = pd.read_csv(StringIO(csv_content), header=None)\n",
    "    data_local.drop(data_local.columns[-1], axis=1, inplace=True)\n",
    "    data_local.columns=new_columns\n",
    "    \n",
    "    return data_local\n",
    "\n",
    "def load_and_concatenate(file_paths):\n",
    "    \"\"\"\n",
    "    Given a list of file paths, load each file, organize it using org_lines function,\n",
    "    and concatenate them into a single dataframe.\n",
    "    \"\"\"\n",
    "    return pd.concat([org_lines(file_path) for file_path in file_paths], ignore_index=True)\n",
    "\n",
    "def get_most_recent_data(df_aggregated):\n",
    "    \"\"\"\n",
    "    Given an aggregated dataframe, this function returns the most recent (max JIF) \n",
    "    entry for each unique ISSN or eISSN.\n",
    "    \"\"\"\n",
    "    # Determine the most recent year in the dataframe\n",
    "    most_recent_year = max([int(col.split(' ')[0]) for col in df_aggregated.columns if ' JIF' in col])\n",
    "\n",
    "    # Combining the data based on ISSN and eISSN, taking the max of JIF\n",
    "    df_grouped_by_issn = df_aggregated.groupby('ISSN', as_index=False).apply(lambda x: x.nlargest(1, f'{most_recent_year} JIF')).reset_index(drop=True)\n",
    "    df_grouped_by_eissn = df_aggregated.groupby('eISSN', as_index=False).apply(lambda x: x.nlargest(1, f'{most_recent_year} JIF')).reset_index(drop=True)\n",
    "    \n",
    "    # Combining the two dataframes and dropping any duplicates\n",
    "    df_most_recent = pd.concat([df_grouped_by_issn, df_grouped_by_eissn]).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_most_recent, most_recent_year\n",
    "\n",
    "\n",
    "def standardize_issn_format(issn):\n",
    "    \"\"\"Convert a given ISSN to the format '0000-0000'.\"\"\"\n",
    "    if isinstance(issn, str) and len(issn) == 8 and \"-\" not in issn:\n",
    "        return issn[:4] + '-' + issn[4:]\n",
    "    return issn\n",
    "\n",
    "def format_issn_list(issn_list):\n",
    "    return [f\"{issn[:4]}-{issn[4:]}\" if issn != np.nan else issn for issn in issn_list]\n",
    "\n",
    "def montar_lista_issn(data):\n",
    "    issn_list = data['issn'].to_list()\n",
    "    str_list=''\n",
    "    for n,i in enumerate(issn_list):\n",
    "        issn = standardize_issn_format(str(i))\n",
    "        if n==0:\n",
    "            str_list = issn\n",
    "        elif issn == 'NULL':\n",
    "            pass\n",
    "        elif issn == 'nan':\n",
    "            pass        \n",
    "        else:\n",
    "            str_list = str_list+(', ')+issn\n",
    "    \n",
    "    print(len(str_list.split(', ')),'ISSN extraídos do currículo')\n",
    "    return str_list\n",
    "\n",
    "def populate_missing_data(df_start, df_most_recent, most_recent_year):\n",
    "    \"\"\"\n",
    "    Given a starting dataframe and a dataframe with the most recent data for each ISSN or eISSN,\n",
    "    this function populates the starting dataframe with missing values.\n",
    "    \"\"\"\n",
    "    for index, row in df_start.iterrows():\n",
    "        formatted_issn = standardize_issn_format(row['data-issn'])\n",
    "        if pd.isna(row['impact-factor']) or pd.isna(row['jcr-year']):\n",
    "            # Search in df_most_recent using ISSN or eISSN\n",
    "            matching_row = df_most_recent[(df_most_recent['ISSN'] == formatted_issn) | (df_most_recent['eISSN'] == formatted_issn)]\n",
    "            \n",
    "            if not matching_row.empty:\n",
    "                most_recent_entry = matching_row.iloc[0]\n",
    "                df_start.at[index, 'impact-factor'] = most_recent_entry[f'{most_recent_year} JIF']\n",
    "                df_start.at[index, 'jcr-year'] = f'(JCR {most_recent_year})'\n",
    "                df_start.at[index, 'journal'] = most_recent_entry['Journal name']\n",
    "\n",
    "    return df_start\n",
    "\n",
    "def fill_na_from_df_aggregated(sorted_df, df_aggregated):\n",
    "    df_aggregated['ISSN'] = df_aggregated['ISSN'].apply(standardize_issn_format)\n",
    "    dt_trab = sorted_df['data-issn'].copy()\n",
    "    sorted_df['issn'] = dt_trab.apply(standardize_issn_format)\n",
    "    \n",
    "    df_nan_eissn = df_aggregated[df_aggregated['eISSN'].isna()]\n",
    "    \n",
    "    for idx, row in df_nan_eissn.iterrows():\n",
    "        issn_value = row['ISSN']\n",
    "        mask = (sorted_df['issn'] == issn_value) & sorted_df['impact-factor'].isna()\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            sorted_df.loc[mask, 'impact-factor'] = row['2022 JIF']\n",
    "            sorted_df.loc[mask, 'journal'] = row['Journal name']\n",
    "\n",
    "    return sorted_df\n",
    "\n",
    "def update_sorted_df_with_jcr(sorted_df, df_aggregated):\n",
    "    years = list(range(2018, 2023))\n",
    "    \n",
    "    # Standardize ISSN formats\n",
    "    df_aggregated['ISSN'] = df_aggregated['ISSN'].apply(standardize_issn_format)\n",
    "    df_aggregated['eISSN'] = df_aggregated['eISSN'].apply(standardize_issn_format)\n",
    "    sorted_df['data-issn'] = sorted_df['data-issn'].apply(standardize_issn_format)\n",
    "    \n",
    "    for year in reversed(years):  # Loop from most recent to oldest year\n",
    "        jif_column = f\"{year} JIF\"\n",
    "        \n",
    "        # Merge based on ISSN and eISSN\n",
    "        merged_issn = pd.merge(sorted_df, df_aggregated, left_on='data-issn', right_on='ISSN', how='left')\n",
    "        merged_eissn = pd.merge(sorted_df, df_aggregated, left_on='data-issn', right_on='eISSN', how='left')\n",
    "        \n",
    "        mask_nan = merged_issn[jif_column].notna()\n",
    "        merged = pd.concat([merged_issn[~mask_nan], merged_eissn], ignore_index=True)\n",
    "        \n",
    "        update_mask = sorted_df['impact-factor'].isna()\n",
    "        update_indices = sorted_df[update_mask].index\n",
    "        \n",
    "        for idx in update_indices:\n",
    "            sorted_df.at[idx, 'impact-factor'] = merged.at[idx, jif_column]\n",
    "            jif_value = merged.at[idx, jif_column]\n",
    "            sorted_df.at[idx, 'jcr-year'] = f\"JCR {year}\" if not pd.isna(jif_value) else np.nan\n",
    "    \n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement_jcr(data, df_aggregated):\n",
    "    \"\"\"\n",
    "    Atualiza os valores NaN em 'data' usando as informações de 'df_aggregated'.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame com as colunas [data-issn, original-title, impact-factor, doi, issn].\n",
    "        df_aggregated (pd.DataFrame): DataFrame com informações sobre JIFs de vários anos.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame 'data' atualizado.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lista de anos em ordem decrescente para procurar os JIFs mais recentes\n",
    "    years = sorted([int(col.split()[0]) for col in df_aggregated.columns if re.match(r\"\\d{4} JIF\", col)], reverse=True)\n",
    "    print(years)\n",
    "\n",
    "    for year in years:\n",
    "        column_name = f\"{year} JIF\"\n",
    "        # Usando o método 'map' para atualizar os NaNs\n",
    "        data.loc[data['impact-factor'].isna(), 'impact-factor'] = data['issn'].map(df_aggregated.set_index('ISSN')[column_name])\n",
    "        \n",
    "        # # Se não houver mais NaNs na coluna 'impact-factor', podemos interromper o loop\n",
    "        # if data['impact-factor'].notna().all():\n",
    "        #     break\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções Criar nós secundários de propriedades no Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JcrHandler:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def _consultar_propriedades_jcr(self, tx, name):\n",
    "        query = (\n",
    "            \"MATCH (p:Person {name: $name})\"\n",
    "            \"RETURN p.JCR AS jcr\"\n",
    "        )\n",
    "        result = tx.run(query, name=name)\n",
    "        return [record[\"jcr\"] for record in result]\n",
    "   \n",
    "    ## Versão para usar com criação de nós secundários retorna JSON\n",
    "    def consultar_propriedades_jcr(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            query = (\n",
    "                \"MATCH (p:Person {name: $name})\"\n",
    "                \"RETURN p.JCR AS jcr\"\n",
    "            )\n",
    "            result = session.run(query, name=name)\n",
    "            jcr_data = result.single()[\"jcr\"]\n",
    "            jcr_properties_list = json.loads(jcr_data)\n",
    "            return jcr_properties_list\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_list_to_dict(lst):\n",
    "        \"\"\"\n",
    "        Converts a list into a dictionary with indices as keys.\n",
    "        \n",
    "        Parameters:\n",
    "        - lst: list, input list to be transformed.\n",
    "        \n",
    "        Returns:\n",
    "        - dict: Transformed dictionary.\n",
    "        \"\"\"\n",
    "        return {str(i): item for i, item in enumerate(lst)}\n",
    "    \n",
    "    def create_person_with_jcr(self, name, jcr_properties):\n",
    "        with self._driver.session() as session:\n",
    "            session.write_transaction(self._create_person_with_jcr, name, jcr_properties)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_person_with_jcr(tx, name, jcr_properties):\n",
    "        # Cria o nó Person\n",
    "        person_query = (\n",
    "            \"CREATE (p:Person {name: $name}) \"\n",
    "            \"RETURN p\"\n",
    "        )\n",
    "        person_result = tx.run(person_query, name=name)\n",
    "        person_node = person_result.single()[0]\n",
    "\n",
    "        # Cria os nós secundários para cada valor único de data-issn\n",
    "        data_issn_values = set(prop.get(\"data-issn\") for prop in jcr_properties)\n",
    "        for data_issn in data_issn_values:\n",
    "            if data_issn:\n",
    "                secondary_node_query = (\n",
    "                    \"CREATE (s:SecondaryNode {data_issn: $data_issn}) \"\n",
    "                    \"RETURN s\"\n",
    "                )\n",
    "                tx.run(secondary_node_query, data_issn=data_issn)\n",
    "\n",
    "                # Cria a relação entre o nó Person e o nó secundário\n",
    "                relation_query = (\n",
    "                    \"MATCH (p:Person {name: $name}), (s:SecondaryNode {data_issn: $data_issn}) \"\n",
    "                    \"CREATE (p)-[:HAS_JCR]->(s)\"\n",
    "                )\n",
    "                tx.run(relation_query, name=name, data_issn=data_issn)\n",
    "\n",
    "    def createJournalsNodes(self, name):\n",
    "        # Get JCR properties\n",
    "        jcr_properties = self.consultar_propriedades_jcr(name)\n",
    "\n",
    "        # Convert the serialized JSON strings back into dictionaries\n",
    "        deserialized_jcr_properties = [json.loads(prop) for prop in jcr_properties.values()]\n",
    "\n",
    "        # Inform the user about the total number of JCR property entries\n",
    "        total_entries = len(deserialized_jcr_properties)\n",
    "        print(f\"Read {total_entries} entries from JCR properties of Person '{name}'.\")\n",
    "\n",
    "        # Extract relevant journal properties and their count\n",
    "        journal_counts = Counter(prop.get(\"data-issn\") for prop in deserialized_jcr_properties)\n",
    "        \n",
    "        # Number of unique ISSNs\n",
    "        unique_issns = len(journal_counts)\n",
    "        print(f\"Identified {unique_issns} unique ISSN values.\")\n",
    "\n",
    "        null_count = journal_counts.pop(None, 0)  # Remove None (null) ISSN and get its count\n",
    "        null_count += journal_counts.pop(\"NULL\", 0)  # Also account for \"NULL\" as a string\n",
    "\n",
    "        # Counters for journals\n",
    "        successful_journal_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for data_issn, count in journal_counts.items():\n",
    "                if data_issn and data_issn != \"NULL\":\n",
    "                    representative_entry = next(prop for prop in deserialized_jcr_properties if prop.get(\"data-issn\") == data_issn)\n",
    "                    journal_name = representative_entry.get(\"original_title\")\n",
    "                    fator_impacto = representative_entry.get(\"impact-factor\")\n",
    "                    jcr_year = representative_entry.get(\"jcr-year\")\n",
    "\n",
    "                    # Create or merge the Journal node\n",
    "                    journal_node_query = (\n",
    "                        \"MERGE (j:Revistas {ISSN: $data_issn}) \"\n",
    "                        \"ON CREATE SET j.name = $journal_name, j.FatorImpacto = $impact_factor, j.JCRYear = $jcr_year \"  # Corrected this line\n",
    "                        \"RETURN j\"\n",
    "                    )\n",
    "                    session.run(journal_node_query, data_issn=data_issn, journal_name=journal_name, impact_factor=fator_impacto, jcr_year=jcr_year)  # And this line\n",
    "\n",
    "                    # Create or update the \"PUBLICOU_EM\" relationship\n",
    "                    relation_query = (\n",
    "                        \"MATCH (p:Person {name: $name}), (j:Revistas {ISSN: $data_issn}) \"  # corrected this line\n",
    "                        \"MERGE (p)-[r:PUBLICOU_EM]->(j) \"\n",
    "                        \"ON CREATE SET r.QuantidadePublicações = $count \"\n",
    "                        \"ON MATCH SET r.QuantidadePublicações = r.QuantidadePublicações + $count\"\n",
    "                    )\n",
    "                    session.run(relation_query, name=name, data_issn=data_issn, count=count)\n",
    "                    \n",
    "                    successful_journal_creations += 1\n",
    "                \n",
    "                if null_count:\n",
    "                    # For example, to print the count:\n",
    "                    pass\n",
    "        \n",
    "        # Inform the user about journals\n",
    "        print(f\"{successful_journal_creations} Revistas adicionadas com sucesso.\")\n",
    "        print(f\"{null_count} Revistas não foram criadas por terem valor NULL de ISSN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import urllib.parse\n",
    "import json\n",
    "import re\n",
    "\n",
    "class AdvisorHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(input_data):\n",
    "        if input_data is None:\n",
    "            return None\n",
    "        if isinstance(input_data, dict):\n",
    "            for key, value in input_data.items():\n",
    "                if isinstance(value, dict):  \n",
    "                    input_data[key] = json.dumps(AdvisorHandler.convert_to_primitives(value), ensure_ascii=False)\n",
    "                else:\n",
    "                    input_data[key] = AdvisorHandler.convert_to_primitives(value)\n",
    "            return input_data\n",
    "        elif isinstance(input_data, list):\n",
    "            return [AdvisorHandler.convert_to_primitives(item) for item in input_data]\n",
    "        elif isinstance(input_data, str):\n",
    "            if 'http://' in input_data or 'https://' in input_data:\n",
    "                parts = input_data.split(\" \")\n",
    "                new_parts = [urllib.parse.quote(part) if part.startswith(('http://', 'https://')) else part for part in parts]\n",
    "                return \" \".join(new_parts)\n",
    "            return input_data\n",
    "        elif isinstance(input_data, (int, float, bool)):\n",
    "            return input_data\n",
    "        else:\n",
    "            return str(input_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return AdvisorHandler.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise\n",
    "\n",
    "    def consult_orientacoes(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            query = (\n",
    "                \"MATCH (p:Person {name: $name})\"\n",
    "                \"RETURN p.Orientações AS orientacoes\"\n",
    "            )\n",
    "            result = session.run(query, name=name)\n",
    "            orient_data = result.single()[\"orientacoes\"]\n",
    "            if orient_data is None:\n",
    "                raise ValueError(f\"No data found for 'Orientações' attribute for Person '{name}'\")\n",
    "            orient_properties_list = json.loads(orient_data)\n",
    "            return orient_properties_list\n",
    "\n",
    "    def create_advisor_relations(self, name):\n",
    "        # Get Orientações properties\n",
    "        orient_properties = self.consult_orientacoes(name)\n",
    "\n",
    "        # Convert the serialized JSON strings back into dictionaries\n",
    "        try:\n",
    "            deserialized_orient_properties = self.debug_and_convert(orient_properties)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deserializing Orientações properties: {e}\")\n",
    "            return\n",
    "\n",
    "        # Advisory relationship mapping\n",
    "        advisory_types = {\n",
    "            \"Dissertação de mestrado\": \"ORIENTOU_MESTRADO\",\n",
    "            \"Tese de doutorado\": \"ORIENTOU_DOUTORADO\",\n",
    "            \"Trabalho de conclusão de curso de graduação\": \"ORIENTOU_GRADUAÇÃO\"\n",
    "        }\n",
    "\n",
    "        successful_advisory_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for orientacao_category, advisories in deserialized_orient_properties.items():\n",
    "                if isinstance(advisories, str):\n",
    "                    try:\n",
    "                        advisories = json.loads(advisories)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Failed to deserialize JSON string in 'Orientações' for category '{orientacao_category}': {advisories}\")\n",
    "                        continue\n",
    "\n",
    "                if not isinstance(advisories, dict):\n",
    "                    print(f\"Unexpected data type in 'Orientações' for category '{orientacao_category}': {advisories}\")\n",
    "                    continue\n",
    "\n",
    "                for advisory_type, relationships in advisories.items():\n",
    "                    relation_label = advisory_types.get(advisory_type)\n",
    "                    if not relation_label:\n",
    "                        continue  # skip if the advisory type is not one of the specified ones\n",
    "\n",
    "                    for _, detail in json.loads(relationships).items():\n",
    "                        try:\n",
    "                            student_name = detail.split(\".\")[0]\n",
    "                            title = detail.split(\".\")[1]\n",
    "                            \n",
    "                            # Extract the year from the detail string\n",
    "                            year_match = re.search(r'(\\d{4})', detail)\n",
    "                            year = year_match.group(1) if year_match else None\n",
    "\n",
    "                            # Create or merge the Orientações node\n",
    "                            node_query = (\n",
    "                                \"MERGE (a:Orientações {Title: $title}) \"\n",
    "                                \"ON CREATE SET a.StudentName = $student_name, a.Tipo = $advisory_type, a.Year = $year \"\n",
    "                                \"ON MATCH SET a.Tipo = $advisory_type, a.Year = $year \"  # Ensure the 'Tipo' and 'Year' are always updated\n",
    "                                \"RETURN a\"\n",
    "                            )\n",
    "                            session.run(node_query, title=title, student_name=student_name, advisory_type=advisory_type, year=year)\n",
    "\n",
    "                            # Create or update the advisory relationship\n",
    "                            relation_query = (\n",
    "                                f\"MATCH (p:Person {{name: $name}}), (a:Orientações {{Title: $title}}) \"\n",
    "                                f\"MERGE (p)-[r:{relation_label}]->(a) \"\n",
    "                            )\n",
    "                            session.run(relation_query, name=name, title=title)\n",
    "\n",
    "                            successful_advisory_creations += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing advisory '{detail}': {e}\")\n",
    "\n",
    "        # Inform the user about advisories\n",
    "        print(f\"{successful_advisory_creations} orientações atualizadas com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class AreasHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "        \n",
    "    def consult_areas_atuacao(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"MATCH (p:Person {name: $name}) RETURN p.`Áreas de atuação` as areas_atuacao\", name=name)\n",
    "            record = result.single()\n",
    "            if record:\n",
    "                return record['areas_atuacao']\n",
    "            return None\n",
    "\n",
    "    def debug_and_convert(self, areas_str):\n",
    "        try:\n",
    "            return json.loads(areas_str)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to deserialize JSON string: {areas_str}\")\n",
    "            return None\n",
    "\n",
    "    def extract_subarea(self, area_detail):\n",
    "        # Extract the 'Subárea' content from the area detail\n",
    "        match = re.search(r'Subárea: ([^/]+)', area_detail)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return None\n",
    "\n",
    "    def extract_areas(self, area_detail):\n",
    "        # Extract the 'Grande Área', 'Área', and 'Subárea' contents from the area detail\n",
    "        grande_area_match = re.search(r'Grande área: ([^/]+)', area_detail)\n",
    "        area_match = re.search(r'Área: ([^/]+)', area_detail)\n",
    "        subarea_match = re.search(r'Subárea: ([^/]+)', area_detail)\n",
    "        \n",
    "        grande_area = grande_area_match.group(1).strip() if grande_area_match else None\n",
    "        area = area_match.group(1).strip() if area_match else None\n",
    "        subarea = subarea_match.group(1).strip() if subarea_match else None\n",
    "        \n",
    "        return grande_area, area, subarea\n",
    "\n",
    "    def create_areas_relations(self, name):\n",
    "        # Get 'Áreas de atuação' properties\n",
    "        areas_properties = self.consult_areas_atuacao(name)\n",
    "\n",
    "        # Convert the serialized JSON strings back into dictionaries\n",
    "        try:\n",
    "            deserialized_areas_properties = self.debug_and_convert(areas_properties)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deserializing 'Áreas de atuação' properties: {e}\")\n",
    "            return\n",
    "\n",
    "        successful_areas_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for _, area_detail in deserialized_areas_properties.items():\n",
    "                try:\n",
    "                    # Extracting Grande Área, Área, and Subárea from the details\n",
    "                    match = re.match(r'Grande área: (.*?) / Área: (.*?) / Subárea: (.*?)(?:/Especialidade: (.*?))?\\.?$', area_detail)\n",
    "                    if not match:\n",
    "                        print(f\"Unexpected format for 'Áreas de atuação' detail: {area_detail}\")\n",
    "                        continue\n",
    "                    grande_area, area, subarea = match.groups()[:3]\n",
    "\n",
    "                    # Creating or merging nodes for Subárea, Área, and Grande Área\n",
    "                    session.run(\"MERGE (s:Subárea {name: $subarea})\", subarea=subarea)\n",
    "                    session.run(\"MERGE (a:Área {name: $area})\", area=area)\n",
    "                    session.run(\"MERGE (ga:GrandeÁrea {name: $grande_area})\", grande_area=grande_area)\n",
    "\n",
    "                    # Creating or merging relationships. Using MERGE ensures no duplicate relationships are created.\n",
    "                    session.run(\"MATCH (p:Person {name: $name}), (s:Subárea {name: $subarea}) MERGE (p)-[r:ATUA_EM]->(s)\", name=name, subarea=subarea)\n",
    "                    session.run(\"MATCH (ga:GrandeÁrea {name: $grande_area}), (a:Área {name: $area}) MERGE (ga)-[r:CONTÉM_ÁREA]->(a)\", grande_area=grande_area, area=area)\n",
    "                    session.run(\"MATCH (a:Área {name: $area}), (s:Subárea {name: $subarea}) MERGE (a)-[r:CONTEM_SUBÁREA]->(s)\", area=area, subarea=subarea)\n",
    "\n",
    "                    successful_areas_creations += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing 'Áreas de atuação' detail '{area_detail}': {e}\")\n",
    "\n",
    "            # Inform the user about areas\n",
    "            print(f\"{successful_areas_creations} 'Áreas de atuação' relations successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectsHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def consult_data_by_property(self, name, property_name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(f\"MATCH (p:Person {{name: $name}}) RETURN p.`{property_name}` as data\", name=name)\n",
    "            record = result.single()\n",
    "            return record['data'] if record else None\n",
    "\n",
    "    def create_projects_relations(self, name):\n",
    "        successful_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            # Process 'Atuação Profissional' data\n",
    "            professional_data = self.consult_data_by_property(name, 'Atuação Profissional')\n",
    "            if professional_data:\n",
    "                for institution_name, _ in json.loads(professional_data).items():\n",
    "                    session.run(\"MERGE (i:Instituição {name: $institution_name})\", institution_name=institution_name)\n",
    "                    print(f\"Institution node created/merged for: {institution_name}\")\n",
    "\n",
    "                    session.run(\"MATCH (p:Person {name: $name}), (i:Instituição {name: $institution_name}) MERGE (p)-[:TEM]->(i)\", name=name, institution_name=institution_name)\n",
    "                    print(f\"Relationship established between {name} and {institution_name}.\")\n",
    "\n",
    "            # Process other dynamic nodes\n",
    "            key_labels_to_check = ['Linhas de pesquisa', 'Projetos de pesquisa', 'Projetos de extensão', 'Projetos de desenvolvimento']\n",
    "            for key in key_labels_to_check:\n",
    "                formatted_key = f\"`{key}`\"  # Wrap the key with backticks\n",
    "                project_data = self.consult_data_by_property(name, key)\n",
    "                if project_data:\n",
    "                    for project_time, project_name in json.loads(project_data).items():\n",
    "                        if project_name:  # to avoid empty names\n",
    "                            session.run(f\"MERGE (p:{formatted_key} {{name: $project_name}})\", project_name=project_name)\n",
    "                            print(f\"{key} node created/merged for: {project_name}\")\n",
    "\n",
    "                            session.run(f\"MATCH (a:Person {{name: $name}}), (p:{formatted_key} {{name: $project_name}}) MERGE (a)-[:TEM]->(p)\", name=name, project_name=project_name)\n",
    "                            print(f\"Relationship established between {name} and {project_name} ({key}).\")\n",
    "                            successful_creations += 1\n",
    "                else:\n",
    "                    print(f\"'{key}' data not found for {name}\")\n",
    "\n",
    "        print(f\"{successful_creations} projetos atualizados com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class ArticleHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def fetch_person_productions(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"MATCH (p:Person {name: $name}) RETURN p.Produções as produções\", name=name)\n",
    "            record = result.single()\n",
    "            return record['produções'] if record else None\n",
    "\n",
    "    def extract_article_info(self, input_str):\n",
    "        # Encontre todas as abreviaturas de iniciais em maiúsculas e seus índices\n",
    "        abbreviations = [(match.group(), match.start()) for match in re.finditer(r'\\b[A-Z]\\.', input_str)]\n",
    "\n",
    "        # Encontre a posição da maior ocorrência de abreviaturas de iniciais, se houver\n",
    "        if abbreviations:\n",
    "            max_abbr_position = max(abbreviations, key=lambda x: x[1])\n",
    "\n",
    "            # Encontre a primeira ocorrência de '. ' ou ' . ' após a maior ocorrência de abreviaturas de iniciais\n",
    "            first_separator_candidates = [\n",
    "                input_str.find('. ', max_abbr_position[1] + 3),\n",
    "                input_str.find(' . ', max_abbr_position[1] + 3),\n",
    "                input_str.find('.. ')\n",
    "            ]\n",
    "            first_separator_candidates = [pos for pos in first_separator_candidates if pos != -1]\n",
    "\n",
    "            if first_separator_candidates:\n",
    "                first_separator = min(first_separator_candidates)\n",
    "\n",
    "                # Encontre a primeira ocorrência de '. ' após o primeiro separador\n",
    "                second_separator = input_str.find('. ', first_separator + 2)\n",
    "\n",
    "                # Encontre a primeira ocorrência de ', ' após o segundo separador\n",
    "                third_separator = input_str.find(', ', second_separator + 2)\n",
    "            else:\n",
    "                first_separator = second_separator = third_separator = -1\n",
    "        else:\n",
    "            first_separator = second_separator = third_separator = -1\n",
    "\n",
    "        # Defina o padrão para encontrar \"p.\" e o conteúdo até a próxima vírgula\n",
    "        pages_match = re.search(r' p\\.\\s*(.*?),', input_str)\n",
    "        pages = pages_match.group(1) if pages_match else \"\"\n",
    "\n",
    "        # Defina o padrão para encontrar \"v.\" e o conteúdo até a próxima vírgula\n",
    "        volume_match = re.search(r' v\\.\\s*(.*?),', input_str)\n",
    "        volume = volume_match.group(1) if volume_match else \"\"\n",
    "\n",
    "        # Encontre a primeira ocorrência de um ano de quatro dígitos seguido de ponto final após o terceiro separador\n",
    "        year_match = re.search(r' \\d{4}\\.', input_str[third_separator + 2:])\n",
    "        year = year_match.group().strip('.').strip() if year_match else \"\"\n",
    "\n",
    "        # Extraia os dados com base nas posições dos separadores\n",
    "        authors = input_str[:first_separator].strip()\n",
    "        title = input_str[first_separator + 2:second_separator].strip()\n",
    "        journal = input_str[second_separator + 2:third_separator].strip()\n",
    "\n",
    "        # Verifique se a lista de autores e o título não estão vazios\n",
    "        if not authors or not title:\n",
    "            return None  # Retorna None para indicar falha\n",
    "\n",
    "        # Crie um dicionário com os dados extraídos\n",
    "        article_info = {\n",
    "            \"authors\": authors,\n",
    "            \"title\": title,\n",
    "            \"original_title\": journal,\n",
    "            \"pages\": pages,\n",
    "            \"volume\": volume,\n",
    "            \"year\": year\n",
    "        }\n",
    "\n",
    "        return article_info\n",
    "    \n",
    "    def deserialize_and_create_nodes(self, name):\n",
    "        print(f\"Fetching 'Produções' data for {name}...\")\n",
    "        productions_data = self.fetch_person_productions(name)\n",
    "        \n",
    "        if not productions_data:\n",
    "            print(f\"'Produções' data not found or empty for {name}.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Attempting to deserialize 'Produções' data for {name}...\")\n",
    "        try:\n",
    "            productions_data = json.loads(productions_data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to deserialize 'Produções' data for {name}: {e}\")\n",
    "            return\n",
    "\n",
    "        successful_articles = 0\n",
    "        unsuccessful_articles = []\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            print(f\"Processing 'Produção bibliográfica' for {name}...\")\n",
    "            bibliographic_production = productions_data.get(\"Produção bibliográfica\", {})\n",
    "            \n",
    "            if isinstance(bibliographic_production, str):\n",
    "                print(f\"Attempting to deserialize 'Produção bibliográfica' for {name}...\")\n",
    "                try:\n",
    "                    bibliographic_production = json.loads(bibliographic_production)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Failed to deserialize 'Produção bibliográfica' for {name}: {e}\")\n",
    "                    return\n",
    "\n",
    "            articles = json.loads(bibliographic_production.get(\"Artigos completos publicados em periódicos\", \"{}\"))\n",
    "\n",
    "            for _, article_str in articles.items():\n",
    "                article_details = self.extract_article_info(article_str)\n",
    "\n",
    "                # Vamos imprimir os detalhes de cada artigo e verificar se os autores estão presentes.\n",
    "                print(f\"Original Article: {article_str}\")\n",
    "                print(f\"Extracted Details: {article_details}\")\n",
    "\n",
    "                if article_details:\n",
    "                    article_details[\"title\"] = article_details[\"title\"].strip()\n",
    "                    article_details[\"original_title\"] = article_details[\"original_title\"].strip()\n",
    "\n",
    "                    session.run(f\"MERGE (a:Artigo {{title: $title}}) SET a += $details\", title=article_details[\"title\"], details=article_details)\n",
    "                    session.run(f\"MATCH (p:Person {{name: $name}}), (a:Artigo {{title: $title}}) MERGE (p)-[:PUBLICOU]->(a)\", name=name, title=article_details[\"title\"])\n",
    "                    successful_articles += 1\n",
    "                else:\n",
    "                    unsuccessful_articles.append(article_str)\n",
    "\n",
    "        print(f\"Processed {successful_articles} articles successfully for {name}.\")\n",
    "\n",
    "        if unsuccessful_articles:\n",
    "            print(\"Failed to process the following articles:\")\n",
    "            for article in unsuccessful_articles:\n",
    "                print(article)\n",
    "\n",
    "    def process_articles(self, name):\n",
    "        self.deserialize_and_create_nodes(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "class DataRemovalHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        \"\"\"\n",
    "        Inicializa a classe DataRemovalHandler com informações de conexão ao banco de dados Neo4j.\n",
    "\n",
    "        Parâmetros:\n",
    "        - uri (str): URI de conexão ao Neo4j.\n",
    "        - user (str): Nome de usuário para autenticação.\n",
    "        - password (str): Senha para autenticação.\n",
    "        \"\"\"\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Fecha a conexão com o banco de dados Neo4j.\n",
    "        \"\"\"\n",
    "        self._driver.close()\n",
    "\n",
    "    def delete_nodes_by_label(self, label):\n",
    "        \"\"\"\n",
    "        Deleta todos os nós associados a um label específico no Neo4j.\n",
    "\n",
    "        Parâmetro:\n",
    "        - label (str): O label dos nós a serem deletados.\n",
    "\n",
    "        Retorna:\n",
    "        - int: O número de nós deletados.\n",
    "        \"\"\"\n",
    "        with self._driver.session() as session:\n",
    "            # Esta consulta combina com todos os nós do label especificado e os deleta\n",
    "            result = session.run(f\"MATCH (n:{label}) DETACH DELETE n RETURN count(n) as deleted_count\")\n",
    "            deleted_count = result.single()[\"deleted_count\"]\n",
    "            return deleted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para extrair Lista de Currículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  #seletorcss=\"div.paginacao:nth-child(4) > a:nth-child(2)\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas\n",
    "\n",
    "def achar_busca(browser, delay):\n",
    "    '''\n",
    "    Função para clicar no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = browser.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "               # expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "               # logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)\n",
    "\n",
    "def preencher_busca(browser, delay, NOME):\n",
    "    '''\n",
    "    Função para passar o nome para campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)\n",
    "\n",
    "def definir_filtros(browser, delay, mestres=True, assunto=False):\n",
    "    '''\n",
    "    Clica nos check-boxes para definir os filtros de buscas\n",
    "    Para buscar por Assuntos usar parâmetro True, caso omitido fará busca por Nome por default\n",
    "    '''\n",
    "    from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.common.by import By\n",
    "    import traceback   \n",
    "    \n",
    "    ## Aguardar carregar e clicar em checkbox de Assunto\n",
    "    try:\n",
    "        if mestres == True:\n",
    "            limite=2\n",
    "            ## Aguardar opção dropdown ser carregada e clicar em sua checkbox\n",
    "            css_buscar_demais = '#buscarDemais'\n",
    "            checkbox_buscar_demais = browser.find_element(By.CSS_SELECTOR, css_buscar_demais)\n",
    "            \n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, css_buscar_demais))),\n",
    "                   wait_ms=150,\n",
    "                   limit=limite, \n",
    "                   on_exhaust=(f'Problema clicar em {checkbox_buscar_demais}, {limite} tentativas sem sucesso.'))\n",
    "            \n",
    "            time.sleep(1)\n",
    "            checkbox_buscar_demais.click()\n",
    "            print(f'Clique efetuado em {checkbox_buscar_demais}')\n",
    "\n",
    "        # css_estrangeiros  = '#buscarEstrangeiros'\n",
    "\n",
    "        if assunto == True:\n",
    "            ## Implementar número de retentativas para casos de conexão muito instável\n",
    "            limite=5\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#buscaAssunto\"))).click(),\n",
    "                #    expected_ex_type=ZeroDivisionError, \n",
    "                   wait_ms=200,\n",
    "                   limit=limite, \n",
    "                #    logger=logger, \n",
    "                   on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "        \n",
    "            ## Aguardar opção Atuação Profissional ser carregada e clicar em sua checkbox\n",
    "            xpath_atuacaoprofissional = \".//*[contains(text(), 'Atuação profissional')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_atuacaoprofissional))).click()\n",
    "\n",
    "            ## Aguardar opção Ciências da Saúde ser carregada e clicar em sua checkbox\n",
    "            xpath_cienciassaude = \".//*[contains(text(), 'Ciências da Saúde')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_cienciassaude))).click()\n",
    "            #browser.find_element_by_xpath(xpath_cienciassaude).click()        \n",
    "\n",
    "            ## Aguardar opção Enfermagem ser carregada e clicar em sua checkbox\n",
    "            xpath_enfermagem    = \".//*[contains(text(), 'Enfermagem')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_enfermagem))).click()\n",
    "            #browser.find_element_by_xpath(xpath_enfermagem).click()\n",
    "            aplicar_link  = browser.find_element(By.LINK_TEXT, 'Aplicar')\n",
    "            aplicar_link.click()\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f'Erro na função definir_filtros()')\n",
    "        print(e)\n",
    "        # traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "        # print(e, traceback_str) \n",
    "\n",
    "def procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite):\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(browser)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(browser)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(browser)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(browser, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', browser\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, browser\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # browser.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = browser.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', browser\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função procurar_vinculos()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, browser\n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, browser\n",
    "\n",
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "\n",
    "        import re\n",
    "\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        import re\n",
    "\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "# Extração com BeaultifulSoap\n",
    "def parse_header(soup):\n",
    "    title = soup.title.string if soup.title else \"Unknown\"\n",
    "    meta_keywords = soup.find(\"meta\", {\"http-equiv\": \"keywords\"})[\"content\"] if soup.find(\"meta\", {\"http-equiv\": \"keywords\"}) else \"Unknown\"\n",
    "    return {\"title\": title, \"meta_keywords\": meta_keywords}\n",
    "\n",
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "\n",
    "\n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    for elem in h1_elements:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # Substitua pelo seu endpoint e credenciais\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'], meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    return header_node\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Exemplo de entrada HTML\n",
    "#     html_content = '''\n",
    "#     <html lang=\"pt-BR\">\n",
    "#         <head>\n",
    "#             <title>Currículo do Sistema de Currículos Lattes (Raimir Holanda Filho)</title>\n",
    "#             <meta content=\"currículo,curriculo,curriculum,cv,vitae,lattes,produção,producao,científica,cientifica,Brasil\" http-equiv=\"keywords\">\n",
    "#         </head>\n",
    "#         <body>\n",
    "#             <h1 tabindex=\"0\">Educação</h1>\n",
    "#             <h1 tabindex=\"0\">Publicações</h1>\n",
    "#             <!-- ... -->\n",
    "#         </body>\n",
    "#     </html>\n",
    "#     '''\n",
    "#     # Inicialização do BeautifulSoup\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     # Extração e Persistência do cabeçalho\n",
    "#     header_data = parse_header(soup)\n",
    "#     header_node = persist_to_neo4j(header_data)\n",
    "    \n",
    "#     # Extração e Persistência dos elementos H1\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     parse_h1_elements(soup, header_node, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_personinfo(soup):\n",
    "    # Step 1: Identify Node Name\n",
    "    properties = {}\n",
    "\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    # div_informacoesautor = soup.find(\"ul\", {\"class\": \"informacoes-autor\"})\n",
    "    if node_name:\n",
    "        properties['name'] = node_name\n",
    "    for li in soup.find_all('li'):\n",
    "        text_content = li.text  # Extract the text content of the 'li' element\n",
    "        span_class = li.span['class'][0] if li.span else 'Unknown'  # Extract the class of the span within the 'li', if present\n",
    "\n",
    "        # Populate dictionary based on span class\n",
    "        if span_class == 'img_link':\n",
    "            properties['CV_URL'] = text_content.replace('Endereço para acessar este CV: ', '').strip()\n",
    "        elif span_class == 'img_identy':\n",
    "            properties['ID_Lattes'] = li.find('span', {'style': 'font-weight: bold; color: #326C99;'}).text.strip() if li.find('span', {'style': 'font-weight: bold; color: #326C99;'}) else 'Unknown'\n",
    "        elif span_class == 'img_cert':\n",
    "            date_str = text_content.replace('Última atualização do currículo em ', '').strip()\n",
    "            \n",
    "            # Assuming that the date string is in the format 'DD/MM/YYYY', parse it into a datetime object\n",
    "            # Modify the format string according to the actual format of date_str\n",
    "            parsed_date = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "\n",
    "            # Convert the datetime object to ISO 8601 format (YYYY-MM-DD)\n",
    "            iso_date = parsed_date.strftime('%Y-%m-%d')\n",
    "            properties['Last_Updated'] = iso_date\n",
    "\n",
    "    return properties\n",
    "\n",
    "\n",
    "def extract_infopessoa(soup):\n",
    "    # Localiza a seção com a classe 'infpessoa'\n",
    "    section = soup.find('div', class_='infpessoa')\n",
    "\n",
    "    # Inicializa um dicionário para armazenar os dados extraídos\n",
    "    extracted_data = {}\n",
    "\n",
    "    # Extrai e armazena o nome\n",
    "    name = section.find('h2', class_='nome').text.strip() if section.find('h2', class_='nome') else None\n",
    "    extracted_data['Nome'] = re.sub(r'\\s+', ' ', name) if name else None\n",
    "\n",
    "    # Extrai e armazena o título ou posição\n",
    "    title = section.find('span', class_='texto').text.strip() if section.find('span', class_='texto') else None\n",
    "    extracted_data['Bolsa'] = re.sub(r'\\s+', ' ', title) if title else None\n",
    "\n",
    "    # Extrai e armazena as informações adicionais\n",
    "    info_list = section.find('ul', class_='informacoes-autor')\n",
    "    if info_list:\n",
    "        for li in info_list.find_all('li'):\n",
    "            text = re.sub(r'\\s+', ' ', li.text.strip())\n",
    "            if 'Endereço para acessar este CV:' in text:\n",
    "                extracted_data['Endereço para acessar este CV'] = text.replace('Endereço para acessar este CV:', '').strip()\n",
    "            elif 'ID Lattes:' in text:\n",
    "                extracted_data['ID Lattes'] = text.replace('ID Lattes:', '').strip()\n",
    "            elif 'Última atualização do currículo em' in text:\n",
    "                extracted_data['Última atualização do currículo em'] = text.replace('Última atualização do currículo em', '').strip()\n",
    "\n",
    "    extracted_data['Resumo'] = soup.find('p', class_='resumo').text.strip()\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def extract_academic(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados da página do Lattes, nas divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Bs4 object (soup): Objeto dod Beaultiful Soap com o conteúdo bruto da página HTML.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "    \"\"\"\n",
    "    # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # Localiza todas as divs com a classe 'title-wrapper'\n",
    "    divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "    for div_key in divs_key:\n",
    "        # Extrai o conteúdo da tag para formar a chave do dicionário\n",
    "        find_div = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        if find_div:\n",
    "            key = find_div.text.strip('\\n')\n",
    "        \n",
    "        # Encontra a div que segue imediatamente para o valor\n",
    "        div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "        # Extrai o conteúdo da div para o valor\n",
    "        value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "        # Armazena no dicionário se ambas chave e valor existirem\n",
    "        if key and value:\n",
    "            extracted_data[key] = value\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def mount_articles(extracted_content):\n",
    "    \"\"\"\n",
    "    Organiza os dados extraídos pelo web scraping em um DataFrame do pandas, melhorando a detecção de '\\n'.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - extracted_content (dict): Um dicionário contendo os conteúdos extraídos organizados por classe.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um DataFrame do pandas contendo os dados organizados.\n",
    "    \"\"\"\n",
    "    # Localiza a lista de artigos completos\n",
    "    artigos_completos = extracted_content.get('artigo-completo', [])\n",
    "    print(f'Total de artigos detectados: {len(artigos_completos)}')\n",
    "    \n",
    "    # Concatena todos os artigos em uma única string\n",
    "    artigos_concatenados = \" \".join(artigos_completos)\n",
    "    \n",
    "    # Remove múltiplos espaços e substitui por um único espaço\n",
    "    artigos_concatenados = re.sub(r'\\s+', ' ', artigos_concatenados)\n",
    "    \n",
    "    # Divide os artigos com base em um padrão específico (exemplo: números seguidos de pontos)\n",
    "    artigos_divididos = re.split(r'\\s\\d+\\.\\s', artigos_concatenados)\n",
    "    \n",
    "    # Remove entradas vazias\n",
    "    artigos_divididos = [artigo.strip() for artigo in artigos_divididos if artigo.strip()]\n",
    "    \n",
    "    # Lista para armazenar os registros para o DataFrame\n",
    "    records = []\n",
    "    ordens = []\n",
    "    \n",
    "    for artigo in artigos_divididos:\n",
    "        # Encontra o primeiro ano mencionado no artigo\n",
    "        match_ano = re.search(r'20\\d{2}', artigo)\n",
    "        \n",
    "        if match_ano:\n",
    "            ano = match_ano.group(0)\n",
    "            indice, resto = artigo.split(ano, 1)\n",
    "            \n",
    "            # Armazena o registro em formato de dicionário\n",
    "            record = {\n",
    "                'Índice': indice.split('. ')[0],\n",
    "                'Ano': ano,\n",
    "                'Título e Autores': resto.strip()\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Cria um DataFrame do pandas com os registros\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_research_project(soup):\n",
    "    project_list = []\n",
    "    projects_section = soup.find('h1', string='Projetos de pesquisa')\n",
    "    periods=[]\n",
    "    titles=[]\n",
    "    descriptions=[]\n",
    "    if projects_section:\n",
    "        project_divs = projects_section.find_all_next('div', {'class': 'layout-cell layout-cell-3 text-align-right'})\n",
    "        \n",
    "        for div in project_divs:\n",
    "            project_dict = {}\n",
    "            period_div = div.find('b')\n",
    "            if period_div:\n",
    "                periods.append(period_div.text.strip())\n",
    "            \n",
    "            title_div_container = div.find_next_sibling('div')\n",
    "            if title_div_container:\n",
    "                title_div = title_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                if title_div:\n",
    "                    titles.append(title_div.text.strip())\n",
    "            \n",
    "            # Locate the div that contains the project description\n",
    "            parent_div = div.find_parent('div')\n",
    "            if parent_div:\n",
    "                description_div_container = parent_div.find_next_sibling('div')\n",
    "                if description_div_container:\n",
    "                    description_div_container = description_div_container.find_next_sibling('div')\n",
    "                    if description_div_container:\n",
    "                        description_div = description_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                        if description_div:\n",
    "                            full_text = description_div.text\n",
    "                            description_start_index = full_text.find('Descrição:')\n",
    "                            if description_start_index != -1:\n",
    "                                descriptions.append(full_text[description_start_index + len('Descrição:'):].strip())\n",
    "\n",
    "    df =pd.DataFrame({\n",
    "        'PERIODO': pd.Series(periods),\n",
    "        'TITULO': pd.Series(titles),\n",
    "        'DESCRICAO': pd.Series(descriptions),\n",
    "            })                \n",
    "    \n",
    "    descricoes = df[df['PERIODO']==\"\"]['TITULO'].values\n",
    "    df = df[df['PERIODO']!=\"\"]\n",
    "    df = df[:len(descricoes)]\n",
    "    df['DESCRICAO']=descricoes\n",
    "    df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_lista(lista, mestres=True, assunto=False):\n",
    "    sucesso=[]\n",
    "    falhas=[]\n",
    "    duvidas=[]\n",
    "    tipo_erro=[]\n",
    "    curriculos=[]\n",
    "    rotulos=[]\n",
    "    conteudos=[]\n",
    "\n",
    "    delay=10\n",
    "    limite=3\n",
    "    instituicao = 'Fundação Oswaldo Cruz'\n",
    "    unidade     = 'Fiocruz Ceará'\n",
    "    termo       = 'Ministerio da Saude'\n",
    "\n",
    "    t0 = time.time()\n",
    "    browser = connect_driver(caminho)\n",
    "    for NOME in lista:\n",
    "        try:\n",
    "            preencher_busca(browser, delay, NOME)\n",
    "            elemento_achado, nome_falha, duvida, erro, browser = procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite)\n",
    "            link_nome     = achar_busca(browser, delay)\n",
    "            window_before = browser.current_window_handle\n",
    "            \n",
    "            if str(elemento_achado) == 'nan':\n",
    "                print('Vínculo não encontrado, passando ao próximo nome...')\n",
    "                falhas.append(nome_falha)\n",
    "                duvidas.append(duvida)\n",
    "                tipo_erro.append(erro)\n",
    "                # print(nome_falha)\n",
    "                # print(erro)\n",
    "                # clear_output(wait=True)\n",
    "                raise Exception\n",
    "            print('Vínculo encontrado no currículo de nome:',elemento_achado.text)\n",
    "\n",
    "            ## Clicar no botão abrir currículo e mudar de aba\n",
    "            try:\n",
    "                ## Aguarda, encontra, clica em buscar nome\n",
    "                link_nome    = achar_busca(browser, delay)\n",
    "                nome_buscado = []\n",
    "                nome_achado  = []\n",
    "                nome_buscado.append(NOME)\n",
    "                \n",
    "                if link_nome.text == None:\n",
    "                    xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "                    # 'Stale file handle'\n",
    "                    print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "                    retry(WebDriverWait(browser, delay).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "                try:\n",
    "                    ActionChains(browser).click(link_nome).perform()\n",
    "                    nome_achado.append(link_nome.text)\n",
    "                except:\n",
    "                    print(f'Currículo não encontrado para: {NOME}.')\n",
    "                    return\n",
    "                \n",
    "                retry(WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "                \n",
    "                # Clicar botão para abrir o currículo\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                time.sleep(0.2)\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "\n",
    "                # Pega o código fonte da página\n",
    "                page_source = browser.page_source\n",
    "\n",
    "                # Usa BeautifulSoup para analisar\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                # Extração e Persistência do cabeçalho\n",
    "                dict_header = parse_header(soup)\n",
    "                header_node = persist_to_neo4j(dict_header)\n",
    "                \n",
    "                # Extração e Persistência dos elementos H1\n",
    "                graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "                parse_h1_elements(soup, header_node, graph)\n",
    "                graph, cv_node, properties = parse_personinfo(soup)                \n",
    "\n",
    "            except Exception as e:\n",
    "                print('Erro',e)\n",
    "                print('Tentando nova requisição ao servidor')\n",
    "                time.sleep(1)\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "                time.sleep(1)\n",
    "\n",
    "            sucesso.append(NOME)\n",
    "\n",
    "        except:\n",
    "            print(f'Currículo não encontrado: {NOME}')\n",
    "            browser.back()\n",
    "            continue\n",
    "\n",
    "    df_dados =pd.DataFrame({\n",
    "        'CURRICULO': pd.Series(curriculos),\n",
    "        'ROTULOS': pd.Series(rotulos),\n",
    "        'CONTEUDOS': pd.Series(conteudos),\n",
    "            })\n",
    "    \n",
    "    t6=time.time()\n",
    "    print('='*95)\n",
    "    # print(f' {len(sucesso)} currículos extraídos com sucesso')\n",
    "    print(f' Tempo total para extrair {len(conteudos)} seções dos currículos: {tempo(t0,t6)}')\n",
    "    # print('='*95)\n",
    "    browser.quit()\n",
    "    \n",
    "    return df_dados, sucesso  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função alternativa para extrair todas seções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    title_elements = soup.find_all('h1')\n",
    "    for title_elem in title_elements:\n",
    "        title_text = title_elem.text.strip()\n",
    "        data_dict = {}\n",
    "        parent_div = title_elem.find_parent('div', {'class': 'title-wrapper'})\n",
    "        if parent_div:\n",
    "            data_cells = parent_div.find_all('div', {'class': 'layout-cell layout-cell-12 data-cell'})\n",
    "\n",
    "            higher_order_key = None  # Inicializa variável para armazenar a chave de ordem superior\n",
    "            data_list = []  # Inicialize lista para armazenar entradas de índices de dataframe\n",
    "            \n",
    "            parag_elements = parent_div.find_all('p')\n",
    "            if parag_elements:\n",
    "                for idx, elem in enumerate(parag_elements):\n",
    "                    class_name = elem.get('class', [None])[0]  # Assumes only one class; otherwise, join them into a single string\n",
    "                    higher_order_key = class_name\n",
    "                    data_list = []  # Redefine a lista para a nova chave de ordem superior\n",
    "                    data_dict[higher_order_key] = data_list  # Cria nova lista para esta chave de ordem superior\n",
    "                    data_entry = {'rotulos': class_name, 'conteudos': elem.text}\n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "\n",
    "            for cell in data_cells:\n",
    "                inst_back_elem = cell.find('div', {'class': 'inst_back'})\n",
    "                kdict_elements = cell.find_all('b')\n",
    "                # print(len(kdict_elements))\n",
    "\n",
    "                if inst_back_elem:\n",
    "                    higher_order_key = inst_back_elem.text.strip()\n",
    "                    data_list = []  # Redefine a lista para a nova chave de ordem superior\n",
    "                    data_dict[higher_order_key] = data_list  # Cria nova lista para esta chave de ordem superior\n",
    "\n",
    "                index_elems   = cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                details_elems = cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "                \n",
    "                for key_elem, details_elem in zip(index_elems, details_elems):\n",
    "                    key_text     = key_elem.text.strip() if key_elem else None\n",
    "                    details_text = details_elem.text.strip() if details_elem else None\n",
    "                    \n",
    "                    data_entry = {'rotulos': key_text, 'conteudos': details_text}\n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "                \n",
    "            if higher_order_key is None:\n",
    "                # Se nenhuma chave de ordem superior for encontrada, associa a lista de dados diretamente ao título\n",
    "                result_dict[title_text] = data_list\n",
    "            else:\n",
    "                # Caso contrário, associa o data_dict contendo chaves de ordem superior ao título\n",
    "                result_dict[title_text] = data_dict\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def generate_dataframe_and_neo4j_dict(data_dict):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame and a dictionary for Neo4j persistence, incorporating section and subsection names.\n",
    "\n",
    "    Parameters:\n",
    "        data_dict (dict): A nested dictionary containing section and subsection data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame aggregating all sections and subsections, with additional columns specifying their names.\n",
    "        dict: A dictionary intended for Neo4j persistence, formatted according to the Neo4j data model.\n",
    "    \"\"\"\n",
    "\n",
    "    all_frames = []  # List to store DataFrames corresponding to each section and subsection\n",
    "    neo4j_dict = {}  # Dictionary for Neo4j persistence\n",
    "\n",
    "    for section, items in data_dict.items():\n",
    "        if section:  # Exclude empty sections\n",
    "            neo4j_dict[section] = {}\n",
    "            if isinstance(items, list):  # If items is a list, convert to DataFrame\n",
    "                df = pd.DataFrame(items)\n",
    "                df['Section'] = section  # Append a column for the section name\n",
    "                all_frames.append(df)\n",
    "                neo4j_dict[section] = items  # For list items, add them as they are\n",
    "\n",
    "            elif isinstance(items, dict):  # If items is a dictionary, explore subsections\n",
    "                for subsection, subitems in items.items():\n",
    "                    if subitems:  # Exclude empty subsections\n",
    "                        df = pd.DataFrame(subitems)\n",
    "                        df['Subsection'] = subsection  # Append a column for the subsection name\n",
    "                        df['Section'] = section  # Append a column for the section name\n",
    "                        all_frames.append(df)\n",
    "                        neo4j_dict[section][subsection] = subitems  # Store subsection data\n",
    "\n",
    "    # Concatenate all DataFrames vertically to form one unified DataFrame\n",
    "    dataframe = pd.concat(all_frames, ignore_index=True)\n",
    "\n",
    "    return dataframe, neo4j_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções alternativas diversas para interagir com Neo4j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database credentials and URI\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    try:\n",
    "        graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "    except:\n",
    "        print('Erro ao conectar ao Neo4j')\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'].split('(')[1].strip(')'), meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    print(type(header_node))\n",
    "\n",
    "    return header_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher, Relationship\n",
    "from typing import Dict, List\n",
    "\n",
    "def create_or_update_publications(graph: Graph, publications_dict: Dict[str, Dict[str, List[Dict[str, str]]]]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node and its associated publications in the Neo4j database.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        publications_dict (Dict[str, Dict[str, List[Dict[str, str]]]]): Dictionary containing publication information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    publications_created = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Create a NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "\n",
    "    # Search for existing researcher node by name\n",
    "    existing_node = matcher.match(\"Researcher\", name=publications_dict['Node Name']).first()\n",
    "\n",
    "    if not existing_node:\n",
    "        raise ValueError(\"Researcher node must exist before adding publications.\")\n",
    "\n",
    "    # Process publications\n",
    "    for publication in publications_dict['Properties']['Produções']:\n",
    "        # Check if a similar publication already exists\n",
    "        existing_pub_node = matcher.match(\"Publication\", doi=publication['doi']).first()\n",
    "\n",
    "        if not existing_pub_node:\n",
    "            pub_node = Node(\"Publication\", **publication)\n",
    "            graph.create(pub_node)\n",
    "            publications_created += 1\n",
    "        else:\n",
    "            for key, value in publication.items():\n",
    "                if key not in existing_pub_node or existing_pub_node[key] != value:\n",
    "                    existing_pub_node[key] = value\n",
    "                    properties_updated += 1\n",
    "            pub_node = existing_pub_node\n",
    "            graph.push(pub_node)\n",
    "\n",
    "        # Create or update relationship between researcher and publication\n",
    "        rel = Relationship(existing_node, \"PUBLICOU\", pub_node)\n",
    "        graph.merge(rel)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Publications created: {publications_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "from typing import Dict\n",
    "\n",
    "def create_researcher_node(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    if 'nome' in researcher_dict and 'resumo' in researcher_dict:\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['nome'][0] if isinstance(researcher_dict['nome'], list) else researcher_dict['nome']\n",
    "        summary = researcher_dict['resumo'][0] if isinstance(researcher_dict['resumo'], list) else researcher_dict['resumo']\n",
    "        \n",
    "        # Create a new node of type 'Researcher'\n",
    "        researcher_node = Node(\"Researcher\", name=name, resumo=summary)\n",
    "        \n",
    "        # Add the node to the Neo4j database\n",
    "        graph.create(researcher_node)\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'nome' and 'resumo'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given researcher dictionary\n",
    "# researcher_dict = {'nome': ['John Doe'], 'resumo': ['This is a summary.']}\n",
    "\n",
    "# Create a Researcher node in Neo4j based on the dictionary\n",
    "# create_researcher_node(graph, researcher_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any\n",
    "\n",
    "def generate_lattes_dict(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a dictionary from a BeautifulSoup object to be persisted in Neo4j.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML/XML data.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the relevant information for Neo4j persistence.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to hold the extracted data\n",
    "    lattes_data = {}\n",
    "    \n",
    "    # Extracting researcher's name as an example\n",
    "    name_section = soup.find('div', {'id': 'name-section'})\n",
    "    if name_section:\n",
    "        lattes_data['researcher_name'] = name_section.text.strip()\n",
    "    \n",
    "    # Extracting list of publications as an example\n",
    "    publications = []\n",
    "    for pub in soup.find_all('div', {'class': 'publication'}):\n",
    "        publication_data = {}\n",
    "        title = pub.find('span', {'class': 'title'})\n",
    "        authors = pub.find('span', {'class': 'authors'})\n",
    "        \n",
    "        if title:\n",
    "            publication_data['title'] = title.text.strip()\n",
    "        \n",
    "        if authors:\n",
    "            publication_data['authors'] = authors.text.strip().split(',')\n",
    "        \n",
    "        publications.append(publication_data)\n",
    "    \n",
    "    lattes_data['publications'] = publications\n",
    "    \n",
    "    # Additional extractions can be performed as per the requirements\n",
    "    \n",
    "    return lattes_data\n",
    "\n",
    "# Example usage (Assuming 'some_html_content' contains the HTML content)\n",
    "# soup = BeautifulSoup(some_html_content, 'html.parser')\n",
    "# lattes_dict = generate_lattes_dict(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "from typing import Dict\n",
    "\n",
    "def create_researcher_node_from_dict(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    required_keys = ['NOME', 'IDLATTES', 'ATUALIZAÇÃO']\n",
    "    if all(key in researcher_dict for key in required_keys):\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['NOME']\n",
    "        id_lattes = researcher_dict['IDLATTES']\n",
    "        last_updated = researcher_dict['ATUALIZAÇÃO']\n",
    "        \n",
    "        # Create a new node of type 'Researcher'\n",
    "        researcher_node = Node(\"Researcher\", name=name, id_lattes=id_lattes, last_updated=last_updated)\n",
    "        \n",
    "        # Add the node to the Neo4j database\n",
    "        graph.create(researcher_node)\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'NOME', 'IDLATTES', and 'ATUALIZAÇÃO'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create a Researcher node in Neo4j based on the dictionary\n",
    "# create_researcher_node_from_dict(graph, dict_pessoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher\n",
    "from typing import Dict\n",
    "\n",
    "def create_or_update_researcher_node(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    nodes_created = 0\n",
    "    nodes_updated = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    required_keys = ['NOME', 'IDLATTES', 'ATUALIZAÇÃO']\n",
    "    if all(key in researcher_dict for key in required_keys):\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['NOME']\n",
    "        id_lattes = researcher_dict['IDLATTES']\n",
    "        last_updated = researcher_dict['ATUALIZAÇÃO']\n",
    "\n",
    "        # Create a NodeMatcher object\n",
    "        matcher = NodeMatcher(graph)\n",
    "\n",
    "        # Look for existing nodes with the same name\n",
    "        existing_node = matcher.match(\"Researcher\", name=name).first()\n",
    "\n",
    "        if existing_node:\n",
    "            # Update properties of existing node\n",
    "            for key, value in researcher_dict.items():\n",
    "                if key.lower() not in existing_node or existing_node[key.lower()] != value:\n",
    "                    existing_node[key.lower()] = value\n",
    "                    properties_updated += 1\n",
    "\n",
    "            # Push the changes to the database\n",
    "            graph.push(existing_node)\n",
    "            nodes_updated += 1\n",
    "\n",
    "        else:\n",
    "            # Create a new node of type 'Researcher'\n",
    "            researcher_node = Node(\"Researcher\", name=name, id_lattes=id_lattes, last_updated=last_updated)\n",
    "            # Add the node to the Neo4j database\n",
    "            graph.create(researcher_node)\n",
    "            nodes_created += 1\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"Nodes created: {nodes_created}\")\n",
    "        print(f\"Nodes updated: {nodes_updated}\")\n",
    "        print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'NOME', 'IDLATTES', and 'ATUALIZAÇÃO'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given researcher dictionary\n",
    "# researcher_dict = {'NOME': 'John Doe', 'IDLATTES': '0000000000000000', 'ATUALIZAÇÃO': '31/12/2023'}\n",
    "\n",
    "# Create or update a Researcher node in Neo4j based on the dictionary\n",
    "# create_or_update_researcher_node(graph, dict_pessoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher, Relationship\n",
    "from typing import Dict\n",
    "\n",
    "def create_or_update_professional_links(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node and its professional links in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's professional information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    relationships_created = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Create a NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "\n",
    "    # Look for existing nodes with the same name\n",
    "    existing_node = matcher.match(\"Researcher\", name=researcher_dict.get('Nome')).first()\n",
    "\n",
    "    if not existing_node:\n",
    "        raise ValueError(\"Researcher node must exist before adding professional links.\")\n",
    "\n",
    "    # Process professional affiliations and activities\n",
    "    for key, value in researcher_dict.items():\n",
    "        if key not in ['Nome', 'Endereço Profissional']:\n",
    "            # Create or find the organization/affiliation node\n",
    "            org_node = matcher.match(\"Organization\", name=key).first()\n",
    "            if not org_node:\n",
    "                org_node = Node(\"Organization\", name=key)\n",
    "                graph.create(org_node)\n",
    "\n",
    "            # Create or update the relationship\n",
    "            rel_type = \"AFFILIATED_WITH\" if 'Atual' in value else \"HAS_BEEN_AFFILIATED_WITH\"\n",
    "            rel = Relationship(existing_node, rel_type, org_node, details=value)\n",
    "\n",
    "            # Check if a similar relationship already exists\n",
    "            existing_rel = None\n",
    "            for r in graph.match((existing_node, org_node), r_type=rel_type):\n",
    "                if r['details'] == value:\n",
    "                    existing_rel = r\n",
    "                    break\n",
    "\n",
    "            # Create new or update existing relationship\n",
    "            if not existing_rel:\n",
    "                graph.create(rel)\n",
    "                relationships_created += 1\n",
    "            else:\n",
    "                for property_name, property_value in rel.items():\n",
    "                    if property_name not in existing_rel or existing_rel[property_name] != property_value:\n",
    "                        existing_rel[property_name] = property_value\n",
    "                        properties_updated += 1\n",
    "                graph.push(existing_rel)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Relationships created: {relationships_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create or update professional links in Neo4j based on the dictionary\n",
    "# create_or_update_professional_links(graph, dict_vinculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "\n",
    "def persist_journals_from_xlsx(graph: Graph, xlsx_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Persist journal information into a Neo4j database from an Excel (.xlsx) file.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        xlsx_path (str): The path to the Excel (.xlsx) file containing the journal information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize counters for tracking\n",
    "    nodes_created = 0\n",
    "    properties_updated = 0\n",
    "    \n",
    "    # Create NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "    \n",
    "    # Read the Excel file into a pandas DataFrame\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract journal properties\n",
    "        properties = {'ISSN': row['ISSN'],\n",
    "                      'Título': row['Título'],\n",
    "                      'Área de Avaliação': row['Área de Avaliação'],\n",
    "                      'Estrato': row['Estrato']}\n",
    "        \n",
    "        # Check if a node with the same ISSN already exists\n",
    "        existing_node = matcher.match(\"original_title\", ISSN=row['ISSN']).first()\n",
    "        \n",
    "        if existing_node:\n",
    "            for key, value in properties.items():\n",
    "                if key not in existing_node or existing_node[key] != value:\n",
    "                    existing_node[key] = value\n",
    "                    properties_updated += 1\n",
    "            graph.push(existing_node)\n",
    "        else:\n",
    "            new_node = Node(\"original_title\", **properties)\n",
    "            graph.create(new_node)\n",
    "            nodes_created += 1\n",
    "    \n",
    "    # Print statistical data\n",
    "    print(f\"Nodes created: {nodes_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given the path to the Excel file\n",
    "# xlsx_path = \"path/to/excel/file.xlsx\"\n",
    "\n",
    "# Persist the journal information from the Excel file\n",
    "# persist_journals_from_xlsx(graph, xlsx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlsx_path = './../data/classificações_publicadas_todas_as_areas_avaliacao1672761192111.xlsx'\n",
    "# xlsx_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Reconhecimento de Entidades Nomeadas (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marcos.aires\\.conda\\envs\\Python38-PyTorch\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marcos.aires\\.conda\\envs\\Python38-PyTorch\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(text):\n",
    "    import spacy\n",
    "    \n",
    "    nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "    nlp_pt = spacy.load(\"pt_core_news_lg\")\n",
    "    doc = nlp_pt(text)\n",
    "    entities = {'ORG': [], 'GPE': [], 'NORP': [], 'PERSON': [], 'PRODUCT': [], 'WORK_OF_ART': []}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in entities:\n",
    "            entities[ent.label_].append(ent.text)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphModel:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def add_entity(self, entity_type, entity_value):\n",
    "        with self._driver.session() as session:\n",
    "            session.write_transaction(self._add_entity, entity_type, entity_value)\n",
    "            \n",
    "    @staticmethod\n",
    "    def _add_entity(tx, entity_type, entity_value):\n",
    "        query = f\"MERGE (a:{entity_type} {{name: $name}})\"\n",
    "        tx.run(query, name=entity_value)\n",
    "    \n",
    "    def add_relation(self, src_type, src_name, rel_type, tgt_type, tgt_name):\n",
    "        with self._driver.session() as session:\n",
    "            session.write_transaction(self._add_relation, src_type, src_name, rel_type, tgt_type, tgt_name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_relation(tx, src_type, src_name, rel_type, tgt_type, tgt_name):\n",
    "        query = (\n",
    "            f\"MATCH (a:{src_type} {{name: $src_name}}), (b:{tgt_type} {{name: $tgt_name}}) \"\n",
    "            f\"MERGE (a)-[:{rel_type}]->(b)\"\n",
    "        )\n",
    "        tx.run(query, src_name=src_name, tgt_name=tgt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    def expand(key, value):\n",
    "        if isinstance(value, dict):\n",
    "            return [(str(key) + '.' + str(k), v) for k, v in flatten_dict(value).items()]\n",
    "        else:\n",
    "            return [(str(key), value)]\n",
    "        \n",
    "    items = [item for k, v in d.items() for item in expand(k, v)]\n",
    "    return dict(items)\n",
    "\n",
    "def main(data_dict):\n",
    "    graph_model = GraphModel(\"bolt://localhost:7687\", \"neo4j\", \"password\")    \n",
    "    flattened_data = flatten_dict(data_dict['Properties']['Identificação'])\n",
    "    person_name = flattened_data.get('Nome', None)[0]\n",
    "    print(f'Nome: {person_name}')\n",
    "    if person_name:\n",
    "        graph_model.add_entity(\"Person\", person_name)\n",
    "    \n",
    "    institutional = flatten_dict(data_dict['Properties'])\n",
    "    for key, value in institutional.items():\n",
    "        if key in ['Endereço.Endereço Profissional']:\n",
    "            text = ' '.join(value)\n",
    "            print(text)\n",
    "            entities = extract_named_entities(text)\n",
    "            print(f'Entidades reconhecidas: {entities}')\n",
    "            entities\n",
    "            \n",
    "            for org in entities.get('ORG', []):\n",
    "                graph_model.add_entity(\"Organization\", org)\n",
    "                graph_model.add_relation(\"Person\", person_name, \"AFFILIATED_WITH\", \"Organization\", org)\n",
    "                \n",
    "            for gpe in entities.get('GPE', []):\n",
    "                graph_model.add_entity(\"Location\", gpe)\n",
    "                graph_model.add_relation(\"Person\", person_name, \"LOCATED_IN\", \"Location\", gpe)\n",
    "\n",
    "    projetos = flatten_dict(data_dict['Properties'])\n",
    "    for key, value in projetos.items():\n",
    "        if key in ['Projetos de pesquisa', 'Projetos de extensão', 'Projetos de desenvolvimento']:\n",
    "            entities = extract_named_entities(value)\n",
    "            \n",
    "            for org in entities.get('ORG', []):\n",
    "                graph_model.add_entity(\"Organization\", org)\n",
    "                graph_model.add_relation(\"Person\", person_name, \"HAS_PROJECT_IN\", \"Organization\", org)\n",
    "                \n",
    "            for gpe in entities.get('GPE', []):\n",
    "                graph_model.add_entity(\"Location\", gpe)\n",
    "                graph_model.add_relation(\"Person\", person_name, \"LOCATED_IN\", \"Location\", gpe)\n",
    "\n",
    "    graph_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy --user --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Processamento para eliminar strings duplicadas e vazias\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_duplicates\u001b[39m(entities):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "# Processamento para eliminar strings duplicadas e vazias\n",
    "def remove_duplicates(entities):\n",
    "    for key in entities.keys():\n",
    "        seen = set()\n",
    "        unique_values = []\n",
    "        \n",
    "        for value in entities[key]:\n",
    "            lower_value = value.lower()\n",
    "            if lower_value not in seen and value.strip():\n",
    "                seen.add(lower_value)\n",
    "                unique_values.append(value)\n",
    "        \n",
    "        # Atualização do dicionário\n",
    "        entities[key] = unique_values\n",
    "    return entities\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    nlp = spacy.load('en_core_web_lg')  # Assuming the use of the small Portuguese model\n",
    "    doc = nlp(text)\n",
    "    entities = {'ORG': [], 'GPE': [], 'PHONE': [], 'URL': []}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in entities.keys():\n",
    "            entities[ent.label_].append(ent.text)\n",
    "    \n",
    "    unique_entities = remove_duplicates(entities)\n",
    "    \n",
    "    return unique_entities\n",
    "\n",
    "text = 'Pós-Doutor em Computação pela Sorbonne Université-Pierre et Marie Curie (França - 2020). Doutor em Ciência da Computação pela Universitat Politecnica de Catalunya (Espanha - 2005). Atualmente é professor titular da Universidade de Fortaleza - UNIFOR na qual é membro permanente dos programas de pós-graduação em Informática Aplicada (mestrado e doutorado) e do mestrado profissional em administração. Possui mais de 100 trabalhos publicados em conferências e periódicos nacionais e internacionais. Tem experiência na área de Ciência da Computação, com ênfase em Ciência de Dados e Teleinformática, atuando principalmente nos seguintes temas: Redes Complexas, Grafos de Conhecimento, Aprendizagem de Máquina, Internet das Coisas, Segurança e Blockchain. (Texto informado pelo autor)\\n'\n",
    "\n",
    "# entities = extract_named_entities(text)\n",
    "# entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_dict['Properties'].keys())\n",
    "\n",
    "# main(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 2: Extrair dados dos Currículos para dicionários</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalization of the DOM extraction from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digital representation of the HTML DOM (Document Object Model) in question follow a consistent class-based structure, where the division of information into various classes within 'div' elements serves as an important taxonomy for organizing and categorizing the information.\n",
    "\n",
    "The nested structure predominantly consists of HTML div elements differentiated by their CSS classes. The div elements appear in a tree-like organization, hierarchically grouped under recursive presence of the div elements within the class 'title-wrapper', followed by the div elements marked with 'layout-cell' and hierarquically organized until reaching the more detailed levels where the data of interest is, contained in the classes such 'data-cell', 'text-align-right' or 'layout-cell-pad-5' and tags like 'a', 'b'.\n",
    "\n",
    "The extraction of data from this intricate nested architecture necessitates a recursive methodology that maintains the hierarchical fidelity of the original data. Thus, one approach to transforming this data into a structured JSON object would be to employ depth-first search (DFS) algorithms to traverse through each node in this tree-like structure. Each traversal would examine the class attributes and potentially the text content within each div. \n",
    "\n",
    "**Formalization:**\n",
    "\n",
    "In the formal language of computational theory, let \\( T \\) be the DOM tree with each node \\( n \\) containing a list of attributes \\( A(n) \\) and a text content \\( C(n) \\). Let \\( JSON(n) \\) be the JSON representation of the node \\( n \\). The recursive function to extract data can be described as:\n",
    "\n",
    "\n",
    "JSON(n) = \n",
    "\\begin{cases} \n",
    "\\{ \"type\": A(n), \"content\": C(n), \"children\": \\{ JSON(c) \\,|\\, c \\in \\text{children of } n \\} \\} & \\text{if } n \\text{ has children} \\\\\n",
    "\\{ \"type\": A(n), \"content\": C(n) \\} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\n",
    "\n",
    "**Python implementation:**\n",
    " \n",
    "In terms of practical implementation, Python's Beautiful Soup library can be particularly effective for this task, allowing for a relatively straightforward traversal of each div element to construct the JSON object.\n",
    "\n",
    "The end result would be a JSON object where each entry corresponds to a 'div' element in the original HTML structure, represented by a dictionary containing the attributes and content of the div, and potentially another dictionary (or list of dictionaries) representing any nested child div elements. This would effectively capture the data within each div while maintaining the hierarchical structure of the original HTML document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair DOM para Dicionários de um Currículo apenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando com o servidor do CNPq...\n",
      "1 resultados para ['Raimir Holanda Filho']\n",
      "Vínculo encontrado no currículo de nome: Raimir Holanda Filho\n",
      "34 células principais de dados encontradas\n",
      "Nenhum dado encontrado na classe \"citado\".\n",
      "Nenhum dado encontrado na classe \"citado\".\n",
      "Dados artigo: {'doi': '10.1016/j.procs.2023.03.018', 'issn': '18770509', 'volume': '220', 'paginaInicial': '119', 'titulo': 'Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain', 'sequencial': '3', 'nomePeriodico': 'PROCEDIA COMPUTER SCIENCE'}\n",
      "Dados artigo: {'doi': '10.9790/487X-2507032735', 'issn': '2278487X', 'volume': '25', 'paginaInicial': '27', 'titulo': 'Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector', 'sequencial': '4', 'nomePeriodico': 'Journal of Business and Management'}\n",
      "Dados artigo: {'doi': '10.1109/ACCESS.2022.3179004', 'issn': '21693536', 'volume': '1', 'paginaInicial': '1', 'titulo': 'A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain', 'sequencial': '5', 'nomePeriodico': 'IEEE Access'}\n",
      "Dados artigo: {'issn': '22360700', 'volume': '13', 'paginaInicial': '1', 'titulo': 'Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública', 'sequencial': '6', 'nomePeriodico': 'Revista Razão Contábil '}\n",
      "Dados artigo: {'doi': '10.3390/app12188939', 'issn': '20763417', 'volume': '12', 'paginaInicial': '8939', 'titulo': 'Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19', 'sequencial': '7', 'nomePeriodico': 'Applied Sciences-Basel'}\n",
      "Dados artigo: {'issn': '22360700', 'volume': '12', 'paginaInicial': '1', 'titulo': 'Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará', 'sequencial': '8', 'nomePeriodico': 'Revista Razão Contábil '}\n",
      "Dados artigo: {'doi': '10.1007/978-3-030-84311-3_2', 'issn': '22138684', 'volume': '2021', 'paginaInicial': '13', 'titulo': 'Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19', 'sequencial': '9', 'nomePeriodico': 'Springer Proceedings in Complexity'}\n",
      "Dados artigo: {'doi': '10.3390/app112110457', 'issn': '20763417', 'volume': '11', 'paginaInicial': '10457', 'titulo': 'An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus', 'sequencial': '10', 'nomePeriodico': 'Applied Sciences-Basel'}\n",
      "Dados artigo: {'doi': '10.3390/s20113068', 'issn': '14248220', 'volume': '20', 'paginaInicial': '3068', 'titulo': 'Enhancing Key Management in LoRaWAN with Permissioned Blockchain', 'sequencial': '11', 'nomePeriodico': 'SENSORS'}\n",
      "Dados artigo: {'doi': '10.1109/access.2020.3009167', 'issn': '21693536', 'volume': '1', 'paginaInicial': '1', 'titulo': 'Integration of the Mobile Robot and Internet of Things to Monitor Older People', 'sequencial': '12', 'nomePeriodico': 'IEEE Access'}\n",
      "Dados artigo: {'issn': '22360700', 'volume': '11', 'paginaInicial': '1', 'titulo': 'Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019', 'sequencial': '13', 'nomePeriodico': 'Revista Razão Contábil '}\n",
      "Dados artigo: {'doi': '10.1002/nem.2062', 'issn': '10557148', 'volume': '1', 'paginaInicial': 'e2062', 'titulo': 'An approach for provisioning virtual sensors in sensor clouds', 'sequencial': '14', 'nomePeriodico': 'International Journal of Network Management'}\n",
      "Dados artigo: {'doi': '10.1109/jiot.2019.2904302', 'issn': '23274662', 'volume': '6', 'paginaInicial': '5631', 'titulo': 'Enabling Online Quantitative Security Analysis in 6LoWPAN Networks', 'sequencial': '15', 'nomePeriodico': 'IEEE Internet of Things Journal'}\n",
      "Dados artigo: {'doi': '10.1109/mnet.2018.1800151', 'issn': '08908044', 'volume': '33', 'paginaInicial': '126', 'titulo': 'Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering', 'sequencial': '16', 'nomePeriodico': 'IEEE NETWORK'}\n",
      "Dados artigo: {'doi': '10.3390/s18020353', 'issn': '14248220', 'volume': '18', 'paginaInicial': '353', 'titulo': 'A Proposal for IoT Dynamic Routes Selection Based on Contextual Information', 'sequencial': '17', 'nomePeriodico': 'SENSORS'}\n",
      "Dados artigo: {'doi': '10.1002/dac.3380', 'issn': '10745351', 'volume': '31', 'paginaInicial': 'e3380', 'titulo': 'Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks', 'sequencial': '18', 'nomePeriodico': 'INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS'}\n",
      "Dados artigo: {'doi': '10.1016/j.jnca.2018.01.015', 'issn': '10848045', 'volume': '107', 'paginaInicial': '56', 'titulo': 'A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs', 'sequencial': '19', 'nomePeriodico': 'JOURNAL OF NETWORK AND COMPUTER APPLICATIONS'}\n",
      "Dados artigo: {'doi': '10.3390/s18030689', 'issn': '14248220', 'volume': '18', 'paginaInicial': '689', 'titulo': 'An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments', 'sequencial': '20', 'nomePeriodico': 'SENSORS'}\n",
      "Dados artigo: {'doi': '10.3390/s18051312', 'issn': '14248220', 'volume': '18', 'paginaInicial': '1312', 'titulo': 'Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks', 'sequencial': '21', 'nomePeriodico': 'SENSORS'}\n",
      "Dados artigo: {'doi': '10.1109/comst.2017.2745505', 'issn': '1553877X', 'volume': '19', 'paginaInicial': '2704', 'titulo': 'Model-Based Quantitative Network Security Metrics: A Survey', 'sequencial': '22', 'nomePeriodico': 'IEEE Communications Surveys and Tutorials'}\n",
      "Dados artigo: {'doi': '10.18464/cybin.v3i1', 'issn': '24942715', 'volume': '3', 'paginaInicial': '30', 'titulo': 'Exploring a P2P Transient Botnet - From Discovery to Enumeration', 'sequencial': '23', 'nomePeriodico': 'The Journal on Cybercrime '}\n",
      "Dados artigo: {'doi': '10.21528/lnlm-vol14-no1-art1', 'issn': '16762789', 'volume': '14', 'paginaInicial': '4', 'titulo': 'UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES', 'sequencial': '24', 'nomePeriodico': 'LEARNING AND NONLINEAR MODELS'}\n",
      "Dados artigo: {'doi': '10.3390/s150102104', 'issn': '14248220', 'volume': '15', 'paginaInicial': '2104', 'titulo': 'Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks', 'sequencial': '25', 'nomePeriodico': 'SENSORS'}\n",
      "Dados artigo: {'doi': '10.1155/2014/506203', 'issn': '15501329', 'volume': '2014', 'paginaInicial': '1', 'titulo': 'A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions', 'sequencial': '26', 'nomePeriodico': 'International Journal of Distributed Sensor Networks'}\n",
      "Dados artigo: {'issn': '15487709', 'volume': '10', 'paginaInicial': '702', 'titulo': 'A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks', 'sequencial': '27', 'nomePeriodico': 'JOURNAL OF COMMUNICATION AND COMPUTER'}\n",
      "Dados artigo: {'issn': '16762789', 'volume': '10', 'paginaInicial': '4', 'titulo': 'Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink', 'sequencial': '28', 'nomePeriodico': 'Learning and Nonlinear Models'}\n",
      "Dados artigo: {'doi': '10.3233/HIS-2012-0147', 'issn': '14485869', 'volume': '9', 'paginaInicial': '61', 'titulo': 'An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks', 'sequencial': '29', 'nomePeriodico': 'INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS'}\n",
      "Dados artigo: {'doi': '10.1016/j.inffus.2012.01.010', 'issn': '15662535', 'volume': '15', 'paginaInicial': '44', 'titulo': 'On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries', 'sequencial': '30', 'nomePeriodico': 'Information Fusion (Print)'}\n",
      "Dados artigo: {'issn': '1980086X', 'volume': 'X', 'paginaInicial': '65', 'titulo': 'Controle Externo da Governança de Tecnologia da Informação', 'sequencial': '31', 'nomePeriodico': 'Revista Controle'}\n",
      "Dados artigo: {'issn': '01018191', 'volume': '28', 'paginaInicial': '33', 'titulo': 'Detecting computer network attacks using statistical discriminators and cluster analysis', 'sequencial': '32', 'nomePeriodico': 'Revista Tecnologia (UNIFOR)'}\n",
      "Dados artigo: {'issn': '01018191', 'volume': '27', 'paginaInicial': '113', 'titulo': 'Broadband network traffic characterization and classification using a multivariate statistical method', 'sequencial': '33', 'nomePeriodico': 'Revista Tecnologia (UNIFOR)'}\n",
      "Dados artigo: {'issn': '14145685', 'volume': '8o.', 'titulo': 'Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários', 'sequencial': '34', 'nomePeriodico': 'Revista Brasileira de Informática na Educação'}\n",
      "Dados de 34 artigos completos extraídos\n",
      "Total de caracteres extraídos:  91782\n",
      "Quantidade extraída de linhas:   3610\n"
     ]
    }
   ],
   "source": [
    "driver = connect_driver(caminho)\n",
    "NOME = ['Raimir Holanda Filho']\n",
    "fill_name(driver, delay, NOME)\n",
    "\n",
    "limite=3\n",
    "instituicao = 'Fundação Oswaldo Cruz'\n",
    "unidade     = 'Fiocruz Ceará'\n",
    "termo       = 'Ministerio da Saude'\n",
    "\n",
    "elm_vinculo, np.NaN, np.NaN, np.NaN, driver = find_terms(NOME, instituicao, unidade, termo, driver, delay, limite)\n",
    "soup = open_curriculum(driver,elm_vinculo)\n",
    "\n",
    "caracteres = len(soup.text)\n",
    "linhas = len(soup.text.split('\\n'))\n",
    "print(f'Total de caracteres extraídos: {caracteres:6d}')\n",
    "print(f'Quantidade extraída de linhas: {linhas:6d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Identificação': {'Nome': 'Raimir Holanda Filho',\n",
       "  'Nome em citações bibliográficas': 'HOLANDA FILHO, R.;FILHO, RAIMIR HOLANDA;FILHO, RAIMIR;HOLANDA, RAIMIR;HOLANDA FILHO, RAIMIR;FILHO, RAIMIR H.',\n",
       "  'Lattes iD': 'http%3A//lattes.cnpq.br/2607811863279622'},\n",
       " 'Endereço': {'Endereço Profissional': 'Universidade de Fortaleza, Mestrado em Informatica Aplicada, Mestrado Em Informatica Aplicada. Av. Washington Soares,1321Edson Queiroz60811341 - Fortaleza, CE - BrasilTelefone: (85) 34773268URL da Homepage: http%3A//www.unifor.br'},\n",
       " 'Formação acadêmica/titulação': {'2001 - 2005': 'Doutorado em Ciência da Computação. Universitat Politècnica de Catalunya, UPC, Espanha.  Título: A New Methodology for Packet Trace Classification and Compression based on Semantic Traffic Characterization, Ano de obtenção: 2005. Orientador: Jorge Garcia Vidal. Bolsista do(a): Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES, Brasil. Palavras-chave: Traffic Characterization; TCP flow clustering; Performance; Packet trace classification; Compression.Grande área: Ciências Exatas e da TerraSetores de atividade: Educação Superior.',\n",
       "  '1996 - 1998': 'Mestrado em Ciências da Computação. Universidade Federal do Ceará, UFC, Brasil.  Título: SAGRES - Um Sistema Baseado em Conhecimento para Apoia a Gerencia de Falhas em Redes de Computadores, Ano de Obtenção: 1998.Orientador: Mauro Oliveira.Palavras-chave: SAGRES; GERENCIA DE REDES; REDES DE COMPUTADORES; SISTEMAS BASEADOS EM CONHECIMENTO; GERENCIA DE FALHAS.Grande área: Ciências Exatas e da TerraSetores de atividade: Educação; Informática.',\n",
       "  '1994 - 1994': 'Especialização em Informatica.  (Carga Horária: 360h). Universidade Federal do Ceará, UFC, Brasil.',\n",
       "  '1985 - 1989': 'Graduação em Engenharia Civil. Universidade Federal do Ceará, UFC, Brasil.'},\n",
       " 'Pós-doutorado': {'2020 - 2020': 'Pós-Doutorado. Université Pierre et Marie Curie, UPMC, França.  Grande área: Ciências Exatas e da TerraGrande Área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Machine Learning.'},\n",
       " 'Formação Complementar': {},\n",
       " 'Atuação Profissional': {'Universidade de Fortaleza, UNIFOR, Brasil.': [{'Vínculo institucional': '',\n",
       "    '1995 - Atual': 'Vínculo: , Enquadramento Funcional: Professor titular, Carga horária: 40',\n",
       "    'Atividades': '',\n",
       "    '04/2008 - Atual': 'Conselhos, Comissões e Consultoria, Reitoria, Conselho de Centro do CCT.',\n",
       "    '': 'Treinamentos ministradosTecnologias de Redes sem Fio',\n",
       "    '03/2006 - Atual': 'Ensino, Informática Aplicada, Nível: Pós-Graduação',\n",
       "    '02/2006 - Atual': 'Ensino, Engenharia de Telecomunicações, Nível: Graduação',\n",
       "    '10/2005 - Atual': 'Pesquisa e desenvolvimento, Mestrado em Informatica Aplicada.',\n",
       "    '3/1995 - Atual': 'Ensino, Informatica, Nível: Graduação',\n",
       "    '1/2006 - 1/2006': 'Treinamentos ministrados , Mestrado em Informatica Aplicada, Mestrado Em Informatica Aplicada.'}]},\n",
       " 'Linhas de pesquisa': {'1.': 'Redes de Comunicação'},\n",
       " 'Projetos de pesquisa': {'2021 - Atual': 'Monitor Fiscal TCE/CE',\n",
       "  '': 'Descrição: ESTUDO E DESENVOLVIMENTO DE SISTEMAS DE DETECÇÃO/PREVENÇÃO DE INTRUSÃO BASEADOS EM REGRAS PARA REDES EM AMBIENTE SEM FIO.. Situação: Concluído; Natureza: Pesquisa. Alunos envolvidos: Graduação: (3)  / Especialização: (0)  / Mestrado acadêmico: (1)  / Mestrado profissional: (0)  / Doutorado: (0) . Integrantes: Raimir Holanda Filho - Coordenador.',\n",
       "  '2011 - 2014': 'CIA2-Construindo Cidades Inteligentes da Instrumentação dos Ambientes ao Desenvolvimento de Aplicações',\n",
       "  '2010 - 2010': 'DeLAtoR - Detecção e localização de disparos de armas baseado em redes de sensores sem fio',\n",
       "  '2009 - 2011': 'MOVERES: Sistema de Monitoramento Veicular Através de Redes de Sensores sem Fio. Projeto financiado pela FINEP (Edital Subvenção Economica a Inovacao - 01/2007)',\n",
       "  '2008 - 2011': 'Especificaçao de uma arquitetura de gerenciamento distribuído para RSSF. Projeto de Pesquisa cadastrado na Unifor.',\n",
       "  '2008 - 2010': 'FATADIST - Uma Ferramenta para Classificação de Tráfego de Ataques. Projeto financiado com recursos do Edital AT - CNPq',\n",
       "  '2007 - 2009': 'CLASTRIN - Um Classificador de Tráfego Internet baseado em Discriminantes Estatísticos. Projeto financiado com recursos do Edital Universal - CNPq',\n",
       "  '2007 - 2008': 'Sistema de Detecção de ataques DoS (Denial of Service) em redes móveis através de técnicas de modelagem.',\n",
       "  '2006 - 2008': 'Caracterização, classificação e modelagem de tráfego de redes convergentes',\n",
       "  '2005 - 2007': 'Telemedicina - Ampliaçao de Laboratório para Pesquisa e Desenvolvimento de Aplicações em Telemedicina. Projeto de pesquisa e desenvolvimento em tecnologia da informacao incentivado através da Lei de Informática. (Celéstica do Brasil Ltda)',\n",
       "  '2005 - 2006': 'Implantacao do laboratorio de Redes Convergentes. Projeto financiado com recursos da Lei de Informática (UNIFOR - SANMINA-SCI DO BRASIL INTEGRATION LTDA)'},\n",
       " 'Projetos de extensão': {'2009 - 2009': 'S2IPro - Sistema de Identificação Inequívoca do Proprietário',\n",
       "  '': 'Descrição: Sistema de Identificação Inequívoca do Proprietário. Situação: Concluído; Natureza: Extensão. Alunos envolvidos: Graduação: (2)  / Mestrado acadêmico: (1) . Integrantes: Raimir Holanda Filho - Coordenador / Luis Henrique Pequeno Almeida - Integrante.Financiador(es): Thinktech Ind e Comercio de Informatica Ltda - Auxílio financeiro.'},\n",
       " 'Projetos de desenvolvimento': {'2010 - 2012': 'Desenvolvimento de Software para Gerenciamento de Arquitetura de Telecomunicações de Baixo Custo para Comunidades Rurais, Pequenas Empresas, Órgãos e Instituições Municipais/Estaduais',\n",
       "  '': 'Situação: Em andamento; Natureza: Desenvolvimento. Alunos envolvidos: Graduação: (4)  / Mestrado acadêmico: (2) . Integrantes: Raimir Holanda Filho - Coordenador / Geneflides Laureno da Silva - Integrante.Financiador(es): Financiadora de Estudos e Projetos - Auxílio financeiro.'},\n",
       " 'Revisor de periódico': {'2020 - Atual': 'Periódico: IEEE Latin America Transactions',\n",
       "  '2021 - Atual': 'Periódico: Computer Networks'},\n",
       " 'Revisor de projeto de fomento': {'2015 - Atual': 'Agência de fomento: Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico'},\n",
       " 'Áreas de atuação': {'1.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Sistemas de Computação/Especialidade: Teleinformática.',\n",
       "  '2.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Security.',\n",
       "  '3.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Machine Learning.',\n",
       "  '4.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Natural Language Processing.',\n",
       "  '5.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Internet of Things.',\n",
       "  '6.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Blockchain.'},\n",
       " 'Idiomas': {'Inglês': 'Compreende Bem, Fala Bem, Lê Bem, Escreve Bem.',\n",
       "  'Espanhol': 'Compreende Bem, Fala Bem, Lê Bem, Escreve Bem.',\n",
       "  'Português': 'Compreende Bem, Fala Bem, Lê Bem, Escreve Bem.',\n",
       "  'Francês': 'Compreende Bem, Fala Razoavelmente, Lê Bem, Escreve Razoavelmente.'},\n",
       " 'Inovação': {'2021 - Atual': 'Monitor Fiscal TCE/CE',\n",
       "  '': 'Descrição: O projeto busca aprimorar os mecanismos e a capacidade de análise e projeção de dados das contas públicas estaduais e municipais já existentes no TCE/CE, mediante o desenvolvimento de novas ferramentas de sistematização e disponibilização de dados das contas públicas estaduais e municipais. A partir de técnicas de análise e integração de dados, em bases públicas e/ou custodiadas, serão utilizados modelos computacionais descritivos/preditivos para tentar antecipar possíveis deteriorações nas contas públicas estaduais e municipais. Deste modo o TCE/CE pode emitir alertas com maior frequência e antecedência.. Situação: Em andamento; Natureza: Pesquisa. Integrantes: Raimir Holanda Filho - Coordenador.Financiador(es): Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico - Auxílio financeiro.'}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_tit1_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Produções': {'Produção bibliográfica': {'Citações': {'Database': 'Outras',\n",
       "    'Total de trabalhos': 210,\n",
       "    'Total de citações': 736,\n",
       "    'Índice_H': None,\n",
       "    'Data': '08/05/2023'},\n",
       "   'Artigos completos publicados em periódicos': {'1.': 'ANDRADE, E. C. ; PINHEIRO, L. I. ;Pinheiro, Placido R.; NUNES, L. C. ; PINHEIRO, M. C. D. ; PEREIRA, M. L. D. ; ABREU, W. C. ;HOLANDA FILHO, RAIMIR; SIMAO FILHO, M. ; PINHEIRO, P. G. C. D. ; NUNES, R. E. C. . Hybrid model for early identification post-Covid-19 sequelae. Journal of Ambient Intelligence and Humanized Computing, p. 1-14, 2023.',\n",
       "    '2.': 'MARINHO, RENATO ;HOLANDA, RAIMIR. Automated Emerging Cyber Threat Identification and Profiling Based on Natural Language Processing. IEEE Access, v. 1, p. 1-1, 2023.CitaÃ§Ãµes:',\n",
       "    '3.': 'FILHO, RAIMIR HOLANDA; DE SOUSA, DEBORA CARLA BARBOZA ; DE BRITO, WELLINGTON ALVES ; CHAVES, JOAN LUCAS MARQUES DE SOUSA ; SÁ, EMANUEL LEÃO ; RIBEIRO, VICTOR PASKNEL DE ALENCAR . Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain. PROCEDIA COMPUTER SCIENCE, v. 220, p. 119-126, 2023.',\n",
       "    '4.': 'HOLANDA FILHO, RAIMIR; CUNHA, G. H. M. ; RAVIOLO, B. P. Y. ; BRITO, R. W. C. . Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector. Journal of Business and Management, v. 25, p. 27-35, 2023.',\n",
       "    '5.': 'Ribeiro, Victor ;HOLANDA, RAIMIR; RAMOS, ALEX ; RODRIGUES, JOEL J. P. C. . A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain. IEEE Access, v. 1, p. 1-1, 2022.CitaÃ§Ãµes:',\n",
       "    '6.': 'MAPURUNGA, M. P. A. ;HOLANDA FILHO, RAIMIR. Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública. Revista Razão Contábil & Finanças, v. 13, p. 1-14, 2022.',\n",
       "    '7.': 'ANDRADE, EVANDRO CARVALHO DE ; PINHEIRO, PLÁCIDO ROGERIO ; BARROS, ANA LUIZA BESSA DE PAULA ; NUNES, LUCIANO COMIN ; PINHEIRO, LUANA IBIAPINA C. C. ; PINHEIRO, PEDRO GABRIEL CALÍOPE DANTAS ;HOLANDA FILHO, RAIMIR. Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19. Applied Sciences-Basel, v. 12, p. 8939, 2022.CitaÃ§Ãµes:',\n",
       "    '8.': 'LIMA, S. M. ; BRITO, R. W. ; MATOS, P. R. F. ; JESUS FILHO, J. ;HOLANDA FILHO, R.; CUNHA, G. H. M. . Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará. Revista Razão Contábil & Finanças, v. 12, p. 1, 2021.',\n",
       "    '9.': 'ANDRADE, E. C. ;Pinheiro, Placido R.;HOLANDA FILHO, R.; NUNES, L. C. ; PINHEIRO, M. C. D. ; ABREU, W. C. ; SIMAO FILHO, M. ; PINHEIRO, L. I. C. C. ; PEREIRA, M. L. D. ; PINHEIRO, P. G. C. D. . Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19. Springer Proceedings in Complexity, v. 2021, p. 13-24, 2021.',\n",
       "    '10.': 'PINHEIRO, LUANA I. C. C. ; PEREIRA, MARIA LÚCIA D. ; ANDRADE, EVANDRO C. DE ; NUNES, LUCIANO C. ; ABREU, WILSON C. DE ; PINHEIRO, PEDRO GABRIEL CALÍOPE D. ;HOLANDA FILHO, RAIMIR; PINHEIRO, PLÁCIDO ROGERIO . An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus. Applied Sciences-Basel, v. 11, p. 10457, 2021.CitaÃ§Ãµes:',\n",
       "    '11.': 'Ribeiro, Victor ;HOLANDA, RAIMIR; RAMOS, ALEX ; RODRIGUES, JOEL J. P. C. . Enhancing Key Management in LoRaWAN with Permissioned Blockchain. SENSORS, v. 20, p. 3068, 2020.CitaÃ§Ãµes:',\n",
       "    '12.': 'Pinheiro, Placido R.; PINHEIRO, PEDRO G. C. D. ;FILHO, RAIMIR H.; BARROZO, JOAO P. A. ; RODRIGUES, JOEL J. P. C. ; PINHEIRO, LUANA I. C. C. ; PEREIRA, MARIA L. D. . Integration of the Mobile Robot and Internet of Things to Monitor Older People. IEEE Access, v. 1, p. 1-1, 2020.CitaÃ§Ãµes:',\n",
       "    '13.': 'LIMA, S. M. ; BRITO, R. W. ; MATOS, P. R. F. ; JESUS FILHO, J. ;HOLANDA FILHO, R.. Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019. Revista Razão Contábil & Finanças, v. 11, p. 1, 2020.',\n",
       "    '14.': 'LEMOS, MARCUS ; RABELO, RICARDO ; MENDES, DOUGLAS ; CARVALHO, CARLOS ;HOLANDA, RAIMIR. An approach for provisioning virtual sensors in sensor clouds. International Journal of Network Management, v. 1, p. e2062, 2019.CitaÃ§Ãµes:',\n",
       "    '15.': 'RAMOS, ALEX ; MILFONT, RONALDO T. P. ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. . Enabling Online Quantitative Security Analysis in 6LoWPAN Networks. IEEE Internet of Things Journal, v. 6, p. 5631-5638, 2019.CitaÃ§Ãµes:',\n",
       "    '16.': 'GUIMARAES, RANIERE ROCHA ; PASSOS, LEANDRO A. ;FILHO, RAIMIR HOLANDA; ALBUQUERQUE, VICTOR HUGO C. DE ; RODRIGUES, JOEL J. P. C. ; KOMAROV, MIKHAIL M. ; PAPA, JOAO PAULO . Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering. IEEE NETWORK, v. 33, p. 126-131, 2019.CitaÃ§Ãµes:',\n",
       "    '17.': 'ARAÚJO, HARILTON ;FILHO, RAIMIR; RODRIGUES, JOEL ; RABELO, RICARDO ; SOUSA, NATANAEL ; FILHO, JOSÉ ; SOBRAL, JOSÉ . A Proposal for IoT Dynamic Routes Selection Based on Contextual Information. SENSORS, v. 18, p. 353, 2018.CitaÃ§Ãµes:',\n",
       "    '18.': 'ARAÚJO, PAULO RÉGIS C. ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. ; OLIVEIRA, JOÃO P. C. M. ; BRAGA, STEPHANIE A. . Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks. INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS, v. 31, p. e3380, 2018.CitaÃ§Ãµes:',\n",
       "    '19.': 'SOBRAL, JOSÉ V.V. ; RODRIGUES, JOEL J.P.C. ; RABELO, RICARDO A.L. ; LIMA FILHO, JOSÉ C. ; SOUSA, NATANAEL ; ARAUJO, HARILTON S. ;HOLANDA FILHO, RAIMIR. A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs. JOURNAL OF NETWORK AND COMPUTER APPLICATIONS, v. 107, p. 56-68, 2018.CitaÃ§Ãµes:',\n",
       "    '20.': 'LEMOS, MARCUS ;FILHO, RAIMIR; RABÊLO, RICARDO ; DE CARVALHO, CARLOS ; MENDES, DOUGLAS ; COSTA, VALNEY . An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments. SENSORS, v. 18, p. 689, 2018.CitaÃ§Ãµes:',\n",
       "    '21.': 'DE ARAÚJO, PAULO ;FILHO, RAIMIR; RODRIGUES, JOEL ; OLIVEIRA, JOÃO ; BRAGA, STEPHANIE . Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks. SENSORS, v. 18, p. 1312, 2018.CitaÃ§Ãµes:',\n",
       "    '22.': 'RAMOS, ALEX ; LAZAR, MARCELLA ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. . Model-Based Quantitative Network Security Metrics: A Survey. IEEE Communications Surveys and Tutorials, v. 19, p. 2704-2734, 2017.CitaÃ§Ãµes:',\n",
       "    '23.': 'MARINHO, R. ;HOLANDA FILHO, RAIMIR. Exploring a P2P Transient Botnet - From Discovery to Enumeration. The Journal on Cybercrime & Digital Investigations, v. 3, p. 30-39, 2017.',\n",
       "    '24.': 'LEMOS, MARCUS ; CARVALHO, CARLOS ; LOPES, DOUGLAS ;HOLANDA, RAIMIR; RABÊLO, RICARDO . UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES. LEARNING AND NONLINEAR MODELS, v. 14, p. 4-14, 2016.',\n",
       "    '25.': 'RAMOS, A. L. ;HOLANDA FILHO, R.. Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks. SENSORS, v. 15, p. 2104-2136, 2015.CitaÃ§Ãµes:',\n",
       "    '26.': 'ARAUJO, Paulo Regis C. ;HOLANDA FILHO, R.; RODRIGUES, Antonio. Wendell. O. ; ARAUJO, Andre. Luiz. C. ; MORAES FILHO, Jose. A. ; OLIVEIRA, Joao. Paolo. C. M. . A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions. International Journal of Distributed Sensor Networks, v. 2014, p. 1-15, 2014.CitaÃ§Ãµes:',\n",
       "    '27.': 'SOBRAL, J. ; SOUSA, A. ; ARAUJO, H. S. ; BALUZ, R. ;HOLANDA FILHO, R.; SOUSA, M. V. ; Rabelo, Ricardo A. L. . A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks. JOURNAL OF COMMUNICATION AND COMPUTER, v. 10, p. 702-712, 2013.',\n",
       "    '28.': 'Rabelo, Ricardo A. L. ; LEAL, L. B. ; SOUSA, M. V. ;HOLANDA FILHO, R.; Borges, Fabio A. S. . Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink. Learning and Nonlinear Models, v. 10, p. 4-18, 2012.',\n",
       "    '29.': 'Rabelo, Ricardo A. L. ; LEAL, L. B. ; SOUSA, M. V. ;HOLANDA FILHO, R.; Borges, Fabio A. S. . An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks. INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS, v. 9, p. 61-74, 2012.',\n",
       "    '30.': 'BRAYNER, A. ; Coelho, André Luis Carvalho ; Marinho, Karine ;HOLANDA FILHO, R.; Castro, Wagner L. T. . On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries. Information Fusion (Print), v. 15, p. 44-55, 2012.CitaÃ§Ãµes:',\n",
       "    '31.': 'HOLANDA FILHO, R.; OLIVEIRA, J. A. . Controle Externo da Governança de Tecnologia da Informação. Revista Controle, v. X, p. 65-85, 2012.',\n",
       "    '32.': 'HOLANDA FILHO, R.; MAIA, J. E. B. ; CARMO, M. F. F. . Detecting computer network attacks using statistical discriminators and cluster analysis. Revista Tecnologia (UNIFOR), v. 28, p. 33-41, 2007.',\n",
       "    '33.': 'HOLANDA FILHO, R.; MAIA, J. E. B. ; PAULINO, G. . Broadband network traffic characterization and classification using a multivariate statistical method. Revista Tecnologia (UNIFOR), v. 27, p. 113-122, 2006.',\n",
       "    '34.': 'FURTADO, E. ; LINCOLN, F. ; FURTADO, V. ;HOLANDA FILHO, R.. Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários. Revista Brasileira de Informática na Educação, v. 8o., 2001.'}},\n",
       "  'Produção técnica': {'Programas de computador sem registro': {'1.': 'HOLANDA FILHO, R.. SAGRES. 1998.'}}},\n",
       " 'JCR': [{'data-issn': '18685137',\n",
       "   'impact-factor': '3.662',\n",
       "   'jcr-year': 'JCR 2021'},\n",
       "  {'data-issn': '21693536', 'impact-factor': '3.9', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': '21693536', 'impact-factor': '3.9', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': '20763417', 'impact-factor': '2.7', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': '20763417', 'impact-factor': '2.7', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': '14248220', 'impact-factor': '3.9', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': '21693536', 'impact-factor': '3.9', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': '10557148', 'impact-factor': '1.5', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': '23274662', 'impact-factor': '10.6', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': '08908044', 'impact-factor': '9.3', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': '14248220', 'impact-factor': '3.9', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': '10745351', 'impact-factor': '2.1', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': '10848045', 'impact-factor': '8.7', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': '14248220', 'impact-factor': '3.9', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': '14248220', 'impact-factor': '3.9', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': '1553877X', 'impact-factor': '35.6', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': '14248220', 'impact-factor': '3.9', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': '15501329', 'impact-factor': '1.614', 'jcr-year': 'JCR 2018'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': '15662535', 'impact-factor': '18.6', 'jcr-year': 'JCR 2022'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'}],\n",
       " 'Bancas': {'Participação em bancas de trabalhos de conclusão': {'Mestrado': {'1.': 'TORRES, A. L. M. M.; BORGES NETO, H.; HOLANDA FILHO, RAIMIR.  Participação em banca de ANDRÉ SANTOS SILVA. TeleMeios: uma proposta de virtualização do ensino ancorada na Sequência Fedathi. 2022. Dissertação (Mestrado em Educação) - Universidade Federal do Ceará.',\n",
       "    '2.': 'MACHADO, V. P.; SANTOS, P. A.; MOURA, R. S.; LEMOS, MARCUS; HOLANDA FILHO, RAIMIR.  Participação em banca de Joselito Mendes de Sousa Junior. Modelo para Classificação de Fornecedores da Adm Publ baseado em Aprend Maquina. 2019. Dissertação (Mestrado em Ciência da Computação) - Universidade Federal do Piauí.',\n",
       "    '3.': 'HOLANDA FILHO, R.; Pinheiro, Placido R.; BESSA, A.; THOMAZ, A. C. F..  Participação em banca de Odecilia Barreira da Silva. C3M - Gerenciamento de Mudanças Estruturado em uma Metodologia de Multicritério. 2013. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '4.': 'HOLANDA FILHO, R.; ANDRADE, R. M. C.; SAMPAIO, A. T. F..  Participação em banca de Alex Lacerda Ramos. Sensor Data Security Estimator: um Framework para Estimativa do Nível de Segurança dos dados de Redes de Sensores sem Fio. 2013. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '5.': 'HOLANDA FILHO, R.; Rabelo, Ricardo A. L.; BRAYNER, A.; FERNANDES, R. A. S..  Participação em banca de Rodrigo Augusto Rocha Souza Baluz. Uma Aplicação de Sistemas Inteligentes Híbridos ACO-Fuzzy para a Otimização do Desempenho em Redes de Sensores sem Fio. 2013. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '6.': 'MENDONCA, N. C.; SAMPAIO, A. T. F.; FONSECA, N. L. S.; HOLANDA FILHO, R..  Participação em banca de Matheus Ciríaco Cerqueira Cunha. Um Ambiente Programável para Avaliar o Desempenho de Aplicações em Nuvens de Infraestrutura. 2012. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '7.': 'SAMPAIO, A. T. F.; HOLANDA FILHO, R.; UEYAMA, J.; VASCONCELOS FILHO, J. E..  Participação em banca de Leonardo Moura Leitão. NaturalCloud: Um framework para integração de Rede de Sensores na Nuvem. 2012. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '8.': 'HOLANDA FILHO, R.; ZIVIANI, A.; MENDONCA, N. C..  Participação em banca de Victor Pasknel de Alencar Ribeiro. Classificação de tráfego online baseada em sub-fluxos. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '9.': 'HOLANDA FILHO, R.; Coelho, André Luis Carvalho; GOMES, D. G.; SANTOS, C. N..  Participação em banca de Gesiel Rios Lopes. GERAU: Um mecanismo de gerenciamento de segurança autonômico baseado em detecção de novidades e mudança de conceito. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '10.': 'HOLANDA FILHO, R.; Rabelo, Ricardo A. L.; SAMPAIO, A. T. F..  Participação em banca de Harilton da Silva Araújo. LARP: UM PROTOCOLO DE ROTEAMENTO TOLERANTE A FALHAS PARA REDES DE SENSORES SEM FIO. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '11.': 'HOLANDA FILHO, R.; NOGUEIRA, M.; MADEIRA, E. R. M.; MENDONCA, N. C.; RODRIGUES, M. A. F..  Participação em banca de Helber Wagner da Silva. Um Esquema de Seleção de Rotas para o Balanceamento de Segurança e Desempenho em Redes em Malha sem Fio. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '12.': 'HOLANDA FILHO, R.; Rabelo, Ricardo A. L.; SANTOS, Aldri L dos; SANTOS, C. N..  Participação em banca de Liliam Barroso Leal. Uma Abordagem para Estimação da Qualidade de Rotas em Redes de Sensores sem Fio Multi-Sink Baseada em Sistemas Fuzzy Genéticos. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '13.': 'HOLANDA FILHO, R.; SILVA, J. S. V.; BESSA, A..  Participação em banca de Denilson Cursino de Oliveira. Uma Sistematização para o Planejamento da Gerência de Mudanças em TI e Modelagem de Uma Ferramenta. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '14.': 'HOLANDA FILHO, R.; ANDRADE, R. M. C.; BRAYNER, A. R. A..  Participação em banca de Marcus Vinicius de Sousa Lemos. Detecção de Intrusão em Redes de Sensores Sem Fio Utilizando uma Abordagem Colaborativa e Cross-layer. 2010. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '15.': 'HOLANDA FILHO, R.; Souza, Jefferson T.; Silva, Jorge Luiz de C..  Participação em banca de Fabiano Carneiro Ribeiro. Estimação de Matrizes de Tráfego Origem-Destino utilizando Algoritmo Genético. 2009. Dissertação (Mestrado em Mestrado Acadêmico em Ciência da Computação) - Universidade Estadual do Ceará.',\n",
       "    '16.': 'HOLANDA FILHO, R.; SANTOS, Aldri L dos; Coelho, André Luis Carvalho.  Participação em banca de MARCUS FÁBIO FONTENELLE DO CARMO. CLASTRIN - Um Classificador de Tráfego de Aplicações Internet utilizando a Abordagem Um-Contra-Todos. 2009. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '17.': 'HOLANDA FILHO, R.; Pinheiro, Placido R.; Fernandez, Marcial P..  Participação em banca de Geneflides Laureno da Silva. Gateway de Voz para Integração IP-PSTN Aderente à Arquitetura das Redes de Nova Geração. 2009. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '18.': 'BRAYNER, A.; Coelho, André Luis Carvalho; Nakamura, Eduardo freire; HOLANDA FILHO, R..  Participação em banca de Karina Marinho de Souza. Processamento de Consultas em Redes de Sensores sem Fio: Uma Abordagem de Detecção de Novidades para o Controle da Qualidade dos Serviços das Consultas. 2009. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '19.': 'HOLANDA FILHO, R.; Silva, Jorge Luiz de C.; Bezerra, Francisco N..  Participação em banca de Gabriel Paulino Siqueira Júnior. Uma Metodologia para Identificação de Classes de Tráfego Baseada em Discriminantes Estatísticos e Análise de Agrupamentos. 2008. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '20.': 'HOLANDA FILHO, R.; BELCHIOR, A. D.; Falbo, Ricardo de A.; Farias, Pedro P. M..  Participação em banca de Karlson Bernardo de Oliveira. Aplicação da Estatística Multivariada para Apoiar a Avaliação Organizacional. 2008. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '21.': 'HOLANDA FILHO, R.; MENDONÇA, Nabor das Chagas; MACHADO, Javam de Castro.  Participação em banca de Fabricio Albuquerque Diógenes. Logmiddle: um middleware para o compartilhamento de dados em redes móveis ad hoc. 2006. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '22.': 'HOLANDA FILHO, R..  Participação em banca de Ricardo Régis Cavalcante Chaves. Uma arquitetura para ambientes virtuais colaborativos com dispositivos de force feedback. 2006. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'},\n",
       "   'Teses de doutorado': {'1.': 'Pinheiro, Placido R.; RABELO, RICARDO A.L.; HOLANDA FILHO, RAIMIR; RODRIGUES, J. J. P. C.; ALMEIDA, O. M.; CRUZ NETO, J. X..  Participação em banca de Jaclason Machado Veras. Home Energy Management System: A Multi-objective optimization Model for Scheduling Loads. 2019. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza.'},\n",
       "   'Trabalhos de conclusão de curso de graduação': {'1.': 'HOLANDA FILHO, R.; MAIA, J. E. B..  Participação em banca de Alvaro Garcia de Miguel.Estudo das Redes Mesh e sua Aplicação na Telemedicina.2011. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '2.': 'HOLANDA FILHO, R.; Brito, Wellington Alves; Rios, Clauson Sales do N..  Participação em banca de Dráulio Brasil Soares Neto.Estudo das Redes de Sensores sem Fio com um Estudo de Caso.2009. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '3.': 'HOLANDA FILHO, R.; Silva, Geneflides L.; MAIA, J. E. B..  Participação em banca de Gustavo Gurgel Nóbrega.QoS em Redes IP.2009. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '4.': 'HOLANDA FILHO, R.; GARCIA, Fernando Parente.  Participação em banca de Leandro Orofino Enck.Estudo Comparativo entre Algoritmos de Roteamento para Redes de Sensores sem Fio.2009. Trabalho de Conclusão de Curso (Graduação em Ciência da Computação)  - Universidade de Fortaleza.',\n",
       "    '5.': 'HOLANDA FILHO, R.; MAIA, J. E. B.; GARCIA, Fernando Parente.  Participação em banca de Thais Lucas da Rocha Sousa.Avaliação da Qualidade Percebida em Serviços de Telecomunicações.2009. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '6.': 'HOLANDA FILHO, R.; Brito, Wellington Alves; Alcocer, Juan C. A..  Participação em banca de Bráulio Crisóstomo de Quental.Transmissão de Dados sobre Redes Elétricas. Estudo de Caso do Projeto Piloto PLC - Coelce.2009. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '7.': 'HOLANDA FILHO, R..  Participação em banca de Emanoela de Jesus Lopes Soares.Redes de Acesso Opticas, HFC e HFW.2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '8.': 'HOLANDA FILHO, R..  Participação em banca de Noele Ingrid da Rosa Silva.Um estudo para convergência TV com outras plataformas para identificar padrões de interação do usuário com serviços de televisão.2007. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.',\n",
       "    '9.': 'HOLANDA FILHO, R..  Participação em banca de Claudio Franklin Mesquita Araújo.Gerência e operação de redes GSM.2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '10.': 'HOLANDA FILHO, R..  Participação em banca de Francisco Arnaldo de Araújo Filho.Segurança em redes sem fio 802.11x.2007. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.',\n",
       "    '11.': 'MAIA, J. E. B.; Antonio Macilio Pereira de Lucena; HOLANDA FILHO, R..  Participação em banca de Liana de Andrade Gomes.Evolucao das Redes GSM para UMTS (3G).2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '12.': 'MAIA, J. E. B.; GARCIA, Fernando Parente; HOLANDA FILHO, R..  Participação em banca de Naiana Edilma Coelho de Freitas.Metodologia em Gestão de Serviços em Telecomunicações e Tecnologia da Informação.2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '13.': 'Colares, Ricardo Fialho; HOLANDA FILHO, R.; Brito, Wellington Alves.  Participação em banca de Diego Buarque Mancera.H.323: Conceitos, Aplicações e Integração com Asterisk.2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '14.': 'GARCIA, Fernando Parente; HOLANDA FILHO, R..  Participação em banca de Samuel Limaverde Verissimo.Redes WMAN sem Fio: Cenários de utilização das tecnologias WiMAX e WiMesh.2007. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.',\n",
       "    '15.': 'HOLANDA FILHO, R.; GARCIA, Fernando Parente.  Participação em banca de Igor Louback de Castro Moura.Implementação de um aplicativo WEB para gerência de redes sem fio.2006. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.',\n",
       "    '16.': 'HOLANDA FILHO, R..  Participação em banca de Marcus Antonio Borges Sales.Tecnologias de acesso fixo sem fio para telecomunicações.2006. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.',\n",
       "    '17.': 'MAIA, J. E. B.; HOLANDA FILHO, R..  Participação em banca de Danilo Roosevelt Perdigão Coimbra.Uma revisao das tecnologias de sistemas de armazenamento de dados.2006. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.'}},\n",
       "  'Participação em bancas de comissões julgadoras': {'Outras participações': {'1.': 'HOLANDA FILHO, R.; Pinheiro, Placido R.; Vasco, J.J. Peixoto Furtado; MENDONÇA, Nabor das Chagas. Comissão de Seleção da Turma XI do MIA.2008.Universidade de Fortaleza.',\n",
       "    '2.': 'HOLANDA FILHO, R.. Membro do Comitê Científico do XII Seminário Apec.2007.Universidade de Fortaleza.',\n",
       "    '3.': 'HOLANDA FILHO, R.. Parecerista no VII Encontro de Pós-Graduação e Pesquisa.2007.Universidade de Fortaleza.'}}},\n",
       " 'Orientações': {'Orientações e supervisões em andamento': {'Dissertação de mestrado': {'1.': 'Marcele Pinho de Arruda Mapurunga. Avaliação do nível de maturidade em transformação digital - estudo de caso em uma organização pública.2022.Dissertação  (Mestrado em Administração)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '2.': 'Pedro Gabriel Caliope Dantas Pinheiro. INTEGRAÇÃO DE IOT E ROBÓTICA MÓVEL EM UMA APLICAÇÃO DE MONITORAMENTO DE AMBIENTES.2019.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '3.': 'Raniere Rocha Guimaraes. Detecção de Anomalias em Redes de Sensores Sem Fio utilizando Agrupamento de Dados Baseado em Floresta de Caminhos Ótimos.2018.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '4.': 'Patrick Robson Saldanha Vasconcelos. Salus Monitum: Um Sistema de Alertas na Nuvem Aplicado em Soluções IoT Voltadas à Saúde.2018.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '5.': 'Carlinha Santos Fujiwara. RECORDATUS: Uma Solução Móvel baseada em Internet das Coisas para Suporte a Adultos com TDAH.2017.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '6.': 'Kerllon Fontenele de Andrade. Discriminação de Tráfego P2P Utilizando Árvores de Decisão e Naive Bayes.2014.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '7.': 'Alex Lacerda Ramos. Sensor Data Security Estimator: um Framework para Estimativa do Nível de Segurança dos dados de Redes de Sensores sem Fio.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '8.': 'Rodrigo Augusto Rocha Souza Baluz. Uma Aplicação de Sistemas Inteligentes Híbridos ACO-Fuzzy para a Otimização do Desempenho em Redes de Sensores sem Fio.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '9.': 'Odecilia Barreira da Silva. C3M - Gerenciamento de Mudanças Estruturado em uma Metodologia de Multicritério.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '10.': 'Renato Rodrigues Marinho. Detecção de Botnets P2P baseada em Discriminantes de Sub-Fluxos.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '11.': 'Leonardo Moura Leitão. NaturalCloud: Um Framework para integração de Rede de Sensores na Nuvem.2012.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Coorientador: Raimir Holanda Filho.',\n",
       "    '12.': 'HARILTON DA SILVA ARAÚJO. LARP: UM PROTOCOLO DE ROTEAMENTO TOLERANTE A FALHAS PARA REDES DE SENSORES SEM FIO.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Conselho Nacional de Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '13.': 'Helber Wagner da Silva. Um Esquema de Seleção de Rotas para o Balanceamento de Segurança e Desempenho em Redes em Malha sem Fio.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '14.': 'Denilson Cursino de Oliveira. Uma Sistematização para o Planejamento da Gerência de Mudanças em TI e Modelagem de Uma Ferramenta.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '15.': 'Liliam Barroso Leal. Uma Abordagem para Estimação da Qualidade de Rotas em Redes de Sensores sem Fio Multi-Sink Baseada em Sistemas Fuzzy Genéticos.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '16.': 'Victor Pasknel de Alencar Ribeiro. Classificação de tráfego online baseada em sub-fluxos.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Conselho Nacional de Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '17.': 'Gesiel Rios Lopes. GERAU: Um mecanismo de gerenciamento de segurança autonômico baseado em detecção de novidades e mudança de conceito..2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '18.': 'Marcus Vinicius de Sousa Lemos. Detecção de Intrusão em Redes de Sensores Sem Fio Utilizando uma Abordagem Colaborativa e Cross-layer.2010.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '19.': 'MARCUS FÁBIO FONTENELLE DO CARMO. CLASTRIN ? UM CLASSIFICADOR DE TRÁFEGO DE APLICAÇÕES INTERNET UTILIZANDO A ABORDAGEM ―UM-CONTRA-TODOS‖.2009.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Orientador: Raimir Holanda Filho.',\n",
       "    '20.': 'Geneflides Laureno da Silva. Gateway de Voz para Integração IP-PSTN Aderente à Arquitetura das Redes de Nova Geração.2009.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '21.': 'Gabriel Paulino Siqueira Júnior. Uma Metodologia para identificação de Classes de Tráfego baseada em discriminantes Estatísticos e Análise de Agrupamentos.2008.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '22.': 'karlson Bernardo de Oliveira. Uso da Estatistica Multivariada para Auxiliar a Avaliaçao Organizacional.2008.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Coorientador: Raimir Holanda Filho.'},\n",
       "   'Tese de doutorado': {'1.': 'Victor Pasknel de Alencar Ribeiro.A FAULT-TOLERANT AND SECURE ARCHITECTURE FOR KEY MANAGEMENT IN LORAWAN BASED ON PERMISSIONED BLOCKCHAIN.2022. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '2.': 'Paulo Regis Carneiro De Araujo.Uma proposta para Integração de Equipamentos Elétricos Legados a uma Smart Grid Utilizando Redes de Sensores Sem Fio.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '3.': 'Marcus Vinicius De Sousa Lemos.An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in the Heterogeneous Environments of Sensor Clouds.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '4.': 'Alex Lacerda Ramos.Network Security Metrics for the Internet of Things.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '5.': 'Harilton da Silva Araújo.Seleção de Rotas em Redes para Internet das Coisas baseada em Requisitos de Aplicações.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.'},\n",
       "   'Trabalho de conclusão de curso de graduação': {'1.': 'Alvaro Garcia de Miguel.Estudo das Redes Mesh e sua Aplicação na Telemedicina.2011.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '2.': 'Iagê Figueiredo de C. Teixeira.Tecnologia WIMAX: Um Estudo Comparativo.2009.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '3.': 'Ana Caroline de Arruda Amorim.Proposta de Arquitetura para prover QoS em Redes 4G.2007.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '4.': 'Jean Rodrigo Landim Costa.Autorização, Autenticação e Bilhetagem baseados no Protocolos SIP.2007.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '5.': 'Carlos Henrique Rodrigues Ximenes.Captura e Análise de Tráfego VoIP.2007.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '6.': 'Matheus Fechine de Moura.Um Estudo Comparativo de QoS em  Tráfego Vox sobre IP.2007.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '7.': 'Marcos Heleno Chagas Quixadá.Uma Proposta para Arquitetura de Qualidade de Serviço em Sistemas de TV Digital.2007.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '8.': 'André Luiz Vieira F.S. Amorim.Mobilidade e interatividade em sistemas de TV digital.2007.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '9.': 'Luciano Leal do Vale.Geração e caracterização de tráfego de aplicações P2P.2006.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.'}},\n",
       "  'Orientações e supervisões concluídas': {'Dissertação de mestrado': {'1.': 'Marcele Pinho de Arruda Mapurunga. Avaliação do nível de maturidade em transformação digital - estudo de caso em uma organização pública.2022.Dissertação  (Mestrado em Administração)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '2.': 'Pedro Gabriel Caliope Dantas Pinheiro. INTEGRAÇÃO DE IOT E ROBÓTICA MÓVEL EM UMA APLICAÇÃO DE MONITORAMENTO DE AMBIENTES.2019.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '3.': 'Raniere Rocha Guimaraes. Detecção de Anomalias em Redes de Sensores Sem Fio utilizando Agrupamento de Dados Baseado em Floresta de Caminhos Ótimos.2018.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '4.': 'Patrick Robson Saldanha Vasconcelos. Salus Monitum: Um Sistema de Alertas na Nuvem Aplicado em Soluções IoT Voltadas à Saúde.2018.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '5.': 'Carlinha Santos Fujiwara. RECORDATUS: Uma Solução Móvel baseada em Internet das Coisas para Suporte a Adultos com TDAH.2017.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '6.': 'Kerllon Fontenele de Andrade. Discriminação de Tráfego P2P Utilizando Árvores de Decisão e Naive Bayes.2014.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '7.': 'Alex Lacerda Ramos. Sensor Data Security Estimator: um Framework para Estimativa do Nível de Segurança dos dados de Redes de Sensores sem Fio.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '8.': 'Rodrigo Augusto Rocha Souza Baluz. Uma Aplicação de Sistemas Inteligentes Híbridos ACO-Fuzzy para a Otimização do Desempenho em Redes de Sensores sem Fio.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '9.': 'Odecilia Barreira da Silva. C3M - Gerenciamento de Mudanças Estruturado em uma Metodologia de Multicritério.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '10.': 'Renato Rodrigues Marinho. Detecção de Botnets P2P baseada em Discriminantes de Sub-Fluxos.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '11.': 'Leonardo Moura Leitão. NaturalCloud: Um Framework para integração de Rede de Sensores na Nuvem.2012.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Coorientador: Raimir Holanda Filho.',\n",
       "    '12.': 'HARILTON DA SILVA ARAÚJO. LARP: UM PROTOCOLO DE ROTEAMENTO TOLERANTE A FALHAS PARA REDES DE SENSORES SEM FIO.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Conselho Nacional de Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '13.': 'Helber Wagner da Silva. Um Esquema de Seleção de Rotas para o Balanceamento de Segurança e Desempenho em Redes em Malha sem Fio.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '14.': 'Denilson Cursino de Oliveira. Uma Sistematização para o Planejamento da Gerência de Mudanças em TI e Modelagem de Uma Ferramenta.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '15.': 'Liliam Barroso Leal. Uma Abordagem para Estimação da Qualidade de Rotas em Redes de Sensores sem Fio Multi-Sink Baseada em Sistemas Fuzzy Genéticos.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '16.': 'Victor Pasknel de Alencar Ribeiro. Classificação de tráfego online baseada em sub-fluxos.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Conselho Nacional de Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '17.': 'Gesiel Rios Lopes. GERAU: Um mecanismo de gerenciamento de segurança autonômico baseado em detecção de novidades e mudança de conceito..2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '18.': 'Marcus Vinicius de Sousa Lemos. Detecção de Intrusão em Redes de Sensores Sem Fio Utilizando uma Abordagem Colaborativa e Cross-layer.2010.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '19.': 'MARCUS FÁBIO FONTENELLE DO CARMO. CLASTRIN ? UM CLASSIFICADOR DE TRÁFEGO DE APLICAÇÕES INTERNET UTILIZANDO A ABORDAGEM ―UM-CONTRA-TODOS‖.2009.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Orientador: Raimir Holanda Filho.',\n",
       "    '20.': 'Geneflides Laureno da Silva. Gateway de Voz para Integração IP-PSTN Aderente à Arquitetura das Redes de Nova Geração.2009.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '21.': 'Gabriel Paulino Siqueira Júnior. Uma Metodologia para identificação de Classes de Tráfego baseada em discriminantes Estatísticos e Análise de Agrupamentos.2008.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '22.': 'karlson Bernardo de Oliveira. Uso da Estatistica Multivariada para Auxiliar a Avaliaçao Organizacional.2008.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Coorientador: Raimir Holanda Filho.'},\n",
       "   'Tese de doutorado': {'1.': 'Victor Pasknel de Alencar Ribeiro.A FAULT-TOLERANT AND SECURE ARCHITECTURE FOR KEY MANAGEMENT IN LORAWAN BASED ON PERMISSIONED BLOCKCHAIN.2022. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '2.': 'Paulo Regis Carneiro De Araujo.Uma proposta para Integração de Equipamentos Elétricos Legados a uma Smart Grid Utilizando Redes de Sensores Sem Fio.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '3.': 'Marcus Vinicius De Sousa Lemos.An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in the Heterogeneous Environments of Sensor Clouds.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '4.': 'Alex Lacerda Ramos.Network Security Metrics for the Internet of Things.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '5.': 'Harilton da Silva Araújo.Seleção de Rotas em Redes para Internet das Coisas baseada em Requisitos de Aplicações.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.'},\n",
       "   'Trabalho de conclusão de curso de graduação': {'1.': 'Alvaro Garcia de Miguel.Estudo das Redes Mesh e sua Aplicação na Telemedicina.2011.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '2.': 'Iagê Figueiredo de C. Teixeira.Tecnologia WIMAX: Um Estudo Comparativo.2009.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '3.': 'Ana Caroline de Arruda Amorim.Proposta de Arquitetura para prover QoS em Redes 4G.2007.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '4.': 'Jean Rodrigo Landim Costa.Autorização, Autenticação e Bilhetagem baseados no Protocolos SIP.2007.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '5.': 'Carlos Henrique Rodrigues Ximenes.Captura e Análise de Tráfego VoIP.2007.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '6.': 'Matheus Fechine de Moura.Um Estudo Comparativo de QoS em  Tráfego Vox sobre IP.2007.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '7.': 'Marcos Heleno Chagas Quixadá.Uma Proposta para Arquitetura de Qualidade de Serviço em Sistemas de TV Digital.2007.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '8.': 'André Luiz Vieira F.S. Amorim.Mobilidade e interatividade em sistemas de TV digital.2007.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '9.': 'Luciano Leal do Vale.Geração e caracterização de tráfego de aplicações P2P.2006.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.'}}},\n",
       " 'JCR2': [{'jcr-ano': 'JCR 2021',\n",
       "   'doi': 'http://dx.doi.org/10.1007/s12652-023-04555-3',\n",
       "   'data-issn': '18685137',\n",
       "   'original_title': 'Fator de impacto (JCR 2021): 3.662',\n",
       "   'impact-factor': '3.662'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': 'http://dx.doi.org/10.1109/ACCESS.2023.3260020',\n",
       "   'data-issn': '21693536',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9'},\n",
       "  {'doi': '10.1016/j.procs.2023.03.018',\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '18770509',\n",
       "   'volume': '220',\n",
       "   'paginaInicial': '119',\n",
       "   'titulo': 'Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain',\n",
       "   'sequencial': '3',\n",
       "   'nomePeriodico': 'PROCEDIA COMPUTER SCIENCE'},\n",
       "  {'doi': '10.9790/487X-2507032735',\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '2278487X',\n",
       "   'volume': '25',\n",
       "   'paginaInicial': '27',\n",
       "   'titulo': 'Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector',\n",
       "   'sequencial': '4',\n",
       "   'nomePeriodico': 'Journal of Business and Management'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1109/ACCESS.2022.3179004',\n",
       "   'data-issn': '21693536',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '21693536',\n",
       "   'volume': '1',\n",
       "   'paginaInicial': '1',\n",
       "   'titulo': 'A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain',\n",
       "   'sequencial': '5',\n",
       "   'nomePeriodico': 'IEEE Access'},\n",
       "  {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '22360700',\n",
       "   'volume': '13',\n",
       "   'paginaInicial': '1',\n",
       "   'titulo': 'Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública',\n",
       "   'sequencial': '6',\n",
       "   'nomePeriodico': 'Revista Razão Contábil '},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/app12188939',\n",
       "   'data-issn': '20763417',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 2.7',\n",
       "   'impact-factor': '2.7',\n",
       "   'issn': '20763417',\n",
       "   'volume': '12',\n",
       "   'paginaInicial': '8939',\n",
       "   'titulo': 'Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19',\n",
       "   'sequencial': '7',\n",
       "   'nomePeriodico': 'Applied Sciences-Basel'},\n",
       "  {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '22360700',\n",
       "   'volume': '12',\n",
       "   'paginaInicial': '1',\n",
       "   'titulo': 'Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará',\n",
       "   'sequencial': '8',\n",
       "   'nomePeriodico': 'Revista Razão Contábil '},\n",
       "  {'doi': '10.1007/978-3-030-84311-3_2',\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '22138684',\n",
       "   'volume': '2021',\n",
       "   'paginaInicial': '13',\n",
       "   'titulo': 'Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19',\n",
       "   'sequencial': '9',\n",
       "   'nomePeriodico': 'Springer Proceedings in Complexity'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/app112110457',\n",
       "   'data-issn': '20763417',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 2.7',\n",
       "   'impact-factor': '2.7',\n",
       "   'issn': '20763417',\n",
       "   'volume': '11',\n",
       "   'paginaInicial': '10457',\n",
       "   'titulo': 'An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus',\n",
       "   'sequencial': '10',\n",
       "   'nomePeriodico': 'Applied Sciences-Basel'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/s20113068',\n",
       "   'data-issn': '14248220',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '14248220',\n",
       "   'volume': '20',\n",
       "   'paginaInicial': '3068',\n",
       "   'titulo': 'Enhancing Key Management in LoRaWAN with Permissioned Blockchain',\n",
       "   'sequencial': '11',\n",
       "   'nomePeriodico': 'SENSORS'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1109/access.2020.3009167',\n",
       "   'data-issn': '21693536',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '21693536',\n",
       "   'volume': '1',\n",
       "   'paginaInicial': '1',\n",
       "   'titulo': 'Integration of the Mobile Robot and Internet of Things to Monitor Older People',\n",
       "   'sequencial': '12',\n",
       "   'nomePeriodico': 'IEEE Access'},\n",
       "  {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '22360700',\n",
       "   'volume': '11',\n",
       "   'paginaInicial': '1',\n",
       "   'titulo': 'Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019',\n",
       "   'sequencial': '13',\n",
       "   'nomePeriodico': 'Revista Razão Contábil '},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1002/nem.2062',\n",
       "   'data-issn': '10557148',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 1.5',\n",
       "   'impact-factor': '1.5',\n",
       "   'issn': '10557148',\n",
       "   'volume': '1',\n",
       "   'paginaInicial': 'e2062',\n",
       "   'titulo': 'An approach for provisioning virtual sensors in sensor clouds',\n",
       "   'sequencial': '14',\n",
       "   'nomePeriodico': 'International Journal of Network Management'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1109/jiot.2019.2904302',\n",
       "   'data-issn': '23274662',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 10.6',\n",
       "   'impact-factor': '10.6',\n",
       "   'issn': '23274662',\n",
       "   'volume': '6',\n",
       "   'paginaInicial': '5631',\n",
       "   'titulo': 'Enabling Online Quantitative Security Analysis in 6LoWPAN Networks',\n",
       "   'sequencial': '15',\n",
       "   'nomePeriodico': 'IEEE Internet of Things Journal'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1109/mnet.2018.1800151',\n",
       "   'data-issn': '08908044',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 9.3',\n",
       "   'impact-factor': '9.3',\n",
       "   'issn': '08908044',\n",
       "   'volume': '33',\n",
       "   'paginaInicial': '126',\n",
       "   'titulo': 'Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering',\n",
       "   'sequencial': '16',\n",
       "   'nomePeriodico': 'IEEE NETWORK'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/s18020353',\n",
       "   'data-issn': '14248220',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '14248220',\n",
       "   'volume': '18',\n",
       "   'paginaInicial': '353',\n",
       "   'titulo': 'A Proposal for IoT Dynamic Routes Selection Based on Contextual Information',\n",
       "   'sequencial': '17',\n",
       "   'nomePeriodico': 'SENSORS'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1002/dac.3380',\n",
       "   'data-issn': '10745351',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 2.1',\n",
       "   'impact-factor': '2.1',\n",
       "   'issn': '10745351',\n",
       "   'volume': '31',\n",
       "   'paginaInicial': 'e3380',\n",
       "   'titulo': 'Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks',\n",
       "   'sequencial': '18',\n",
       "   'nomePeriodico': 'INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1016/j.jnca.2018.01.015',\n",
       "   'data-issn': '10848045',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 8.7',\n",
       "   'impact-factor': '8.7',\n",
       "   'issn': '10848045',\n",
       "   'volume': '107',\n",
       "   'paginaInicial': '56',\n",
       "   'titulo': 'A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs',\n",
       "   'sequencial': '19',\n",
       "   'nomePeriodico': 'JOURNAL OF NETWORK AND COMPUTER APPLICATIONS'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/s18030689',\n",
       "   'data-issn': '14248220',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '14248220',\n",
       "   'volume': '18',\n",
       "   'paginaInicial': '689',\n",
       "   'titulo': 'An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments',\n",
       "   'sequencial': '20',\n",
       "   'nomePeriodico': 'SENSORS'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/s18051312',\n",
       "   'data-issn': '14248220',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '14248220',\n",
       "   'volume': '18',\n",
       "   'paginaInicial': '1312',\n",
       "   'titulo': 'Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks',\n",
       "   'sequencial': '21',\n",
       "   'nomePeriodico': 'SENSORS'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1109/comst.2017.2745505',\n",
       "   'data-issn': '1553877X',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 35.6',\n",
       "   'impact-factor': '35.6',\n",
       "   'issn': '1553877X',\n",
       "   'volume': '19',\n",
       "   'paginaInicial': '2704',\n",
       "   'titulo': 'Model-Based Quantitative Network Security Metrics: A Survey',\n",
       "   'sequencial': '22',\n",
       "   'nomePeriodico': 'IEEE Communications Surveys and Tutorials'},\n",
       "  {'doi': '10.18464/cybin.v3i1',\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '24942715',\n",
       "   'volume': '3',\n",
       "   'paginaInicial': '30',\n",
       "   'titulo': 'Exploring a P2P Transient Botnet - From Discovery to Enumeration',\n",
       "   'sequencial': '23',\n",
       "   'nomePeriodico': 'The Journal on Cybercrime '},\n",
       "  {'doi': '10.21528/lnlm-vol14-no1-art1',\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '16762789',\n",
       "   'volume': '14',\n",
       "   'paginaInicial': '4',\n",
       "   'titulo': 'UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES',\n",
       "   'sequencial': '24',\n",
       "   'nomePeriodico': 'LEARNING AND NONLINEAR MODELS'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/s150102104',\n",
       "   'data-issn': '14248220',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '14248220',\n",
       "   'volume': '15',\n",
       "   'paginaInicial': '2104',\n",
       "   'titulo': 'Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks',\n",
       "   'sequencial': '25',\n",
       "   'nomePeriodico': 'SENSORS'},\n",
       "  {'jcr-ano': 'JCR 2018',\n",
       "   'doi': '10.1155/2014/506203',\n",
       "   'data-issn': '15501329',\n",
       "   'original_title': 'Fator de impacto (JCR 2018): 1.614',\n",
       "   'impact-factor': '1.614',\n",
       "   'issn': '15501329',\n",
       "   'volume': '2014',\n",
       "   'paginaInicial': '1',\n",
       "   'titulo': 'A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions',\n",
       "   'sequencial': '26',\n",
       "   'nomePeriodico': 'International Journal of Distributed Sensor Networks'},\n",
       "  {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '15487709',\n",
       "   'volume': '10',\n",
       "   'paginaInicial': '702',\n",
       "   'titulo': 'A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks',\n",
       "   'sequencial': '27',\n",
       "   'nomePeriodico': 'JOURNAL OF COMMUNICATION AND COMPUTER'},\n",
       "  {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '16762789',\n",
       "   'volume': '10',\n",
       "   'paginaInicial': '4',\n",
       "   'titulo': 'Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink',\n",
       "   'sequencial': '28',\n",
       "   'nomePeriodico': 'Learning and Nonlinear Models'},\n",
       "  {'doi': '10.3233/HIS-2012-0147',\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '14485869',\n",
       "   'volume': '9',\n",
       "   'paginaInicial': '61',\n",
       "   'titulo': 'An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks',\n",
       "   'sequencial': '29',\n",
       "   'nomePeriodico': 'INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS'},\n",
       "  {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1016/j.inffus.2012.01.010',\n",
       "   'data-issn': '15662535',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 18.6',\n",
       "   'impact-factor': '18.6',\n",
       "   'issn': '15662535',\n",
       "   'volume': '15',\n",
       "   'paginaInicial': '44',\n",
       "   'titulo': 'On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries',\n",
       "   'sequencial': '30',\n",
       "   'nomePeriodico': 'Information Fusion (Print)'},\n",
       "  {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '1980086X',\n",
       "   'volume': 'X',\n",
       "   'paginaInicial': '65',\n",
       "   'titulo': 'Controle Externo da Governança de Tecnologia da Informação',\n",
       "   'sequencial': '31',\n",
       "   'nomePeriodico': 'Revista Controle'},\n",
       "  {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '01018191',\n",
       "   'volume': '28',\n",
       "   'paginaInicial': '33',\n",
       "   'titulo': 'Detecting computer network attacks using statistical discriminators and cluster analysis',\n",
       "   'sequencial': '32',\n",
       "   'nomePeriodico': 'Revista Tecnologia (UNIFOR)'},\n",
       "  {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '01018191',\n",
       "   'volume': '27',\n",
       "   'paginaInicial': '113',\n",
       "   'titulo': 'Broadband network traffic characterization and classification using a multivariate statistical method',\n",
       "   'sequencial': '33',\n",
       "   'nomePeriodico': 'Revista Tecnologia (UNIFOR)'},\n",
       "  {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '14145685',\n",
       "   'volume': '8o.',\n",
       "   'titulo': 'Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários',\n",
       "   'sequencial': '34',\n",
       "   'nomePeriodico': 'Revista Brasileira de Informática na Educação'}]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_tit2_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Eventos': {'Participação em eventos, congressos, exposições e feiras': {'1.': 'IEEE International Conference on Engineering Veracruz. Explainable Intelligence: Opportunities and Challenges. 2021. (Congresso).',\n",
       "   '2.': 'Mundo Unifor.Panorama Atual das Criptomoedas: da ilegalidade à adoção como moeda oficial.2021. (Encontro).',\n",
       "   '3.': 'Encontro Unificado de Computação em Parnaíba.Análise de Protocolos de Roteamento em RSSF: uma abordagem hierárquica utilizando agregaçao de dados.2009. (Encontro).',\n",
       "   '4.': 'Escola Regional de Computação: Ceará, Maranhão e Piauí.Simulação em RSSF oara Protocolos de Roteamento usando uma Abordagem Geocast.2009. (Encontro).',\n",
       "   '5.': 'Mundo Unifor 2009.Tecnologia de Redes de Sensores sem Fio.2009. (Encontro).',\n",
       "   '6.': 'Congresso Ceará de Gestão Pública.Gestão Pública.2008. (Encontro).',\n",
       "   '7.': 'Escola Regional de Computação: Ceará, Maranhão e Piauí.ERCEMAPI 2008.2008. (Encontro).',\n",
       "   '8.': '6a Semana de Tecnologia da UNIFOR.Novas Tecnologias de Redes sem Fio.2007. (Encontro).',\n",
       "   '9.': 'Escola Regional de Computação: Ceará, Maranhão e Piauí.Administração Avançada de Redes.2007. (Encontro).',\n",
       "   '10.': 'XIII Encontro de Iniciação à Pesquisa da Unifor.Sistema de Detecção de Ataques baseado em Anomalias de Tráfego.2007. (Encontro).',\n",
       "   '11.': 'Curso AWFSS - Aironet Wireless LAN Fundamentals and Cisco Aironet Wireless Site Survey.Curso AWFSS - Aironet Wireless LAN Fundamentals and Cisco Aironet Wireless Site Survey. 2006. (Outra).',\n",
       "   '12.': 'First Euro-NGI Summer School.Traffic Engineering for the Next Generation Internet. 2004. (Oficina).',\n",
       "   '13.': 'INTEL Internship Program.INTEL Internship Program. 2004. (Outra).',\n",
       "   '14.': 'Curso CCNA 4 - Wan Technologies.Curso CCNA 4 - Wan Technologies. 2003. (Outra).',\n",
       "   '15.': 'Curso CISCO CCNA 1.Curso CISCO CCNA 1. 2003. (Outra).',\n",
       "   '16.': 'Curso CISCO CCNA 2 - Routers and Routing Basics.Curso CISCO CCNA 2 - Routers and Routing Basics. 2003. (Outra).',\n",
       "   '17.': 'Curso CISCO CCNA 3 - Switching Basics and Intermediate Routing.Curso CISCO CCNA 3 - Switching Basics and Intermediate Routing. 2003. (Outra).',\n",
       "   '18.': 'COST 279 First European Summer School.Fundamentals of Wireless Networks. 2002. (Oficina).',\n",
       "   '19.': 'COST 279 First European Summer School.Queueing Systems and Loss Networks. 2002. (Oficina).',\n",
       "   '20.': 'COST 279 First European Summer School.Spatial Queueing Models. 2002. (Oficina).',\n",
       "   '21.': 'Curso Administering Unicenter TNG Asset Management Option / AimIT.Curso Administering Unicenter TNG Asset Management Option / AimIT. 2001. (Outra).',\n",
       "   '22.': 'Curso Administering Unicenter TNG Software Delivery Option / ShipIT.Curso Administering Unicenter TNG Software Delivery Option / ShipIT. 2001. (Outra).',\n",
       "   '23.': 'Curso ArcserverIT Administration Fundamentals.Curso ArcserverIT Administration Fundamentals. 2001. (Outra).',\n",
       "   '24.': 'Curso E Trust Admin Seminar.Curso E Trust Admin Seminar. 2001. (Outra).',\n",
       "   '25.': 'Curso Unicenter TNG Basics.Curso Unicenter TNG Basics. 2001. (Outra).',\n",
       "   '26.': 'Curso Unicenter TNG Remote Control Option.Curso Unicenter TNG Remote Control Option. 2001. (Outra).',\n",
       "   '27.': 'I Encontro de Pós-Graduação e Pesquisa.Trabalho Científico Intitulado Utilização da Tecnologia de Agentes Inteligentes para a Construção de Ambientes Colaborativos de Ensino-Aprendizagem.2001. (Encontro).',\n",
       "   '28.': 'XV Encontro de Pesquisa Educacional das Regiões Norte e Nordeste.Encontro de Pesquisa Educacional das Regiões Norte e Nordeste.2001. (Encontro).',\n",
       "   '29.': '5a. Semana Insoft de Tecnologia da Informação.5a. Semana Insoft de tecnologia da Informação.2000. (Encontro).',\n",
       "   '30.': 'Curso Lotus.Curso Implementando uma infra-estrutura DOMINO. 2000. (Outra).',\n",
       "   '31.': 'Projeto de Pesquisa Tele-Ambiente (Protem CC-IE).Palestra com o tema Ambientes Virtuais Cooperativos. 2000. (Oficina).',\n",
       "   '32.': 'Simpósio Brasilieiro de Informática na Educação.XI Simpósio Brasileiro de Informática na Educação.2000. (Simpósio).',\n",
       "   '33.': 'Curso.Curso Lotus Notes - System Administration 2. 1999. (Outra).'},\n",
       "  'Organização de eventos, congressos, exposições e feiras': {'1.': 'HOLANDA FILHO, R.. Euro-Par 2008 Conference. 2008. (Congresso).',\n",
       "   '2.': 'HOLANDA FILHO, R.. ERCEMAPI - Escola Regional de Computacao Ceará-Maranhao-Piaui. 2007. (Congresso).',\n",
       "   '3.': 'HOLANDA FILHO, R.. XII Seminario Associacao dos Estudantes e Pesquisadores Brasileiros na Catalunha. 2007. (Outro).'}}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_tit3_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = aggregate_data_dicts(soup)\n",
    "# pprint(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': 'Person',\n",
       " 'name': 'Raimir Holanda Filho',\n",
       " 'InfPes': ['Raimir Holanda Filho',\n",
       "  'Bolsista de Produtividade em Pesquisa do CNPq - Nível 2',\n",
       "  'Endereço para acessar este CV: http://lattes.cnpq.br/2607811863279622',\n",
       "  'ID Lattes: 2607811863279622',\n",
       "  'Última atualização do currículo em 05/09/2023'],\n",
       " 'Resumo': ['Pós-Doutor em Computação pela Sorbonne Université-Pierre et Marie Curie (França - 2020). Doutor em Ciência da Computação pela Universitat Politecnica de Catalunya (Espanha - 2005). Atualmente é professor titular da Universidade de Fortaleza - UNIFOR na qual é membro permanente dos programas de pós-graduação em Informática Aplicada (mestrado e doutorado) e do mestrado profissional em administração. Possui mais de 100 trabalhos publicados em conferências e periódicos nacionais e internacionais. Tem experiência na área de Ciência da Computação, com ênfase em Ciência de Dados e Teleinformática, atuando principalmente nos seguintes temas: Redes Complexas, Grafos de Conhecimento, Aprendizagem de Máquina, Internet das Coisas, Segurança e Blockchain. (Texto informado pelo autor)'],\n",
       " 'Identificação': {'Nome': 'Raimir Holanda Filho',\n",
       "  'Nome em citações bibliográficas': 'HOLANDA FILHO, R.;FILHO, RAIMIR HOLANDA;FILHO, RAIMIR;HOLANDA, RAIMIR;HOLANDA FILHO, RAIMIR;FILHO, RAIMIR H.',\n",
       "  'Lattes iD': 'http%3A//lattes.cnpq.br/2607811863279622'},\n",
       " 'Endereço': {'Endereço Profissional': 'Universidade de Fortaleza, Mestrado em Informatica Aplicada, Mestrado Em Informatica Aplicada. Av. Washington Soares,1321Edson Queiroz60811341 - Fortaleza, CE - BrasilTelefone: (85) 34773268URL da Homepage: http%3A//www.unifor.br'},\n",
       " 'Formação acadêmica/titulação': {'2001 - 2005': 'Doutorado em Ciência da Computação. Universitat Politècnica de Catalunya, UPC, Espanha.  Título: A New Methodology for Packet Trace Classification and Compression based on Semantic Traffic Characterization, Ano de obtenção: 2005. Orientador: Jorge Garcia Vidal. Bolsista do(a): Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES, Brasil. Palavras-chave: Traffic Characterization; TCP flow clustering; Performance; Packet trace classification; Compression.Grande área: Ciências Exatas e da TerraSetores de atividade: Educação Superior.',\n",
       "  '1996 - 1998': 'Mestrado em Ciências da Computação. Universidade Federal do Ceará, UFC, Brasil.  Título: SAGRES - Um Sistema Baseado em Conhecimento para Apoia a Gerencia de Falhas em Redes de Computadores, Ano de Obtenção: 1998.Orientador: Mauro Oliveira.Palavras-chave: SAGRES; GERENCIA DE REDES; REDES DE COMPUTADORES; SISTEMAS BASEADOS EM CONHECIMENTO; GERENCIA DE FALHAS.Grande área: Ciências Exatas e da TerraSetores de atividade: Educação; Informática.',\n",
       "  '1994 - 1994': 'Especialização em Informatica.  (Carga Horária: 360h). Universidade Federal do Ceará, UFC, Brasil.',\n",
       "  '1985 - 1989': 'Graduação em Engenharia Civil. Universidade Federal do Ceará, UFC, Brasil.'},\n",
       " 'Pós-doutorado': {'2020 - 2020': 'Pós-Doutorado. Université Pierre et Marie Curie, UPMC, França.  Grande área: Ciências Exatas e da TerraGrande Área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Machine Learning.'},\n",
       " 'Formação Complementar': {},\n",
       " 'Atuação Profissional': {'Universidade de Fortaleza, UNIFOR, Brasil.': [{'Vínculo institucional': '',\n",
       "    '1995 - Atual': 'Vínculo: , Enquadramento Funcional: Professor titular, Carga horária: 40',\n",
       "    'Atividades': '',\n",
       "    '04/2008 - Atual': 'Conselhos, Comissões e Consultoria, Reitoria, Conselho de Centro do CCT.',\n",
       "    '': 'Treinamentos ministradosTecnologias de Redes sem Fio',\n",
       "    '03/2006 - Atual': 'Ensino, Informática Aplicada, Nível: Pós-Graduação',\n",
       "    '02/2006 - Atual': 'Ensino, Engenharia de Telecomunicações, Nível: Graduação',\n",
       "    '10/2005 - Atual': 'Pesquisa e desenvolvimento, Mestrado em Informatica Aplicada.',\n",
       "    '3/1995 - Atual': 'Ensino, Informatica, Nível: Graduação',\n",
       "    '1/2006 - 1/2006': 'Treinamentos ministrados , Mestrado em Informatica Aplicada, Mestrado Em Informatica Aplicada.'}]},\n",
       " 'Linhas de pesquisa': {'1.': 'Redes de Comunicação'},\n",
       " 'Projetos de pesquisa': {'2021 - Atual': 'Monitor Fiscal TCE/CE',\n",
       "  '': 'Descrição: ESTUDO E DESENVOLVIMENTO DE SISTEMAS DE DETECÇÃO/PREVENÇÃO DE INTRUSÃO BASEADOS EM REGRAS PARA REDES EM AMBIENTE SEM FIO.. Situação: Concluído; Natureza: Pesquisa. Alunos envolvidos: Graduação: (3)  / Especialização: (0)  / Mestrado acadêmico: (1)  / Mestrado profissional: (0)  / Doutorado: (0) . Integrantes: Raimir Holanda Filho - Coordenador.',\n",
       "  '2011 - 2014': 'CIA2-Construindo Cidades Inteligentes da Instrumentação dos Ambientes ao Desenvolvimento de Aplicações',\n",
       "  '2010 - 2010': 'DeLAtoR - Detecção e localização de disparos de armas baseado em redes de sensores sem fio',\n",
       "  '2009 - 2011': 'MOVERES: Sistema de Monitoramento Veicular Através de Redes de Sensores sem Fio. Projeto financiado pela FINEP (Edital Subvenção Economica a Inovacao - 01/2007)',\n",
       "  '2008 - 2011': 'Especificaçao de uma arquitetura de gerenciamento distribuído para RSSF. Projeto de Pesquisa cadastrado na Unifor.',\n",
       "  '2008 - 2010': 'FATADIST - Uma Ferramenta para Classificação de Tráfego de Ataques. Projeto financiado com recursos do Edital AT - CNPq',\n",
       "  '2007 - 2009': 'CLASTRIN - Um Classificador de Tráfego Internet baseado em Discriminantes Estatísticos. Projeto financiado com recursos do Edital Universal - CNPq',\n",
       "  '2007 - 2008': 'Sistema de Detecção de ataques DoS (Denial of Service) em redes móveis através de técnicas de modelagem.',\n",
       "  '2006 - 2008': 'Caracterização, classificação e modelagem de tráfego de redes convergentes',\n",
       "  '2005 - 2007': 'Telemedicina - Ampliaçao de Laboratório para Pesquisa e Desenvolvimento de Aplicações em Telemedicina. Projeto de pesquisa e desenvolvimento em tecnologia da informacao incentivado através da Lei de Informática. (Celéstica do Brasil Ltda)',\n",
       "  '2005 - 2006': 'Implantacao do laboratorio de Redes Convergentes. Projeto financiado com recursos da Lei de Informática (UNIFOR - SANMINA-SCI DO BRASIL INTEGRATION LTDA)'},\n",
       " 'Projetos de extensão': {'2009 - 2009': 'S2IPro - Sistema de Identificação Inequívoca do Proprietário',\n",
       "  '': 'Descrição: Sistema de Identificação Inequívoca do Proprietário. Situação: Concluído; Natureza: Extensão. Alunos envolvidos: Graduação: (2)  / Mestrado acadêmico: (1) . Integrantes: Raimir Holanda Filho - Coordenador / Luis Henrique Pequeno Almeida - Integrante.Financiador(es): Thinktech Ind e Comercio de Informatica Ltda - Auxílio financeiro.'},\n",
       " 'Projetos de desenvolvimento': {'2010 - 2012': 'Desenvolvimento de Software para Gerenciamento de Arquitetura de Telecomunicações de Baixo Custo para Comunidades Rurais, Pequenas Empresas, Órgãos e Instituições Municipais/Estaduais',\n",
       "  '': 'Situação: Em andamento; Natureza: Desenvolvimento. Alunos envolvidos: Graduação: (4)  / Mestrado acadêmico: (2) . Integrantes: Raimir Holanda Filho - Coordenador / Geneflides Laureno da Silva - Integrante.Financiador(es): Financiadora de Estudos e Projetos - Auxílio financeiro.'},\n",
       " 'Revisor de periódico': {'2020 - Atual': 'Periódico: IEEE Latin America Transactions',\n",
       "  '2021 - Atual': 'Periódico: Computer Networks'},\n",
       " 'Revisor de projeto de fomento': {'2015 - Atual': 'Agência de fomento: Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico'},\n",
       " 'Áreas de atuação': {'1.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Sistemas de Computação/Especialidade: Teleinformática.',\n",
       "  '2.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Security.',\n",
       "  '3.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Machine Learning.',\n",
       "  '4.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Natural Language Processing.',\n",
       "  '5.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Internet of Things.',\n",
       "  '6.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Blockchain.'},\n",
       " 'Idiomas': {'Inglês': 'Compreende Bem, Fala Bem, Lê Bem, Escreve Bem.',\n",
       "  'Espanhol': 'Compreende Bem, Fala Bem, Lê Bem, Escreve Bem.',\n",
       "  'Português': 'Compreende Bem, Fala Bem, Lê Bem, Escreve Bem.',\n",
       "  'Francês': 'Compreende Bem, Fala Razoavelmente, Lê Bem, Escreve Razoavelmente.'},\n",
       " 'Inovação': {'2021 - Atual': 'Monitor Fiscal TCE/CE',\n",
       "  '': 'Descrição: O projeto busca aprimorar os mecanismos e a capacidade de análise e projeção de dados das contas públicas estaduais e municipais já existentes no TCE/CE, mediante o desenvolvimento de novas ferramentas de sistematização e disponibilização de dados das contas públicas estaduais e municipais. A partir de técnicas de análise e integração de dados, em bases públicas e/ou custodiadas, serão utilizados modelos computacionais descritivos/preditivos para tentar antecipar possíveis deteriorações nas contas públicas estaduais e municipais. Deste modo o TCE/CE pode emitir alertas com maior frequência e antecedência.. Situação: Em andamento; Natureza: Pesquisa. Integrantes: Raimir Holanda Filho - Coordenador.Financiador(es): Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico - Auxílio financeiro.'},\n",
       " 'Produções': {'Produção bibliográfica': {'Citações': {'Database': 'Outras',\n",
       "    'Total de trabalhos': 210,\n",
       "    'Total de citações': 736,\n",
       "    'Índice_H': None,\n",
       "    'Data': '08/05/2023'},\n",
       "   'Artigos completos publicados em periódicos': {'1.': 'ANDRADE, E. C. ; PINHEIRO, L. I. ;Pinheiro, Placido R.; NUNES, L. C. ; PINHEIRO, M. C. D. ; PEREIRA, M. L. D. ; ABREU, W. C. ;HOLANDA FILHO, RAIMIR; SIMAO FILHO, M. ; PINHEIRO, P. G. C. D. ; NUNES, R. E. C. . Hybrid model for early identification post-Covid-19 sequelae. Journal of Ambient Intelligence and Humanized Computing, p. 1-14, 2023.',\n",
       "    '2.': 'MARINHO, RENATO ;HOLANDA, RAIMIR. Automated Emerging Cyber Threat Identification and Profiling Based on Natural Language Processing. IEEE Access, v. 1, p. 1-1, 2023.CitaÃ§Ãµes:',\n",
       "    '3.': 'FILHO, RAIMIR HOLANDA; DE SOUSA, DEBORA CARLA BARBOZA ; DE BRITO, WELLINGTON ALVES ; CHAVES, JOAN LUCAS MARQUES DE SOUSA ; SÁ, EMANUEL LEÃO ; RIBEIRO, VICTOR PASKNEL DE ALENCAR . Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain. PROCEDIA COMPUTER SCIENCE, v. 220, p. 119-126, 2023.',\n",
       "    '4.': 'HOLANDA FILHO, RAIMIR; CUNHA, G. H. M. ; RAVIOLO, B. P. Y. ; BRITO, R. W. C. . Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector. Journal of Business and Management, v. 25, p. 27-35, 2023.',\n",
       "    '5.': 'Ribeiro, Victor ;HOLANDA, RAIMIR; RAMOS, ALEX ; RODRIGUES, JOEL J. P. C. . A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain. IEEE Access, v. 1, p. 1-1, 2022.CitaÃ§Ãµes:',\n",
       "    '6.': 'MAPURUNGA, M. P. A. ;HOLANDA FILHO, RAIMIR. Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública. Revista Razão Contábil & Finanças, v. 13, p. 1-14, 2022.',\n",
       "    '7.': 'ANDRADE, EVANDRO CARVALHO DE ; PINHEIRO, PLÁCIDO ROGERIO ; BARROS, ANA LUIZA BESSA DE PAULA ; NUNES, LUCIANO COMIN ; PINHEIRO, LUANA IBIAPINA C. C. ; PINHEIRO, PEDRO GABRIEL CALÍOPE DANTAS ;HOLANDA FILHO, RAIMIR. Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19. Applied Sciences-Basel, v. 12, p. 8939, 2022.CitaÃ§Ãµes:',\n",
       "    '8.': 'LIMA, S. M. ; BRITO, R. W. ; MATOS, P. R. F. ; JESUS FILHO, J. ;HOLANDA FILHO, R.; CUNHA, G. H. M. . Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará. Revista Razão Contábil & Finanças, v. 12, p. 1, 2021.',\n",
       "    '9.': 'ANDRADE, E. C. ;Pinheiro, Placido R.;HOLANDA FILHO, R.; NUNES, L. C. ; PINHEIRO, M. C. D. ; ABREU, W. C. ; SIMAO FILHO, M. ; PINHEIRO, L. I. C. C. ; PEREIRA, M. L. D. ; PINHEIRO, P. G. C. D. . Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19. Springer Proceedings in Complexity, v. 2021, p. 13-24, 2021.',\n",
       "    '10.': 'PINHEIRO, LUANA I. C. C. ; PEREIRA, MARIA LÚCIA D. ; ANDRADE, EVANDRO C. DE ; NUNES, LUCIANO C. ; ABREU, WILSON C. DE ; PINHEIRO, PEDRO GABRIEL CALÍOPE D. ;HOLANDA FILHO, RAIMIR; PINHEIRO, PLÁCIDO ROGERIO . An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus. Applied Sciences-Basel, v. 11, p. 10457, 2021.CitaÃ§Ãµes:',\n",
       "    '11.': 'Ribeiro, Victor ;HOLANDA, RAIMIR; RAMOS, ALEX ; RODRIGUES, JOEL J. P. C. . Enhancing Key Management in LoRaWAN with Permissioned Blockchain. SENSORS, v. 20, p. 3068, 2020.CitaÃ§Ãµes:',\n",
       "    '12.': 'Pinheiro, Placido R.; PINHEIRO, PEDRO G. C. D. ;FILHO, RAIMIR H.; BARROZO, JOAO P. A. ; RODRIGUES, JOEL J. P. C. ; PINHEIRO, LUANA I. C. C. ; PEREIRA, MARIA L. D. . Integration of the Mobile Robot and Internet of Things to Monitor Older People. IEEE Access, v. 1, p. 1-1, 2020.CitaÃ§Ãµes:',\n",
       "    '13.': 'LIMA, S. M. ; BRITO, R. W. ; MATOS, P. R. F. ; JESUS FILHO, J. ;HOLANDA FILHO, R.. Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019. Revista Razão Contábil & Finanças, v. 11, p. 1, 2020.',\n",
       "    '14.': 'LEMOS, MARCUS ; RABELO, RICARDO ; MENDES, DOUGLAS ; CARVALHO, CARLOS ;HOLANDA, RAIMIR. An approach for provisioning virtual sensors in sensor clouds. International Journal of Network Management, v. 1, p. e2062, 2019.CitaÃ§Ãµes:',\n",
       "    '15.': 'RAMOS, ALEX ; MILFONT, RONALDO T. P. ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. . Enabling Online Quantitative Security Analysis in 6LoWPAN Networks. IEEE Internet of Things Journal, v. 6, p. 5631-5638, 2019.CitaÃ§Ãµes:',\n",
       "    '16.': 'GUIMARAES, RANIERE ROCHA ; PASSOS, LEANDRO A. ;FILHO, RAIMIR HOLANDA; ALBUQUERQUE, VICTOR HUGO C. DE ; RODRIGUES, JOEL J. P. C. ; KOMAROV, MIKHAIL M. ; PAPA, JOAO PAULO . Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering. IEEE NETWORK, v. 33, p. 126-131, 2019.CitaÃ§Ãµes:',\n",
       "    '17.': 'ARAÚJO, HARILTON ;FILHO, RAIMIR; RODRIGUES, JOEL ; RABELO, RICARDO ; SOUSA, NATANAEL ; FILHO, JOSÉ ; SOBRAL, JOSÉ . A Proposal for IoT Dynamic Routes Selection Based on Contextual Information. SENSORS, v. 18, p. 353, 2018.CitaÃ§Ãµes:',\n",
       "    '18.': 'ARAÚJO, PAULO RÉGIS C. ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. ; OLIVEIRA, JOÃO P. C. M. ; BRAGA, STEPHANIE A. . Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks. INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS, v. 31, p. e3380, 2018.CitaÃ§Ãµes:',\n",
       "    '19.': 'SOBRAL, JOSÉ V.V. ; RODRIGUES, JOEL J.P.C. ; RABELO, RICARDO A.L. ; LIMA FILHO, JOSÉ C. ; SOUSA, NATANAEL ; ARAUJO, HARILTON S. ;HOLANDA FILHO, RAIMIR. A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs. JOURNAL OF NETWORK AND COMPUTER APPLICATIONS, v. 107, p. 56-68, 2018.CitaÃ§Ãµes:',\n",
       "    '20.': 'LEMOS, MARCUS ;FILHO, RAIMIR; RABÊLO, RICARDO ; DE CARVALHO, CARLOS ; MENDES, DOUGLAS ; COSTA, VALNEY . An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments. SENSORS, v. 18, p. 689, 2018.CitaÃ§Ãµes:',\n",
       "    '21.': 'DE ARAÚJO, PAULO ;FILHO, RAIMIR; RODRIGUES, JOEL ; OLIVEIRA, JOÃO ; BRAGA, STEPHANIE . Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks. SENSORS, v. 18, p. 1312, 2018.CitaÃ§Ãµes:',\n",
       "    '22.': 'RAMOS, ALEX ; LAZAR, MARCELLA ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. . Model-Based Quantitative Network Security Metrics: A Survey. IEEE Communications Surveys and Tutorials, v. 19, p. 2704-2734, 2017.CitaÃ§Ãµes:',\n",
       "    '23.': 'MARINHO, R. ;HOLANDA FILHO, RAIMIR. Exploring a P2P Transient Botnet - From Discovery to Enumeration. The Journal on Cybercrime & Digital Investigations, v. 3, p. 30-39, 2017.',\n",
       "    '24.': 'LEMOS, MARCUS ; CARVALHO, CARLOS ; LOPES, DOUGLAS ;HOLANDA, RAIMIR; RABÊLO, RICARDO . UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES. LEARNING AND NONLINEAR MODELS, v. 14, p. 4-14, 2016.',\n",
       "    '25.': 'RAMOS, A. L. ;HOLANDA FILHO, R.. Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks. SENSORS, v. 15, p. 2104-2136, 2015.CitaÃ§Ãµes:',\n",
       "    '26.': 'ARAUJO, Paulo Regis C. ;HOLANDA FILHO, R.; RODRIGUES, Antonio. Wendell. O. ; ARAUJO, Andre. Luiz. C. ; MORAES FILHO, Jose. A. ; OLIVEIRA, Joao. Paolo. C. M. . A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions. International Journal of Distributed Sensor Networks, v. 2014, p. 1-15, 2014.CitaÃ§Ãµes:',\n",
       "    '27.': 'SOBRAL, J. ; SOUSA, A. ; ARAUJO, H. S. ; BALUZ, R. ;HOLANDA FILHO, R.; SOUSA, M. V. ; Rabelo, Ricardo A. L. . A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks. JOURNAL OF COMMUNICATION AND COMPUTER, v. 10, p. 702-712, 2013.',\n",
       "    '28.': 'Rabelo, Ricardo A. L. ; LEAL, L. B. ; SOUSA, M. V. ;HOLANDA FILHO, R.; Borges, Fabio A. S. . Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink. Learning and Nonlinear Models, v. 10, p. 4-18, 2012.',\n",
       "    '29.': 'Rabelo, Ricardo A. L. ; LEAL, L. B. ; SOUSA, M. V. ;HOLANDA FILHO, R.; Borges, Fabio A. S. . An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks. INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS, v. 9, p. 61-74, 2012.',\n",
       "    '30.': 'BRAYNER, A. ; Coelho, André Luis Carvalho ; Marinho, Karine ;HOLANDA FILHO, R.; Castro, Wagner L. T. . On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries. Information Fusion (Print), v. 15, p. 44-55, 2012.CitaÃ§Ãµes:',\n",
       "    '31.': 'HOLANDA FILHO, R.; OLIVEIRA, J. A. . Controle Externo da Governança de Tecnologia da Informação. Revista Controle, v. X, p. 65-85, 2012.',\n",
       "    '32.': 'HOLANDA FILHO, R.; MAIA, J. E. B. ; CARMO, M. F. F. . Detecting computer network attacks using statistical discriminators and cluster analysis. Revista Tecnologia (UNIFOR), v. 28, p. 33-41, 2007.',\n",
       "    '33.': 'HOLANDA FILHO, R.; MAIA, J. E. B. ; PAULINO, G. . Broadband network traffic characterization and classification using a multivariate statistical method. Revista Tecnologia (UNIFOR), v. 27, p. 113-122, 2006.',\n",
       "    '34.': 'FURTADO, E. ; LINCOLN, F. ; FURTADO, V. ;HOLANDA FILHO, R.. Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários. Revista Brasileira de Informática na Educação, v. 8o., 2001.'}},\n",
       "  'Produção técnica': {'Programas de computador sem registro': {'1.': 'HOLANDA FILHO, R.. SAGRES. 1998.'}}},\n",
       " 'JCR': {'0': {'data-issn': '18685137',\n",
       "   'impact-factor': '3.662',\n",
       "   'jcr-year': 'JCR 2021'},\n",
       "  '1': {'data-issn': '21693536',\n",
       "   'impact-factor': '3.9',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '2': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '3': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '4': {'data-issn': '21693536',\n",
       "   'impact-factor': '3.9',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '5': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '6': {'data-issn': '20763417',\n",
       "   'impact-factor': '2.7',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '7': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '8': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '9': {'data-issn': '20763417',\n",
       "   'impact-factor': '2.7',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '10': {'data-issn': '14248220',\n",
       "   'impact-factor': '3.9',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '11': {'data-issn': '21693536',\n",
       "   'impact-factor': '3.9',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '12': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '13': {'data-issn': '10557148',\n",
       "   'impact-factor': '1.5',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '14': {'data-issn': '23274662',\n",
       "   'impact-factor': '10.6',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '15': {'data-issn': '08908044',\n",
       "   'impact-factor': '9.3',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '16': {'data-issn': '14248220',\n",
       "   'impact-factor': '3.9',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '17': {'data-issn': '10745351',\n",
       "   'impact-factor': '2.1',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '18': {'data-issn': '10848045',\n",
       "   'impact-factor': '8.7',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '19': {'data-issn': '14248220',\n",
       "   'impact-factor': '3.9',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '20': {'data-issn': '14248220',\n",
       "   'impact-factor': '3.9',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '21': {'data-issn': '1553877X',\n",
       "   'impact-factor': '35.6',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '22': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '23': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '24': {'data-issn': '14248220',\n",
       "   'impact-factor': '3.9',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '25': {'data-issn': '15501329',\n",
       "   'impact-factor': '1.614',\n",
       "   'jcr-year': 'JCR 2018'},\n",
       "  '26': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '27': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '28': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '29': {'data-issn': '15662535',\n",
       "   'impact-factor': '18.6',\n",
       "   'jcr-year': 'JCR 2022'},\n",
       "  '30': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '31': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '32': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'},\n",
       "  '33': {'data-issn': 'NULL', 'impact-factor': 'NULL', 'jcr-year': 'NULL'}},\n",
       " 'Bancas': {'Participação em bancas de trabalhos de conclusão': {'Mestrado': {'1.': 'TORRES, A. L. M. M.; BORGES NETO, H.; HOLANDA FILHO, RAIMIR.  Participação em banca de ANDRÉ SANTOS SILVA. TeleMeios: uma proposta de virtualização do ensino ancorada na Sequência Fedathi. 2022. Dissertação (Mestrado em Educação) - Universidade Federal do Ceará.',\n",
       "    '2.': 'MACHADO, V. P.; SANTOS, P. A.; MOURA, R. S.; LEMOS, MARCUS; HOLANDA FILHO, RAIMIR.  Participação em banca de Joselito Mendes de Sousa Junior. Modelo para Classificação de Fornecedores da Adm Publ baseado em Aprend Maquina. 2019. Dissertação (Mestrado em Ciência da Computação) - Universidade Federal do Piauí.',\n",
       "    '3.': 'HOLANDA FILHO, R.; Pinheiro, Placido R.; BESSA, A.; THOMAZ, A. C. F..  Participação em banca de Odecilia Barreira da Silva. C3M - Gerenciamento de Mudanças Estruturado em uma Metodologia de Multicritério. 2013. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '4.': 'HOLANDA FILHO, R.; ANDRADE, R. M. C.; SAMPAIO, A. T. F..  Participação em banca de Alex Lacerda Ramos. Sensor Data Security Estimator: um Framework para Estimativa do Nível de Segurança dos dados de Redes de Sensores sem Fio. 2013. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '5.': 'HOLANDA FILHO, R.; Rabelo, Ricardo A. L.; BRAYNER, A.; FERNANDES, R. A. S..  Participação em banca de Rodrigo Augusto Rocha Souza Baluz. Uma Aplicação de Sistemas Inteligentes Híbridos ACO-Fuzzy para a Otimização do Desempenho em Redes de Sensores sem Fio. 2013. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '6.': 'MENDONCA, N. C.; SAMPAIO, A. T. F.; FONSECA, N. L. S.; HOLANDA FILHO, R..  Participação em banca de Matheus Ciríaco Cerqueira Cunha. Um Ambiente Programável para Avaliar o Desempenho de Aplicações em Nuvens de Infraestrutura. 2012. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '7.': 'SAMPAIO, A. T. F.; HOLANDA FILHO, R.; UEYAMA, J.; VASCONCELOS FILHO, J. E..  Participação em banca de Leonardo Moura Leitão. NaturalCloud: Um framework para integração de Rede de Sensores na Nuvem. 2012. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '8.': 'HOLANDA FILHO, R.; ZIVIANI, A.; MENDONCA, N. C..  Participação em banca de Victor Pasknel de Alencar Ribeiro. Classificação de tráfego online baseada em sub-fluxos. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '9.': 'HOLANDA FILHO, R.; Coelho, André Luis Carvalho; GOMES, D. G.; SANTOS, C. N..  Participação em banca de Gesiel Rios Lopes. GERAU: Um mecanismo de gerenciamento de segurança autonômico baseado em detecção de novidades e mudança de conceito. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '10.': 'HOLANDA FILHO, R.; Rabelo, Ricardo A. L.; SAMPAIO, A. T. F..  Participação em banca de Harilton da Silva Araújo. LARP: UM PROTOCOLO DE ROTEAMENTO TOLERANTE A FALHAS PARA REDES DE SENSORES SEM FIO. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '11.': 'HOLANDA FILHO, R.; NOGUEIRA, M.; MADEIRA, E. R. M.; MENDONCA, N. C.; RODRIGUES, M. A. F..  Participação em banca de Helber Wagner da Silva. Um Esquema de Seleção de Rotas para o Balanceamento de Segurança e Desempenho em Redes em Malha sem Fio. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '12.': 'HOLANDA FILHO, R.; Rabelo, Ricardo A. L.; SANTOS, Aldri L dos; SANTOS, C. N..  Participação em banca de Liliam Barroso Leal. Uma Abordagem para Estimação da Qualidade de Rotas em Redes de Sensores sem Fio Multi-Sink Baseada em Sistemas Fuzzy Genéticos. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '13.': 'HOLANDA FILHO, R.; SILVA, J. S. V.; BESSA, A..  Participação em banca de Denilson Cursino de Oliveira. Uma Sistematização para o Planejamento da Gerência de Mudanças em TI e Modelagem de Uma Ferramenta. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '14.': 'HOLANDA FILHO, R.; ANDRADE, R. M. C.; BRAYNER, A. R. A..  Participação em banca de Marcus Vinicius de Sousa Lemos. Detecção de Intrusão em Redes de Sensores Sem Fio Utilizando uma Abordagem Colaborativa e Cross-layer. 2010. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '15.': 'HOLANDA FILHO, R.; Souza, Jefferson T.; Silva, Jorge Luiz de C..  Participação em banca de Fabiano Carneiro Ribeiro. Estimação de Matrizes de Tráfego Origem-Destino utilizando Algoritmo Genético. 2009. Dissertação (Mestrado em Mestrado Acadêmico em Ciência da Computação) - Universidade Estadual do Ceará.',\n",
       "    '16.': 'HOLANDA FILHO, R.; SANTOS, Aldri L dos; Coelho, André Luis Carvalho.  Participação em banca de MARCUS FÁBIO FONTENELLE DO CARMO. CLASTRIN - Um Classificador de Tráfego de Aplicações Internet utilizando a Abordagem Um-Contra-Todos. 2009. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '17.': 'HOLANDA FILHO, R.; Pinheiro, Placido R.; Fernandez, Marcial P..  Participação em banca de Geneflides Laureno da Silva. Gateway de Voz para Integração IP-PSTN Aderente à Arquitetura das Redes de Nova Geração. 2009. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '18.': 'BRAYNER, A.; Coelho, André Luis Carvalho; Nakamura, Eduardo freire; HOLANDA FILHO, R..  Participação em banca de Karina Marinho de Souza. Processamento de Consultas em Redes de Sensores sem Fio: Uma Abordagem de Detecção de Novidades para o Controle da Qualidade dos Serviços das Consultas. 2009. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '19.': 'HOLANDA FILHO, R.; Silva, Jorge Luiz de C.; Bezerra, Francisco N..  Participação em banca de Gabriel Paulino Siqueira Júnior. Uma Metodologia para Identificação de Classes de Tráfego Baseada em Discriminantes Estatísticos e Análise de Agrupamentos. 2008. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '20.': 'HOLANDA FILHO, R.; BELCHIOR, A. D.; Falbo, Ricardo de A.; Farias, Pedro P. M..  Participação em banca de Karlson Bernardo de Oliveira. Aplicação da Estatística Multivariada para Apoiar a Avaliação Organizacional. 2008. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '21.': 'HOLANDA FILHO, R.; MENDONÇA, Nabor das Chagas; MACHADO, Javam de Castro.  Participação em banca de Fabricio Albuquerque Diógenes. Logmiddle: um middleware para o compartilhamento de dados em redes móveis ad hoc. 2006. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.',\n",
       "    '22.': 'HOLANDA FILHO, R..  Participação em banca de Ricardo Régis Cavalcante Chaves. Uma arquitetura para ambientes virtuais colaborativos com dispositivos de force feedback. 2006. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'},\n",
       "   'Teses de doutorado': {'1.': 'Pinheiro, Placido R.; RABELO, RICARDO A.L.; HOLANDA FILHO, RAIMIR; RODRIGUES, J. J. P. C.; ALMEIDA, O. M.; CRUZ NETO, J. X..  Participação em banca de Jaclason Machado Veras. Home Energy Management System: A Multi-objective optimization Model for Scheduling Loads. 2019. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza.'},\n",
       "   'Trabalhos de conclusão de curso de graduação': {'1.': 'HOLANDA FILHO, R.; MAIA, J. E. B..  Participação em banca de Alvaro Garcia de Miguel.Estudo das Redes Mesh e sua Aplicação na Telemedicina.2011. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '2.': 'HOLANDA FILHO, R.; Brito, Wellington Alves; Rios, Clauson Sales do N..  Participação em banca de Dráulio Brasil Soares Neto.Estudo das Redes de Sensores sem Fio com um Estudo de Caso.2009. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '3.': 'HOLANDA FILHO, R.; Silva, Geneflides L.; MAIA, J. E. B..  Participação em banca de Gustavo Gurgel Nóbrega.QoS em Redes IP.2009. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '4.': 'HOLANDA FILHO, R.; GARCIA, Fernando Parente.  Participação em banca de Leandro Orofino Enck.Estudo Comparativo entre Algoritmos de Roteamento para Redes de Sensores sem Fio.2009. Trabalho de Conclusão de Curso (Graduação em Ciência da Computação)  - Universidade de Fortaleza.',\n",
       "    '5.': 'HOLANDA FILHO, R.; MAIA, J. E. B.; GARCIA, Fernando Parente.  Participação em banca de Thais Lucas da Rocha Sousa.Avaliação da Qualidade Percebida em Serviços de Telecomunicações.2009. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '6.': 'HOLANDA FILHO, R.; Brito, Wellington Alves; Alcocer, Juan C. A..  Participação em banca de Bráulio Crisóstomo de Quental.Transmissão de Dados sobre Redes Elétricas. Estudo de Caso do Projeto Piloto PLC - Coelce.2009. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '7.': 'HOLANDA FILHO, R..  Participação em banca de Emanoela de Jesus Lopes Soares.Redes de Acesso Opticas, HFC e HFW.2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '8.': 'HOLANDA FILHO, R..  Participação em banca de Noele Ingrid da Rosa Silva.Um estudo para convergência TV com outras plataformas para identificar padrões de interação do usuário com serviços de televisão.2007. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.',\n",
       "    '9.': 'HOLANDA FILHO, R..  Participação em banca de Claudio Franklin Mesquita Araújo.Gerência e operação de redes GSM.2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '10.': 'HOLANDA FILHO, R..  Participação em banca de Francisco Arnaldo de Araújo Filho.Segurança em redes sem fio 802.11x.2007. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.',\n",
       "    '11.': 'MAIA, J. E. B.; Antonio Macilio Pereira de Lucena; HOLANDA FILHO, R..  Participação em banca de Liana de Andrade Gomes.Evolucao das Redes GSM para UMTS (3G).2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '12.': 'MAIA, J. E. B.; GARCIA, Fernando Parente; HOLANDA FILHO, R..  Participação em banca de Naiana Edilma Coelho de Freitas.Metodologia em Gestão de Serviços em Telecomunicações e Tecnologia da Informação.2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '13.': 'Colares, Ricardo Fialho; HOLANDA FILHO, R.; Brito, Wellington Alves.  Participação em banca de Diego Buarque Mancera.H.323: Conceitos, Aplicações e Integração com Asterisk.2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.',\n",
       "    '14.': 'GARCIA, Fernando Parente; HOLANDA FILHO, R..  Participação em banca de Samuel Limaverde Verissimo.Redes WMAN sem Fio: Cenários de utilização das tecnologias WiMAX e WiMesh.2007. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.',\n",
       "    '15.': 'HOLANDA FILHO, R.; GARCIA, Fernando Parente.  Participação em banca de Igor Louback de Castro Moura.Implementação de um aplicativo WEB para gerência de redes sem fio.2006. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.',\n",
       "    '16.': 'HOLANDA FILHO, R..  Participação em banca de Marcus Antonio Borges Sales.Tecnologias de acesso fixo sem fio para telecomunicações.2006. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.',\n",
       "    '17.': 'MAIA, J. E. B.; HOLANDA FILHO, R..  Participação em banca de Danilo Roosevelt Perdigão Coimbra.Uma revisao das tecnologias de sistemas de armazenamento de dados.2006. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.'}},\n",
       "  'Participação em bancas de comissões julgadoras': {'Outras participações': {'1.': 'HOLANDA FILHO, R.; Pinheiro, Placido R.; Vasco, J.J. Peixoto Furtado; MENDONÇA, Nabor das Chagas. Comissão de Seleção da Turma XI do MIA.2008.Universidade de Fortaleza.',\n",
       "    '2.': 'HOLANDA FILHO, R.. Membro do Comitê Científico do XII Seminário Apec.2007.Universidade de Fortaleza.',\n",
       "    '3.': 'HOLANDA FILHO, R.. Parecerista no VII Encontro de Pós-Graduação e Pesquisa.2007.Universidade de Fortaleza.'}}},\n",
       " 'Orientações': {'Orientações e supervisões em andamento': {'Dissertação de mestrado': {'1.': 'Marcele Pinho de Arruda Mapurunga. Avaliação do nível de maturidade em transformação digital - estudo de caso em uma organização pública.2022.Dissertação  (Mestrado em Administração)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '2.': 'Pedro Gabriel Caliope Dantas Pinheiro. INTEGRAÇÃO DE IOT E ROBÓTICA MÓVEL EM UMA APLICAÇÃO DE MONITORAMENTO DE AMBIENTES.2019.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '3.': 'Raniere Rocha Guimaraes. Detecção de Anomalias em Redes de Sensores Sem Fio utilizando Agrupamento de Dados Baseado em Floresta de Caminhos Ótimos.2018.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '4.': 'Patrick Robson Saldanha Vasconcelos. Salus Monitum: Um Sistema de Alertas na Nuvem Aplicado em Soluções IoT Voltadas à Saúde.2018.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '5.': 'Carlinha Santos Fujiwara. RECORDATUS: Uma Solução Móvel baseada em Internet das Coisas para Suporte a Adultos com TDAH.2017.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '6.': 'Kerllon Fontenele de Andrade. Discriminação de Tráfego P2P Utilizando Árvores de Decisão e Naive Bayes.2014.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '7.': 'Alex Lacerda Ramos. Sensor Data Security Estimator: um Framework para Estimativa do Nível de Segurança dos dados de Redes de Sensores sem Fio.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '8.': 'Rodrigo Augusto Rocha Souza Baluz. Uma Aplicação de Sistemas Inteligentes Híbridos ACO-Fuzzy para a Otimização do Desempenho em Redes de Sensores sem Fio.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '9.': 'Odecilia Barreira da Silva. C3M - Gerenciamento de Mudanças Estruturado em uma Metodologia de Multicritério.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '10.': 'Renato Rodrigues Marinho. Detecção de Botnets P2P baseada em Discriminantes de Sub-Fluxos.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '11.': 'Leonardo Moura Leitão. NaturalCloud: Um Framework para integração de Rede de Sensores na Nuvem.2012.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Coorientador: Raimir Holanda Filho.',\n",
       "    '12.': 'HARILTON DA SILVA ARAÚJO. LARP: UM PROTOCOLO DE ROTEAMENTO TOLERANTE A FALHAS PARA REDES DE SENSORES SEM FIO.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Conselho Nacional de Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '13.': 'Helber Wagner da Silva. Um Esquema de Seleção de Rotas para o Balanceamento de Segurança e Desempenho em Redes em Malha sem Fio.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '14.': 'Denilson Cursino de Oliveira. Uma Sistematização para o Planejamento da Gerência de Mudanças em TI e Modelagem de Uma Ferramenta.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '15.': 'Liliam Barroso Leal. Uma Abordagem para Estimação da Qualidade de Rotas em Redes de Sensores sem Fio Multi-Sink Baseada em Sistemas Fuzzy Genéticos.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '16.': 'Victor Pasknel de Alencar Ribeiro. Classificação de tráfego online baseada em sub-fluxos.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Conselho Nacional de Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '17.': 'Gesiel Rios Lopes. GERAU: Um mecanismo de gerenciamento de segurança autonômico baseado em detecção de novidades e mudança de conceito..2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '18.': 'Marcus Vinicius de Sousa Lemos. Detecção de Intrusão em Redes de Sensores Sem Fio Utilizando uma Abordagem Colaborativa e Cross-layer.2010.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '19.': 'MARCUS FÁBIO FONTENELLE DO CARMO. CLASTRIN ? UM CLASSIFICADOR DE TRÁFEGO DE APLICAÇÕES INTERNET UTILIZANDO A ABORDAGEM ―UM-CONTRA-TODOS‖.2009.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Orientador: Raimir Holanda Filho.',\n",
       "    '20.': 'Geneflides Laureno da Silva. Gateway de Voz para Integração IP-PSTN Aderente à Arquitetura das Redes de Nova Geração.2009.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '21.': 'Gabriel Paulino Siqueira Júnior. Uma Metodologia para identificação de Classes de Tráfego baseada em discriminantes Estatísticos e Análise de Agrupamentos.2008.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '22.': 'karlson Bernardo de Oliveira. Uso da Estatistica Multivariada para Auxiliar a Avaliaçao Organizacional.2008.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Coorientador: Raimir Holanda Filho.'},\n",
       "   'Tese de doutorado': {'1.': 'Victor Pasknel de Alencar Ribeiro.A FAULT-TOLERANT AND SECURE ARCHITECTURE FOR KEY MANAGEMENT IN LORAWAN BASED ON PERMISSIONED BLOCKCHAIN.2022. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '2.': 'Paulo Regis Carneiro De Araujo.Uma proposta para Integração de Equipamentos Elétricos Legados a uma Smart Grid Utilizando Redes de Sensores Sem Fio.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '3.': 'Marcus Vinicius De Sousa Lemos.An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in the Heterogeneous Environments of Sensor Clouds.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '4.': 'Alex Lacerda Ramos.Network Security Metrics for the Internet of Things.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '5.': 'Harilton da Silva Araújo.Seleção de Rotas em Redes para Internet das Coisas baseada em Requisitos de Aplicações.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.'},\n",
       "   'Trabalho de conclusão de curso de graduação': {'1.': 'Alvaro Garcia de Miguel.Estudo das Redes Mesh e sua Aplicação na Telemedicina.2011.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '2.': 'Iagê Figueiredo de C. Teixeira.Tecnologia WIMAX: Um Estudo Comparativo.2009.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '3.': 'Ana Caroline de Arruda Amorim.Proposta de Arquitetura para prover QoS em Redes 4G.2007.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '4.': 'Jean Rodrigo Landim Costa.Autorização, Autenticação e Bilhetagem baseados no Protocolos SIP.2007.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '5.': 'Carlos Henrique Rodrigues Ximenes.Captura e Análise de Tráfego VoIP.2007.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '6.': 'Matheus Fechine de Moura.Um Estudo Comparativo de QoS em  Tráfego Vox sobre IP.2007.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '7.': 'Marcos Heleno Chagas Quixadá.Uma Proposta para Arquitetura de Qualidade de Serviço em Sistemas de TV Digital.2007.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '8.': 'André Luiz Vieira F.S. Amorim.Mobilidade e interatividade em sistemas de TV digital.2007.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '9.': 'Luciano Leal do Vale.Geração e caracterização de tráfego de aplicações P2P.2006.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.'}},\n",
       "  'Orientações e supervisões concluídas': {'Dissertação de mestrado': {'1.': 'Marcele Pinho de Arruda Mapurunga. Avaliação do nível de maturidade em transformação digital - estudo de caso em uma organização pública.2022.Dissertação  (Mestrado em Administração)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '2.': 'Pedro Gabriel Caliope Dantas Pinheiro. INTEGRAÇÃO DE IOT E ROBÓTICA MÓVEL EM UMA APLICAÇÃO DE MONITORAMENTO DE AMBIENTES.2019.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '3.': 'Raniere Rocha Guimaraes. Detecção de Anomalias em Redes de Sensores Sem Fio utilizando Agrupamento de Dados Baseado em Floresta de Caminhos Ótimos.2018.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '4.': 'Patrick Robson Saldanha Vasconcelos. Salus Monitum: Um Sistema de Alertas na Nuvem Aplicado em Soluções IoT Voltadas à Saúde.2018.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '5.': 'Carlinha Santos Fujiwara. RECORDATUS: Uma Solução Móvel baseada em Internet das Coisas para Suporte a Adultos com TDAH.2017.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '6.': 'Kerllon Fontenele de Andrade. Discriminação de Tráfego P2P Utilizando Árvores de Decisão e Naive Bayes.2014.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '7.': 'Alex Lacerda Ramos. Sensor Data Security Estimator: um Framework para Estimativa do Nível de Segurança dos dados de Redes de Sensores sem Fio.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '8.': 'Rodrigo Augusto Rocha Souza Baluz. Uma Aplicação de Sistemas Inteligentes Híbridos ACO-Fuzzy para a Otimização do Desempenho em Redes de Sensores sem Fio.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '9.': 'Odecilia Barreira da Silva. C3M - Gerenciamento de Mudanças Estruturado em uma Metodologia de Multicritério.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '10.': 'Renato Rodrigues Marinho. Detecção de Botnets P2P baseada em Discriminantes de Sub-Fluxos.2013.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '11.': 'Leonardo Moura Leitão. NaturalCloud: Um Framework para integração de Rede de Sensores na Nuvem.2012.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Coorientador: Raimir Holanda Filho.',\n",
       "    '12.': 'HARILTON DA SILVA ARAÚJO. LARP: UM PROTOCOLO DE ROTEAMENTO TOLERANTE A FALHAS PARA REDES DE SENSORES SEM FIO.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Conselho Nacional de Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '13.': 'Helber Wagner da Silva. Um Esquema de Seleção de Rotas para o Balanceamento de Segurança e Desempenho em Redes em Malha sem Fio.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '14.': 'Denilson Cursino de Oliveira. Uma Sistematização para o Planejamento da Gerência de Mudanças em TI e Modelagem de Uma Ferramenta.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '15.': 'Liliam Barroso Leal. Uma Abordagem para Estimação da Qualidade de Rotas em Redes de Sensores sem Fio Multi-Sink Baseada em Sistemas Fuzzy Genéticos.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '16.': 'Victor Pasknel de Alencar Ribeiro. Classificação de tráfego online baseada em sub-fluxos.2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Conselho Nacional de Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '17.': 'Gesiel Rios Lopes. GERAU: Um mecanismo de gerenciamento de segurança autonômico baseado em detecção de novidades e mudança de conceito..2011.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico. Orientador: Raimir Holanda Filho.',\n",
       "    '18.': 'Marcus Vinicius de Sousa Lemos. Detecção de Intrusão em Redes de Sensores Sem Fio Utilizando uma Abordagem Colaborativa e Cross-layer.2010.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '19.': 'MARCUS FÁBIO FONTENELLE DO CARMO. CLASTRIN ? UM CLASSIFICADOR DE TRÁFEGO DE APLICAÇÕES INTERNET UTILIZANDO A ABORDAGEM ―UM-CONTRA-TODOS‖.2009.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Orientador: Raimir Holanda Filho.',\n",
       "    '20.': 'Geneflides Laureno da Silva. Gateway de Voz para Integração IP-PSTN Aderente à Arquitetura das Redes de Nova Geração.2009.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '21.': 'Gabriel Paulino Siqueira Júnior. Uma Metodologia para identificação de Classes de Tráfego baseada em discriminantes Estatísticos e Análise de Agrupamentos.2008.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '22.': 'karlson Bernardo de Oliveira. Uso da Estatistica Multivariada para Auxiliar a Avaliaçao Organizacional.2008.Dissertação  (Mestrado em Informática Aplicada)  - Universidade de Fortaleza, . Coorientador: Raimir Holanda Filho.'},\n",
       "   'Tese de doutorado': {'1.': 'Victor Pasknel de Alencar Ribeiro.A FAULT-TOLERANT AND SECURE ARCHITECTURE FOR KEY MANAGEMENT IN LORAWAN BASED ON PERMISSIONED BLOCKCHAIN.2022. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '2.': 'Paulo Regis Carneiro De Araujo.Uma proposta para Integração de Equipamentos Elétricos Legados a uma Smart Grid Utilizando Redes de Sensores Sem Fio.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '3.': 'Marcus Vinicius De Sousa Lemos.An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in the Heterogeneous Environments of Sensor Clouds.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '4.': 'Alex Lacerda Ramos.Network Security Metrics for the Internet of Things.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.',\n",
       "    '5.': 'Harilton da Silva Araújo.Seleção de Rotas em Redes para Internet das Coisas baseada em Requisitos de Aplicações.2018. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza, . Orientador: Raimir Holanda Filho.'},\n",
       "   'Trabalho de conclusão de curso de graduação': {'1.': 'Alvaro Garcia de Miguel.Estudo das Redes Mesh e sua Aplicação na Telemedicina.2011.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '2.': 'Iagê Figueiredo de C. Teixeira.Tecnologia WIMAX: Um Estudo Comparativo.2009.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '3.': 'Ana Caroline de Arruda Amorim.Proposta de Arquitetura para prover QoS em Redes 4G.2007.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '4.': 'Jean Rodrigo Landim Costa.Autorização, Autenticação e Bilhetagem baseados no Protocolos SIP.2007.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '5.': 'Carlos Henrique Rodrigues Ximenes.Captura e Análise de Tráfego VoIP.2007.Trabalho de Conclusão de Curso. (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '6.': 'Matheus Fechine de Moura.Um Estudo Comparativo de QoS em  Tráfego Vox sobre IP.2007.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '7.': 'Marcos Heleno Chagas Quixadá.Uma Proposta para Arquitetura de Qualidade de Serviço em Sistemas de TV Digital.2007.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '8.': 'André Luiz Vieira F.S. Amorim.Mobilidade e interatividade em sistemas de TV digital.2007.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.',\n",
       "    '9.': 'Luciano Leal do Vale.Geração e caracterização de tráfego de aplicações P2P.2006.Trabalho de Conclusão de Curso. (Graduação em Informatica)  - Universidade de Fortaleza. Orientador: Raimir Holanda Filho.'}}},\n",
       " 'JCR2': {'0': {'jcr-ano': 'JCR 2021',\n",
       "   'doi': 'http://dx.doi.org/10.1007/s12652-023-04555-3',\n",
       "   'data-issn': '18685137',\n",
       "   'original_title': 'Fator de impacto (JCR 2021): 3.662',\n",
       "   'impact-factor': '3.662'},\n",
       "  '1': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': 'http://dx.doi.org/10.1109/ACCESS.2023.3260020',\n",
       "   'data-issn': '21693536',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9'},\n",
       "  '2': {'doi': '10.1016/j.procs.2023.03.018',\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '18770509',\n",
       "   'volume': '220',\n",
       "   'paginaInicial': '119',\n",
       "   'titulo': 'Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain',\n",
       "   'sequencial': '3',\n",
       "   'nomePeriodico': 'PROCEDIA COMPUTER SCIENCE'},\n",
       "  '3': {'doi': '10.9790/487X-2507032735',\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '2278487X',\n",
       "   'volume': '25',\n",
       "   'paginaInicial': '27',\n",
       "   'titulo': 'Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector',\n",
       "   'sequencial': '4',\n",
       "   'nomePeriodico': 'Journal of Business and Management'},\n",
       "  '4': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1109/ACCESS.2022.3179004',\n",
       "   'data-issn': '21693536',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '21693536',\n",
       "   'volume': '1',\n",
       "   'paginaInicial': '1',\n",
       "   'titulo': 'A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain',\n",
       "   'sequencial': '5',\n",
       "   'nomePeriodico': 'IEEE Access'},\n",
       "  '5': {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '22360700',\n",
       "   'volume': '13',\n",
       "   'paginaInicial': '1',\n",
       "   'titulo': 'Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública',\n",
       "   'sequencial': '6',\n",
       "   'nomePeriodico': 'Revista Razão Contábil '},\n",
       "  '6': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/app12188939',\n",
       "   'data-issn': '20763417',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 2.7',\n",
       "   'impact-factor': '2.7',\n",
       "   'issn': '20763417',\n",
       "   'volume': '12',\n",
       "   'paginaInicial': '8939',\n",
       "   'titulo': 'Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19',\n",
       "   'sequencial': '7',\n",
       "   'nomePeriodico': 'Applied Sciences-Basel'},\n",
       "  '7': {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '22360700',\n",
       "   'volume': '12',\n",
       "   'paginaInicial': '1',\n",
       "   'titulo': 'Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará',\n",
       "   'sequencial': '8',\n",
       "   'nomePeriodico': 'Revista Razão Contábil '},\n",
       "  '8': {'doi': '10.1007/978-3-030-84311-3_2',\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '22138684',\n",
       "   'volume': '2021',\n",
       "   'paginaInicial': '13',\n",
       "   'titulo': 'Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19',\n",
       "   'sequencial': '9',\n",
       "   'nomePeriodico': 'Springer Proceedings in Complexity'},\n",
       "  '9': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/app112110457',\n",
       "   'data-issn': '20763417',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 2.7',\n",
       "   'impact-factor': '2.7',\n",
       "   'issn': '20763417',\n",
       "   'volume': '11',\n",
       "   'paginaInicial': '10457',\n",
       "   'titulo': 'An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus',\n",
       "   'sequencial': '10',\n",
       "   'nomePeriodico': 'Applied Sciences-Basel'},\n",
       "  '10': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/s20113068',\n",
       "   'data-issn': '14248220',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '14248220',\n",
       "   'volume': '20',\n",
       "   'paginaInicial': '3068',\n",
       "   'titulo': 'Enhancing Key Management in LoRaWAN with Permissioned Blockchain',\n",
       "   'sequencial': '11',\n",
       "   'nomePeriodico': 'SENSORS'},\n",
       "  '11': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1109/access.2020.3009167',\n",
       "   'data-issn': '21693536',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '21693536',\n",
       "   'volume': '1',\n",
       "   'paginaInicial': '1',\n",
       "   'titulo': 'Integration of the Mobile Robot and Internet of Things to Monitor Older People',\n",
       "   'sequencial': '12',\n",
       "   'nomePeriodico': 'IEEE Access'},\n",
       "  '12': {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '22360700',\n",
       "   'volume': '11',\n",
       "   'paginaInicial': '1',\n",
       "   'titulo': 'Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019',\n",
       "   'sequencial': '13',\n",
       "   'nomePeriodico': 'Revista Razão Contábil '},\n",
       "  '13': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1002/nem.2062',\n",
       "   'data-issn': '10557148',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 1.5',\n",
       "   'impact-factor': '1.5',\n",
       "   'issn': '10557148',\n",
       "   'volume': '1',\n",
       "   'paginaInicial': 'e2062',\n",
       "   'titulo': 'An approach for provisioning virtual sensors in sensor clouds',\n",
       "   'sequencial': '14',\n",
       "   'nomePeriodico': 'International Journal of Network Management'},\n",
       "  '14': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1109/jiot.2019.2904302',\n",
       "   'data-issn': '23274662',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 10.6',\n",
       "   'impact-factor': '10.6',\n",
       "   'issn': '23274662',\n",
       "   'volume': '6',\n",
       "   'paginaInicial': '5631',\n",
       "   'titulo': 'Enabling Online Quantitative Security Analysis in 6LoWPAN Networks',\n",
       "   'sequencial': '15',\n",
       "   'nomePeriodico': 'IEEE Internet of Things Journal'},\n",
       "  '15': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1109/mnet.2018.1800151',\n",
       "   'data-issn': '08908044',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 9.3',\n",
       "   'impact-factor': '9.3',\n",
       "   'issn': '08908044',\n",
       "   'volume': '33',\n",
       "   'paginaInicial': '126',\n",
       "   'titulo': 'Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering',\n",
       "   'sequencial': '16',\n",
       "   'nomePeriodico': 'IEEE NETWORK'},\n",
       "  '16': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/s18020353',\n",
       "   'data-issn': '14248220',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '14248220',\n",
       "   'volume': '18',\n",
       "   'paginaInicial': '353',\n",
       "   'titulo': 'A Proposal for IoT Dynamic Routes Selection Based on Contextual Information',\n",
       "   'sequencial': '17',\n",
       "   'nomePeriodico': 'SENSORS'},\n",
       "  '17': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1002/dac.3380',\n",
       "   'data-issn': '10745351',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 2.1',\n",
       "   'impact-factor': '2.1',\n",
       "   'issn': '10745351',\n",
       "   'volume': '31',\n",
       "   'paginaInicial': 'e3380',\n",
       "   'titulo': 'Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks',\n",
       "   'sequencial': '18',\n",
       "   'nomePeriodico': 'INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS'},\n",
       "  '18': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1016/j.jnca.2018.01.015',\n",
       "   'data-issn': '10848045',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 8.7',\n",
       "   'impact-factor': '8.7',\n",
       "   'issn': '10848045',\n",
       "   'volume': '107',\n",
       "   'paginaInicial': '56',\n",
       "   'titulo': 'A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs',\n",
       "   'sequencial': '19',\n",
       "   'nomePeriodico': 'JOURNAL OF NETWORK AND COMPUTER APPLICATIONS'},\n",
       "  '19': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/s18030689',\n",
       "   'data-issn': '14248220',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '14248220',\n",
       "   'volume': '18',\n",
       "   'paginaInicial': '689',\n",
       "   'titulo': 'An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments',\n",
       "   'sequencial': '20',\n",
       "   'nomePeriodico': 'SENSORS'},\n",
       "  '20': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/s18051312',\n",
       "   'data-issn': '14248220',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '14248220',\n",
       "   'volume': '18',\n",
       "   'paginaInicial': '1312',\n",
       "   'titulo': 'Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks',\n",
       "   'sequencial': '21',\n",
       "   'nomePeriodico': 'SENSORS'},\n",
       "  '21': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1109/comst.2017.2745505',\n",
       "   'data-issn': '1553877X',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 35.6',\n",
       "   'impact-factor': '35.6',\n",
       "   'issn': '1553877X',\n",
       "   'volume': '19',\n",
       "   'paginaInicial': '2704',\n",
       "   'titulo': 'Model-Based Quantitative Network Security Metrics: A Survey',\n",
       "   'sequencial': '22',\n",
       "   'nomePeriodico': 'IEEE Communications Surveys and Tutorials'},\n",
       "  '22': {'doi': '10.18464/cybin.v3i1',\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '24942715',\n",
       "   'volume': '3',\n",
       "   'paginaInicial': '30',\n",
       "   'titulo': 'Exploring a P2P Transient Botnet - From Discovery to Enumeration',\n",
       "   'sequencial': '23',\n",
       "   'nomePeriodico': 'The Journal on Cybercrime '},\n",
       "  '23': {'doi': '10.21528/lnlm-vol14-no1-art1',\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '16762789',\n",
       "   'volume': '14',\n",
       "   'paginaInicial': '4',\n",
       "   'titulo': 'UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES',\n",
       "   'sequencial': '24',\n",
       "   'nomePeriodico': 'LEARNING AND NONLINEAR MODELS'},\n",
       "  '24': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.3390/s150102104',\n",
       "   'data-issn': '14248220',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 3.9',\n",
       "   'impact-factor': '3.9',\n",
       "   'issn': '14248220',\n",
       "   'volume': '15',\n",
       "   'paginaInicial': '2104',\n",
       "   'titulo': 'Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks',\n",
       "   'sequencial': '25',\n",
       "   'nomePeriodico': 'SENSORS'},\n",
       "  '25': {'jcr-ano': 'JCR 2018',\n",
       "   'doi': '10.1155/2014/506203',\n",
       "   'data-issn': '15501329',\n",
       "   'original_title': 'Fator de impacto (JCR 2018): 1.614',\n",
       "   'impact-factor': '1.614',\n",
       "   'issn': '15501329',\n",
       "   'volume': '2014',\n",
       "   'paginaInicial': '1',\n",
       "   'titulo': 'A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions',\n",
       "   'sequencial': '26',\n",
       "   'nomePeriodico': 'International Journal of Distributed Sensor Networks'},\n",
       "  '26': {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '15487709',\n",
       "   'volume': '10',\n",
       "   'paginaInicial': '702',\n",
       "   'titulo': 'A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks',\n",
       "   'sequencial': '27',\n",
       "   'nomePeriodico': 'JOURNAL OF COMMUNICATION AND COMPUTER'},\n",
       "  '27': {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '16762789',\n",
       "   'volume': '10',\n",
       "   'paginaInicial': '4',\n",
       "   'titulo': 'Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink',\n",
       "   'sequencial': '28',\n",
       "   'nomePeriodico': 'Learning and Nonlinear Models'},\n",
       "  '28': {'doi': '10.3233/HIS-2012-0147',\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '14485869',\n",
       "   'volume': '9',\n",
       "   'paginaInicial': '61',\n",
       "   'titulo': 'An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks',\n",
       "   'sequencial': '29',\n",
       "   'nomePeriodico': 'INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS'},\n",
       "  '29': {'jcr-ano': 'JCR 2022',\n",
       "   'doi': '10.1016/j.inffus.2012.01.010',\n",
       "   'data-issn': '15662535',\n",
       "   'original_title': 'Fator de impacto (JCR 2022): 18.6',\n",
       "   'impact-factor': '18.6',\n",
       "   'issn': '15662535',\n",
       "   'volume': '15',\n",
       "   'paginaInicial': '44',\n",
       "   'titulo': 'On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries',\n",
       "   'sequencial': '30',\n",
       "   'nomePeriodico': 'Information Fusion (Print)'},\n",
       "  '30': {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '1980086X',\n",
       "   'volume': 'X',\n",
       "   'paginaInicial': '65',\n",
       "   'titulo': 'Controle Externo da Governança de Tecnologia da Informação',\n",
       "   'sequencial': '31',\n",
       "   'nomePeriodico': 'Revista Controle'},\n",
       "  '31': {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '01018191',\n",
       "   'volume': '28',\n",
       "   'paginaInicial': '33',\n",
       "   'titulo': 'Detecting computer network attacks using statistical discriminators and cluster analysis',\n",
       "   'sequencial': '32',\n",
       "   'nomePeriodico': 'Revista Tecnologia (UNIFOR)'},\n",
       "  '32': {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '01018191',\n",
       "   'volume': '27',\n",
       "   'paginaInicial': '113',\n",
       "   'titulo': 'Broadband network traffic characterization and classification using a multivariate statistical method',\n",
       "   'sequencial': '33',\n",
       "   'nomePeriodico': 'Revista Tecnologia (UNIFOR)'},\n",
       "  '33': {'doi': None,\n",
       "   'data-issn': None,\n",
       "   'original_title': None,\n",
       "   'impact-factor': None,\n",
       "   'issn': '14145685',\n",
       "   'volume': '8o.',\n",
       "   'titulo': 'Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários',\n",
       "   'sequencial': '34',\n",
       "   'nomePeriodico': 'Revista Brasileira de Informática na Educação'}},\n",
       " 'Eventos': {'Participação em eventos, congressos, exposições e feiras': {'1.': 'IEEE International Conference on Engineering Veracruz. Explainable Intelligence: Opportunities and Challenges. 2021. (Congresso).',\n",
       "   '2.': 'Mundo Unifor.Panorama Atual das Criptomoedas: da ilegalidade à adoção como moeda oficial.2021. (Encontro).',\n",
       "   '3.': 'Encontro Unificado de Computação em Parnaíba.Análise de Protocolos de Roteamento em RSSF: uma abordagem hierárquica utilizando agregaçao de dados.2009. (Encontro).',\n",
       "   '4.': 'Escola Regional de Computação: Ceará, Maranhão e Piauí.Simulação em RSSF oara Protocolos de Roteamento usando uma Abordagem Geocast.2009. (Encontro).',\n",
       "   '5.': 'Mundo Unifor 2009.Tecnologia de Redes de Sensores sem Fio.2009. (Encontro).',\n",
       "   '6.': 'Congresso Ceará de Gestão Pública.Gestão Pública.2008. (Encontro).',\n",
       "   '7.': 'Escola Regional de Computação: Ceará, Maranhão e Piauí.ERCEMAPI 2008.2008. (Encontro).',\n",
       "   '8.': '6a Semana de Tecnologia da UNIFOR.Novas Tecnologias de Redes sem Fio.2007. (Encontro).',\n",
       "   '9.': 'Escola Regional de Computação: Ceará, Maranhão e Piauí.Administração Avançada de Redes.2007. (Encontro).',\n",
       "   '10.': 'XIII Encontro de Iniciação à Pesquisa da Unifor.Sistema de Detecção de Ataques baseado em Anomalias de Tráfego.2007. (Encontro).',\n",
       "   '11.': 'Curso AWFSS - Aironet Wireless LAN Fundamentals and Cisco Aironet Wireless Site Survey.Curso AWFSS - Aironet Wireless LAN Fundamentals and Cisco Aironet Wireless Site Survey. 2006. (Outra).',\n",
       "   '12.': 'First Euro-NGI Summer School.Traffic Engineering for the Next Generation Internet. 2004. (Oficina).',\n",
       "   '13.': 'INTEL Internship Program.INTEL Internship Program. 2004. (Outra).',\n",
       "   '14.': 'Curso CCNA 4 - Wan Technologies.Curso CCNA 4 - Wan Technologies. 2003. (Outra).',\n",
       "   '15.': 'Curso CISCO CCNA 1.Curso CISCO CCNA 1. 2003. (Outra).',\n",
       "   '16.': 'Curso CISCO CCNA 2 - Routers and Routing Basics.Curso CISCO CCNA 2 - Routers and Routing Basics. 2003. (Outra).',\n",
       "   '17.': 'Curso CISCO CCNA 3 - Switching Basics and Intermediate Routing.Curso CISCO CCNA 3 - Switching Basics and Intermediate Routing. 2003. (Outra).',\n",
       "   '18.': 'COST 279 First European Summer School.Fundamentals of Wireless Networks. 2002. (Oficina).',\n",
       "   '19.': 'COST 279 First European Summer School.Queueing Systems and Loss Networks. 2002. (Oficina).',\n",
       "   '20.': 'COST 279 First European Summer School.Spatial Queueing Models. 2002. (Oficina).',\n",
       "   '21.': 'Curso Administering Unicenter TNG Asset Management Option / AimIT.Curso Administering Unicenter TNG Asset Management Option / AimIT. 2001. (Outra).',\n",
       "   '22.': 'Curso Administering Unicenter TNG Software Delivery Option / ShipIT.Curso Administering Unicenter TNG Software Delivery Option / ShipIT. 2001. (Outra).',\n",
       "   '23.': 'Curso ArcserverIT Administration Fundamentals.Curso ArcserverIT Administration Fundamentals. 2001. (Outra).',\n",
       "   '24.': 'Curso E Trust Admin Seminar.Curso E Trust Admin Seminar. 2001. (Outra).',\n",
       "   '25.': 'Curso Unicenter TNG Basics.Curso Unicenter TNG Basics. 2001. (Outra).',\n",
       "   '26.': 'Curso Unicenter TNG Remote Control Option.Curso Unicenter TNG Remote Control Option. 2001. (Outra).',\n",
       "   '27.': 'I Encontro de Pós-Graduação e Pesquisa.Trabalho Científico Intitulado Utilização da Tecnologia de Agentes Inteligentes para a Construção de Ambientes Colaborativos de Ensino-Aprendizagem.2001. (Encontro).',\n",
       "   '28.': 'XV Encontro de Pesquisa Educacional das Regiões Norte e Nordeste.Encontro de Pesquisa Educacional das Regiões Norte e Nordeste.2001. (Encontro).',\n",
       "   '29.': '5a. Semana Insoft de Tecnologia da Informação.5a. Semana Insoft de tecnologia da Informação.2000. (Encontro).',\n",
       "   '30.': 'Curso Lotus.Curso Implementando uma infra-estrutura DOMINO. 2000. (Outra).',\n",
       "   '31.': 'Projeto de Pesquisa Tele-Ambiente (Protem CC-IE).Palestra com o tema Ambientes Virtuais Cooperativos. 2000. (Oficina).',\n",
       "   '32.': 'Simpósio Brasilieiro de Informática na Educação.XI Simpósio Brasileiro de Informática na Educação.2000. (Simpósio).',\n",
       "   '33.': 'Curso.Curso Lotus Notes - System Administration 2. 1999. (Outra).'},\n",
       "  'Organização de eventos, congressos, exposições e feiras': {'1.': 'HOLANDA FILHO, R.. Euro-Par 2008 Conference. 2008. (Congresso).',\n",
       "   '2.': 'HOLANDA FILHO, R.. ERCEMAPI - Escola Regional de Computacao Ceará-Maranhao-Piaui. 2007. (Congresso).',\n",
       "   '3.': 'HOLANDA FILHO, R.. XII Seminario Associacao dos Estudantes e Pesquisadores Brasileiros na Catalunha. 2007. (Outro).'}}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráficos dos dados extraídos e persistidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar uma lista de dicionários\n",
    "data_list = dict_doi_list(data_dict)\n",
    "# pprint(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Caso não esteja como dicionário e sim listas ou strings\n",
    "# # Convert the entire string representation to a dictionary\n",
    "# outer_dict = json.loads(data_list[0])\n",
    "# # Iterate over the outer dictionary and convert the inner JSON-like strings to valid dictionaries\n",
    "# for key, value in outer_dict.items():\n",
    "#     outer_dict[key] = json.loads(value)\n",
    "\n",
    "# # Now, outer_dict holds the data in nested dictionary format\n",
    "# pprint(outer_dict)\n",
    "\n",
    "# jcr_properties_list = outer_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 ISSN recuperados, 2 ISSN ausentes\n",
      "25  DOI recuperados, 15  DOI ausentes\n",
      "19  JCI recuperados, 15  JCI ausentes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>jcr-ano</th>\n",
       "      <th>doi</th>\n",
       "      <th>data-issn</th>\n",
       "      <th>original_title</th>\n",
       "      <th>impact-factor</th>\n",
       "      <th>issn</th>\n",
       "      <th>volume</th>\n",
       "      <th>paginaInicial</th>\n",
       "      <th>titulo</th>\n",
       "      <th>sequencial</th>\n",
       "      <th>nomePeriodico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/comst.2017.2745505</td>\n",
       "      <td>1553877X</td>\n",
       "      <td>Fator de impacto (JCR 2022): 35.6</td>\n",
       "      <td>35.6</td>\n",
       "      <td>1553877X</td>\n",
       "      <td>19</td>\n",
       "      <td>2704</td>\n",
       "      <td>Model-Based Quantitative Network Security Metrics: A Survey</td>\n",
       "      <td>22</td>\n",
       "      <td>IEEE Communications Surveys and Tutorials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1016/j.inffus.2012.01.010</td>\n",
       "      <td>15662535</td>\n",
       "      <td>Fator de impacto (JCR 2022): 18.6</td>\n",
       "      <td>18.6</td>\n",
       "      <td>15662535</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries</td>\n",
       "      <td>30</td>\n",
       "      <td>Information Fusion (Print)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/jiot.2019.2904302</td>\n",
       "      <td>23274662</td>\n",
       "      <td>Fator de impacto (JCR 2022): 10.6</td>\n",
       "      <td>10.6</td>\n",
       "      <td>23274662</td>\n",
       "      <td>6</td>\n",
       "      <td>5631</td>\n",
       "      <td>Enabling Online Quantitative Security Analysis in 6LoWPAN Networks</td>\n",
       "      <td>15</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/mnet.2018.1800151</td>\n",
       "      <td>08908044</td>\n",
       "      <td>Fator de impacto (JCR 2022): 9.3</td>\n",
       "      <td>9.3</td>\n",
       "      <td>08908044</td>\n",
       "      <td>33</td>\n",
       "      <td>126</td>\n",
       "      <td>Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering</td>\n",
       "      <td>16</td>\n",
       "      <td>IEEE NETWORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1016/j.jnca.2018.01.015</td>\n",
       "      <td>10848045</td>\n",
       "      <td>Fator de impacto (JCR 2022): 8.7</td>\n",
       "      <td>8.7</td>\n",
       "      <td>10848045</td>\n",
       "      <td>107</td>\n",
       "      <td>56</td>\n",
       "      <td>A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs</td>\n",
       "      <td>19</td>\n",
       "      <td>JOURNAL OF NETWORK AND COMPUTER APPLICATIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/access.2020.3009167</td>\n",
       "      <td>21693536</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>21693536</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Integration of the Mobile Robot and Internet of Things to Monitor Older People</td>\n",
       "      <td>12</td>\n",
       "      <td>IEEE Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/ACCESS.2022.3179004</td>\n",
       "      <td>21693536</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>21693536</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain</td>\n",
       "      <td>5</td>\n",
       "      <td>IEEE Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s20113068</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>14248220</td>\n",
       "      <td>20</td>\n",
       "      <td>3068</td>\n",
       "      <td>Enhancing Key Management in LoRaWAN with Permissioned Blockchain</td>\n",
       "      <td>11</td>\n",
       "      <td>SENSORS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s18051312</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>14248220</td>\n",
       "      <td>18</td>\n",
       "      <td>1312</td>\n",
       "      <td>Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks</td>\n",
       "      <td>21</td>\n",
       "      <td>SENSORS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s18030689</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>14248220</td>\n",
       "      <td>18</td>\n",
       "      <td>689</td>\n",
       "      <td>An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments</td>\n",
       "      <td>20</td>\n",
       "      <td>SENSORS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s18020353</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>14248220</td>\n",
       "      <td>18</td>\n",
       "      <td>353</td>\n",
       "      <td>A Proposal for IoT Dynamic Routes Selection Based on Contextual Information</td>\n",
       "      <td>17</td>\n",
       "      <td>SENSORS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s150102104</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>14248220</td>\n",
       "      <td>15</td>\n",
       "      <td>2104</td>\n",
       "      <td>Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks</td>\n",
       "      <td>25</td>\n",
       "      <td>SENSORS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>http://dx.doi.org/10.1109/ACCESS.2023.3260020</td>\n",
       "      <td>21693536</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JCR 2021</td>\n",
       "      <td>http://dx.doi.org/10.1007/s12652-023-04555-3</td>\n",
       "      <td>18685137</td>\n",
       "      <td>Fator de impacto (JCR 2021): 3.662</td>\n",
       "      <td>3.662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/app12188939</td>\n",
       "      <td>20763417</td>\n",
       "      <td>Fator de impacto (JCR 2022): 2.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>20763417</td>\n",
       "      <td>12</td>\n",
       "      <td>8939</td>\n",
       "      <td>Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19</td>\n",
       "      <td>7</td>\n",
       "      <td>Applied Sciences-Basel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/app112110457</td>\n",
       "      <td>20763417</td>\n",
       "      <td>Fator de impacto (JCR 2022): 2.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>20763417</td>\n",
       "      <td>11</td>\n",
       "      <td>10457</td>\n",
       "      <td>An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus</td>\n",
       "      <td>10</td>\n",
       "      <td>Applied Sciences-Basel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1002/dac.3380</td>\n",
       "      <td>10745351</td>\n",
       "      <td>Fator de impacto (JCR 2022): 2.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>10745351</td>\n",
       "      <td>31</td>\n",
       "      <td>e3380</td>\n",
       "      <td>Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks</td>\n",
       "      <td>18</td>\n",
       "      <td>INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>JCR 2018</td>\n",
       "      <td>10.1155/2014/506203</td>\n",
       "      <td>15501329</td>\n",
       "      <td>Fator de impacto (JCR 2018): 1.614</td>\n",
       "      <td>1.614</td>\n",
       "      <td>15501329</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions</td>\n",
       "      <td>26</td>\n",
       "      <td>International Journal of Distributed Sensor Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1002/nem.2062</td>\n",
       "      <td>10557148</td>\n",
       "      <td>Fator de impacto (JCR 2022): 1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>10557148</td>\n",
       "      <td>1</td>\n",
       "      <td>e2062</td>\n",
       "      <td>An approach for provisioning virtual sensors in sensor clouds</td>\n",
       "      <td>14</td>\n",
       "      <td>International Journal of Network Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18464/cybin.v3i1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24942715</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>Exploring a P2P Transient Botnet - From Discovery to Enumeration</td>\n",
       "      <td>23</td>\n",
       "      <td>The Journal on Cybercrime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.9790/487X-2507032735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2278487X</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector</td>\n",
       "      <td>4</td>\n",
       "      <td>Journal of Business and Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22360700</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública</td>\n",
       "      <td>6</td>\n",
       "      <td>Revista Razão Contábil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22360700</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará</td>\n",
       "      <td>8</td>\n",
       "      <td>Revista Razão Contábil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22360700</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019</td>\n",
       "      <td>13</td>\n",
       "      <td>Revista Razão Contábil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/978-3-030-84311-3_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22138684</td>\n",
       "      <td>2021</td>\n",
       "      <td>13</td>\n",
       "      <td>Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19</td>\n",
       "      <td>9</td>\n",
       "      <td>Springer Proceedings in Complexity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980086X</td>\n",
       "      <td>X</td>\n",
       "      <td>65</td>\n",
       "      <td>Controle Externo da Governança de Tecnologia da Informação</td>\n",
       "      <td>31</td>\n",
       "      <td>Revista Controle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.procs.2023.03.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18770509</td>\n",
       "      <td>220</td>\n",
       "      <td>119</td>\n",
       "      <td>Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain</td>\n",
       "      <td>3</td>\n",
       "      <td>PROCEDIA COMPUTER SCIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.21528/lnlm-vol14-no1-art1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16762789</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES</td>\n",
       "      <td>24</td>\n",
       "      <td>LEARNING AND NONLINEAR MODELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16762789</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink</td>\n",
       "      <td>28</td>\n",
       "      <td>Learning and Nonlinear Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15487709</td>\n",
       "      <td>10</td>\n",
       "      <td>702</td>\n",
       "      <td>A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks</td>\n",
       "      <td>27</td>\n",
       "      <td>JOURNAL OF COMMUNICATION AND COMPUTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.3233/HIS-2012-0147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14485869</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks</td>\n",
       "      <td>29</td>\n",
       "      <td>INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14145685</td>\n",
       "      <td>8o.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários</td>\n",
       "      <td>34</td>\n",
       "      <td>Revista Brasileira de Informática na Educação</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01018191</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>Detecting computer network attacks using statistical discriminators and cluster analysis</td>\n",
       "      <td>32</td>\n",
       "      <td>Revista Tecnologia (UNIFOR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01018191</td>\n",
       "      <td>27</td>\n",
       "      <td>113</td>\n",
       "      <td>Broadband network traffic characterization and classification using a multivariate statistical method</td>\n",
       "      <td>33</td>\n",
       "      <td>Revista Tecnologia (UNIFOR)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jcr-ano   doi                                            data-issn   \n",
       "21  JCR 2022                     10.1109/comst.2017.2745505  1553877X  \\\n",
       "29  JCR 2022                   10.1016/j.inffus.2012.01.010  15662535   \n",
       "14  JCR 2022                      10.1109/jiot.2019.2904302  23274662   \n",
       "15  JCR 2022                      10.1109/mnet.2018.1800151  08908044   \n",
       "18  JCR 2022                     10.1016/j.jnca.2018.01.015  10848045   \n",
       "11  JCR 2022                    10.1109/access.2020.3009167  21693536   \n",
       "4   JCR 2022                    10.1109/ACCESS.2022.3179004  21693536   \n",
       "10  JCR 2022                              10.3390/s20113068  14248220   \n",
       "20  JCR 2022                              10.3390/s18051312  14248220   \n",
       "19  JCR 2022                              10.3390/s18030689  14248220   \n",
       "16  JCR 2022                              10.3390/s18020353  14248220   \n",
       "24  JCR 2022                             10.3390/s150102104  14248220   \n",
       "1   JCR 2022  http://dx.doi.org/10.1109/ACCESS.2023.3260020  21693536   \n",
       "0   JCR 2021   http://dx.doi.org/10.1007/s12652-023-04555-3  18685137   \n",
       "6   JCR 2022                            10.3390/app12188939  20763417   \n",
       "9   JCR 2022                           10.3390/app112110457  20763417   \n",
       "17  JCR 2022                               10.1002/dac.3380  10745351   \n",
       "25  JCR 2018                            10.1155/2014/506203  15501329   \n",
       "13  JCR 2022                               10.1002/nem.2062  10557148   \n",
       "22       NaN                            10.18464/cybin.v3i1       NaN   \n",
       "3        NaN                        10.9790/487X-2507032735       NaN   \n",
       "5        NaN                                            NaN       NaN   \n",
       "7        NaN                                            NaN       NaN   \n",
       "12       NaN                                            NaN       NaN   \n",
       "8        NaN                    10.1007/978-3-030-84311-3_2       NaN   \n",
       "30       NaN                                            NaN       NaN   \n",
       "2        NaN                    10.1016/j.procs.2023.03.018       NaN   \n",
       "23       NaN                   10.21528/lnlm-vol14-no1-art1       NaN   \n",
       "27       NaN                                            NaN       NaN   \n",
       "26       NaN                                            NaN       NaN   \n",
       "28       NaN                          10.3233/HIS-2012-0147       NaN   \n",
       "33       NaN                                            NaN       NaN   \n",
       "31       NaN                                            NaN       NaN   \n",
       "32       NaN                                            NaN       NaN   \n",
       "\n",
       "   original_title                      impact-factor issn      volume   \n",
       "21   Fator de impacto (JCR 2022): 35.6   35.6         1553877X    19   \\\n",
       "29   Fator de impacto (JCR 2022): 18.6   18.6         15662535    15    \n",
       "14   Fator de impacto (JCR 2022): 10.6   10.6         23274662     6    \n",
       "15    Fator de impacto (JCR 2022): 9.3    9.3         08908044    33    \n",
       "18    Fator de impacto (JCR 2022): 8.7    8.7         10848045   107    \n",
       "11    Fator de impacto (JCR 2022): 3.9    3.9         21693536     1    \n",
       "4     Fator de impacto (JCR 2022): 3.9    3.9         21693536     1    \n",
       "10    Fator de impacto (JCR 2022): 3.9    3.9         14248220    20    \n",
       "20    Fator de impacto (JCR 2022): 3.9    3.9         14248220    18    \n",
       "19    Fator de impacto (JCR 2022): 3.9    3.9         14248220    18    \n",
       "16    Fator de impacto (JCR 2022): 3.9    3.9         14248220    18    \n",
       "24    Fator de impacto (JCR 2022): 3.9    3.9         14248220    15    \n",
       "1     Fator de impacto (JCR 2022): 3.9    3.9              NaN   NaN    \n",
       "0   Fator de impacto (JCR 2021): 3.662  3.662              NaN   NaN    \n",
       "6     Fator de impacto (JCR 2022): 2.7    2.7         20763417    12    \n",
       "9     Fator de impacto (JCR 2022): 2.7    2.7         20763417    11    \n",
       "17    Fator de impacto (JCR 2022): 2.1    2.1         10745351    31    \n",
       "25  Fator de impacto (JCR 2018): 1.614  1.614         15501329  2014    \n",
       "13    Fator de impacto (JCR 2022): 1.5    1.5         10557148     1    \n",
       "22                                 NaN    NaN         24942715     3    \n",
       "3                                  NaN    NaN         2278487X    25    \n",
       "5                                  NaN    NaN         22360700    13    \n",
       "7                                  NaN    NaN         22360700    12    \n",
       "12                                 NaN    NaN         22360700    11    \n",
       "8                                  NaN    NaN         22138684  2021    \n",
       "30                                 NaN    NaN         1980086X     X    \n",
       "2                                  NaN    NaN         18770509   220    \n",
       "23                                 NaN    NaN         16762789    14    \n",
       "27                                 NaN    NaN         16762789    10    \n",
       "26                                 NaN    NaN         15487709    10    \n",
       "28                                 NaN    NaN         14485869     9    \n",
       "33                                 NaN    NaN         14145685   8o.    \n",
       "31                                 NaN    NaN         01018191    28    \n",
       "32                                 NaN    NaN         01018191    27    \n",
       "\n",
       "   paginaInicial   \n",
       "21   2704         \\\n",
       "29     44          \n",
       "14   5631          \n",
       "15    126          \n",
       "18     56          \n",
       "11      1          \n",
       "4       1          \n",
       "10   3068          \n",
       "20   1312          \n",
       "19    689          \n",
       "16    353          \n",
       "24   2104          \n",
       "1     NaN          \n",
       "0     NaN          \n",
       "6    8939          \n",
       "9   10457          \n",
       "17  e3380          \n",
       "25      1          \n",
       "13  e2062          \n",
       "22     30          \n",
       "3      27          \n",
       "5       1          \n",
       "7       1          \n",
       "12      1          \n",
       "8      13          \n",
       "30     65          \n",
       "2     119          \n",
       "23      4          \n",
       "27      4          \n",
       "26    702          \n",
       "28     61          \n",
       "33    NaN          \n",
       "31     33          \n",
       "32    113          \n",
       "\n",
       "   titulo                                                                                                                                                                  \n",
       "21                                                                                                           Model-Based Quantitative Network Security Metrics: A Survey  \\\n",
       "29                                                                                   On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries   \n",
       "14                                                                                                    Enabling Online Quantitative Security Analysis in 6LoWPAN Networks   \n",
       "15                                                                                       Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering   \n",
       "18                                                                   A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs   \n",
       "11                                                                                        Integration of the Mobile Robot and Internet of Things to Monitor Older People   \n",
       "4                                                                A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain   \n",
       "10                                                                                                      Enhancing Key Management in LoRaWAN with Permissioned Blockchain   \n",
       "20                                                        Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks   \n",
       "19                                                                    An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments   \n",
       "16                                                                                           A Proposal for IoT Dynamic Routes Selection Based on Contextual Information   \n",
       "24                                                                                             Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks   \n",
       "1                                                                                                                                                                    NaN   \n",
       "0                                                                                                                                                                    NaN   \n",
       "6                                                           Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19   \n",
       "9                                                        An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus   \n",
       "17                                               Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks   \n",
       "25                                                                                      A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions   \n",
       "13                                                                                                         An approach for provisioning virtual sensors in sensor clouds   \n",
       "22                                                                                                      Exploring a P2P Transient Botnet - From Discovery to Enumeration   \n",
       "3                                                     Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector   \n",
       "5                                                                  Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública   \n",
       "7                                                                                 Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará   \n",
       "12                                                                  Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019   \n",
       "8                                                                                         Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19   \n",
       "30                                                                                                            Controle Externo da Governança de Tecnologia da Informação   \n",
       "2                                                          Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain   \n",
       "23                                                                                       UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES   \n",
       "27  Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink   \n",
       "26                                                                   A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks   \n",
       "28                                                                         An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks   \n",
       "33                                                                                               Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários   \n",
       "31                                                                              Detecting computer network attacks using statistical discriminators and cluster analysis   \n",
       "32                                                                 Broadband network traffic characterization and classification using a multivariate statistical method   \n",
       "\n",
       "   sequencial nomePeriodico                                          \n",
       "21   22                   IEEE Communications Surveys and Tutorials  \n",
       "29   30                                  Information Fusion (Print)  \n",
       "14   15                             IEEE Internet of Things Journal  \n",
       "15   16                                                IEEE NETWORK  \n",
       "18   19                JOURNAL OF NETWORK AND COMPUTER APPLICATIONS  \n",
       "11   12                                                 IEEE Access  \n",
       "4     5                                                 IEEE Access  \n",
       "10   11                                                     SENSORS  \n",
       "20   21                                                     SENSORS  \n",
       "19   20                                                     SENSORS  \n",
       "16   17                                                     SENSORS  \n",
       "24   25                                                     SENSORS  \n",
       "1   NaN                                                         NaN  \n",
       "0   NaN                                                         NaN  \n",
       "6     7                                      Applied Sciences-Basel  \n",
       "9    10                                      Applied Sciences-Basel  \n",
       "17   18              INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS  \n",
       "25   26        International Journal of Distributed Sensor Networks  \n",
       "13   14                 International Journal of Network Management  \n",
       "22   23                                  The Journal on Cybercrime   \n",
       "3     4                          Journal of Business and Management  \n",
       "5     6                                     Revista Razão Contábil   \n",
       "7     8                                     Revista Razão Contábil   \n",
       "12   13                                     Revista Razão Contábil   \n",
       "8     9                          Springer Proceedings in Complexity  \n",
       "30   31                                            Revista Controle  \n",
       "2     3                                   PROCEDIA COMPUTER SCIENCE  \n",
       "23   24                               LEARNING AND NONLINEAR MODELS  \n",
       "27   28                               Learning and Nonlinear Models  \n",
       "26   27                       JOURNAL OF COMMUNICATION AND COMPUTER  \n",
       "28   29         INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS  \n",
       "33   34               Revista Brasileira de Informática na Educação  \n",
       "31   32                                 Revista Tecnologia (UNIFOR)  \n",
       "32   33                                 Revista Tecnologia (UNIFOR)  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data_list)\n",
    "\n",
    "# Substituir 'NULL' e None por np.nan em todo o dataframe\n",
    "data.replace(['NULL', None], np.nan, inplace=True)\n",
    "data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "data = data.sort_values(by=['impact-factor','issn','doi'], ascending=False)\n",
    "count_jci = data['impact-factor'].notna().sum()\n",
    "count_doi = data['doi'].notna().sum()\n",
    "count_jcrissn = data['issn'].notna().sum()\n",
    "print(f'{count_jcrissn} ISSN recuperados, {len(data.index)-count_jcrissn} ISSN ausentes')\n",
    "print(f'{count_doi}  DOI recuperados, {len(data.index)-count_jci}  DOI ausentes')\n",
    "print(f'{count_jci}  JCI recuperados, {len(data.index)-count_jci}  JCI ausentes')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complementar com dados do CrossRef "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25  DOI recuperados, 9  DOI ausentes\n",
      "22 ISSN recuperados, 12 ISSN ausentes\n",
      "19  JCI recuperados, 15  JCI ausentes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>jcr-ano</th>\n",
       "      <th>doi</th>\n",
       "      <th>data-issn</th>\n",
       "      <th>original_title</th>\n",
       "      <th>impact-factor</th>\n",
       "      <th>issn</th>\n",
       "      <th>volume</th>\n",
       "      <th>paginaInicial</th>\n",
       "      <th>titulo</th>\n",
       "      <th>sequencial</th>\n",
       "      <th>nomePeriodico</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.procs.2023.03.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1877-0509</td>\n",
       "      <td>220</td>\n",
       "      <td>119</td>\n",
       "      <td>Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain</td>\n",
       "      <td>3</td>\n",
       "      <td>PROCEDIA COMPUTER SCIENCE</td>\n",
       "      <td>Procedia Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.21528/lnlm-vol14-no1-art1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1676-2789</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES</td>\n",
       "      <td>24</td>\n",
       "      <td>LEARNING AND NONLINEAR MODELS</td>\n",
       "      <td>Learning and Nonlinear Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.3233/HIS-2012-0147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1875-8819</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks</td>\n",
       "      <td>29</td>\n",
       "      <td>INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS</td>\n",
       "      <td>International Journal of Hybrid Intelligent Systems</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jcr-ano doi                           data-issn original_title   \n",
       "2   NaN      10.1016/j.procs.2023.03.018  NaN       NaN            \\\n",
       "23  NaN     10.21528/lnlm-vol14-no1-art1  NaN       NaN             \n",
       "28  NaN            10.3233/HIS-2012-0147  NaN       NaN             \n",
       "\n",
       "    impact-factor issn       volume paginaInicial   \n",
       "2  NaN             1877-0509  220    119           \\\n",
       "23 NaN             1676-2789   14      4            \n",
       "28 NaN             1875-8819    9     61            \n",
       "\n",
       "   titulo                                                                                                           \n",
       "2   Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain  \\\n",
       "23                                UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES   \n",
       "28                  An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks   \n",
       "\n",
       "   sequencial nomePeriodico                                          \n",
       "2    3                                   PROCEDIA COMPUTER SCIENCE  \\\n",
       "23  24                               LEARNING AND NONLINEAR MODELS   \n",
       "28  29         INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS   \n",
       "\n",
       "   journal                                               \n",
       "2                             Procedia Computer Science  \n",
       "23                        Learning and Nonlinear Models  \n",
       "28  International Journal of Hybrid Intelligent Systems  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = crossref_complement(data)\n",
    "\n",
    "filtro1 = data['impact-factor'].isna()\n",
    "temp = data[filtro1]\n",
    "filtro2 = temp['issn'].notna()\n",
    "consultar = temp[filtro2]\n",
    "consultar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complementar ISSN de arquivos de texto do JCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ISSN extraídos do currículo\n",
      "1553-877X, 1566-2535, 2327-4662, 0890-8044, 1084-8045, 2169-3536, 2169-3536, 1424-8220, 1424-8220, 1424-8220, 1424-8220, 1424-8220, 2169-3536, 1868-5137, 2076-3417, 2076-3417, 1074-5351, 1550-1477, 1055-7148, 1877-0509, 1676-2789, 1875-8819\n"
     ]
    }
   ],
   "source": [
    "df_start = data\n",
    "print(montar_lista_issn(df_start))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 30 buscados\n",
    "'1566-2535, 2327-4662, 0890-8044, 1084-8045, 2169-3536, 2169-3536, 2169-3536, 1868-5137, 2076-3417, 1074-5351, 1055-7148, 2494-2715, 2236-0700, 2236-0700, 2076-3417, 1980-086X, 1676-2789, 1676-2789, 1553-877X, 1550-1329, 1548-7709, 1448-5869, 1424-8220, 1424-8220, 1424-8220, 1424-8220, 1424-8220, 1414-5685, 0101-8191, 0101-8191'\n",
    "\n",
    "# 09 duplicados\n",
    "'1980-086X, 1676-2789, 1548-7709, 1448-5869, 2494-2715, 1424-5685, 0101-8191, 2327-4662, 2236-0700'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>Journal name</th>\n",
       "      <th>JCR Abbreviation</th>\n",
       "      <th>ISSN</th>\n",
       "      <th>eISSN</th>\n",
       "      <th>Category</th>\n",
       "      <th>Total Citations</th>\n",
       "      <th>2018 JIF</th>\n",
       "      <th>JIF Quartile</th>\n",
       "      <th>2018 JCI</th>\n",
       "      <th>% of OA Gold</th>\n",
       "      <th>2019 JIF</th>\n",
       "      <th>2019 JCI</th>\n",
       "      <th>2020 JIF</th>\n",
       "      <th>2020 JCI</th>\n",
       "      <th>2021 JIF</th>\n",
       "      <th>2021 JCI</th>\n",
       "      <th>2022 JIF</th>\n",
       "      <th>2022 JCI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IEEE Communications Surveys and Tutorials</td>\n",
       "      <td>IEEE COMMUN SURV TUT</td>\n",
       "      <td>1553-877X</td>\n",
       "      <td>1553-877X</td>\n",
       "      <td>COMPUTER SCIENCE, INFORMATION SYSTEMS - SCIE</td>\n",
       "      <td>16,408</td>\n",
       "      <td>22.973</td>\n",
       "      <td>Q1</td>\n",
       "      <td>8.81</td>\n",
       "      <td>0%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IEEE Communications Surveys and Tutorials</td>\n",
       "      <td>IEEE COMMUN SURV TUT</td>\n",
       "      <td>1553-877X</td>\n",
       "      <td>1553-877X</td>\n",
       "      <td>TELECOMMUNICATIONS - SCIE</td>\n",
       "      <td>16,408</td>\n",
       "      <td>22.973</td>\n",
       "      <td>Q1</td>\n",
       "      <td>8.81</td>\n",
       "      <td>0%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Information Fusion</td>\n",
       "      <td>INFORM FUSION</td>\n",
       "      <td>1566-2535</td>\n",
       "      <td>1872-6305</td>\n",
       "      <td>COMPUTER SCIENCE, ARTIFICIAL INTELLIGENCE - SCIE</td>\n",
       "      <td>4,746</td>\n",
       "      <td>10.716</td>\n",
       "      <td>Q1</td>\n",
       "      <td>3.42</td>\n",
       "      <td>2.1%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Information Fusion</td>\n",
       "      <td>INFORM FUSION</td>\n",
       "      <td>1566-2535</td>\n",
       "      <td>1872-6305</td>\n",
       "      <td>COMPUTER SCIENCE, THEORY &amp; METHODS - SCIE</td>\n",
       "      <td>4,746</td>\n",
       "      <td>10.716</td>\n",
       "      <td>Q1</td>\n",
       "      <td>3.42</td>\n",
       "      <td>2.1%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>IEEE INTERNET THINGS</td>\n",
       "      <td>2327-4662</td>\n",
       "      <td>2327-4662</td>\n",
       "      <td>COMPUTER SCIENCE, INFORMATION SYSTEMS - SCIE</td>\n",
       "      <td>6,119</td>\n",
       "      <td>9.515</td>\n",
       "      <td>Q1</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Journal name                               JCR Abbreviation      ISSN         \n",
       "0  IEEE Communications Surveys and Tutorials  IEEE COMMUN SURV TUT  1553-877X  \\\n",
       "1  IEEE Communications Surveys and Tutorials  IEEE COMMUN SURV TUT  1553-877X   \n",
       "2                         Information Fusion         INFORM FUSION  1566-2535   \n",
       "3                         Information Fusion         INFORM FUSION  1566-2535   \n",
       "4            IEEE Internet of Things Journal  IEEE INTERNET THINGS  2327-4662   \n",
       "\n",
       "  eISSN      Category                                            \n",
       "0  1553-877X      COMPUTER SCIENCE, INFORMATION SYSTEMS - SCIE  \\\n",
       "1  1553-877X                         TELECOMMUNICATIONS - SCIE   \n",
       "2  1872-6305  COMPUTER SCIENCE, ARTIFICIAL INTELLIGENCE - SCIE   \n",
       "3  1872-6305         COMPUTER SCIENCE, THEORY & METHODS - SCIE   \n",
       "4  2327-4662      COMPUTER SCIENCE, INFORMATION SYSTEMS - SCIE   \n",
       "\n",
       "  Total Citations  2018 JIF JIF Quartile  2018 JCI % of OA Gold  2019 JIF   \n",
       "0  16,408          22.973    Q1           8.81        0%        NaN        \\\n",
       "1  16,408          22.973    Q1           8.81        0%        NaN         \n",
       "2   4,746          10.716    Q1           3.42      2.1%        NaN         \n",
       "3   4,746          10.716    Q1           3.42      2.1%        NaN         \n",
       "4   6,119           9.515    Q1           2.59        0%        NaN         \n",
       "\n",
       "   2019 JCI  2020 JIF  2020 JCI  2021 JIF  2021 JCI  2022 JIF  2022 JCI  \n",
       "0 NaN       NaN       NaN       NaN       NaN       NaN       NaN        \n",
       "1 NaN       NaN       NaN       NaN       NaN       NaN       NaN        \n",
       "2 NaN       NaN       NaN       NaN       NaN       NaN       NaN        \n",
       "3 NaN       NaN       NaN       NaN       NaN       NaN       NaN        \n",
       "4 NaN       NaN       NaN       NaN       NaN       NaN       NaN        "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File paths\n",
    "file_paths = [\n",
    "    '../data/RaimirHolandaFilho_JCR_JournalResults2018.csv',\n",
    "    '../data/RaimirHolandaFilho_JCR_JournalResults2019.csv',\n",
    "    '../data/RaimirHolandaFilho_JCR_JournalResults2020.csv',\n",
    "    '../data/RaimirHolandaFilho_JCR_JournalResults2021.csv',\n",
    "    '../data/RaimirHolandaFilho_JCR_JournalResults2022.csv'\n",
    "]\n",
    "\n",
    "# Generate df_aggregated by concatenating all the annual dataframes\n",
    "df_aggregated = load_and_concatenate(file_paths)\n",
    "df_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>Journal name</th>\n",
       "      <th>JCR Abbreviation</th>\n",
       "      <th>ISSN</th>\n",
       "      <th>eISSN</th>\n",
       "      <th>Category</th>\n",
       "      <th>Total Citations</th>\n",
       "      <th>2018 JIF</th>\n",
       "      <th>JIF Quartile</th>\n",
       "      <th>2018 JCI</th>\n",
       "      <th>% of OA Gold</th>\n",
       "      <th>2019 JIF</th>\n",
       "      <th>2019 JCI</th>\n",
       "      <th>2020 JIF</th>\n",
       "      <th>2020 JCI</th>\n",
       "      <th>2021 JIF</th>\n",
       "      <th>2021 JCI</th>\n",
       "      <th>2022 JIF</th>\n",
       "      <th>2022 JCI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IEEE Communications Surveys and Tutorials</td>\n",
       "      <td>IEEE COMMUN SURV TUT</td>\n",
       "      <td>1553-877X</td>\n",
       "      <td>1553-877X</td>\n",
       "      <td>COMPUTER SCIENCE, INFORMATION SYSTEMS - SCIE</td>\n",
       "      <td>16,408</td>\n",
       "      <td>22.973</td>\n",
       "      <td>Q1</td>\n",
       "      <td>8.81</td>\n",
       "      <td>0%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IEEE Communications Surveys and Tutorials</td>\n",
       "      <td>IEEE COMMUN SURV TUT</td>\n",
       "      <td>1553-877X</td>\n",
       "      <td>1553-877X</td>\n",
       "      <td>TELECOMMUNICATIONS - SCIE</td>\n",
       "      <td>16,408</td>\n",
       "      <td>22.973</td>\n",
       "      <td>Q1</td>\n",
       "      <td>8.81</td>\n",
       "      <td>0%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Information Fusion</td>\n",
       "      <td>INFORM FUSION</td>\n",
       "      <td>1566-2535</td>\n",
       "      <td>1872-6305</td>\n",
       "      <td>COMPUTER SCIENCE, ARTIFICIAL INTELLIGENCE - SCIE</td>\n",
       "      <td>4,746</td>\n",
       "      <td>10.716</td>\n",
       "      <td>Q1</td>\n",
       "      <td>3.42</td>\n",
       "      <td>2.1%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Information Fusion</td>\n",
       "      <td>INFORM FUSION</td>\n",
       "      <td>1566-2535</td>\n",
       "      <td>1872-6305</td>\n",
       "      <td>COMPUTER SCIENCE, THEORY &amp; METHODS - SCIE</td>\n",
       "      <td>4,746</td>\n",
       "      <td>10.716</td>\n",
       "      <td>Q1</td>\n",
       "      <td>3.42</td>\n",
       "      <td>2.1%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>IEEE INTERNET THINGS</td>\n",
       "      <td>2327-4662</td>\n",
       "      <td>2327-4662</td>\n",
       "      <td>COMPUTER SCIENCE, INFORMATION SYSTEMS - SCIE</td>\n",
       "      <td>6,119</td>\n",
       "      <td>9.515</td>\n",
       "      <td>Q1</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Journal name                               JCR Abbreviation      ISSN         \n",
       "0  IEEE Communications Surveys and Tutorials  IEEE COMMUN SURV TUT  1553-877X  \\\n",
       "1  IEEE Communications Surveys and Tutorials  IEEE COMMUN SURV TUT  1553-877X   \n",
       "2                         Information Fusion         INFORM FUSION  1566-2535   \n",
       "3                         Information Fusion         INFORM FUSION  1566-2535   \n",
       "4            IEEE Internet of Things Journal  IEEE INTERNET THINGS  2327-4662   \n",
       "\n",
       "  eISSN      Category                                            \n",
       "0  1553-877X      COMPUTER SCIENCE, INFORMATION SYSTEMS - SCIE  \\\n",
       "1  1553-877X                         TELECOMMUNICATIONS - SCIE   \n",
       "2  1872-6305  COMPUTER SCIENCE, ARTIFICIAL INTELLIGENCE - SCIE   \n",
       "3  1872-6305         COMPUTER SCIENCE, THEORY & METHODS - SCIE   \n",
       "4  2327-4662      COMPUTER SCIENCE, INFORMATION SYSTEMS - SCIE   \n",
       "\n",
       "  Total Citations  2018 JIF JIF Quartile  2018 JCI % of OA Gold  2019 JIF   \n",
       "0  16,408          22.973    Q1           8.81        0%        NaN        \\\n",
       "1  16,408          22.973    Q1           8.81        0%        NaN         \n",
       "2   4,746          10.716    Q1           3.42      2.1%        NaN         \n",
       "3   4,746          10.716    Q1           3.42      2.1%        NaN         \n",
       "4   6,119           9.515    Q1           2.59        0%        NaN         \n",
       "\n",
       "   2019 JCI  2020 JIF  2020 JCI  2021 JIF  2021 JCI  2022 JIF  2022 JCI  \n",
       "0 NaN       NaN       NaN       NaN       NaN       NaN       NaN        \n",
       "1 NaN       NaN       NaN       NaN       NaN       NaN       NaN        \n",
       "2 NaN       NaN       NaN       NaN       NaN       NaN       NaN        \n",
       "3 NaN       NaN       NaN       NaN       NaN       NaN       NaN        \n",
       "4 NaN       NaN       NaN       NaN       NaN       NaN       NaN        "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>Journal name</th>\n",
       "      <th>JCR Abbreviation</th>\n",
       "      <th>ISSN</th>\n",
       "      <th>eISSN</th>\n",
       "      <th>Category</th>\n",
       "      <th>Total Citations</th>\n",
       "      <th>2018 JIF</th>\n",
       "      <th>JIF Quartile</th>\n",
       "      <th>2018 JCI</th>\n",
       "      <th>% of OA Gold</th>\n",
       "      <th>2019 JIF</th>\n",
       "      <th>2019 JCI</th>\n",
       "      <th>2020 JIF</th>\n",
       "      <th>2020 JCI</th>\n",
       "      <th>2021 JIF</th>\n",
       "      <th>2021 JCI</th>\n",
       "      <th>2022 JIF</th>\n",
       "      <th>2022 JCI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IEEE NETWORK</td>\n",
       "      <td>IEEE NETWORK</td>\n",
       "      <td>0890-8044</td>\n",
       "      <td>1558-156X</td>\n",
       "      <td>COMPUTER SCIENCE, HARDWARE &amp; ARCHITECTURE - SCIE</td>\n",
       "      <td>10,306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.3</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>International Journal of Network Management</td>\n",
       "      <td>INT J NETW MANAG</td>\n",
       "      <td>1055-7148</td>\n",
       "      <td>1099-1190</td>\n",
       "      <td>COMPUTER SCIENCE, INFORMATION SYSTEMS - SCIE</td>\n",
       "      <td>451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.96%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS</td>\n",
       "      <td>INT J COMMUN SYST</td>\n",
       "      <td>1074-5351</td>\n",
       "      <td>1099-1131</td>\n",
       "      <td>ENGINEERING, ELECTRICAL &amp; ELECTRONIC - SCIE</td>\n",
       "      <td>4,108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.91%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JOURNAL OF NETWORK AND COMPUTER APPLICATIONS</td>\n",
       "      <td>J NETW COMPUT APPL</td>\n",
       "      <td>1084-8045</td>\n",
       "      <td>1095-8592</td>\n",
       "      <td>COMPUTER SCIENCE, HARDWARE &amp; ARCHITECTURE - SCIE</td>\n",
       "      <td>11,654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.69%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SENSORS</td>\n",
       "      <td>SENSORS-BASEL</td>\n",
       "      <td>1424-8220</td>\n",
       "      <td>1424-8220</td>\n",
       "      <td>CHEMISTRY, ANALYTICAL - SCIE</td>\n",
       "      <td>46,222</td>\n",
       "      <td>3.031</td>\n",
       "      <td>Q2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>100%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Journal name                                    JCR Abbreviation      \n",
       "0                                    IEEE NETWORK        IEEE NETWORK  \\\n",
       "1     International Journal of Network Management    INT J NETW MANAG   \n",
       "2  INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS   INT J COMMUN SYST   \n",
       "3    JOURNAL OF NETWORK AND COMPUTER APPLICATIONS  J NETW COMPUT APPL   \n",
       "4                                         SENSORS       SENSORS-BASEL   \n",
       "\n",
       "  ISSN       eISSN      Category                                            \n",
       "0  0890-8044  1558-156X  COMPUTER SCIENCE, HARDWARE & ARCHITECTURE - SCIE  \\\n",
       "1  1055-7148  1099-1190      COMPUTER SCIENCE, INFORMATION SYSTEMS - SCIE   \n",
       "2  1074-5351  1099-1131       ENGINEERING, ELECTRICAL & ELECTRONIC - SCIE   \n",
       "3  1084-8045  1095-8592  COMPUTER SCIENCE, HARDWARE & ARCHITECTURE - SCIE   \n",
       "4  1424-8220  1424-8220                      CHEMISTRY, ANALYTICAL - SCIE   \n",
       "\n",
       "  Total Citations  2018 JIF JIF Quartile  2018 JCI % of OA Gold  2019 JIF   \n",
       "0  10,306            NaN     Q1            NaN         0%       NaN        \\\n",
       "1     451            NaN     Q4            NaN      6.96%       NaN         \n",
       "2   4,108            NaN     Q3            NaN      0.91%       NaN         \n",
       "3  11,654            NaN     Q1            NaN      5.69%       NaN         \n",
       "4  46,222          3.031     Q2           0.85       100%       NaN         \n",
       "\n",
       "   2019 JCI  2020 JIF  2020 JCI  2021 JIF  2021 JCI  2022 JIF  2022 JCI  \n",
       "0 NaN       NaN       NaN       NaN       NaN        9.3       2.28      \n",
       "1 NaN       NaN       NaN       NaN       NaN        1.5       0.40      \n",
       "2 NaN       NaN       NaN       NaN       NaN        2.1       0.47      \n",
       "3 NaN       NaN       NaN       NaN       NaN        8.7       1.60      \n",
       "4 NaN       NaN       NaN       NaN       NaN        NaN        NaN      "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the most recent data for each ISSN or eISSN and get the most recent year\n",
    "df_most_recent, most_recent_year = get_most_recent_data(df_aggregated)\n",
    "df_most_recent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022, 2021, 2020, 2019, 2018]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>jcr-ano</th>\n",
       "      <th>doi</th>\n",
       "      <th>data-issn</th>\n",
       "      <th>original_title</th>\n",
       "      <th>impact-factor</th>\n",
       "      <th>issn</th>\n",
       "      <th>volume</th>\n",
       "      <th>paginaInicial</th>\n",
       "      <th>titulo</th>\n",
       "      <th>sequencial</th>\n",
       "      <th>nomePeriodico</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.procs.2023.03.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1877-0509</td>\n",
       "      <td>220</td>\n",
       "      <td>119</td>\n",
       "      <td>Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain</td>\n",
       "      <td>3</td>\n",
       "      <td>PROCEDIA COMPUTER SCIENCE</td>\n",
       "      <td>Procedia Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.21528/lnlm-vol14-no1-art1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1676-2789</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES</td>\n",
       "      <td>24</td>\n",
       "      <td>LEARNING AND NONLINEAR MODELS</td>\n",
       "      <td>Learning and Nonlinear Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.3233/HIS-2012-0147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1875-8819</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks</td>\n",
       "      <td>29</td>\n",
       "      <td>INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS</td>\n",
       "      <td>International Journal of Hybrid Intelligent Systems</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jcr-ano doi                           data-issn original_title   \n",
       "2   NaN      10.1016/j.procs.2023.03.018  NaN       NaN            \\\n",
       "23  NaN     10.21528/lnlm-vol14-no1-art1  NaN       NaN             \n",
       "28  NaN            10.3233/HIS-2012-0147  NaN       NaN             \n",
       "\n",
       "    impact-factor issn       volume paginaInicial   \n",
       "2  NaN             1877-0509  220    119           \\\n",
       "23 NaN             1676-2789   14      4            \n",
       "28 NaN             1875-8819    9     61            \n",
       "\n",
       "   titulo                                                                                                           \n",
       "2   Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain  \\\n",
       "23                                UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES   \n",
       "28                  An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks   \n",
       "\n",
       "   sequencial nomePeriodico                                          \n",
       "2    3                                   PROCEDIA COMPUTER SCIENCE  \\\n",
       "23  24                               LEARNING AND NONLINEAR MODELS   \n",
       "28  29         INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS   \n",
       "\n",
       "   journal                                               \n",
       "2                             Procedia Computer Science  \n",
       "23                        Learning and Nonlinear Models  \n",
       "28  International Journal of Hybrid Intelligent Systems  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removendo entradas duplicadas\n",
    "df_unic = df_aggregated.drop_duplicates(subset='ISSN', keep='last')\n",
    "\n",
    "# Uso da função\n",
    "updated_data = complement_jcr(consultar, df_unic)\n",
    "updated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>jcr-ano</th>\n",
       "      <th>doi</th>\n",
       "      <th>data-issn</th>\n",
       "      <th>original_title</th>\n",
       "      <th>impact-factor</th>\n",
       "      <th>issn</th>\n",
       "      <th>volume</th>\n",
       "      <th>paginaInicial</th>\n",
       "      <th>titulo</th>\n",
       "      <th>sequencial</th>\n",
       "      <th>nomePeriodico</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.procs.2023.03.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1877-0509</td>\n",
       "      <td>220</td>\n",
       "      <td>119</td>\n",
       "      <td>Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain</td>\n",
       "      <td>3</td>\n",
       "      <td>PROCEDIA COMPUTER SCIENCE</td>\n",
       "      <td>Procedia Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.21528/lnlm-vol14-no1-art1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1676-2789</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES</td>\n",
       "      <td>24</td>\n",
       "      <td>LEARNING AND NONLINEAR MODELS</td>\n",
       "      <td>Learning and Nonlinear Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.3233/HIS-2012-0147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1875-8819</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks</td>\n",
       "      <td>29</td>\n",
       "      <td>INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS</td>\n",
       "      <td>International Journal of Hybrid Intelligent Systems</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jcr-ano doi                           data-issn original_title   \n",
       "2   NaN      10.1016/j.procs.2023.03.018  NaN       NaN            \\\n",
       "23  NaN     10.21528/lnlm-vol14-no1-art1  NaN       NaN             \n",
       "28  NaN            10.3233/HIS-2012-0147  NaN       NaN             \n",
       "\n",
       "    impact-factor issn       volume paginaInicial   \n",
       "2  NaN             1877-0509  220    119           \\\n",
       "23 NaN             1676-2789   14      4            \n",
       "28 NaN             1875-8819    9     61            \n",
       "\n",
       "   titulo                                                                                                           \n",
       "2   Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain  \\\n",
       "23                                UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES   \n",
       "28                  An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks   \n",
       "\n",
       "   sequencial nomePeriodico                                          \n",
       "2    3                                   PROCEDIA COMPUTER SCIENCE  \\\n",
       "23  24                               LEARNING AND NONLINEAR MODELS   \n",
       "28  29         INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS   \n",
       "\n",
       "   journal                                               \n",
       "2                             Procedia Computer Science  \n",
       "23                        Learning and Nonlinear Models  \n",
       "28  International Journal of Hybrid Intelligent Systems  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Populate the starting dataframe with missing data\n",
    "updated_df = populate_missing_data(updated_data, df_most_recent, most_recent_year)\n",
    "updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 valores de DOI identificados\n",
      "22 valores de ISSN não nulos extraídos\n",
      "19 valores de fator de impacto não nulos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>jcr-ano</th>\n",
       "      <th>doi</th>\n",
       "      <th>data-issn</th>\n",
       "      <th>original_title</th>\n",
       "      <th>impact-factor</th>\n",
       "      <th>issn</th>\n",
       "      <th>volume</th>\n",
       "      <th>paginaInicial</th>\n",
       "      <th>titulo</th>\n",
       "      <th>sequencial</th>\n",
       "      <th>nomePeriodico</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/comst.2017.2745505</td>\n",
       "      <td>1553877X</td>\n",
       "      <td>Fator de impacto (JCR 2022): 35.6</td>\n",
       "      <td>35.600</td>\n",
       "      <td>1553-877X</td>\n",
       "      <td>19</td>\n",
       "      <td>2704</td>\n",
       "      <td>Model-Based Quantitative Network Security Metrics: A Survey</td>\n",
       "      <td>22</td>\n",
       "      <td>IEEE Communications Surveys and Tutorials</td>\n",
       "      <td>IEEE Communications Surveys &amp; Tutorials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1016/j.inffus.2012.01.010</td>\n",
       "      <td>15662535</td>\n",
       "      <td>Fator de impacto (JCR 2022): 18.6</td>\n",
       "      <td>18.600</td>\n",
       "      <td>1566-2535</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries</td>\n",
       "      <td>30</td>\n",
       "      <td>Information Fusion (Print)</td>\n",
       "      <td>Information Fusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/jiot.2019.2904302</td>\n",
       "      <td>23274662</td>\n",
       "      <td>Fator de impacto (JCR 2022): 10.6</td>\n",
       "      <td>10.600</td>\n",
       "      <td>2327-4662</td>\n",
       "      <td>6</td>\n",
       "      <td>5631</td>\n",
       "      <td>Enabling Online Quantitative Security Analysis in 6LoWPAN Networks</td>\n",
       "      <td>15</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/mnet.2018.1800151</td>\n",
       "      <td>08908044</td>\n",
       "      <td>Fator de impacto (JCR 2022): 9.3</td>\n",
       "      <td>9.300</td>\n",
       "      <td>0890-8044</td>\n",
       "      <td>33</td>\n",
       "      <td>126</td>\n",
       "      <td>Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering</td>\n",
       "      <td>16</td>\n",
       "      <td>IEEE NETWORK</td>\n",
       "      <td>IEEE Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1016/j.jnca.2018.01.015</td>\n",
       "      <td>10848045</td>\n",
       "      <td>Fator de impacto (JCR 2022): 8.7</td>\n",
       "      <td>8.700</td>\n",
       "      <td>1084-8045</td>\n",
       "      <td>107</td>\n",
       "      <td>56</td>\n",
       "      <td>A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs</td>\n",
       "      <td>19</td>\n",
       "      <td>JOURNAL OF NETWORK AND COMPUTER APPLICATIONS</td>\n",
       "      <td>Journal of Network and Computer Applications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>http://dx.doi.org/10.1109/ACCESS.2023.3260020</td>\n",
       "      <td>21693536</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>2169-3536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IEEE Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/access.2020.3009167</td>\n",
       "      <td>21693536</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>2169-3536</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Integration of the Mobile Robot and Internet of Things to Monitor Older People</td>\n",
       "      <td>12</td>\n",
       "      <td>IEEE Access</td>\n",
       "      <td>IEEE Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/ACCESS.2022.3179004</td>\n",
       "      <td>21693536</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>2169-3536</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain</td>\n",
       "      <td>5</td>\n",
       "      <td>IEEE Access</td>\n",
       "      <td>IEEE Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s20113068</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1424-8220</td>\n",
       "      <td>20</td>\n",
       "      <td>3068</td>\n",
       "      <td>Enhancing Key Management in LoRaWAN with Permissioned Blockchain</td>\n",
       "      <td>11</td>\n",
       "      <td>SENSORS</td>\n",
       "      <td>Sensors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s18051312</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1424-8220</td>\n",
       "      <td>18</td>\n",
       "      <td>1312</td>\n",
       "      <td>Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks</td>\n",
       "      <td>21</td>\n",
       "      <td>SENSORS</td>\n",
       "      <td>Sensors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s18030689</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1424-8220</td>\n",
       "      <td>18</td>\n",
       "      <td>689</td>\n",
       "      <td>An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments</td>\n",
       "      <td>20</td>\n",
       "      <td>SENSORS</td>\n",
       "      <td>Sensors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s18020353</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1424-8220</td>\n",
       "      <td>18</td>\n",
       "      <td>353</td>\n",
       "      <td>A Proposal for IoT Dynamic Routes Selection Based on Contextual Information</td>\n",
       "      <td>17</td>\n",
       "      <td>SENSORS</td>\n",
       "      <td>Sensors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s150102104</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1424-8220</td>\n",
       "      <td>15</td>\n",
       "      <td>2104</td>\n",
       "      <td>Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks</td>\n",
       "      <td>25</td>\n",
       "      <td>SENSORS</td>\n",
       "      <td>Sensors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JCR 2021</td>\n",
       "      <td>http://dx.doi.org/10.1007/s12652-023-04555-3</td>\n",
       "      <td>18685137</td>\n",
       "      <td>Fator de impacto (JCR 2021): 3.662</td>\n",
       "      <td>3.662</td>\n",
       "      <td>1868-5137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Journal of Ambient Intelligence and Humanized Computing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/app12188939</td>\n",
       "      <td>20763417</td>\n",
       "      <td>Fator de impacto (JCR 2022): 2.7</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2076-3417</td>\n",
       "      <td>12</td>\n",
       "      <td>8939</td>\n",
       "      <td>Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19</td>\n",
       "      <td>7</td>\n",
       "      <td>Applied Sciences-Basel</td>\n",
       "      <td>Applied Sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/app112110457</td>\n",
       "      <td>20763417</td>\n",
       "      <td>Fator de impacto (JCR 2022): 2.7</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2076-3417</td>\n",
       "      <td>11</td>\n",
       "      <td>10457</td>\n",
       "      <td>An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus</td>\n",
       "      <td>10</td>\n",
       "      <td>Applied Sciences-Basel</td>\n",
       "      <td>Applied Sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1002/dac.3380</td>\n",
       "      <td>10745351</td>\n",
       "      <td>Fator de impacto (JCR 2022): 2.1</td>\n",
       "      <td>2.100</td>\n",
       "      <td>1074-5351</td>\n",
       "      <td>31</td>\n",
       "      <td>e3380</td>\n",
       "      <td>Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks</td>\n",
       "      <td>18</td>\n",
       "      <td>INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS</td>\n",
       "      <td>International Journal of Communication Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>JCR 2018</td>\n",
       "      <td>10.1155/2014/506203</td>\n",
       "      <td>15501329</td>\n",
       "      <td>Fator de impacto (JCR 2018): 1.614</td>\n",
       "      <td>1.614</td>\n",
       "      <td>1550-1477</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions</td>\n",
       "      <td>26</td>\n",
       "      <td>International Journal of Distributed Sensor Networks</td>\n",
       "      <td>International Journal of Distributed Sensor Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1002/nem.2062</td>\n",
       "      <td>10557148</td>\n",
       "      <td>Fator de impacto (JCR 2022): 1.5</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1055-7148</td>\n",
       "      <td>1</td>\n",
       "      <td>e2062</td>\n",
       "      <td>An approach for provisioning virtual sensors in sensor clouds</td>\n",
       "      <td>14</td>\n",
       "      <td>International Journal of Network Management</td>\n",
       "      <td>International Journal of Network Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.procs.2023.03.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1877-0509</td>\n",
       "      <td>220</td>\n",
       "      <td>119</td>\n",
       "      <td>Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain</td>\n",
       "      <td>3</td>\n",
       "      <td>PROCEDIA COMPUTER SCIENCE</td>\n",
       "      <td>Procedia Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.3233/HIS-2012-0147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1875-8819</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks</td>\n",
       "      <td>29</td>\n",
       "      <td>INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS</td>\n",
       "      <td>International Journal of Hybrid Intelligent Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.21528/lnlm-vol14-no1-art1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1676-2789</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES</td>\n",
       "      <td>24</td>\n",
       "      <td>LEARNING AND NONLINEAR MODELS</td>\n",
       "      <td>Learning and Nonlinear Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.9790/487X-2507032735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector</td>\n",
       "      <td>4</td>\n",
       "      <td>Journal of Business and Management</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18464/cybin.v3i1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>Exploring a P2P Transient Botnet - From Discovery to Enumeration</td>\n",
       "      <td>23</td>\n",
       "      <td>The Journal on Cybercrime</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/978-3-030-84311-3_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021</td>\n",
       "      <td>13</td>\n",
       "      <td>Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19</td>\n",
       "      <td>9</td>\n",
       "      <td>Springer Proceedings in Complexity</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública</td>\n",
       "      <td>6</td>\n",
       "      <td>Revista Razão Contábil</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará</td>\n",
       "      <td>8</td>\n",
       "      <td>Revista Razão Contábil</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019</td>\n",
       "      <td>13</td>\n",
       "      <td>Revista Razão Contábil</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>65</td>\n",
       "      <td>Controle Externo da Governança de Tecnologia da Informação</td>\n",
       "      <td>31</td>\n",
       "      <td>Revista Controle</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink</td>\n",
       "      <td>28</td>\n",
       "      <td>Learning and Nonlinear Models</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>702</td>\n",
       "      <td>A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks</td>\n",
       "      <td>27</td>\n",
       "      <td>JOURNAL OF COMMUNICATION AND COMPUTER</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8o.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários</td>\n",
       "      <td>34</td>\n",
       "      <td>Revista Brasileira de Informática na Educação</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>Detecting computer network attacks using statistical discriminators and cluster analysis</td>\n",
       "      <td>32</td>\n",
       "      <td>Revista Tecnologia (UNIFOR)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>113</td>\n",
       "      <td>Broadband network traffic characterization and classification using a multivariate statistical method</td>\n",
       "      <td>33</td>\n",
       "      <td>Revista Tecnologia (UNIFOR)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jcr-ano   doi                                            data-issn   \n",
       "21  JCR 2022                     10.1109/comst.2017.2745505  1553877X  \\\n",
       "29  JCR 2022                   10.1016/j.inffus.2012.01.010  15662535   \n",
       "14  JCR 2022                      10.1109/jiot.2019.2904302  23274662   \n",
       "15  JCR 2022                      10.1109/mnet.2018.1800151  08908044   \n",
       "18  JCR 2022                     10.1016/j.jnca.2018.01.015  10848045   \n",
       "1   JCR 2022  http://dx.doi.org/10.1109/ACCESS.2023.3260020  21693536   \n",
       "11  JCR 2022                    10.1109/access.2020.3009167  21693536   \n",
       "4   JCR 2022                    10.1109/ACCESS.2022.3179004  21693536   \n",
       "10  JCR 2022                              10.3390/s20113068  14248220   \n",
       "20  JCR 2022                              10.3390/s18051312  14248220   \n",
       "19  JCR 2022                              10.3390/s18030689  14248220   \n",
       "16  JCR 2022                              10.3390/s18020353  14248220   \n",
       "24  JCR 2022                             10.3390/s150102104  14248220   \n",
       "0   JCR 2021   http://dx.doi.org/10.1007/s12652-023-04555-3  18685137   \n",
       "6   JCR 2022                            10.3390/app12188939  20763417   \n",
       "9   JCR 2022                           10.3390/app112110457  20763417   \n",
       "17  JCR 2022                               10.1002/dac.3380  10745351   \n",
       "25  JCR 2018                            10.1155/2014/506203  15501329   \n",
       "13  JCR 2022                               10.1002/nem.2062  10557148   \n",
       "2        NaN                    10.1016/j.procs.2023.03.018       NaN   \n",
       "28       NaN                          10.3233/HIS-2012-0147       NaN   \n",
       "23       NaN                   10.21528/lnlm-vol14-no1-art1       NaN   \n",
       "3        NaN                        10.9790/487X-2507032735       NaN   \n",
       "22       NaN                            10.18464/cybin.v3i1       NaN   \n",
       "8        NaN                    10.1007/978-3-030-84311-3_2       NaN   \n",
       "5        NaN                                            NaN       NaN   \n",
       "7        NaN                                            NaN       NaN   \n",
       "12       NaN                                            NaN       NaN   \n",
       "30       NaN                                            NaN       NaN   \n",
       "27       NaN                                            NaN       NaN   \n",
       "26       NaN                                            NaN       NaN   \n",
       "33       NaN                                            NaN       NaN   \n",
       "31       NaN                                            NaN       NaN   \n",
       "32       NaN                                            NaN       NaN   \n",
       "\n",
       "   original_title                       impact-factor issn       volume   \n",
       "21   Fator de impacto (JCR 2022): 35.6  35.600         1553-877X    19   \\\n",
       "29   Fator de impacto (JCR 2022): 18.6  18.600         1566-2535    15    \n",
       "14   Fator de impacto (JCR 2022): 10.6  10.600         2327-4662     6    \n",
       "15    Fator de impacto (JCR 2022): 9.3   9.300         0890-8044    33    \n",
       "18    Fator de impacto (JCR 2022): 8.7   8.700         1084-8045   107    \n",
       "1     Fator de impacto (JCR 2022): 3.9   3.900         2169-3536   NaN    \n",
       "11    Fator de impacto (JCR 2022): 3.9   3.900         2169-3536     1    \n",
       "4     Fator de impacto (JCR 2022): 3.9   3.900         2169-3536     1    \n",
       "10    Fator de impacto (JCR 2022): 3.9   3.900         1424-8220    20    \n",
       "20    Fator de impacto (JCR 2022): 3.9   3.900         1424-8220    18    \n",
       "19    Fator de impacto (JCR 2022): 3.9   3.900         1424-8220    18    \n",
       "16    Fator de impacto (JCR 2022): 3.9   3.900         1424-8220    18    \n",
       "24    Fator de impacto (JCR 2022): 3.9   3.900         1424-8220    15    \n",
       "0   Fator de impacto (JCR 2021): 3.662   3.662         1868-5137   NaN    \n",
       "6     Fator de impacto (JCR 2022): 2.7   2.700         2076-3417    12    \n",
       "9     Fator de impacto (JCR 2022): 2.7   2.700         2076-3417    11    \n",
       "17    Fator de impacto (JCR 2022): 2.1   2.100         1074-5351    31    \n",
       "25  Fator de impacto (JCR 2018): 1.614   1.614         1550-1477  2014    \n",
       "13    Fator de impacto (JCR 2022): 1.5   1.500         1055-7148     1    \n",
       "2                                  NaN     NaN         1877-0509   220    \n",
       "28                                 NaN     NaN         1875-8819     9    \n",
       "23                                 NaN     NaN         1676-2789    14    \n",
       "3                                  NaN     NaN               NaN    25    \n",
       "22                                 NaN     NaN               NaN     3    \n",
       "8                                  NaN     NaN               NaN  2021    \n",
       "5                                  NaN     NaN               NaN    13    \n",
       "7                                  NaN     NaN               NaN    12    \n",
       "12                                 NaN     NaN               NaN    11    \n",
       "30                                 NaN     NaN               NaN     X    \n",
       "27                                 NaN     NaN               NaN    10    \n",
       "26                                 NaN     NaN               NaN    10    \n",
       "33                                 NaN     NaN               NaN   8o.    \n",
       "31                                 NaN     NaN               NaN    28    \n",
       "32                                 NaN     NaN               NaN    27    \n",
       "\n",
       "   paginaInicial   \n",
       "21   2704         \\\n",
       "29     44          \n",
       "14   5631          \n",
       "15    126          \n",
       "18     56          \n",
       "1     NaN          \n",
       "11      1          \n",
       "4       1          \n",
       "10   3068          \n",
       "20   1312          \n",
       "19    689          \n",
       "16    353          \n",
       "24   2104          \n",
       "0     NaN          \n",
       "6    8939          \n",
       "9   10457          \n",
       "17  e3380          \n",
       "25      1          \n",
       "13  e2062          \n",
       "2     119          \n",
       "28     61          \n",
       "23      4          \n",
       "3      27          \n",
       "22     30          \n",
       "8      13          \n",
       "5       1          \n",
       "7       1          \n",
       "12      1          \n",
       "30     65          \n",
       "27      4          \n",
       "26    702          \n",
       "33    NaN          \n",
       "31     33          \n",
       "32    113          \n",
       "\n",
       "   titulo                                                                                                                                                                  \n",
       "21                                                                                                           Model-Based Quantitative Network Security Metrics: A Survey  \\\n",
       "29                                                                                   On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries   \n",
       "14                                                                                                    Enabling Online Quantitative Security Analysis in 6LoWPAN Networks   \n",
       "15                                                                                       Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering   \n",
       "18                                                                   A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs   \n",
       "1                                                                                                                                                                    NaN   \n",
       "11                                                                                        Integration of the Mobile Robot and Internet of Things to Monitor Older People   \n",
       "4                                                                A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain   \n",
       "10                                                                                                      Enhancing Key Management in LoRaWAN with Permissioned Blockchain   \n",
       "20                                                        Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks   \n",
       "19                                                                    An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments   \n",
       "16                                                                                           A Proposal for IoT Dynamic Routes Selection Based on Contextual Information   \n",
       "24                                                                                             Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks   \n",
       "0                                                                                                                                                                    NaN   \n",
       "6                                                           Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19   \n",
       "9                                                        An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus   \n",
       "17                                               Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks   \n",
       "25                                                                                      A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions   \n",
       "13                                                                                                         An approach for provisioning virtual sensors in sensor clouds   \n",
       "2                                                          Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain   \n",
       "28                                                                         An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks   \n",
       "23                                                                                       UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES   \n",
       "3                                                     Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector   \n",
       "22                                                                                                      Exploring a P2P Transient Botnet - From Discovery to Enumeration   \n",
       "8                                                                                         Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19   \n",
       "5                                                                  Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública   \n",
       "7                                                                                 Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará   \n",
       "12                                                                  Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019   \n",
       "30                                                                                                            Controle Externo da Governança de Tecnologia da Informação   \n",
       "27  Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink   \n",
       "26                                                                   A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks   \n",
       "33                                                                                               Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários   \n",
       "31                                                                              Detecting computer network attacks using statistical discriminators and cluster analysis   \n",
       "32                                                                 Broadband network traffic characterization and classification using a multivariate statistical method   \n",
       "\n",
       "   sequencial nomePeriodico                                           \n",
       "21   22                   IEEE Communications Surveys and Tutorials  \\\n",
       "29   30                                  Information Fusion (Print)   \n",
       "14   15                             IEEE Internet of Things Journal   \n",
       "15   16                                                IEEE NETWORK   \n",
       "18   19                JOURNAL OF NETWORK AND COMPUTER APPLICATIONS   \n",
       "1   NaN                                                         NaN   \n",
       "11   12                                                 IEEE Access   \n",
       "4     5                                                 IEEE Access   \n",
       "10   11                                                     SENSORS   \n",
       "20   21                                                     SENSORS   \n",
       "19   20                                                     SENSORS   \n",
       "16   17                                                     SENSORS   \n",
       "24   25                                                     SENSORS   \n",
       "0   NaN                                                         NaN   \n",
       "6     7                                      Applied Sciences-Basel   \n",
       "9    10                                      Applied Sciences-Basel   \n",
       "17   18              INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS   \n",
       "25   26        International Journal of Distributed Sensor Networks   \n",
       "13   14                 International Journal of Network Management   \n",
       "2     3                                   PROCEDIA COMPUTER SCIENCE   \n",
       "28   29         INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS   \n",
       "23   24                               LEARNING AND NONLINEAR MODELS   \n",
       "3     4                          Journal of Business and Management   \n",
       "22   23                                  The Journal on Cybercrime    \n",
       "8     9                          Springer Proceedings in Complexity   \n",
       "5     6                                     Revista Razão Contábil    \n",
       "7     8                                     Revista Razão Contábil    \n",
       "12   13                                     Revista Razão Contábil    \n",
       "30   31                                            Revista Controle   \n",
       "27   28                               Learning and Nonlinear Models   \n",
       "26   27                       JOURNAL OF COMMUNICATION AND COMPUTER   \n",
       "33   34               Revista Brasileira de Informática na Educação   \n",
       "31   32                                 Revista Tecnologia (UNIFOR)   \n",
       "32   33                                 Revista Tecnologia (UNIFOR)   \n",
       "\n",
       "   journal                                                   \n",
       "21                  IEEE Communications Surveys & Tutorials  \n",
       "29                                       Information Fusion  \n",
       "14                          IEEE Internet of Things Journal  \n",
       "15                                             IEEE Network  \n",
       "18             Journal of Network and Computer Applications  \n",
       "1                                               IEEE Access  \n",
       "11                                              IEEE Access  \n",
       "4                                               IEEE Access  \n",
       "10                                                  Sensors  \n",
       "20                                                  Sensors  \n",
       "19                                                  Sensors  \n",
       "16                                                  Sensors  \n",
       "24                                                  Sensors  \n",
       "0   Journal of Ambient Intelligence and Humanized Computing  \n",
       "6                                          Applied Sciences  \n",
       "9                                          Applied Sciences  \n",
       "17           International Journal of Communication Systems  \n",
       "25     International Journal of Distributed Sensor Networks  \n",
       "13              International Journal of Network Management  \n",
       "2                                 Procedia Computer Science  \n",
       "28      International Journal of Hybrid Intelligent Systems  \n",
       "23                            Learning and Nonlinear Models  \n",
       "3                                                       NaN  \n",
       "22                                                      NaN  \n",
       "8                                                       NaN  \n",
       "5                                                       NaN  \n",
       "7                                                       NaN  \n",
       "12                                                      NaN  \n",
       "30                                                      NaN  \n",
       "27                                                      NaN  \n",
       "26                                                      NaN  \n",
       "33                                                      NaN  \n",
       "31                                                      NaN  \n",
       "32                                                      NaN  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final  = data.sort_values(by=['impact-factor','issn','jcr-ano','doi','journal'], ascending=False)\n",
    "\n",
    "count_doi_non_nan = df_final['doi'].notna().sum()\n",
    "count_jci_non_nan = df_final['impact-factor'].notna().sum()\n",
    "count_issn_non_nan = df_final['issn'].notna().sum()\n",
    "print(f'{count_doi_non_nan} valores de DOI identificados')\n",
    "print(f'{count_issn_non_nan} valores de ISSN não nulos extraídos')\n",
    "print(f'{count_jci_non_nan} valores de fator de impacto não nulos')\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_9992\\1269987069.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sorted_df['issn'] = dt_trab.apply(standardize_issn_format)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>jcr-ano</th>\n",
       "      <th>doi</th>\n",
       "      <th>data-issn</th>\n",
       "      <th>original_title</th>\n",
       "      <th>impact-factor</th>\n",
       "      <th>issn</th>\n",
       "      <th>volume</th>\n",
       "      <th>paginaInicial</th>\n",
       "      <th>titulo</th>\n",
       "      <th>sequencial</th>\n",
       "      <th>nomePeriodico</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.procs.2023.03.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>220</td>\n",
       "      <td>119</td>\n",
       "      <td>Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain</td>\n",
       "      <td>3</td>\n",
       "      <td>PROCEDIA COMPUTER SCIENCE</td>\n",
       "      <td>Procedia Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.21528/lnlm-vol14-no1-art1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES</td>\n",
       "      <td>24</td>\n",
       "      <td>LEARNING AND NONLINEAR MODELS</td>\n",
       "      <td>Learning and Nonlinear Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.3233/HIS-2012-0147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks</td>\n",
       "      <td>29</td>\n",
       "      <td>INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS</td>\n",
       "      <td>International Journal of Hybrid Intelligent Systems</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jcr-ano doi                           data-issn original_title   \n",
       "2   NaN      10.1016/j.procs.2023.03.018  NaN       NaN            \\\n",
       "23  NaN     10.21528/lnlm-vol14-no1-art1  NaN       NaN             \n",
       "28  NaN            10.3233/HIS-2012-0147  NaN       NaN             \n",
       "\n",
       "    impact-factor  issn volume paginaInicial   \n",
       "2  NaN            NaN    220    119           \\\n",
       "23 NaN            NaN     14      4            \n",
       "28 NaN            NaN      9     61            \n",
       "\n",
       "   titulo                                                                                                           \n",
       "2   Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain  \\\n",
       "23                                UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES   \n",
       "28                  An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks   \n",
       "\n",
       "   sequencial nomePeriodico                                          \n",
       "2    3                                   PROCEDIA COMPUTER SCIENCE  \\\n",
       "23  24                               LEARNING AND NONLINEAR MODELS   \n",
       "28  29         INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS   \n",
       "\n",
       "   journal                                               \n",
       "2                             Procedia Computer Science  \n",
       "23                        Learning and Nonlinear Models  \n",
       "28  International Journal of Hybrid Intelligent Systems  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df_updated = fill_na_from_df_aggregated(consultar, df_aggregated)\n",
    "sorted_df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Substituir 'NULL' e None por np.nan em todo o dataframe\n",
    "# data.replace(['NULL', None], np.nan, inplace=True)\n",
    "# data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "# data = data.sort_values(by=['impact-factor','issn'], ascending=False)\n",
    "\n",
    "# # Agrupar pelo 'data-issn', contar ocorrências, maior valor de 'impact-factor' e pegar o 'journal'\n",
    "# grouped = data.groupby('issn').agg(\n",
    "#     count=('issn', 'size'),\n",
    "#     impact_factor_max=('impact-factor', 'max'),\n",
    "#     journal=('journal', max_len_string)\n",
    "# ).reset_index()\n",
    "\n",
    "# # Renomear as colunas para corresponder às suas descrições\n",
    "# grouped.rename(columns={\"count\": \"issn_count\", \"impact_factor_max\": \"impact-factor\"}, inplace=True)\n",
    "\n",
    "# count_nan_issn  = data['issn'].isna().sum()\n",
    "# null_row        = pd.DataFrame({'issn': ['NULL'], 'issn_count': [count_nan_issn]})\n",
    "# data_all_counts = pd.concat([grouped, null_row], ignore_index=True)\n",
    "# data_all_counts = data_all_counts.sort_values(by=['impact-factor'], ascending=False)\n",
    "# print(data_all_counts['issn_count'].sum())\n",
    "# # data_all_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           1.5,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#ffff00",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          1.5
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "10557148"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           1.614,
           "JCR 2018"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#feff00",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          1.61
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "15501329"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           2.1,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#fbfd00",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          2.1
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "10745351"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           2.7,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#f6fb00",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          2.7
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "20763417"
         ],
         "xaxis": "x",
         "y": [
          2
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           3.662,
           "JCR 2021"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#eff700",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          3.66
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "18685137"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           3.9,
           "JCR 2022"
          ],
          [
           3.9,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#edf600",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          3.9,
          3.9
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "14248220",
          "21693536"
         ],
         "xaxis": "x",
         "y": [
          5,
          3
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           8.7,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#c9e400",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          8.7
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "10848045"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           9.3,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#c5e200",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          9.3
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "08908044"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           10.6,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#bbdd00",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          10.6
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "23274662"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           18.6,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#7fbf00",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          18.6
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "15662535"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           35.6,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#008000",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          35.6
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "1553877X"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           null,
           null
          ],
          [
           null,
           null
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#808080",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          null,
          null
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          null,
          null
         ],
         "xaxis": "x",
         "y": [
          15,
          12
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "height": 800,
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "b": 150,
         "l": 50,
         "r": 50,
         "t": 50
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Frequência de publicação acumulada no período completo versus fator de impacto de cada periódico"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "tickangle": -45,
         "ticktext": [
          "International Journal of Network Management",
          "International Journal of Distributed Sensor Networks",
          "International Journal of Communication Systems",
          "Applied Sciences",
          "Journal of Ambient Intelligence and Humanized Computing",
          "Sensors",
          "IEEE Access",
          "Journal of Network and Computer Applications",
          "IEEE Network",
          "IEEE Internet of Things Journal",
          "Information Fusion",
          "IEEE Communications Surveys & Tutorials",
          "International Journal of Hybrid Intelligent Systems",
          null
         ],
         "tickvals": [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "MISSING"
         ],
         "title": {
          "text": "Periódicos"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": true,
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Frequência de publicação no periódico"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "IEEE Communications Surveys & Tutorials"
          ],
          [
           "Information Fusion"
          ],
          [
           "IEEE Internet of Things Journal"
          ],
          [
           "IEEE Network"
          ],
          [
           "Journal of Network and Computer Applications"
          ],
          [
           "Sensors"
          ],
          [
           "IEEE Access"
          ],
          [
           "Journal of Ambient Intelligence and Humanized Computing"
          ],
          [
           "Applied Sciences"
          ],
          [
           "International Journal of Communication Systems"
          ],
          [
           "International Journal of Distributed Sensor Networks"
          ],
          [
           "International Journal of Network Management"
          ],
          [
           "Learning and Nonlinear Models"
          ],
          [
           "International Journal of Hybrid Intelligent Systems"
          ],
          [
           "Procedia Computer Science"
          ],
          [
           null
          ]
         ],
         "hovertemplate": "ISSN=%{x}<br>Fator de Impacto=%{y}<br>Quantidade=%{marker.color}<br>journal=%{customdata[0]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           1,
           1,
           1,
           1,
           1,
           5,
           3,
           1,
           2,
           1,
           1,
           1,
           1,
           1,
           1,
           12
          ],
          "coloraxis": "coloraxis",
          "opacity": 1,
          "size": [
           2,
           2,
           2,
           2,
           2,
           10,
           6,
           2,
           4,
           2,
           2,
           2,
           2,
           2,
           2,
           24
          ],
          "sizemode": "area",
          "sizeref": 0.03,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "1553-877X",
          "1566-2535",
          "2327-4662",
          "0890-8044",
          "1084-8045",
          "1424-8220",
          "2169-3536",
          "1868-5137",
          "2076-3417",
          "1074-5351",
          "1550-1477",
          "1055-7148",
          "1676-2789",
          "1875-8819",
          "1877-0509",
          null
         ],
         "xaxis": "x",
         "y": [
          35.6,
          18.6,
          10.6,
          9.3,
          8.7,
          3.9,
          3.9,
          3.662,
          2.7,
          2.1,
          1.614,
          1.5,
          null,
          null,
          null,
          null
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "Quantidade"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(255,255,204)"
          ],
          [
           0.125,
           "rgb(255,237,160)"
          ],
          [
           0.25,
           "rgb(254,217,118)"
          ],
          [
           0.375,
           "rgb(254,178,76)"
          ],
          [
           0.5,
           "rgb(253,141,60)"
          ],
          [
           0.625,
           "rgb(252,78,42)"
          ],
          [
           0.75,
           "rgb(227,26,28)"
          ],
          [
           0.875,
           "rgb(189,0,38)"
          ],
          [
           1,
           "rgb(128,0,38)"
          ]
         ]
        },
        "legend": {
         "itemsizing": "constant",
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Gráfico de Dispersão de Fator de Impacto versus ISSN no período completo"
        },
        "xaxis": {
         "anchor": "y",
         "categoryorder": "total ascending",
         "domain": [
          0,
          1
         ],
         "tickangle": -45,
         "tickvals": [
          "1553-877X",
          "1566-2535",
          "2327-4662",
          "0890-8044",
          "1084-8045",
          "1424-8220",
          "2169-3536",
          "1868-5137",
          "2076-3417",
          "1074-5351",
          "1550-1477",
          "1055-7148",
          "1676-2789",
          "1875-8819",
          "1877-0509",
          null
         ],
         "title": {
          "text": "ISSN"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": true,
         "domain": [
          0,
          1
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "title": {
          "text": "Fator de Impacto"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>issn</th>\n",
       "      <th>issn_count</th>\n",
       "      <th>impact-factor</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1553-877X</td>\n",
       "      <td>1</td>\n",
       "      <td>35.6</td>\n",
       "      <td>IEEE Communications Surveys &amp; Tutorials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1566-2535</td>\n",
       "      <td>1</td>\n",
       "      <td>18.6</td>\n",
       "      <td>Information Fusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2327-4662</td>\n",
       "      <td>1</td>\n",
       "      <td>10.6</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0890-8044</td>\n",
       "      <td>1</td>\n",
       "      <td>9.3</td>\n",
       "      <td>IEEE Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1084-8045</td>\n",
       "      <td>1</td>\n",
       "      <td>8.7</td>\n",
       "      <td>Journal of Network and Computer Applications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1424-8220</td>\n",
       "      <td>5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Sensors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2169-3536</td>\n",
       "      <td>3</td>\n",
       "      <td>3.9</td>\n",
       "      <td>IEEE Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1868-5137</td>\n",
       "      <td>1</td>\n",
       "      <td>3.662</td>\n",
       "      <td>Journal of Ambient Intelligence and Humanized Computing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2076-3417</td>\n",
       "      <td>2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>Applied Sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1074-5351</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>International Journal of Communication Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1550-1477</td>\n",
       "      <td>1</td>\n",
       "      <td>1.614</td>\n",
       "      <td>International Journal of Distributed Sensor Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1055-7148</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>International Journal of Network Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1676-2789</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Learning and Nonlinear Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1875-8819</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>International Journal of Hybrid Intelligent Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1877-0509</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Procedia Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   issn        issn_count impact-factor   \n",
       "6   1553-877X   1           35.6         \\\n",
       "7   1566-2535   1           18.6          \n",
       "14  2327-4662   1           10.6          \n",
       "0   0890-8044   1            9.3          \n",
       "3   1084-8045   1            8.7          \n",
       "4   1424-8220   5            3.9          \n",
       "13  2169-3536   3            3.9          \n",
       "9   1868-5137   1          3.662          \n",
       "12  2076-3417   2            2.7          \n",
       "2   1074-5351   1            2.1          \n",
       "5   1550-1477   1          1.614          \n",
       "1   1055-7148   1            1.5          \n",
       "8   1676-2789   1            NaN          \n",
       "10  1875-8819   1            NaN          \n",
       "11  1877-0509   1            NaN          \n",
       "15        NaN  12            NaN          \n",
       "\n",
       "   journal                                                   \n",
       "6                   IEEE Communications Surveys & Tutorials  \n",
       "7                                        Information Fusion  \n",
       "14                          IEEE Internet of Things Journal  \n",
       "0                                              IEEE Network  \n",
       "3              Journal of Network and Computer Applications  \n",
       "4                                                   Sensors  \n",
       "13                                              IEEE Access  \n",
       "9   Journal of Ambient Intelligence and Humanized Computing  \n",
       "12                                         Applied Sciences  \n",
       "2            International Journal of Communication Systems  \n",
       "5      International Journal of Distributed Sensor Networks  \n",
       "1               International Journal of Network Management  \n",
       "8                             Learning and Nonlinear Models  \n",
       "10      International Journal of Hybrid Intelligent Systems  \n",
       "11                                Procedia Computer Science  \n",
       "15                                                      NaN  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_records = dataframe_to_list_of_dicts(df_final)\n",
    "jcr_properties_list = formatted_records\n",
    "\n",
    "plot_vertbar(jcr_properties_list)\n",
    "plot_scatter(jcr_properties_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultar dados do JCR a cada ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Avaliar a evolução de cada revista ao longo dos anos quanto ao JCI\n",
    "# file_path = '../data/RaimirHolandaFilho_JCR_JournalResults2018.csv'\n",
    "# df2018 = org_lines(file_path)\n",
    "\n",
    "# file_path = '../data/RaimirHolandaFilho_JCR_JournalResults2019.csv'\n",
    "# df2019 = org_lines(file_path)\n",
    "\n",
    "# file_path = '../data/RaimirHolandaFilho_JCR_JournalResults2020.csv'\n",
    "# df2020 = org_lines(file_path)\n",
    "\n",
    "# file_path = '../data/RaimirHolandaFilho_JCR_JournalResults2021.csv'\n",
    "# df2021 = org_lines(file_path)\n",
    "\n",
    "# file_path = '../data/RaimirHolandaFilho_JCR_JournalResults2022.csv'\n",
    "# df2022 = org_lines(file_path)\n",
    "\n",
    "# df_issn = pd.concat([df2018, df2019, df2020, df2021, df2022], ignore_index=True)\n",
    "\n",
    "# df_issn = pd.merge(df1, df2, on='issn', how='inner')\n",
    "# df_issn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesclar eISSN com ISSN para achar mais dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_na_from_df_aggregated(df_final, df_aggregated):\n",
    "    # Filter df_aggregated for rows where eISSN is NaN\n",
    "    df_nan_eissn = df_aggregated[df_aggregated['eISSN'].isna()]\n",
    "\n",
    "    # Iterate through the rows of df_nan_eissn\n",
    "    for idx, row in df_nan_eissn.iterrows():\n",
    "        issn_value = row['ISSN']\n",
    "        \n",
    "        # Find the rows in sorted_df with matching 'issn' and NaN 'impact-factor'\n",
    "        mask = (df_final['issn'] == issn_value) & df_final['impact-factor'].isna()\n",
    "        \n",
    "        if mask.sum() > 0:  # if there are matching rows in sorted_df\n",
    "            df_final.loc[mask, 'impact-factor'] = row['2022 JIF']  # Assuming you want the most recent JIF\n",
    "            df_final.loc[mask, 'journal'] = row['Journal name']\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>jcr-ano</th>\n",
       "      <th>doi</th>\n",
       "      <th>data-issn</th>\n",
       "      <th>original_title</th>\n",
       "      <th>impact-factor</th>\n",
       "      <th>issn</th>\n",
       "      <th>volume</th>\n",
       "      <th>paginaInicial</th>\n",
       "      <th>titulo</th>\n",
       "      <th>sequencial</th>\n",
       "      <th>nomePeriodico</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/comst.2017.2745505</td>\n",
       "      <td>1553877X</td>\n",
       "      <td>Fator de impacto (JCR 2022): 35.6</td>\n",
       "      <td>35.600</td>\n",
       "      <td>1553-877X</td>\n",
       "      <td>19</td>\n",
       "      <td>2704</td>\n",
       "      <td>Model-Based Quantitative Network Security Metrics: A Survey</td>\n",
       "      <td>22</td>\n",
       "      <td>IEEE Communications Surveys and Tutorials</td>\n",
       "      <td>IEEE Communications Surveys &amp; Tutorials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1016/j.inffus.2012.01.010</td>\n",
       "      <td>15662535</td>\n",
       "      <td>Fator de impacto (JCR 2022): 18.6</td>\n",
       "      <td>18.600</td>\n",
       "      <td>1566-2535</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries</td>\n",
       "      <td>30</td>\n",
       "      <td>Information Fusion (Print)</td>\n",
       "      <td>Information Fusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/jiot.2019.2904302</td>\n",
       "      <td>23274662</td>\n",
       "      <td>Fator de impacto (JCR 2022): 10.6</td>\n",
       "      <td>10.600</td>\n",
       "      <td>2327-4662</td>\n",
       "      <td>6</td>\n",
       "      <td>5631</td>\n",
       "      <td>Enabling Online Quantitative Security Analysis in 6LoWPAN Networks</td>\n",
       "      <td>15</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/mnet.2018.1800151</td>\n",
       "      <td>08908044</td>\n",
       "      <td>Fator de impacto (JCR 2022): 9.3</td>\n",
       "      <td>9.300</td>\n",
       "      <td>0890-8044</td>\n",
       "      <td>33</td>\n",
       "      <td>126</td>\n",
       "      <td>Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering</td>\n",
       "      <td>16</td>\n",
       "      <td>IEEE NETWORK</td>\n",
       "      <td>IEEE Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1016/j.jnca.2018.01.015</td>\n",
       "      <td>10848045</td>\n",
       "      <td>Fator de impacto (JCR 2022): 8.7</td>\n",
       "      <td>8.700</td>\n",
       "      <td>1084-8045</td>\n",
       "      <td>107</td>\n",
       "      <td>56</td>\n",
       "      <td>A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs</td>\n",
       "      <td>19</td>\n",
       "      <td>JOURNAL OF NETWORK AND COMPUTER APPLICATIONS</td>\n",
       "      <td>Journal of Network and Computer Applications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>http://dx.doi.org/10.1109/ACCESS.2023.3260020</td>\n",
       "      <td>21693536</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>2169-3536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IEEE Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/access.2020.3009167</td>\n",
       "      <td>21693536</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>2169-3536</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Integration of the Mobile Robot and Internet of Things to Monitor Older People</td>\n",
       "      <td>12</td>\n",
       "      <td>IEEE Access</td>\n",
       "      <td>IEEE Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1109/ACCESS.2022.3179004</td>\n",
       "      <td>21693536</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>2169-3536</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain</td>\n",
       "      <td>5</td>\n",
       "      <td>IEEE Access</td>\n",
       "      <td>IEEE Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s20113068</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1424-8220</td>\n",
       "      <td>20</td>\n",
       "      <td>3068</td>\n",
       "      <td>Enhancing Key Management in LoRaWAN with Permissioned Blockchain</td>\n",
       "      <td>11</td>\n",
       "      <td>SENSORS</td>\n",
       "      <td>Sensors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s18051312</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1424-8220</td>\n",
       "      <td>18</td>\n",
       "      <td>1312</td>\n",
       "      <td>Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks</td>\n",
       "      <td>21</td>\n",
       "      <td>SENSORS</td>\n",
       "      <td>Sensors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s18030689</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1424-8220</td>\n",
       "      <td>18</td>\n",
       "      <td>689</td>\n",
       "      <td>An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments</td>\n",
       "      <td>20</td>\n",
       "      <td>SENSORS</td>\n",
       "      <td>Sensors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s18020353</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1424-8220</td>\n",
       "      <td>18</td>\n",
       "      <td>353</td>\n",
       "      <td>A Proposal for IoT Dynamic Routes Selection Based on Contextual Information</td>\n",
       "      <td>17</td>\n",
       "      <td>SENSORS</td>\n",
       "      <td>Sensors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/s150102104</td>\n",
       "      <td>14248220</td>\n",
       "      <td>Fator de impacto (JCR 2022): 3.9</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1424-8220</td>\n",
       "      <td>15</td>\n",
       "      <td>2104</td>\n",
       "      <td>Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks</td>\n",
       "      <td>25</td>\n",
       "      <td>SENSORS</td>\n",
       "      <td>Sensors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JCR 2021</td>\n",
       "      <td>http://dx.doi.org/10.1007/s12652-023-04555-3</td>\n",
       "      <td>18685137</td>\n",
       "      <td>Fator de impacto (JCR 2021): 3.662</td>\n",
       "      <td>3.662</td>\n",
       "      <td>1868-5137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Journal of Ambient Intelligence and Humanized Computing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/app12188939</td>\n",
       "      <td>20763417</td>\n",
       "      <td>Fator de impacto (JCR 2022): 2.7</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2076-3417</td>\n",
       "      <td>12</td>\n",
       "      <td>8939</td>\n",
       "      <td>Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19</td>\n",
       "      <td>7</td>\n",
       "      <td>Applied Sciences-Basel</td>\n",
       "      <td>Applied Sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.3390/app112110457</td>\n",
       "      <td>20763417</td>\n",
       "      <td>Fator de impacto (JCR 2022): 2.7</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2076-3417</td>\n",
       "      <td>11</td>\n",
       "      <td>10457</td>\n",
       "      <td>An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus</td>\n",
       "      <td>10</td>\n",
       "      <td>Applied Sciences-Basel</td>\n",
       "      <td>Applied Sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1002/dac.3380</td>\n",
       "      <td>10745351</td>\n",
       "      <td>Fator de impacto (JCR 2022): 2.1</td>\n",
       "      <td>2.100</td>\n",
       "      <td>1074-5351</td>\n",
       "      <td>31</td>\n",
       "      <td>e3380</td>\n",
       "      <td>Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks</td>\n",
       "      <td>18</td>\n",
       "      <td>INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS</td>\n",
       "      <td>International Journal of Communication Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>JCR 2018</td>\n",
       "      <td>10.1155/2014/506203</td>\n",
       "      <td>15501329</td>\n",
       "      <td>Fator de impacto (JCR 2018): 1.614</td>\n",
       "      <td>1.614</td>\n",
       "      <td>1550-1477</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions</td>\n",
       "      <td>26</td>\n",
       "      <td>International Journal of Distributed Sensor Networks</td>\n",
       "      <td>International Journal of Distributed Sensor Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JCR 2022</td>\n",
       "      <td>10.1002/nem.2062</td>\n",
       "      <td>10557148</td>\n",
       "      <td>Fator de impacto (JCR 2022): 1.5</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1055-7148</td>\n",
       "      <td>1</td>\n",
       "      <td>e2062</td>\n",
       "      <td>An approach for provisioning virtual sensors in sensor clouds</td>\n",
       "      <td>14</td>\n",
       "      <td>International Journal of Network Management</td>\n",
       "      <td>International Journal of Network Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.procs.2023.03.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1877-0509</td>\n",
       "      <td>220</td>\n",
       "      <td>119</td>\n",
       "      <td>Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain</td>\n",
       "      <td>3</td>\n",
       "      <td>PROCEDIA COMPUTER SCIENCE</td>\n",
       "      <td>Procedia Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.3233/HIS-2012-0147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1875-8819</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks</td>\n",
       "      <td>29</td>\n",
       "      <td>INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS</td>\n",
       "      <td>International Journal of Hybrid Intelligent Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.21528/lnlm-vol14-no1-art1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1676-2789</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES</td>\n",
       "      <td>24</td>\n",
       "      <td>LEARNING AND NONLINEAR MODELS</td>\n",
       "      <td>Learning and Nonlinear Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.9790/487X-2507032735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector</td>\n",
       "      <td>4</td>\n",
       "      <td>Journal of Business and Management</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18464/cybin.v3i1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>Exploring a P2P Transient Botnet - From Discovery to Enumeration</td>\n",
       "      <td>23</td>\n",
       "      <td>The Journal on Cybercrime</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/978-3-030-84311-3_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021</td>\n",
       "      <td>13</td>\n",
       "      <td>Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19</td>\n",
       "      <td>9</td>\n",
       "      <td>Springer Proceedings in Complexity</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública</td>\n",
       "      <td>6</td>\n",
       "      <td>Revista Razão Contábil</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará</td>\n",
       "      <td>8</td>\n",
       "      <td>Revista Razão Contábil</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019</td>\n",
       "      <td>13</td>\n",
       "      <td>Revista Razão Contábil</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>65</td>\n",
       "      <td>Controle Externo da Governança de Tecnologia da Informação</td>\n",
       "      <td>31</td>\n",
       "      <td>Revista Controle</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink</td>\n",
       "      <td>28</td>\n",
       "      <td>Learning and Nonlinear Models</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>702</td>\n",
       "      <td>A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks</td>\n",
       "      <td>27</td>\n",
       "      <td>JOURNAL OF COMMUNICATION AND COMPUTER</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8o.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários</td>\n",
       "      <td>34</td>\n",
       "      <td>Revista Brasileira de Informática na Educação</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>Detecting computer network attacks using statistical discriminators and cluster analysis</td>\n",
       "      <td>32</td>\n",
       "      <td>Revista Tecnologia (UNIFOR)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>113</td>\n",
       "      <td>Broadband network traffic characterization and classification using a multivariate statistical method</td>\n",
       "      <td>33</td>\n",
       "      <td>Revista Tecnologia (UNIFOR)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jcr-ano   doi                                            data-issn   \n",
       "21  JCR 2022                     10.1109/comst.2017.2745505  1553877X  \\\n",
       "29  JCR 2022                   10.1016/j.inffus.2012.01.010  15662535   \n",
       "14  JCR 2022                      10.1109/jiot.2019.2904302  23274662   \n",
       "15  JCR 2022                      10.1109/mnet.2018.1800151  08908044   \n",
       "18  JCR 2022                     10.1016/j.jnca.2018.01.015  10848045   \n",
       "1   JCR 2022  http://dx.doi.org/10.1109/ACCESS.2023.3260020  21693536   \n",
       "11  JCR 2022                    10.1109/access.2020.3009167  21693536   \n",
       "4   JCR 2022                    10.1109/ACCESS.2022.3179004  21693536   \n",
       "10  JCR 2022                              10.3390/s20113068  14248220   \n",
       "20  JCR 2022                              10.3390/s18051312  14248220   \n",
       "19  JCR 2022                              10.3390/s18030689  14248220   \n",
       "16  JCR 2022                              10.3390/s18020353  14248220   \n",
       "24  JCR 2022                             10.3390/s150102104  14248220   \n",
       "0   JCR 2021   http://dx.doi.org/10.1007/s12652-023-04555-3  18685137   \n",
       "6   JCR 2022                            10.3390/app12188939  20763417   \n",
       "9   JCR 2022                           10.3390/app112110457  20763417   \n",
       "17  JCR 2022                               10.1002/dac.3380  10745351   \n",
       "25  JCR 2018                            10.1155/2014/506203  15501329   \n",
       "13  JCR 2022                               10.1002/nem.2062  10557148   \n",
       "2        NaN                    10.1016/j.procs.2023.03.018       NaN   \n",
       "28       NaN                          10.3233/HIS-2012-0147       NaN   \n",
       "23       NaN                   10.21528/lnlm-vol14-no1-art1       NaN   \n",
       "3        NaN                        10.9790/487X-2507032735       NaN   \n",
       "22       NaN                            10.18464/cybin.v3i1       NaN   \n",
       "8        NaN                    10.1007/978-3-030-84311-3_2       NaN   \n",
       "5        NaN                                            NaN       NaN   \n",
       "7        NaN                                            NaN       NaN   \n",
       "12       NaN                                            NaN       NaN   \n",
       "30       NaN                                            NaN       NaN   \n",
       "27       NaN                                            NaN       NaN   \n",
       "26       NaN                                            NaN       NaN   \n",
       "33       NaN                                            NaN       NaN   \n",
       "31       NaN                                            NaN       NaN   \n",
       "32       NaN                                            NaN       NaN   \n",
       "\n",
       "   original_title                       impact-factor issn       volume   \n",
       "21   Fator de impacto (JCR 2022): 35.6  35.600         1553-877X    19   \\\n",
       "29   Fator de impacto (JCR 2022): 18.6  18.600         1566-2535    15    \n",
       "14   Fator de impacto (JCR 2022): 10.6  10.600         2327-4662     6    \n",
       "15    Fator de impacto (JCR 2022): 9.3   9.300         0890-8044    33    \n",
       "18    Fator de impacto (JCR 2022): 8.7   8.700         1084-8045   107    \n",
       "1     Fator de impacto (JCR 2022): 3.9   3.900         2169-3536   NaN    \n",
       "11    Fator de impacto (JCR 2022): 3.9   3.900         2169-3536     1    \n",
       "4     Fator de impacto (JCR 2022): 3.9   3.900         2169-3536     1    \n",
       "10    Fator de impacto (JCR 2022): 3.9   3.900         1424-8220    20    \n",
       "20    Fator de impacto (JCR 2022): 3.9   3.900         1424-8220    18    \n",
       "19    Fator de impacto (JCR 2022): 3.9   3.900         1424-8220    18    \n",
       "16    Fator de impacto (JCR 2022): 3.9   3.900         1424-8220    18    \n",
       "24    Fator de impacto (JCR 2022): 3.9   3.900         1424-8220    15    \n",
       "0   Fator de impacto (JCR 2021): 3.662   3.662         1868-5137   NaN    \n",
       "6     Fator de impacto (JCR 2022): 2.7   2.700         2076-3417    12    \n",
       "9     Fator de impacto (JCR 2022): 2.7   2.700         2076-3417    11    \n",
       "17    Fator de impacto (JCR 2022): 2.1   2.100         1074-5351    31    \n",
       "25  Fator de impacto (JCR 2018): 1.614   1.614         1550-1477  2014    \n",
       "13    Fator de impacto (JCR 2022): 1.5   1.500         1055-7148     1    \n",
       "2                                  NaN     NaN         1877-0509   220    \n",
       "28                                 NaN     NaN         1875-8819     9    \n",
       "23                                 NaN     NaN         1676-2789    14    \n",
       "3                                  NaN     NaN               NaN    25    \n",
       "22                                 NaN     NaN               NaN     3    \n",
       "8                                  NaN     NaN               NaN  2021    \n",
       "5                                  NaN     NaN               NaN    13    \n",
       "7                                  NaN     NaN               NaN    12    \n",
       "12                                 NaN     NaN               NaN    11    \n",
       "30                                 NaN     NaN               NaN     X    \n",
       "27                                 NaN     NaN               NaN    10    \n",
       "26                                 NaN     NaN               NaN    10    \n",
       "33                                 NaN     NaN               NaN   8o.    \n",
       "31                                 NaN     NaN               NaN    28    \n",
       "32                                 NaN     NaN               NaN    27    \n",
       "\n",
       "   paginaInicial   \n",
       "21   2704         \\\n",
       "29     44          \n",
       "14   5631          \n",
       "15    126          \n",
       "18     56          \n",
       "1     NaN          \n",
       "11      1          \n",
       "4       1          \n",
       "10   3068          \n",
       "20   1312          \n",
       "19    689          \n",
       "16    353          \n",
       "24   2104          \n",
       "0     NaN          \n",
       "6    8939          \n",
       "9   10457          \n",
       "17  e3380          \n",
       "25      1          \n",
       "13  e2062          \n",
       "2     119          \n",
       "28     61          \n",
       "23      4          \n",
       "3      27          \n",
       "22     30          \n",
       "8      13          \n",
       "5       1          \n",
       "7       1          \n",
       "12      1          \n",
       "30     65          \n",
       "27      4          \n",
       "26    702          \n",
       "33    NaN          \n",
       "31     33          \n",
       "32    113          \n",
       "\n",
       "   titulo                                                                                                                                                                  \n",
       "21                                                                                                           Model-Based Quantitative Network Security Metrics: A Survey  \\\n",
       "29                                                                                   On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries   \n",
       "14                                                                                                    Enabling Online Quantitative Security Analysis in 6LoWPAN Networks   \n",
       "15                                                                                       Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering   \n",
       "18                                                                   A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs   \n",
       "1                                                                                                                                                                    NaN   \n",
       "11                                                                                        Integration of the Mobile Robot and Internet of Things to Monitor Older People   \n",
       "4                                                                A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain   \n",
       "10                                                                                                      Enhancing Key Management in LoRaWAN with Permissioned Blockchain   \n",
       "20                                                        Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks   \n",
       "19                                                                    An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments   \n",
       "16                                                                                           A Proposal for IoT Dynamic Routes Selection Based on Contextual Information   \n",
       "24                                                                                             Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks   \n",
       "0                                                                                                                                                                    NaN   \n",
       "6                                                           Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19   \n",
       "9                                                        An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus   \n",
       "17                                               Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks   \n",
       "25                                                                                      A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions   \n",
       "13                                                                                                         An approach for provisioning virtual sensors in sensor clouds   \n",
       "2                                                          Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain   \n",
       "28                                                                         An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks   \n",
       "23                                                                                       UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES   \n",
       "3                                                     Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector   \n",
       "22                                                                                                      Exploring a P2P Transient Botnet - From Discovery to Enumeration   \n",
       "8                                                                                         Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19   \n",
       "5                                                                  Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública   \n",
       "7                                                                                 Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará   \n",
       "12                                                                  Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019   \n",
       "30                                                                                                            Controle Externo da Governança de Tecnologia da Informação   \n",
       "27  Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink   \n",
       "26                                                                   A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks   \n",
       "33                                                                                               Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários   \n",
       "31                                                                              Detecting computer network attacks using statistical discriminators and cluster analysis   \n",
       "32                                                                 Broadband network traffic characterization and classification using a multivariate statistical method   \n",
       "\n",
       "   sequencial nomePeriodico                                           \n",
       "21   22                   IEEE Communications Surveys and Tutorials  \\\n",
       "29   30                                  Information Fusion (Print)   \n",
       "14   15                             IEEE Internet of Things Journal   \n",
       "15   16                                                IEEE NETWORK   \n",
       "18   19                JOURNAL OF NETWORK AND COMPUTER APPLICATIONS   \n",
       "1   NaN                                                         NaN   \n",
       "11   12                                                 IEEE Access   \n",
       "4     5                                                 IEEE Access   \n",
       "10   11                                                     SENSORS   \n",
       "20   21                                                     SENSORS   \n",
       "19   20                                                     SENSORS   \n",
       "16   17                                                     SENSORS   \n",
       "24   25                                                     SENSORS   \n",
       "0   NaN                                                         NaN   \n",
       "6     7                                      Applied Sciences-Basel   \n",
       "9    10                                      Applied Sciences-Basel   \n",
       "17   18              INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS   \n",
       "25   26        International Journal of Distributed Sensor Networks   \n",
       "13   14                 International Journal of Network Management   \n",
       "2     3                                   PROCEDIA COMPUTER SCIENCE   \n",
       "28   29         INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS   \n",
       "23   24                               LEARNING AND NONLINEAR MODELS   \n",
       "3     4                          Journal of Business and Management   \n",
       "22   23                                  The Journal on Cybercrime    \n",
       "8     9                          Springer Proceedings in Complexity   \n",
       "5     6                                     Revista Razão Contábil    \n",
       "7     8                                     Revista Razão Contábil    \n",
       "12   13                                     Revista Razão Contábil    \n",
       "30   31                                            Revista Controle   \n",
       "27   28                               Learning and Nonlinear Models   \n",
       "26   27                       JOURNAL OF COMMUNICATION AND COMPUTER   \n",
       "33   34               Revista Brasileira de Informática na Educação   \n",
       "31   32                                 Revista Tecnologia (UNIFOR)   \n",
       "32   33                                 Revista Tecnologia (UNIFOR)   \n",
       "\n",
       "   journal                                                   \n",
       "21                  IEEE Communications Surveys & Tutorials  \n",
       "29                                       Information Fusion  \n",
       "14                          IEEE Internet of Things Journal  \n",
       "15                                             IEEE Network  \n",
       "18             Journal of Network and Computer Applications  \n",
       "1                                               IEEE Access  \n",
       "11                                              IEEE Access  \n",
       "4                                               IEEE Access  \n",
       "10                                                  Sensors  \n",
       "20                                                  Sensors  \n",
       "19                                                  Sensors  \n",
       "16                                                  Sensors  \n",
       "24                                                  Sensors  \n",
       "0   Journal of Ambient Intelligence and Humanized Computing  \n",
       "6                                          Applied Sciences  \n",
       "9                                          Applied Sciences  \n",
       "17           International Journal of Communication Systems  \n",
       "25     International Journal of Distributed Sensor Networks  \n",
       "13              International Journal of Network Management  \n",
       "2                                 Procedia Computer Science  \n",
       "28      International Journal of Hybrid Intelligent Systems  \n",
       "23                            Learning and Nonlinear Models  \n",
       "3                                                       NaN  \n",
       "22                                                      NaN  \n",
       "8                                                       NaN  \n",
       "5                                                       NaN  \n",
       "7                                                       NaN  \n",
       "12                                                      NaN  \n",
       "30                                                      NaN  \n",
       "27                                                      NaN  \n",
       "26                                                      NaN  \n",
       "33                                                      NaN  \n",
       "31                                                      NaN  \n",
       "32                                                      NaN  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_json = fill_na_from_df_aggregated(df_final, df_aggregated)\n",
    "print(len(result_json))\n",
    "result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           1.5,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#ffff00",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          1.5
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "10557148"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           1.614,
           "JCR 2018"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#feff00",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          1.61
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "15501329"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           2.1,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#fbfd00",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          2.1
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "10745351"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           2.7,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#f6fb00",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          2.7
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "20763417"
         ],
         "xaxis": "x",
         "y": [
          2
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           3.662,
           "JCR 2021"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#eff700",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          3.66
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "18685137"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           3.9,
           "JCR 2022"
          ],
          [
           3.9,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#edf600",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          3.9,
          3.9
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "14248220",
          "21693536"
         ],
         "xaxis": "x",
         "y": [
          5,
          3
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           8.7,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#c9e400",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          8.7
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "10848045"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           9.3,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#c5e200",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          9.3
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "08908044"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           10.6,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#bbdd00",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          10.6
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "23274662"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           18.6,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#7fbf00",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          18.6
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "15662535"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           35.6,
           "JCR 2022"
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#008000",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          35.6
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          "1553877X"
         ],
         "xaxis": "x",
         "y": [
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           null,
           null
          ],
          [
           null,
           null
          ]
         ],
         "hovertemplate": "data-issn=%{x}<br>issn_count=%{y}<br>text=%{text}<br>impact-factor=%{customdata[0]}<br>jcr_year=%{customdata[1]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#808080",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          null,
          null
         ],
         "textfont": {
          "size": 10
         },
         "textposition": "outside",
         "texttemplate": "%{text}",
         "type": "bar",
         "x": [
          null,
          null
         ],
         "xaxis": "x",
         "y": [
          15,
          12
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "height": 800,
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "b": 150,
         "l": 50,
         "r": 50,
         "t": 50
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Frequência de publicação acumulada no período completo versus fator de impacto de cada periódico"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "tickangle": -45,
         "ticktext": [
          "International Journal of Network Management",
          "International Journal of Distributed Sensor Networks",
          "International Journal of Communication Systems",
          "Applied Sciences",
          "Journal of Ambient Intelligence and Humanized Computing",
          "Sensors",
          "IEEE Access",
          "Journal of Network and Computer Applications",
          "IEEE Network",
          "IEEE Internet of Things Journal",
          "Information Fusion",
          "IEEE Communications Surveys & Tutorials",
          "International Journal of Hybrid Intelligent Systems",
          null
         ],
         "tickvals": [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "MISSING"
         ],
         "title": {
          "text": "Periódicos"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": true,
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Frequência de publicação no periódico"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chame a função com sua lista de dados jcr_properties_list\n",
    "plot_vertbar(result_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscal avaliação Qualis Periódicos em arquivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>ISSN</th>\n",
       "      <th>Título</th>\n",
       "      <th>Área de Avaliação</th>\n",
       "      <th>Estrato</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0149-1423</td>\n",
       "      <td>AAPG BULLETIN (PRINT)</td>\n",
       "      <td>ADMINISTRAÇÃO PÚBLICA E DE EMPRESAS, CIÊNCIAS CONTÁBEIS E TURISMO</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001-4273</td>\n",
       "      <td>ACADEMY OF MANAGEMENT JOURNAL</td>\n",
       "      <td>ADMINISTRAÇÃO PÚBLICA E DE EMPRESAS, CIÊNCIAS CONTÁBEIS E TURISMO</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0951-3574</td>\n",
       "      <td>ACCOUNTING, AUDITING &amp; ACCOUNTABILITY JOURNAL</td>\n",
       "      <td>ADMINISTRAÇÃO PÚBLICA E DE EMPRESAS, CIÊNCIAS CONTÁBEIS E TURISMO</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467-6303</td>\n",
       "      <td>ACCOUNTING FORUM</td>\n",
       "      <td>ADMINISTRAÇÃO PÚBLICA E DE EMPRESAS, CIÊNCIAS CONTÁBEIS E TURISMO</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0155-9982</td>\n",
       "      <td>ACCOUNTING FORUM (ADELAIDE)</td>\n",
       "      <td>ADMINISTRAÇÃO PÚBLICA E DE EMPRESAS, CIÊNCIAS CONTÁBEIS E TURISMO</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154404</th>\n",
       "      <td>2359-5086</td>\n",
       "      <td>VETERINARY AND SCIENCE</td>\n",
       "      <td>ZOOTECNIA / RECURSOS PESQUEIROS</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154405</th>\n",
       "      <td>0976-996X</td>\n",
       "      <td>VETERINARY SCIENCE RESEARCH</td>\n",
       "      <td>ZOOTECNIA / RECURSOS PESQUEIROS</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154406</th>\n",
       "      <td>0102-7352</td>\n",
       "      <td>VETOR (FURG)</td>\n",
       "      <td>ZOOTECNIA / RECURSOS PESQUEIROS</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154407</th>\n",
       "      <td>1518-8361</td>\n",
       "      <td>VISÃO ACADÊMICA (ONLINE)</td>\n",
       "      <td>ZOOTECNIA / RECURSOS PESQUEIROS</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154408</th>\n",
       "      <td>1041-5602</td>\n",
       "      <td>WORLD AQUACULTURE</td>\n",
       "      <td>ZOOTECNIA / RECURSOS PESQUEIROS</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154409 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ISSN       Título                                           \n",
       "0       0149-1423                          AAPG BULLETIN (PRINT)  \\\n",
       "1       0001-4273                  ACADEMY OF MANAGEMENT JOURNAL   \n",
       "2       0951-3574  ACCOUNTING, AUDITING & ACCOUNTABILITY JOURNAL   \n",
       "3       1467-6303                               ACCOUNTING FORUM   \n",
       "4       0155-9982                    ACCOUNTING FORUM (ADELAIDE)   \n",
       "...           ...                                            ...   \n",
       "154404  2359-5086                         VETERINARY AND SCIENCE   \n",
       "154405  0976-996X                    VETERINARY SCIENCE RESEARCH   \n",
       "154406  0102-7352                                   VETOR (FURG)   \n",
       "154407  1518-8361                       VISÃO ACADÊMICA (ONLINE)   \n",
       "154408  1041-5602                              WORLD AQUACULTURE   \n",
       "\n",
       "       Área de Avaliação                                                    \n",
       "0       ADMINISTRAÇÃO PÚBLICA E DE EMPRESAS, CIÊNCIAS CONTÁBEIS E TURISMO  \\\n",
       "1       ADMINISTRAÇÃO PÚBLICA E DE EMPRESAS, CIÊNCIAS CONTÁBEIS E TURISMO   \n",
       "2       ADMINISTRAÇÃO PÚBLICA E DE EMPRESAS, CIÊNCIAS CONTÁBEIS E TURISMO   \n",
       "3       ADMINISTRAÇÃO PÚBLICA E DE EMPRESAS, CIÊNCIAS CONTÁBEIS E TURISMO   \n",
       "4       ADMINISTRAÇÃO PÚBLICA E DE EMPRESAS, CIÊNCIAS CONTÁBEIS E TURISMO   \n",
       "...                                                                   ...   \n",
       "154404                 ZOOTECNIA / RECURSOS PESQUEIROS                      \n",
       "154405                 ZOOTECNIA / RECURSOS PESQUEIROS                      \n",
       "154406                 ZOOTECNIA / RECURSOS PESQUEIROS                      \n",
       "154407                 ZOOTECNIA / RECURSOS PESQUEIROS                      \n",
       "154408                 ZOOTECNIA / RECURSOS PESQUEIROS                      \n",
       "\n",
       "       Estrato  \n",
       "0       A1      \n",
       "1       A1      \n",
       "2       A1      \n",
       "3       A1      \n",
       "4       A1      \n",
       "...        ...  \n",
       "154404   C      \n",
       "154405   C      \n",
       "154406   C      \n",
       "154407   C      \n",
       "154408   C      \n",
       "\n",
       "[154409 rows x 4 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/makaires77/fioce/main/source/adapters/input/data/sucupira_todas_as_areas_avaliacao1672761192111.csv\"\n",
    "data_sucupira = pd.read_csv(url, delimiter=';')\n",
    "data_sucupira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 3: Persistir em em Neo4j</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistir todos dados como propriedades do nó de Label: Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neo4j_persister import Neo4jPersister  # Para quando já houver pacote neo4j_persister.py\n",
    "# Create a Neo4jPersister instance and persist the data\n",
    "neo4j_persister = Neo4jPersister(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n",
    "neo4j_persister.persist_data(data_dict, \"Person\")\n",
    "\n",
    "# Close the Neo4j connection\n",
    "neo4j_persister.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação dos nós secundários e relacionamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 34 entries from JCR properties of Person 'Raimir Holanda Filho'.\n",
      "Identified 13 unique ISSN values.\n",
      "12 Revistas adicionadas com sucesso.\n",
      "15 Revistas não foram criadas por terem valor NULL de ISSN.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the JcrHandler object with the required connection details\n",
    "journals_analysis_obj = JcrHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to create Journals nodes and establish relationships for 'Raimir Holanda Filho'\n",
    "journals_analysis_obj.createJournalsNodes(name='Raimir Holanda Filho')\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "journals_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 orientações atualizadas com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the AdvisorHandler object with the required connection details\n",
    "advisor_analysis_obj = AdvisorHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to process advisories data and establish relationships for 'Raimir Holanda Filho'\n",
    "advisor_analysis_obj.create_advisor_relations(name='Raimir Holanda Filho')\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "advisor_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 'Áreas de atuação' relations successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the AreasHandler object with the required connection details\n",
    "areas_analysis_obj = AreasHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to process advisories data and establish relationships for 'Raimir Holanda Filho'\n",
    "areas_analysis_obj.create_areas_relations(name='Raimir Holanda Filho')\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "areas_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Institution node created/merged for: Universidade de Fortaleza, UNIFOR, Brasil.\n",
      "Relationship established between Raimir Holanda Filho and Universidade de Fortaleza, UNIFOR, Brasil..\n",
      "Linhas de pesquisa node created/merged for: Redes de Comunicação\n",
      "Relationship established between Raimir Holanda Filho and Redes de Comunicação (Linhas de pesquisa).\n",
      "Projetos de pesquisa node created/merged for: Monitor Fiscal TCE/CE\n",
      "Relationship established between Raimir Holanda Filho and Monitor Fiscal TCE/CE (Projetos de pesquisa).\n",
      "Projetos de pesquisa node created/merged for: Descrição: ESTUDO E DESENVOLVIMENTO DE SISTEMAS DE DETECÇÃO/PREVENÇÃO DE INTRUSÃO BASEADOS EM REGRAS PARA REDES EM AMBIENTE SEM FIO.. Situação: Concluído; Natureza: Pesquisa. Alunos envolvidos: Graduação: (3)  / Especialização: (0)  / Mestrado acadêmico: (1)  / Mestrado profissional: (0)  / Doutorado: (0) . Integrantes: Raimir Holanda Filho - Coordenador.\n",
      "Relationship established between Raimir Holanda Filho and Descrição: ESTUDO E DESENVOLVIMENTO DE SISTEMAS DE DETECÇÃO/PREVENÇÃO DE INTRUSÃO BASEADOS EM REGRAS PARA REDES EM AMBIENTE SEM FIO.. Situação: Concluído; Natureza: Pesquisa. Alunos envolvidos: Graduação: (3)  / Especialização: (0)  / Mestrado acadêmico: (1)  / Mestrado profissional: (0)  / Doutorado: (0) . Integrantes: Raimir Holanda Filho - Coordenador. (Projetos de pesquisa).\n",
      "Projetos de pesquisa node created/merged for: CIA2-Construindo Cidades Inteligentes da Instrumentação dos Ambientes ao Desenvolvimento de Aplicações\n",
      "Relationship established between Raimir Holanda Filho and CIA2-Construindo Cidades Inteligentes da Instrumentação dos Ambientes ao Desenvolvimento de Aplicações (Projetos de pesquisa).\n",
      "Projetos de pesquisa node created/merged for: DeLAtoR - Detecção e localização de disparos de armas baseado em redes de sensores sem fio\n",
      "Relationship established between Raimir Holanda Filho and DeLAtoR - Detecção e localização de disparos de armas baseado em redes de sensores sem fio (Projetos de pesquisa).\n",
      "Projetos de pesquisa node created/merged for: MOVERES: Sistema de Monitoramento Veicular Através de Redes de Sensores sem Fio. Projeto financiado pela FINEP (Edital Subvenção Economica a Inovacao - 01/2007)\n",
      "Relationship established between Raimir Holanda Filho and MOVERES: Sistema de Monitoramento Veicular Através de Redes de Sensores sem Fio. Projeto financiado pela FINEP (Edital Subvenção Economica a Inovacao - 01/2007) (Projetos de pesquisa).\n",
      "Projetos de pesquisa node created/merged for: Especificaçao de uma arquitetura de gerenciamento distribuído para RSSF. Projeto de Pesquisa cadastrado na Unifor.\n",
      "Relationship established between Raimir Holanda Filho and Especificaçao de uma arquitetura de gerenciamento distribuído para RSSF. Projeto de Pesquisa cadastrado na Unifor. (Projetos de pesquisa).\n",
      "Projetos de pesquisa node created/merged for: FATADIST - Uma Ferramenta para Classificação de Tráfego de Ataques. Projeto financiado com recursos do Edital AT - CNPq\n",
      "Relationship established between Raimir Holanda Filho and FATADIST - Uma Ferramenta para Classificação de Tráfego de Ataques. Projeto financiado com recursos do Edital AT - CNPq (Projetos de pesquisa).\n",
      "Projetos de pesquisa node created/merged for: CLASTRIN - Um Classificador de Tráfego Internet baseado em Discriminantes Estatísticos. Projeto financiado com recursos do Edital Universal - CNPq\n",
      "Relationship established between Raimir Holanda Filho and CLASTRIN - Um Classificador de Tráfego Internet baseado em Discriminantes Estatísticos. Projeto financiado com recursos do Edital Universal - CNPq (Projetos de pesquisa).\n",
      "Projetos de pesquisa node created/merged for: Sistema de Detecção de ataques DoS (Denial of Service) em redes móveis através de técnicas de modelagem.\n",
      "Relationship established between Raimir Holanda Filho and Sistema de Detecção de ataques DoS (Denial of Service) em redes móveis através de técnicas de modelagem. (Projetos de pesquisa).\n",
      "Projetos de pesquisa node created/merged for: Caracterização, classificação e modelagem de tráfego de redes convergentes\n",
      "Relationship established between Raimir Holanda Filho and Caracterização, classificação e modelagem de tráfego de redes convergentes (Projetos de pesquisa).\n",
      "Projetos de pesquisa node created/merged for: Telemedicina - Ampliaçao de Laboratório para Pesquisa e Desenvolvimento de Aplicações em Telemedicina. Projeto de pesquisa e desenvolvimento em tecnologia da informacao incentivado através da Lei de Informática. (Celéstica do Brasil Ltda)\n",
      "Relationship established between Raimir Holanda Filho and Telemedicina - Ampliaçao de Laboratório para Pesquisa e Desenvolvimento de Aplicações em Telemedicina. Projeto de pesquisa e desenvolvimento em tecnologia da informacao incentivado através da Lei de Informática. (Celéstica do Brasil Ltda) (Projetos de pesquisa).\n",
      "Projetos de pesquisa node created/merged for: Implantacao do laboratorio de Redes Convergentes. Projeto financiado com recursos da Lei de Informática (UNIFOR - SANMINA-SCI DO BRASIL INTEGRATION LTDA)\n",
      "Relationship established between Raimir Holanda Filho and Implantacao do laboratorio de Redes Convergentes. Projeto financiado com recursos da Lei de Informática (UNIFOR - SANMINA-SCI DO BRASIL INTEGRATION LTDA) (Projetos de pesquisa).\n",
      "Projetos de extensão node created/merged for: S2IPro - Sistema de Identificação Inequívoca do Proprietário\n",
      "Relationship established between Raimir Holanda Filho and S2IPro - Sistema de Identificação Inequívoca do Proprietário (Projetos de extensão).\n",
      "Projetos de extensão node created/merged for: Descrição: Sistema de Identificação Inequívoca do Proprietário. Situação: Concluído; Natureza: Extensão. Alunos envolvidos: Graduação: (2)  / Mestrado acadêmico: (1) . Integrantes: Raimir Holanda Filho - Coordenador / Luis Henrique Pequeno Almeida - Integrante.Financiador(es): Thinktech Ind e Comercio de Informatica Ltda - Auxílio financeiro.\n",
      "Relationship established between Raimir Holanda Filho and Descrição: Sistema de Identificação Inequívoca do Proprietário. Situação: Concluído; Natureza: Extensão. Alunos envolvidos: Graduação: (2)  / Mestrado acadêmico: (1) . Integrantes: Raimir Holanda Filho - Coordenador / Luis Henrique Pequeno Almeida - Integrante.Financiador(es): Thinktech Ind e Comercio de Informatica Ltda - Auxílio financeiro. (Projetos de extensão).\n",
      "Projetos de desenvolvimento node created/merged for: Desenvolvimento de Software para Gerenciamento de Arquitetura de Telecomunicações de Baixo Custo para Comunidades Rurais, Pequenas Empresas, Órgãos e Instituições Municipais/Estaduais\n",
      "Relationship established between Raimir Holanda Filho and Desenvolvimento de Software para Gerenciamento de Arquitetura de Telecomunicações de Baixo Custo para Comunidades Rurais, Pequenas Empresas, Órgãos e Instituições Municipais/Estaduais (Projetos de desenvolvimento).\n",
      "Projetos de desenvolvimento node created/merged for: Situação: Em andamento; Natureza: Desenvolvimento. Alunos envolvidos: Graduação: (4)  / Mestrado acadêmico: (2) . Integrantes: Raimir Holanda Filho - Coordenador / Geneflides Laureno da Silva - Integrante.Financiador(es): Financiadora de Estudos e Projetos - Auxílio financeiro.\n",
      "Relationship established between Raimir Holanda Filho and Situação: Em andamento; Natureza: Desenvolvimento. Alunos envolvidos: Graduação: (4)  / Mestrado acadêmico: (2) . Integrantes: Raimir Holanda Filho - Coordenador / Geneflides Laureno da Silva - Integrante.Financiador(es): Financiadora de Estudos e Projetos - Auxílio financeiro. (Projetos de desenvolvimento).\n",
      "17 projetos atualizados com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ProjectsHandler object with the required connection details\n",
    "projects_analysis_obj = ProjectsHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to process advisories data and establish relationships for 'Raimir Holanda Filho'\n",
    "projects_analysis_obj.create_projects_relations(name='Raimir Holanda Filho')\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "projects_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 'Produções' data for Raimir Holanda Filho...\n",
      "Attempting to deserialize 'Produções' data for Raimir Holanda Filho...\n",
      "Processing 'Produção bibliográfica' for Raimir Holanda Filho...\n",
      "Attempting to deserialize 'Produção bibliográfica' for Raimir Holanda Filho...\n",
      "Original Article: ANDRADE, E. C. ; PINHEIRO, L. I. ;Pinheiro, Placido R.; NUNES, L. C. ; PINHEIRO, M. C. D. ; PEREIRA, M. L. D. ; ABREU, W. C. ;HOLANDA FILHO, RAIMIR; SIMAO FILHO, M. ; PINHEIRO, P. G. C. D. ; NUNES, R. E. C. . Hybrid model for early identification post-Covid-19 sequelae. Journal of Ambient Intelligence and Humanized Computing, p. 1-14, 2023.\n",
      "Extracted Details: {'authors': 'ANDRADE, E. C. ; PINHEIRO, L. I. ;Pinheiro, Placido R.; NUNES, L. C. ; PINHEIRO, M. C. D. ; PEREIRA, M. L. D. ; ABREU, W. C. ;HOLANDA FILHO, RAIMIR; SIMAO FILHO, M. ; PINHEIRO, P. G. C. D. ; NUNES, R. E. C.', 'title': 'Hybrid model for early identification post-Covid-19 sequelae', 'original_title': 'Journal of Ambient Intelligence and Humanized Computing', 'pages': '1-14', 'volume': '', 'year': '2023'}\n",
      "Original Article: MARINHO, RENATO ;HOLANDA, RAIMIR. Automated Emerging Cyber Threat Identification and Profiling Based on Natural Language Processing. IEEE Access, v. 1, p. 1-1, 2023.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'MARINHO, RENATO ;HOLANDA, RAIMIR. Automated Emerging Cyber Threat Identification and Profiling Based on Natural Language Processing. IEEE Access, v. 1, p. 1-1, 2023.CitaÃ§Ãµes', 'title': 'ARINHO, RENATO ;HOLANDA, RAIMIR. Automated Emerging Cyber Threat Identification and Profiling Based on Natural Language Processing. IEEE Access, v. 1, p. 1-1, 2023.CitaÃ§Ãµes', 'original_title': 'ARINHO, RENATO ;HOLANDA, RAIMIR. Automated Emerging Cyber Threat Identification and Profiling Based on Natural Language Processing. IEEE Access, v. 1, p. 1-1, 2023.CitaÃ§Ãµes', 'pages': '1-1', 'volume': '1', 'year': '2023'}\n",
      "Original Article: FILHO, RAIMIR HOLANDA; DE SOUSA, DEBORA CARLA BARBOZA ; DE BRITO, WELLINGTON ALVES ; CHAVES, JOAN LUCAS MARQUES DE SOUSA ; SÁ, EMANUEL LEÃO ; RIBEIRO, VICTOR PASKNEL DE ALENCAR . Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain. PROCEDIA COMPUTER SCIENCE, v. 220, p. 119-126, 2023.\n",
      "Extracted Details: {'authors': 'FILHO, RAIMIR HOLANDA; DE SOUSA, DEBORA CARLA BARBOZA ; DE BRITO, WELLINGTON ALVES ; CHAVES, JOAN LUCAS MARQUES DE SOUSA ; SÁ, EMANUEL LEÃO ; RIBEIRO, VICTOR PASKNEL DE ALENCAR . Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain. PROCEDIA COMPUTER SCIENCE, v. 220, p. 119-126, 2023', 'title': 'ILHO, RAIMIR HOLANDA; DE SOUSA, DEBORA CARLA BARBOZA ; DE BRITO, WELLINGTON ALVES ; CHAVES, JOAN LUCAS MARQUES DE SOUSA ; SÁ, EMANUEL LEÃO ; RIBEIRO, VICTOR PASKNEL DE ALENCAR . Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain. PROCEDIA COMPUTER SCIENCE, v. 220, p. 119-126, 2023', 'original_title': 'ILHO, RAIMIR HOLANDA; DE SOUSA, DEBORA CARLA BARBOZA ; DE BRITO, WELLINGTON ALVES ; CHAVES, JOAN LUCAS MARQUES DE SOUSA ; SÁ, EMANUEL LEÃO ; RIBEIRO, VICTOR PASKNEL DE ALENCAR . Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain. PROCEDIA COMPUTER SCIENCE, v. 220, p. 119-126, 2023', 'pages': '119-126', 'volume': '220', 'year': '2023'}\n",
      "Original Article: HOLANDA FILHO, RAIMIR; CUNHA, G. H. M. ; RAVIOLO, B. P. Y. ; BRITO, R. W. C. . Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector. Journal of Business and Management, v. 25, p. 27-35, 2023.\n",
      "Extracted Details: {'authors': 'HOLANDA FILHO, RAIMIR; CUNHA, G. H. M. ; RAVIOLO, B. P. Y. ; BRITO, R. W. C.', 'title': 'Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector', 'original_title': 'Journal of Business and Management', 'pages': '27-35', 'volume': '25', 'year': '2023'}\n",
      "Original Article: Ribeiro, Victor ;HOLANDA, RAIMIR; RAMOS, ALEX ; RODRIGUES, JOEL J. P. C. . A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain. IEEE Access, v. 1, p. 1-1, 2022.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'Ribeiro, Victor ;HOLANDA, RAIMIR; RAMOS, ALEX ; RODRIGUES, JOEL J. P. C.', 'title': 'A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain', 'original_title': 'IEEE Access', 'pages': '1-1', 'volume': '1', 'year': '2022'}\n",
      "Original Article: MAPURUNGA, M. P. A. ;HOLANDA FILHO, RAIMIR. Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública. Revista Razão Contábil & Finanças, v. 13, p. 1-14, 2022.\n",
      "Extracted Details: {'authors': 'MAPURUNGA, M. P. A. ;HOLANDA FILHO, RAIMIR', 'title': 'Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública', 'original_title': 'Revista Razão Contábil & Finanças', 'pages': '1-14', 'volume': '13', 'year': '2022'}\n",
      "Original Article: ANDRADE, EVANDRO CARVALHO DE ; PINHEIRO, PLÁCIDO ROGERIO ; BARROS, ANA LUIZA BESSA DE PAULA ; NUNES, LUCIANO COMIN ; PINHEIRO, LUANA IBIAPINA C. C. ; PINHEIRO, PEDRO GABRIEL CALÍOPE DANTAS ;HOLANDA FILHO, RAIMIR. Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19. Applied Sciences-Basel, v. 12, p. 8939, 2022.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'ANDRADE, EVANDRO CARVALHO DE ; PINHEIRO, PLÁCIDO ROGERIO ; BARROS, ANA LUIZA BESSA DE PAULA ; NUNES, LUCIANO COMIN ; PINHEIRO, LUANA IBIAPINA C. C. ; PINHEIRO, PEDRO GABRIEL CALÍOPE DANTAS ;HOLANDA FILHO, RAIMIR', 'title': 'Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19', 'original_title': 'Applied Sciences-Basel', 'pages': '8939', 'volume': '12', 'year': '2022'}\n",
      "Original Article: LIMA, S. M. ; BRITO, R. W. ; MATOS, P. R. F. ; JESUS FILHO, J. ;HOLANDA FILHO, R.; CUNHA, G. H. M. . Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará. Revista Razão Contábil & Finanças, v. 12, p. 1, 2021.\n",
      "Extracted Details: {'authors': 'LIMA, S. M. ; BRITO, R. W. ; MATOS, P. R. F. ; JESUS FILHO, J. ;HOLANDA FILHO, R.; CUNHA, G. H. M.', 'title': 'Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará', 'original_title': 'Revista Razão Contábil & Finanças', 'pages': '1', 'volume': '12', 'year': '2021'}\n",
      "Original Article: ANDRADE, E. C. ;Pinheiro, Placido R.;HOLANDA FILHO, R.; NUNES, L. C. ; PINHEIRO, M. C. D. ; ABREU, W. C. ; SIMAO FILHO, M. ; PINHEIRO, L. I. C. C. ; PEREIRA, M. L. D. ; PINHEIRO, P. G. C. D. . Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19. Springer Proceedings in Complexity, v. 2021, p. 13-24, 2021.\n",
      "Extracted Details: {'authors': 'ANDRADE, E. C. ;Pinheiro, Placido R.;HOLANDA FILHO, R.; NUNES, L. C. ; PINHEIRO, M. C. D. ; ABREU, W. C. ; SIMAO FILHO, M. ; PINHEIRO, L. I. C. C. ; PEREIRA, M. L. D. ; PINHEIRO, P. G. C. D.', 'title': 'Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19', 'original_title': 'Springer Proceedings in Complexity', 'pages': '13-24', 'volume': '2021', 'year': '2021'}\n",
      "Original Article: PINHEIRO, LUANA I. C. C. ; PEREIRA, MARIA LÚCIA D. ; ANDRADE, EVANDRO C. DE ; NUNES, LUCIANO C. ; ABREU, WILSON C. DE ; PINHEIRO, PEDRO GABRIEL CALÍOPE D. ;HOLANDA FILHO, RAIMIR; PINHEIRO, PLÁCIDO ROGERIO . An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus. Applied Sciences-Basel, v. 11, p. 10457, 2021.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'PINHEIRO, LUANA I. C. C. ; PEREIRA, MARIA LÚCIA D. ; ANDRADE, EVANDRO C. DE ; NUNES, LUCIANO C. ; ABREU, WILSON C. DE ; PINHEIRO, PEDRO GABRIEL CALÍOPE D. ;HOLANDA FILHO, RAIMIR; PINHEIRO, PLÁCIDO ROGERIO', 'title': 'An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus', 'original_title': 'Applied Sciences-Basel', 'pages': '10457', 'volume': '11', 'year': '2021'}\n",
      "Original Article: Ribeiro, Victor ;HOLANDA, RAIMIR; RAMOS, ALEX ; RODRIGUES, JOEL J. P. C. . Enhancing Key Management in LoRaWAN with Permissioned Blockchain. SENSORS, v. 20, p. 3068, 2020.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'Ribeiro, Victor ;HOLANDA, RAIMIR; RAMOS, ALEX ; RODRIGUES, JOEL J. P. C.', 'title': 'Enhancing Key Management in LoRaWAN with Permissioned Blockchain', 'original_title': 'SENSORS', 'pages': '3068', 'volume': '20', 'year': '2020'}\n",
      "Original Article: Pinheiro, Placido R.; PINHEIRO, PEDRO G. C. D. ;FILHO, RAIMIR H.; BARROZO, JOAO P. A. ; RODRIGUES, JOEL J. P. C. ; PINHEIRO, LUANA I. C. C. ; PEREIRA, MARIA L. D. . Integration of the Mobile Robot and Internet of Things to Monitor Older People. IEEE Access, v. 1, p. 1-1, 2020.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'Pinheiro, Placido R.; PINHEIRO, PEDRO G. C. D. ;FILHO, RAIMIR H.; BARROZO, JOAO P. A. ; RODRIGUES, JOEL J. P. C. ; PINHEIRO, LUANA I. C. C. ; PEREIRA, MARIA L. D.', 'title': 'Integration of the Mobile Robot and Internet of Things to Monitor Older People', 'original_title': 'IEEE Access', 'pages': '1-1', 'volume': '1', 'year': '2020'}\n",
      "Original Article: LIMA, S. M. ; BRITO, R. W. ; MATOS, P. R. F. ; JESUS FILHO, J. ;HOLANDA FILHO, R.. Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019. Revista Razão Contábil & Finanças, v. 11, p. 1, 2020.\n",
      "Extracted Details: {'authors': 'LIMA, S. M. ; BRITO, R. W. ; MATOS, P. R. F. ; JESUS FILHO, J. ;HOLANDA FILHO, R', 'title': 'Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019', 'original_title': 'Revista Razão Contábil & Finanças', 'pages': '1', 'volume': '11', 'year': '2020'}\n",
      "Original Article: LEMOS, MARCUS ; RABELO, RICARDO ; MENDES, DOUGLAS ; CARVALHO, CARLOS ;HOLANDA, RAIMIR. An approach for provisioning virtual sensors in sensor clouds. International Journal of Network Management, v. 1, p. e2062, 2019.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'LEMOS, MARCUS ; RABELO, RICARDO ; MENDES, DOUGLAS ; CARVALHO, CARLOS ;HOLANDA, RAIMIR. An approach for provisioning virtual sensors in sensor clouds. International Journal of Network Management, v. 1, p. e2062, 2019.CitaÃ§Ãµes', 'title': 'EMOS, MARCUS ; RABELO, RICARDO ; MENDES, DOUGLAS ; CARVALHO, CARLOS ;HOLANDA, RAIMIR. An approach for provisioning virtual sensors in sensor clouds. International Journal of Network Management, v. 1, p. e2062, 2019.CitaÃ§Ãµes', 'original_title': 'EMOS, MARCUS ; RABELO, RICARDO ; MENDES, DOUGLAS ; CARVALHO, CARLOS ;HOLANDA, RAIMIR. An approach for provisioning virtual sensors in sensor clouds. International Journal of Network Management, v. 1, p. e2062, 2019.CitaÃ§Ãµes', 'pages': 'e2062', 'volume': '1', 'year': '2019'}\n",
      "Original Article: RAMOS, ALEX ; MILFONT, RONALDO T. P. ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. . Enabling Online Quantitative Security Analysis in 6LoWPAN Networks. IEEE Internet of Things Journal, v. 6, p. 5631-5638, 2019.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'RAMOS, ALEX ; MILFONT, RONALDO T. P. ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C.', 'title': 'Enabling Online Quantitative Security Analysis in 6LoWPAN Networks', 'original_title': 'IEEE Internet of Things Journal', 'pages': '5631-5638', 'volume': '6', 'year': '2019'}\n",
      "Original Article: GUIMARAES, RANIERE ROCHA ; PASSOS, LEANDRO A. ;FILHO, RAIMIR HOLANDA; ALBUQUERQUE, VICTOR HUGO C. DE ; RODRIGUES, JOEL J. P. C. ; KOMAROV, MIKHAIL M. ; PAPA, JOAO PAULO . Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering. IEEE NETWORK, v. 33, p. 126-131, 2019.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'GUIMARAES, RANIERE ROCHA ; PASSOS, LEANDRO A. ;FILHO, RAIMIR HOLANDA; ALBUQUERQUE, VICTOR HUGO C. DE ; RODRIGUES, JOEL J. P. C. ; KOMAROV, MIKHAIL M. ; PAPA, JOAO PAULO', 'title': 'Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering', 'original_title': 'IEEE NETWORK', 'pages': '126-131', 'volume': '33', 'year': '2019'}\n",
      "Original Article: ARAÚJO, HARILTON ;FILHO, RAIMIR; RODRIGUES, JOEL ; RABELO, RICARDO ; SOUSA, NATANAEL ; FILHO, JOSÉ ; SOBRAL, JOSÉ . A Proposal for IoT Dynamic Routes Selection Based on Contextual Information. SENSORS, v. 18, p. 353, 2018.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'ARAÚJO, HARILTON ;FILHO, RAIMIR; RODRIGUES, JOEL ; RABELO, RICARDO ; SOUSA, NATANAEL ; FILHO, JOSÉ ; SOBRAL, JOSÉ . A Proposal for IoT Dynamic Routes Selection Based on Contextual Information. SENSORS, v. 18, p. 353, 2018.CitaÃ§Ãµes', 'title': 'RAÚJO, HARILTON ;FILHO, RAIMIR; RODRIGUES, JOEL ; RABELO, RICARDO ; SOUSA, NATANAEL ; FILHO, JOSÉ ; SOBRAL, JOSÉ . A Proposal for IoT Dynamic Routes Selection Based on Contextual Information. SENSORS, v. 18, p. 353, 2018.CitaÃ§Ãµes', 'original_title': 'RAÚJO, HARILTON ;FILHO, RAIMIR; RODRIGUES, JOEL ; RABELO, RICARDO ; SOUSA, NATANAEL ; FILHO, JOSÉ ; SOBRAL, JOSÉ . A Proposal for IoT Dynamic Routes Selection Based on Contextual Information. SENSORS, v. 18, p. 353, 2018.CitaÃ§Ãµes', 'pages': '353', 'volume': '18', 'year': '2018'}\n",
      "Original Article: ARAÚJO, PAULO RÉGIS C. ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. ; OLIVEIRA, JOÃO P. C. M. ; BRAGA, STEPHANIE A. . Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks. INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS, v. 31, p. e3380, 2018.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'ARAÚJO, PAULO RÉGIS C. ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. ; OLIVEIRA, JOÃO P. C. M. ; BRAGA, STEPHANIE A.', 'title': 'Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks', 'original_title': 'INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS', 'pages': 'e3380', 'volume': '31', 'year': '2018'}\n",
      "Original Article: SOBRAL, JOSÉ V.V. ; RODRIGUES, JOEL J.P.C. ; RABELO, RICARDO A.L. ; LIMA FILHO, JOSÉ C. ; SOUSA, NATANAEL ; ARAUJO, HARILTON S. ;HOLANDA FILHO, RAIMIR. A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs. JOURNAL OF NETWORK AND COMPUTER APPLICATIONS, v. 107, p. 56-68, 2018.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'SOBRAL, JOSÉ V.V. ; RODRIGUES, JOEL J.P.C. ; RABELO, RICARDO A.L. ; LIMA FILHO, JOSÉ C. ; SOUSA, NATANAEL ; ARAUJO, HARILTON S. ;HOLANDA FILHO, RAIMIR', 'title': 'A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs', 'original_title': 'JOURNAL OF NETWORK AND COMPUTER APPLICATIONS', 'pages': '56-68', 'volume': '107', 'year': '2018'}\n",
      "Original Article: LEMOS, MARCUS ;FILHO, RAIMIR; RABÊLO, RICARDO ; DE CARVALHO, CARLOS ; MENDES, DOUGLAS ; COSTA, VALNEY . An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments. SENSORS, v. 18, p. 689, 2018.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'LEMOS, MARCUS ;FILHO, RAIMIR; RABÊLO, RICARDO ; DE CARVALHO, CARLOS ; MENDES, DOUGLAS ; COSTA, VALNEY . An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments. SENSORS, v. 18, p. 689, 2018.CitaÃ§Ãµes', 'title': 'EMOS, MARCUS ;FILHO, RAIMIR; RABÊLO, RICARDO ; DE CARVALHO, CARLOS ; MENDES, DOUGLAS ; COSTA, VALNEY . An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments. SENSORS, v. 18, p. 689, 2018.CitaÃ§Ãµes', 'original_title': 'EMOS, MARCUS ;FILHO, RAIMIR; RABÊLO, RICARDO ; DE CARVALHO, CARLOS ; MENDES, DOUGLAS ; COSTA, VALNEY . An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments. SENSORS, v. 18, p. 689, 2018.CitaÃ§Ãµes', 'pages': '689', 'volume': '18', 'year': '2018'}\n",
      "Original Article: DE ARAÚJO, PAULO ;FILHO, RAIMIR; RODRIGUES, JOEL ; OLIVEIRA, JOÃO ; BRAGA, STEPHANIE . Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks. SENSORS, v. 18, p. 1312, 2018.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'DE ARAÚJO, PAULO ;FILHO, RAIMIR; RODRIGUES, JOEL ; OLIVEIRA, JOÃO ; BRAGA, STEPHANIE . Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks. SENSORS, v. 18, p. 1312, 2018.CitaÃ§Ãµes', 'title': 'E ARAÚJO, PAULO ;FILHO, RAIMIR; RODRIGUES, JOEL ; OLIVEIRA, JOÃO ; BRAGA, STEPHANIE . Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks. SENSORS, v. 18, p. 1312, 2018.CitaÃ§Ãµes', 'original_title': 'E ARAÚJO, PAULO ;FILHO, RAIMIR; RODRIGUES, JOEL ; OLIVEIRA, JOÃO ; BRAGA, STEPHANIE . Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks. SENSORS, v. 18, p. 1312, 2018.CitaÃ§Ãµes', 'pages': '1312', 'volume': '18', 'year': '2018'}\n",
      "Original Article: RAMOS, ALEX ; LAZAR, MARCELLA ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. . Model-Based Quantitative Network Security Metrics: A Survey. IEEE Communications Surveys and Tutorials, v. 19, p. 2704-2734, 2017.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'RAMOS, ALEX ; LAZAR, MARCELLA ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C.', 'title': 'Model-Based Quantitative Network Security Metrics: A Survey', 'original_title': 'IEEE Communications Surveys and Tutorials', 'pages': '2704-2734', 'volume': '19', 'year': '2017'}\n",
      "Original Article: MARINHO, R. ;HOLANDA FILHO, RAIMIR. Exploring a P2P Transient Botnet - From Discovery to Enumeration. The Journal on Cybercrime & Digital Investigations, v. 3, p. 30-39, 2017.\n",
      "Extracted Details: {'authors': 'MARINHO, R. ;HOLANDA FILHO, RAIMIR', 'title': 'Exploring a P2P Transient Botnet - From Discovery to Enumeration', 'original_title': 'The Journal on Cybercrime & Digital Investigations', 'pages': '30-39', 'volume': '3', 'year': '2017'}\n",
      "Original Article: LEMOS, MARCUS ; CARVALHO, CARLOS ; LOPES, DOUGLAS ;HOLANDA, RAIMIR; RABÊLO, RICARDO . UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES. LEARNING AND NONLINEAR MODELS, v. 14, p. 4-14, 2016.\n",
      "Extracted Details: {'authors': 'LEMOS, MARCUS ; CARVALHO, CARLOS ; LOPES, DOUGLAS ;HOLANDA, RAIMIR; RABÊLO, RICARDO . UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES. LEARNING AND NONLINEAR MODELS, v. 14, p. 4-14, 2016', 'title': 'EMOS, MARCUS ; CARVALHO, CARLOS ; LOPES, DOUGLAS ;HOLANDA, RAIMIR; RABÊLO, RICARDO . UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES. LEARNING AND NONLINEAR MODELS, v. 14, p. 4-14, 2016', 'original_title': 'EMOS, MARCUS ; CARVALHO, CARLOS ; LOPES, DOUGLAS ;HOLANDA, RAIMIR; RABÊLO, RICARDO . UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES. LEARNING AND NONLINEAR MODELS, v. 14, p. 4-14, 2016', 'pages': '4-14', 'volume': '14', 'year': '2016'}\n",
      "Original Article: RAMOS, A. L. ;HOLANDA FILHO, R.. Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks. SENSORS, v. 15, p. 2104-2136, 2015.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'RAMOS, A. L. ;HOLANDA FILHO, R', 'title': 'Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks', 'original_title': 'SENSORS', 'pages': '2104-2136', 'volume': '15', 'year': '2015'}\n",
      "Original Article: ARAUJO, Paulo Regis C. ;HOLANDA FILHO, R.; RODRIGUES, Antonio. Wendell. O. ; ARAUJO, Andre. Luiz. C. ; MORAES FILHO, Jose. A. ; OLIVEIRA, Joao. Paolo. C. M. . A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions. International Journal of Distributed Sensor Networks, v. 2014, p. 1-15, 2014.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'ARAUJO, Paulo Regis C. ;HOLANDA FILHO, R.; RODRIGUES, Antonio. Wendell. O. ; ARAUJO, Andre. Luiz. C. ; MORAES FILHO, Jose. A. ; OLIVEIRA, Joao. Paolo. C. M.', 'title': 'A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions', 'original_title': 'International Journal of Distributed Sensor Networks', 'pages': '1-15', 'volume': '2014', 'year': '2014'}\n",
      "Original Article: SOBRAL, J. ; SOUSA, A. ; ARAUJO, H. S. ; BALUZ, R. ;HOLANDA FILHO, R.; SOUSA, M. V. ; Rabelo, Ricardo A. L. . A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks. JOURNAL OF COMMUNICATION AND COMPUTER, v. 10, p. 702-712, 2013.\n",
      "Extracted Details: {'authors': 'SOBRAL, J. ; SOUSA, A. ; ARAUJO, H. S. ; BALUZ, R. ;HOLANDA FILHO, R.; SOUSA, M. V. ; Rabelo, Ricardo A. L.', 'title': 'A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks', 'original_title': 'JOURNAL OF COMMUNICATION AND COMPUTER', 'pages': '702-712', 'volume': '10', 'year': '2013'}\n",
      "Original Article: Rabelo, Ricardo A. L. ; LEAL, L. B. ; SOUSA, M. V. ;HOLANDA FILHO, R.; Borges, Fabio A. S. . Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink. Learning and Nonlinear Models, v. 10, p. 4-18, 2012.\n",
      "Extracted Details: {'authors': 'Rabelo, Ricardo A. L. ; LEAL, L. B. ; SOUSA, M. V. ;HOLANDA FILHO, R.; Borges, Fabio A. S.', 'title': 'Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink', 'original_title': 'Learning and Nonlinear Models', 'pages': '4-18', 'volume': '10', 'year': '2012'}\n",
      "Original Article: Rabelo, Ricardo A. L. ; LEAL, L. B. ; SOUSA, M. V. ;HOLANDA FILHO, R.; Borges, Fabio A. S. . An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks. INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS, v. 9, p. 61-74, 2012.\n",
      "Extracted Details: {'authors': 'Rabelo, Ricardo A. L. ; LEAL, L. B. ; SOUSA, M. V. ;HOLANDA FILHO, R.; Borges, Fabio A. S.', 'title': 'An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks', 'original_title': 'INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS', 'pages': '61-74', 'volume': '9', 'year': '2012'}\n",
      "Original Article: BRAYNER, A. ; Coelho, André Luis Carvalho ; Marinho, Karine ;HOLANDA FILHO, R.; Castro, Wagner L. T. . On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries. Information Fusion (Print), v. 15, p. 44-55, 2012.CitaÃ§Ãµes:\n",
      "Extracted Details: {'authors': 'BRAYNER, A. ; Coelho, André Luis Carvalho ; Marinho, Karine ;HOLANDA FILHO, R.; Castro, Wagner L. T.', 'title': 'On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries', 'original_title': 'Information Fusion (Print)', 'pages': '44-55', 'volume': '15', 'year': '2012'}\n",
      "Original Article: HOLANDA FILHO, R.; OLIVEIRA, J. A. . Controle Externo da Governança de Tecnologia da Informação. Revista Controle, v. X, p. 65-85, 2012.\n",
      "Extracted Details: {'authors': 'HOLANDA FILHO, R.; OLIVEIRA, J. A.', 'title': 'Controle Externo da Governança de Tecnologia da Informação', 'original_title': 'Revista Controle', 'pages': '65-85', 'volume': 'X', 'year': '2012'}\n",
      "Original Article: HOLANDA FILHO, R.; MAIA, J. E. B. ; CARMO, M. F. F. . Detecting computer network attacks using statistical discriminators and cluster analysis. Revista Tecnologia (UNIFOR), v. 28, p. 33-41, 2007.\n",
      "Extracted Details: {'authors': 'HOLANDA FILHO, R.; MAIA, J. E. B. ; CARMO, M. F. F.', 'title': 'Detecting computer network attacks using statistical discriminators and cluster analysis', 'original_title': 'Revista Tecnologia (UNIFOR)', 'pages': '33-41', 'volume': '28', 'year': '2007'}\n",
      "Original Article: HOLANDA FILHO, R.; MAIA, J. E. B. ; PAULINO, G. . Broadband network traffic characterization and classification using a multivariate statistical method. Revista Tecnologia (UNIFOR), v. 27, p. 113-122, 2006.\n",
      "Extracted Details: {'authors': 'HOLANDA FILHO, R.; MAIA, J. E. B. ; PAULINO, G.', 'title': 'Broadband network traffic characterization and classification using a multivariate statistical method', 'original_title': 'Revista Tecnologia (UNIFOR)', 'pages': '113-122', 'volume': '27', 'year': '2006'}\n",
      "Original Article: FURTADO, E. ; LINCOLN, F. ; FURTADO, V. ;HOLANDA FILHO, R.. Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários. Revista Brasileira de Informática na Educação, v. 8o., 2001.\n",
      "Extracted Details: {'authors': 'FURTADO, E. ; LINCOLN, F. ; FURTADO, V. ;HOLANDA FILHO, R', 'title': 'Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários', 'original_title': 'Revista Brasileira de Informática na Educação', 'pages': '', 'volume': '8o.', 'year': '2001'}\n",
      "Processed 34 articles successfully for Raimir Holanda Filho.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ProjectsHandler object with the required connection details\n",
    "articles_analysis_obj = ArticleHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to process advisories data and establish relationships for 'Raimir Holanda Filho'\n",
    "articles_analysis_obj.process_articles(name='Raimir Holanda Filho')\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "articles_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletar nós por Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handler = DataRemovalHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "# # labels=\"GrandeÁrea\"\n",
    "# # labels=\"Área\"\n",
    "# # labels=\"Subárea\"\n",
    "# labels = 'Artigo'\n",
    "# deleted_count = handler.delete_nodes_by_label(labels)\n",
    "# print(f\"{deleted_count} nós deletados com rótulo {labels}.\")\n",
    "# handler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> FASE 4: Extrações em lote de lista de pesquisadores</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatizar busca por dados ISSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://developer.clarivate.com/\n",
    "## https://developer.clarivate.com/apis?filter=on&Web+of+Science=on\n",
    "# Subscription and access: Access to this API requires a paid license. The license is available as an add-on subscription to Journal Citation Reports™ or InCites Benchmarking & Analytics™. Don't hesitate to get in touch with us with any questions you may have. This API uses API Key access. All APIs in our Developer Portal require registering an application.\n",
    "# Plans: This API has only one plan (Journals API Plan) and allows a maximum of 5 requests per second.\n",
    "\n",
    "# !pip install git+https://github.com/clarivate/wosjournals-python-client.git\n",
    "# !pip install git+https://github.com/Clarivate-SAR/woslite_py_client.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    Formata uma lista de strings no formato ISSN, que é 0000-0000.\n",
    "\n",
    "    Args:\n",
    "    - issn_list (list): Lista de strings a ser formatada.\n",
    "\n",
    "    Returns:\n",
    "    - list: Lista de strings formatada no formato ISSN.\n",
    "\n",
    "    consultar até 600 ISSN por vez na página e salvar o CSV:\n",
    "    login em:\n",
    "    https://www-periodicos-capes-gov-br.ezl.periodicos.capes.gov.br/index.php/buscador-primo.html\n",
    "\n",
    "    https://www-periodicos-capes-gov-br.ezl.periodicos.capes.gov.br/index.php/acesso-cafe.html\n",
    "\n",
    "    clicar no box:\n",
    "    <span class=\"select2-selection__rendered\" id=\"select2-listaInstituicoesCafe-container\" role=\"textbox\" aria-readonly=\"true\" title=\"Selecione uma instituição\"><span class=\"select2-selection__placeholder\">Selecione uma instituição</span></span>\n",
    "\n",
    "    clicar no input:\n",
    "    <input class=\"select2-search__field\" type=\"search\" tabindex=\"0\" autocorrect=\"off\" autocapitalize=\"none\" spellcheck=\"false\" role=\"searchbox\" aria-autocomplete=\"list\" autocomplete=\"off\" aria-label=\"Search\" aria-controls=\"select2-listaInstituicoesCafe-results\" aria-activedescendant=\"select2-listaInstituicoesCafe-result-2yz3-https://www#periodicos#capes#gov#br/Shibboleth.sso/Login?target=https://www#periodicos#capes#gov#br/secure&amp;entityID=https://idp#fiocruz#br/idp/shibboleth\">\n",
    "\n",
    "    entrar com o texto:\n",
    "    fiocruz\n",
    "\n",
    "    clicar no elemento:\n",
    "    <li class=\"select2-results__option select2-results__option--selectable select2-results__option--selected select2-results__option--highlighted\" id=\"select2-listaInstituicoesCafe-result-2yz3-https://www#periodicos#capes#gov#br/Shibboleth.sso/Login?target=https://www#periodicos#capes#gov#br/secure&amp;entityID=https://idp#fiocruz#br/idp/shibboleth\" role=\"option\" data-select2-id=\"select2-data-select2-listaInstituicoesCafe-result-2yz3-https://www#periodicos#capes#gov#br/Shibboleth.sso/Login?target=https://www#periodicos#capes#gov#br/secure&amp;entityID=https://idp#fiocruz#br/idp/shibboleth\" aria-selected=\"true\">FIOCRUZ - FUNDAÇÃO OSWALDO CRUZ</li>\n",
    "\n",
    "    clicar no enviar:\n",
    "    <button id=\"enviarInstituicaoCafe\" type=\"button\" class=\"btn btn-outline-dark btn-veja-mais w-100\"> Enviar </button>\n",
    "\n",
    "    clicar no input:\n",
    "    <input class=\"form-control\" aria-describedby=\"emailHelp\" id=\"username\" name=\"j_username\" type=\"text\" value=\"\">\n",
    "\n",
    "    entrar com usuário:\n",
    "\n",
    "    \n",
    "    entrar com PIN\n",
    "\n",
    "    ## PIN Dispensa:\n",
    "        # clicar no input:\n",
    "        # <input class=\"form-control\" id=\"password\" name=\"j_password\" type=\"password\" value=\"\">\n",
    "\n",
    "        # entrar com senha:\n",
    "\n",
    "    clicar no submit:\n",
    "    <button type=\"submit\" class=\"btn btn-primary btn-block mb-1\" name=\"_eventId_proceed\" onclick=\"this.childNodes[0].nodeValue='Realizando login...';if(document.getElementById('cache').checked){  document.getElementById('nocache').disabled = true;}\">Entrar</button>\n",
    "\n",
    "    \n",
    "    ## Selecionar Base de Dados\n",
    "    Clicar no link:\n",
    "    <a href=\"#\">Acervo</a>\n",
    "    Clicar no texto parcial:\n",
    "    'lista de bases'\n",
    "    Ou clicar no link:\n",
    "    https://www-periodicos-capes-gov-br.ez68.periodicos.capes.gov.br/index.php/acervo/lista-a-z-bases.html\n",
    "\n",
    "    Clicar no input:\n",
    "    <input class=\"bordered_input\" type=\"text\" name=\"scan_start\" id=\"scan_start\" value=\"\" size=\"100\" style=\"width: 500px;\">\n",
    "\n",
    "    Entrar com valor:\n",
    "    'Journal Citation Reports (JCR)'\n",
    "\n",
    "    Clicar no button:\n",
    "    <input class=\"button btn btn-primary\" type=\"submit\" onclick=\"document.form1.submit();\" value=\"Enviar\">\n",
    "\n",
    "    Clicar no primeiro link do span:\n",
    "    <span id=\"nomedabase\" name=\"nomedabase\"><a href=\"https://buscador-periodicos-capes-gov-br.ez68.periodicos.capes.gov.br/V/ESMK4GB1KMMPQJAG6LUGJFGTNPQSCABMGHHYUTQ9XL3BHXQQD4-22432?func=native-link&amp;resource=CAP04171\" title=\"Incites Journal Citation Reports - JCR (Clarivate Analytics)\" target=\"_blank\">Incites Journal Citation Reports - JCR (Clarivate Analytics)</a></span>\n",
    "\n",
    "    Ou direto no link (pode mudar):\n",
    "    https://buscador-periodicos-capes-gov-br.ez68.periodicos.capes.gov.br/V/ESMK4GB1KMMPQJAG6LUGJFGTNPQSCABMGHHYUTQ9XL3BHXQQD4-22432?func=native-link&resource=CAP04171\n",
    "\n",
    "    \n",
    "    # Na página do JCR\n",
    "    clicar no button:\n",
    "    <div role=\"button\" aria-label=\"Journals\" tabindex=\"0\" class=\"col-sm-3 col-md-3 col-lg-3 pr-0 pl-0 action\"><em class=\"common browse-journals\"></em><p class=\"list-of-pages-subtitle\">Journals</p></div>\n",
    "\n",
    "\n",
    "    clicar na barra lateral:\n",
    "    <mat-sidenav-content fxflex=\"\" fxlayout=\"column\" fxlayoutalign=\"center center\" class=\"mat-drawer-content mat-sidenav-content\"><div tabindex=\"0\" class=\"filter-box-with-badge\"><mat-icon role=\"img\" matsuffix=\"\" class=\"mat-icon notranslate filter-icon-style material-icons mat-ligature-font mat-icon-no-color\" aria-hidden=\"true\" data-mat-icon-type=\"font\">filter_list</mat-icon><p class=\"filter-icon-sub-header\">Filter</p><span matbadgeoverlap=\"false\" class=\"mat-badge notification-badge mat-badge-above mat-badge-after mat-badge-medium ng-star-inserted\"><span id=\"mat-badge-content-0\" aria-hidden=\"true\" class=\"mat-badge-content mat-badge-active\">10</span></span><!----></div></mat-sidenav-content>\n",
    "\n",
    "    Ou no link:\n",
    "    <div tabindex=\"0\" class=\"filter-box-with-badge\"><mat-icon role=\"img\" matsuffix=\"\" class=\"mat-icon notranslate filter-icon-style material-icons mat-ligature-font mat-icon-no-color\" aria-hidden=\"true\" data-mat-icon-type=\"font\">filter_list</mat-icon><p class=\"filter-icon-sub-header\">Filter</p><span matbadgeoverlap=\"false\" class=\"mat-badge notification-badge mat-badge-above mat-badge-after mat-badge-medium ng-star-inserted\"><span id=\"mat-badge-content-0\" aria-hidden=\"true\" class=\"mat-badge-content mat-badge-active\">10</span></span><!----></div>\n",
    "\n",
    "\n",
    "    clicar no subitem:\n",
    "    <mat-panel-title class=\"mat-expansion-panel-header-title ng-tns-c112-8\"> ISSN/eISSN <span class=\"hide ng-star-inserted\">&nbsp;(0)</span><!----><span class=\"green-dot ng-star-inserted\"></span><!----></mat-panel-title>\n",
    "\n",
    "    clicar na área de texto:\n",
    "    <textarea matinput=\"\" placeholder=\"Type in ISSN/eISSNs\" rows=\"13\" class=\"mat-input-element mat-form-field-autofill-control ng-tns-c120-9 ng-untouched ng-pristine ng-valid cdk-text-field-autofill-monitored\" id=\"mat-input-2\" data-placeholder=\"Type in ISSN/eISSNs\" aria-invalid=\"false\" aria-required=\"false\"></textarea>\n",
    "\n",
    "    duplo clique e backspace para limpar\n",
    "\n",
    "    colar lista de ISSN separada por vírgula em string único\n",
    "\n",
    "\n",
    "    ## Busca Simples            \n",
    "    clicar no input:\n",
    "    <input name=\"query\" type=\"search\" id=\"input-busca-primo\" class=\"form-control\" placeholder=\"O que você está procurando?\" required=\"\">\n",
    "\n",
    "    entrar com:\n",
    "    'JCR'\n",
    "\n",
    "    clicar no button:\n",
    "    <i class=\"fas fa-search\"></i>\n",
    "\n",
    "    ## Busca Avançada\n",
    "    clicar no button:\n",
    "    <span translate=\"label.advanced_search\">Busca avançada</span>\n",
    "\n",
    "\n",
    "\n",
    "    https://jcr-clarivate.ez68.periodicos.capes.gov.br/jcr/browse-journals\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTRAS FORMAS E TESTES DE EXTRAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe, neo4j_dict  = generate_dataframe_and_neo4j_dict(soup)\n",
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def infopessoa(data_dict):\n",
    "#     nome,_,_,endereco_lattes,_,idlattes,_,_,atualizacao = data_dict['infpessoa'][0].split('\\n')\n",
    "\n",
    "#     dict_info = {\n",
    "#         'NOME' : nome,\n",
    "#         # 'link_lattes' : endereco_lattes.split(': ')[1],\n",
    "#         'IDLATTES' : idlattes.split(': ')[1],\n",
    "#         'ATUALIZAÇÃO' : atualizacao.split('Última atualização do currículo em ')[1],\n",
    "#     }\n",
    "\n",
    "#     df = pd.DataFrame(dict_info, index=[0]).T\n",
    "#     df.columns = ['DADOS_CURRICULO']\n",
    "#     return df, dict_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_infopessoa, dict_pessoa = infopessoa(data_dict)\n",
    "# df_infopessoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_pessoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_vinculo = extract_academic(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.listdir('./../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = pd.DataFrame([extract_academic(soup)]).T\n",
    "df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_projects = extract_research_project(soup)\n",
    "extracted_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração e Persistência do cabeçalho\n",
    "header_data = parse_header(soup)\n",
    "header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_node = \n",
    "# header_node = persist_to_neo4j(header_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_article = data_dict['artigo-completo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados=[]\n",
    "for i in data_dict['artigo-completo']:\n",
    "    dados.append(i.split('\\n\\n'))\n",
    "\n",
    "def quebra(linha):\n",
    "    return [x.split('\\n') for x in linha]\n",
    "\n",
    "for i in dados:\n",
    "    print(quebra(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\n",
    "        'artigo-completo', 'cita', 'cita-artigos', 'citacoes', 'detalhes', 'fator', \n",
    "        'foto', 'informacao-artigo', 'informacoes-autor', 'infpessoa', 'nome', \n",
    "        'resumo', 'rodape-cv', 'science_cont', 'texto', 'trab'\n",
    "        ]\n",
    "\n",
    "def extract_selected_classes(soup, target_classes):\n",
    "    \"\"\"\n",
    "    Extrai conteúdos de classes específicas de um objeto soup extraído de documento HTML.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Objeto soup e lista de classes a serem extraídas.\n",
    "\n",
    "    Retorno:\n",
    "    - Um dicionário que mapeia o nome da classe à lista de conteúdos extraídos.\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Dicionário para armazenar os conteúdos\n",
    "    content_dict = defaultdict(list)\n",
    "    \n",
    "    # Iteração através das classes alvo para extração de conteúdo\n",
    "    for target_class in target_classes:\n",
    "        elements = soup.find_all(class_=target_class)\n",
    "        for element in elements:\n",
    "            print(element.text)\n",
    "            dado = element.text.split('\\n')\n",
    "\n",
    "            for i in dado:\n",
    "                if i != '':\n",
    "                    content_dict[target_class].append(dado)\n",
    "            \n",
    "    return dict(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\n",
    "        'informacao-artigo'\n",
    "        ]\n",
    "extract_selected_classes(soup, target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Completamente funcional porém falha ao extrair tooltips ocultos por padrão\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# def open_curriculum(driver,elm_vinculo):\n",
    "#     link_nome = achar_busca(driver, delay)\n",
    "#     window_before = driver.current_window_handle\n",
    "\n",
    "#     limite = 5\n",
    "#     if str(elm_vinculo) == 'nan':\n",
    "#         print('Vínculo não encontrado, passando para o próximo nome...')\n",
    "#         raise Exception\n",
    "#     print('Vínculo encontrado no currículo de nome:', elm_vinculo.text)\n",
    "\n",
    "#     # Clicar no botão \"Abrir Currículo\" e mudar de aba\n",
    "#     try:\n",
    "#         link_nome = achar_busca(driver, delay)\n",
    "#     except Exception as e:\n",
    "#         print('Erro')\n",
    "#         print(e)\n",
    "\n",
    "#     if link_nome.text == None:\n",
    "#         xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "#         print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "#         retry(WebDriverWait(driver, delay).until(\n",
    "#             EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "#             wait_ms=200,\n",
    "#             limit=limite,\n",
    "#             on_exhaust=(f'Problema ao acessar ao servidor do CNPq. {limite} tentativas sem sucesso.'))\n",
    "#     try:\n",
    "#         ActionChains(driver).click(link_nome).perform()\n",
    "#     except:\n",
    "#         print(f'Currículo não encontrado.')\n",
    "\n",
    "#     retry(WebDriverWait(driver, delay).until(\n",
    "#         EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "#         wait_ms=200,\n",
    "#         limit=limite,\n",
    "#         on_exhaust=(f'Problema ao acessar ao servidor do CNPq. {limite} tentativas sem sucesso.'))\n",
    "\n",
    "#     # Clicar no botão para abrir o currículo\n",
    "#     btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "#         EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "#     time.sleep(0.2)\n",
    "\n",
    "#     ActionChains(driver).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "#     # Gerenciamento das janelas abertas no navegador\n",
    "#     WebDriverWait(driver, delay).until(EC.number_of_windows_to_be(2))\n",
    "#     window_after = driver.window_handles\n",
    "#     new_window = [x for x in window_after if x != window_before][0]\n",
    "#     driver.switch_to.window(new_window)\n",
    "\n",
    "#     # Aguardar até que o elemento <div id=\"artigos-completos\"> seja encontrado\n",
    "#     artigos_completos_element = WebDriverWait(driver, 10).until(\n",
    "#         EC.presence_of_element_located((By.ID, 'artigos-completos'))\n",
    "#     )\n",
    "#     print('Extraindo dados das publicações...')\n",
    "#     passou_artigos_completos = True\n",
    "\n",
    "#     # Encontrar todos os elementos <sup> com a classe \"ajaxJCR\"\n",
    "#     sup_tags = driver.find_elements(By.CLASS_NAME, 'ajaxJCR.jcrTip')\n",
    "\n",
    "#     for sup_tag in sup_tags:\n",
    "#         try:\n",
    "#             # Verificar se o marcador foi encontrado\n",
    "#             if not passou_artigos_completos:\n",
    "#                 # Verificar se o elemento atual contém o texto desejado\n",
    "#                 if \"Artigos completos publicados em periódicos\" in sup_tag.text:\n",
    "#                     passou_artigos_completos = True\n",
    "#                 continue  # Continue para o próximo elemento\n",
    "\n",
    "#             # Usar o Selenium para ativar o tooltip\n",
    "#             ActionChains(driver).move_to_element(sup_tag).perform()\n",
    "\n",
    "#             # Aguardar até que o tooltip seja exibido\n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.CLASS_NAME, 'jcrTip'))\n",
    "#             )\n",
    "\n",
    "#             # Aguardar até que o elemento seja visível (após a ativação)\n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.visibility_of_element_located((By.CLASS_NAME, 'jcrTip'))\n",
    "#             )\n",
    "\n",
    "#             # Aguardar até que o elemento seja clicável\n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.element_to_be_clickable((By.CLASS_NAME, 'jcrTip'))\n",
    "#             )\n",
    "\n",
    "#             # Obter o elemento do tooltip\n",
    "#             tooltip_element = driver.find_element(By.CLASS_NAME, 'jcrTip')\n",
    "\n",
    "#             # Verificar se o elemento do tooltip é visível\n",
    "#             if tooltip_element.is_displayed():\n",
    "#                 # Obter o texto do tooltip usando JavaScript\n",
    "#                 tooltip_text = driver.execute_script(\"return arguments[0].textContent;\", tooltip_element)\n",
    "\n",
    "#                 # Validar se os campos de interesse estão presentes no texto do tooltip\n",
    "#                 if 'data-issn' in tooltip_text and 'impact-factor' in tooltip_text:\n",
    "#                     # Extrair os dados relevantes do texto do tooltip\n",
    "#                     # Adicionar os dados como atributos personalizados no elemento <sup>\n",
    "#                     driver.execute_script(\"arguments[0].setAttribute('data-tooltip', arguments[1]);\", sup_tag, tooltip_text)\n",
    "\n",
    "#                     # Aguardar até que o elemento do tooltip se torne obsoleto (stale)\n",
    "#                     WebDriverWait(driver, 10).until(\n",
    "#                         EC.staleness_of(tooltip_element)\n",
    "#                     )\n",
    "#         except Exception as e:\n",
    "#             print(f\"Erro ao ativar o tooltip: {e}\")\n",
    "\n",
    "#     # Após adicionar os dados dos tooltips, obter o conteúdo HTML atualizado\n",
    "#     page_source = driver.page_source\n",
    "#     driver.quit()\n",
    "\n",
    "#     # Usar BeautifulSoup para analisar\n",
    "#     soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "#     return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_test = '''\n",
    "<div class=\"layout-cell-pad-5\"><span class=\"informacao-artigo\" data-tipo-ordenacao=\"jcr\">2.7</span><span class=\"informacao-artigo\" data-tipo-ordenacao=\"importancia\"></span><span class=\"informacao-artigo\" data-tipo-ordenacao=\"autor\">PINHEIRO, LUANA I. C. C.</span><span class=\"informacao-artigo\" data-tipo-ordenacao=\"ano\">2021</span><a class=\"icone-producao icone-doi\" href=\"http://dx.doi.org/10.3390/app112110457\" target=\"_blank\" tabindex=\"143\"></a>PINHEIRO, LUANA I. C. C. ; PEREIRA, MARIA LÚCIA D. ; ANDRADE, EVANDRO C. DE ; NUNES, LUCIANO C. ; ABREU, WILSON C. DE ; PINHEIRO, PEDRO GABRIEL CALÍOPE D. ; <b>HOLANDA FILHO, RAIMIR</b> ; PINHEIRO, PLÁCIDO ROGERIO . An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus. Applied Sciences-Basel<sup><img id=\"20763417_10\" src=\"/buscatextual/images/curriculo/jcr.gif\" class=\"ajaxJCR jcrTip\" data-issn=\"20763417\" original-title=\"Applied Sciences-Basel (2076-3417)<br />Fator de impacto (JCR 2022): 2.7\"></sup>, v. 11, p. 10457, 2021. <div class=\"citado\" cvuri=\"/buscatextual/servletcitacoes?doi=10.3390/app112110457&amp;issn=20763417&amp;volume=11&amp;issue=&amp;paginaInicial=10457&amp;titulo=An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus&amp;sequencial=10&amp;nomePeriodico=Applied Sciences-Basel\" tooltip=\"Citações a partir de 1996\"><b class=\"label-citacoes\">CitaÃ§Ãµes:</b><a href=\"https://www.webofscience.com/api/gateway?GWVersion=2&amp;SrcApp=PARTNER_APP&amp;SrcAuth=LinksAMR&amp;KeyUT=WOS:000726550300001&amp;DestLinkType=CitingArticles&amp;DestApp=ALL_WOS&amp;UsrCustomerID=83e8e710a8e621f095e069bfe7c1e2bd\" target=\"_blank\" class=\"citacaoTip\" original-title=\"CitaÃ§Ãµes a partir de 1996\"><img src=\"images/v2/isi.gif\"><span class=\"numero-citacao\" data-tipo-ordenacao=\"1\">2</span></a></div></div>\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Para Titulos do grupo 02 a estrutura é diferente:\n",
    "\n",
    "dentro de class=title-wraper há uma nova div class=layout-cell-12 data-cell que contém como filhos uma div class=inst_back, e dentro dela uma tag 'b' (seção),\n",
    "seguindo ainda em siblings (filhos de div title-wrapper) há várias div class=cita-artigos (subseção) e \n",
    "cada div class=cita-artigos é seguida (em siblings) de vários pares de layout-cell-1 e layout-cell-11 (dados de interesse), há uma div id=artigos-completos...\n",
    "como filhos da div id=artigos-completos há uma div class=cita-artigos com nome da subseção seguida (em siblings) de várias divs class=artigo-completo\n",
    "só dentro de cada div class=artigo-completo é que há um ou vários pares de layout-cell-1 e layout-cell-11 com os dados de interesse\n",
    "dentro da div class=layout-cell-1 há os dados para chave do dicionário, dentro de uma tag 'b' que está dentro de uma div layout-cell-pad-5 text-align-right\n",
    "dentro da div class=layout-cell-11 há os dados para valores do dicionário, dentro de uma tag 'sup' que está dentro de uma div layout-cell-pad-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarqia básica das classes nas divs recorrentes para dados básicos e Publicação de Artigos é:\n",
    "# N00> elem_principal \"layout-cell-pad-main\" | \"p\" (v: resumo)                          [elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")]\n",
    "#   N01> \"infpessoa\"                                                                    [dict_infopessoa = elm_main_cell.find(\"div\", class_=\"infpessoa\")]\n",
    "#   N01> \"title-wrapper\" | \"h1\" (k: Títulos)                                            [for t in elm_main_cell.fid_all(\"div\", class_=\"title-wrapper\")]\n",
    "#       N02> \"layout-cell-12 data-cell\"                                                 [for title in titles: t.find(\"h1\") e i.get_text() para pegar nome de cada Título ] \n",
    "#           N03> \"int_back\" | \"b\" (k: Seções)                                           [USAR FIND_ALL PARA CAPTAR TODAS ENTRADAS RECORRENTES e b.get_text() para pegar Nome de Seção]\n",
    "#               N04> \"layout-cell-12\" >>> \"web_s\" (v: ValoresCitações)  \n",
    "#           N03> \"artigos-completos\"                                                    [i.find(\"div\", class_=\"artigos-completos\")]\n",
    "#               N04> \"cita-artigos\" | \"b\" (k: Subseção)                                 [USAR FIND_ALL PARA CAPTAR TODAS ENTRADAS RECORRENTES e b.get_text() para pegar Nome de Subseção]\n",
    "#               N04> \"artigo-completo\"                                                  [34 Artigos USAR FIND_ALL PARA CAPTAR TODAS ENTRADAS RECORRENTES DE DICIONÁRIO CONFORME ABAIXO]\n",
    "##                   N05> \"layout-cell layout-cell-1 text-align-right\" \n",
    "##                       N06> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do artigo]\n",
    "##                   N05> \"layout-cell layout-cell-11\" \n",
    "#                       N06> \"layout-cell-pad-5\" | \"span\" \"informacao-artigo\" ({k:JCR v:ValoresJCR}, {k:autor v:NomeAutor}, {k:ano v:AnoPub}, {k:doi v:LinkDOI})\n",
    "#                                                | .get_text() \n",
    "#                                                | <sup><... data-issn=\"18685137\" original-title=\"Journal of Ambient Intelligence and Humanized Computing (1868-5137)<br />Fator de impacto (JCR 2021): 3.662\"></sup>\n",
    "#                                                | <div class=\"citado\" cvuri=\"/buscatextual/servletcitacoes?doi=10.1007/s12652-023-04555-3&amp;issn=18685137&amp;volume=&amp;issue=&amp;paginaInicial=1&amp;titulo=Hybrid model for early identification post-Covid-19 sequelae&amp;sequencial=1&amp;nomePeriodico=Journal of Ambient Intelligence and Humanized Computing\" tooltip=\"Citações a partir de 1996\"></div>\n",
    "#\n",
    "# CONTEÚDO PARA EXTRAÇÃO ITERATIVA E RECORRENTE COM FUNÇÃO: extract_pairs()\n",
    "#           N03> \"cita-artigos\" | \"b\" (k: Subseção)                              [02 Livros publicados/organizados ou edições]\n",
    "##           N03> \"layout-cell layout-cell-1 text-align-right\"\n",
    "##               N04> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do Livro]\n",
    "##           N03> \"layout-cell layout-cell-11\"\n",
    "##               N04> \"layout-cell-pad-5\" | \"b\" (k:Autor)                        [b.get_text() para pegar Autor do Livro]\n",
    "##                                        | .get_text() (v:DadosLivro)           [.get_text() para pegar Dados do Livro]\n",
    "##           N03> <br class=\"clear\">\n",
    "#           N03> \"cita-artigos\" | \"b\" (k: Subseção)                              [24 Capítulos de Livros]\n",
    "##           N03> \"layout-cell layout-cell-1 text-align-right\"\n",
    "##               N04> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do Livro]\n",
    "##           N03> \"layout-cell layout-cell-11\"\n",
    "##               N04> \"layout-cell-pad-5\" | \"b\" (k:Autor)                        [b.get_text() para pegar Autor do Livro]\n",
    "##                                        | .get_text() (v:DadosCapítulo)        [.get_text() para pegar Dados do Livro]\n",
    "##           N03> <br class=\"clear\">\n",
    "#           N03> \"cita-artigos\" | \"b\" (k: Subseção)                              [74 Trabalhos completos publicados em anais de congressos]\n",
    "##           N03> \"layout-cell layout-cell-1 text-align-right\"\n",
    "##               N04> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do Livro]\n",
    "##           N03> \"layout-cell layout-cell-11\"\n",
    "##               N04> \"layout-cell-pad-5\" | \"b\" (k:Autor)                        [b.get_text() para pegar Autor do Livro]\n",
    "##                                        | .get_text() (v:DadosCapítulo)        [.get_text() para pegar Dados do Livro]\n",
    "##           N03> <br class=\"clear\">\n",
    "##           N03> <br class=\"clear\">\n",
    "#         Bancas\n",
    "#          Participação em bancas de trabalhos de conclusão  \n",
    "#           N03> \"cita-artigos\" | \"b\" (k: Subseção)                              [22 Mestrado]\n",
    "##           N03> \"layout-cell layout-cell-1 text-align-right\"\n",
    "##               N04> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do Livro]\n",
    "##           N03> \"layout-cell layout-cell-11\"\n",
    "##               N04> \"layout-cell-pad-5\" | \"b\" (k:Autor)                        [b.get_text() para pegar Autor do Livro]\n",
    "##                                        | .get_text() (v:DadosCapítulo)        [.get_text() para pegar Dados do Livro]\n",
    "##           N03> <br class=\"clear\">\n",
    "#           N03> \"cita-artigos\" | \"b\" (k: Subseção)                              [1 Teses de doutorado]\n",
    "##           N03> \"layout-cell layout-cell-1 text-align-right\"\n",
    "##               N04> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do Livro]\n",
    "##           N03> \"layout-cell layout-cell-11\"\n",
    "##               N04> \"layout-cell-pad-5\" | \"b\" (k:Autor)                        [b.get_text() para pegar Autor do Livro]\n",
    "##                                        | .get_text() (v:DadosCapítulo)        [.get_text() para pegar Dados do Livro]\n",
    "##           N03> <br class=\"clear\">\n",
    "#           N03> \"cita-artigos\" | \"b\" (k: Subseção)                              [17 Trabalhos de conclusão de curso de graduação]\n",
    "##           N03> \"layout-cell layout-cell-1 text-align-right\"\n",
    "##               N04> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do Livro]\n",
    "##           N03> \"layout-cell layout-cell-11\"\n",
    "##               N04> \"layout-cell-pad-5\" | \"b\" (k:Autor)                        [b.get_text() para pegar Autor do Livro]\n",
    "##                                        | .get_text() (v:DadosCapítulo)        [.get_text() para pegar Dados do Livro]\n",
    "##           N03> <br class=\"clear\">\n",
    "#          Participação em bancas de comissões julgadoras\n",
    "# EVENTOS \n",
    "#  (33 Participação em eventos, congressos, exposições e feiras)\n",
    "#  (03 Organização de eventos, congressos, exposições e feiras)\n",
    "# ORIENTAÇÕES (22 Dissertações, 05 Teses, 09 TCC)\n",
    "#  Orientações e supervisões em andamento\n",
    "#    (01 Dissertação de Mestrado, 05 Tese de doutorado)\n",
    "# INOVAÇÃO (02 Projetos de Pesquisa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# import re\n",
    "\n",
    "# article_pattern_primary = r'(?P<authors>.*?)\\s* \\. \\s*(?P<title>.*?)\\.\\s*(?P<journal>.*?)\\s*(?:v\\.\\s*(?P<volume>\\d+))?\\s*(?:p\\.\\s*(?P<page>.*?))?\\s*,\\s*(?P<year>\\d{4})'\n",
    "\n",
    "# article_pattern_secondary = r'(?P<authors>.*?)\\s*\\. \\s*(?P<title>.*?)\\.\\s*(?P<journal>.*?)\\s*(?:v\\.\\s*(?P<volume>\\d+))?\\s*(?:p\\.\\s*(?P<page>.*?))?\\s*,\\s*(?P<year>\\d{4})'\n",
    "\n",
    "# article_str = \"ANDRADE, E. C. ; PINHEIRO, L. I. ;Pinheiro, Placido R.; NUNES, L. C. ; PINHEIRO, M. C. D. ; PEREIRA, M. L. D. ; ABREU, W. C. ;HOLANDA FILHO, RAIMIR; SIMAO FILHO, M. ; PINHEIRO, P. G. C. D. ; NUNES, R. E. C.. Hybrid model for early identification post-Covid-19 sequelae. Journal of Ambient Intelligence and Humanized Computing, p. 1-14, 2023.\"\n",
    "\n",
    "# match = re.search(article_pattern_primary, article_str)\n",
    "# if match:\n",
    "#     article_details = match.groupdict()\n",
    "#     print('Caso 01:')\n",
    "#     pprint(article_details)\n",
    "# else:\n",
    "#     match = re.search(article_pattern_secondary, article_str, re.IGNORECASE)\n",
    "#     if match:\n",
    "#         print('Caso 02:')\n",
    "#         article_details = match.groupdict()\n",
    "#         pprint(article_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_articles = {'1.': 'ANDRADE, E. C. ; PINHEIRO, L. I. ;Pinheiro, Placido R.; NUNES, L. C. ; PINHEIRO, M. C. D. ; PEREIRA, M. L. D. ; ABREU, W. C. ;HOLANDA FILHO, RAIMIR; SIMAO FILHO, M. ; PINHEIRO, P. G. C. D. ; NUNES, R. E. C. . Hybrid model for early identification post-Covid-19 sequelae. Journal of Ambient Intelligence and Humanized Computing, p. 1-14, 2023.', '2.': 'MARINHO, RENATO ;HOLANDA, RAIMIR. Automated Emerging Cyber Threat Identification and Profiling Based on Natural Language Processing. IEEE Access, v. 1, p. 1-1, 2023.', '3.': 'FILHO, RAIMIR HOLANDA; DE SOUSA, DEBORA CARLA BARBOZA ; DE BRITO, WELLINGTON ALVES ; CHAVES, JOAN LUCAS MARQUES DE SOUSA ; SÁ, EMANUEL LEÃO ; RIBEIRO, VICTOR PASKNEL DE ALENCAR . Increasing Data Availability for Solid Waste Collection Using an IoT Platform based on LoRaWAN and Blockchain. PROCEDIA COMPUTER SCIENCE, v. 220, p. 119-126, 2023.', '4.': 'HOLANDA FILHO, RAIMIR; CUNHA, G. H. M. ; RAVIOLO, B. P. Y. ; BRITO, R. W. C. . Multi-Criteria Decision Analysis Applied To External Control Of Agreements Signed With The Brazilian Public Sector. Journal of Business and Management, v. 25, p. 27-35, 2023.', '5.': 'Ribeiro, Victor ;HOLANDA, RAIMIR; RAMOS, ALEX ; RODRIGUES, JOEL J. P. C. . A Fault-Tolerant and Secure Architecture for Key Management in LoRaWAN based on Permissioned Blockchain. IEEE Access, v. 1, p. 1-1, 2022.', '6.': 'MAPURUNGA, M. P. A. ;HOLANDA FILHO, RAIMIR. Avaliação do Nível de Maturidade em Transformação Digital - Estudo de Caso em uma Organização Pública. Revista Razão Contábil & Finanças, v. 13, p. 1-14, 2022.', '7.': 'ANDRADE, EVANDRO CARVALHO DE ; PINHEIRO, PLÁCIDO ROGERIO ; BARROS, ANA LUIZA BESSA DE PAULA ; NUNES, LUCIANO COMIN ; PINHEIRO, LUANA IBIAPINA C. C. ; PINHEIRO, PEDRO GABRIEL CALÍOPE DANTAS ;HOLANDA FILHO, RAIMIR. Towards Machine Learning Algorithms in Predicting the Clinical Evolution of Patients Diagnosed with COVID-19. Applied Sciences-Basel, v. 12, p. 8939, 2022.', '8.': 'LIMA, S. M. ; BRITO, R. W. ; MATOS, P. R. F. ; JESUS FILHO, J. ;HOLANDA FILHO, R.; CUNHA, G. H. M. . Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso no estado do Ceará. Revista Razão Contábil & Finanças, v. 12, p. 1, 2021.', '9.': 'ANDRADE, E. C. ;Pinheiro, Placido R.;HOLANDA FILHO, R.; NUNES, L. C. ; PINHEIRO, M. C. D. ; ABREU, W. C. ; SIMAO FILHO, M. ; PINHEIRO, L. I. C. C. ; PEREIRA, M. L. D. ; PINHEIRO, P. G. C. D. . Application of Machine Learning to Infer Symptoms and Risk Factors of Covid-19. Springer Proceedings in Complexity, v. 2021, p. 13-24, 2021.', '10.': 'PINHEIRO, LUANA I. C. C. ; PEREIRA, MARIA LÚCIA D. ; ANDRADE, EVANDRO C. DE ; NUNES, LUCIANO C. ; ABREU, WILSON C. DE ; PINHEIRO, PEDRO GABRIEL CALÍOPE D. ;HOLANDA FILHO, RAIMIR; PINHEIRO, PLÁCIDO ROGERIO . An Intelligent Multicriteria Model for Diagnosing Dementia in People Infected with Human Immunodeficiency Virus. Applied Sciences-Basel, v. 11, p. 10457, 2021.', '11.': 'Ribeiro, Victor ;HOLANDA, RAIMIR; RAMOS, ALEX ; RODRIGUES, JOEL J. P. C. . Enhancing Key Management in LoRaWAN with Permissioned Blockchain. SENSORS, v. 20, p. 3068, 2020.', '12.': 'Pinheiro, Placido R.; PINHEIRO, PEDRO G. C. D. ;FILHO, RAIMIR H.; BARROZO, JOAO P. A. ; RODRIGUES, JOEL J. P. C. ; PINHEIRO, LUANA I. C. C. ; PEREIRA, MARIA L. D. . Integration of the Mobile Robot and Internet of Things to Monitor Older People. IEEE Access, v. 1, p. 1-1, 2020.', '13.': 'LIMA, S. M. ; BRITO, R. W. ; MATOS, P. R. F. ; JESUS FILHO, J. ;HOLANDA FILHO, R.. Modelagem do Gasto Eficiente com Medicamentos: um estudo de caso para o estado do Ceará: 2006 a 2019. Revista Razão Contábil & Finanças, v. 11, p. 1, 2020.', '14.': 'LEMOS, MARCUS ; RABELO, RICARDO ; MENDES, DOUGLAS ; CARVALHO, CARLOS ;HOLANDA, RAIMIR. An approach for provisioning virtual sensors in sensor clouds. International Journal of Network Management, v. 1, p. e2062, 2019.', '15.': 'RAMOS, ALEX ; MILFONT, RONALDO T. P. ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. . Enabling Online Quantitative Security Analysis in 6LoWPAN Networks. IEEE Internet of Things Journal, v. 6, p. 5631-5638, 2019.', '16.': 'GUIMARAES, RANIERE ROCHA ; PASSOS, LEANDRO A. ;FILHO, RAIMIR HOLANDA; ALBUQUERQUE, VICTOR HUGO C. DE ; RODRIGUES, JOEL J. P. C. ; KOMAROV, MIKHAIL M. ; PAPA, JOAO PAULO . Intelligent Network Security Monitoring Based on Optimum-Path Forest Clustering. IEEE NETWORK, v. 33, p. 126-131, 2019.', '17.': 'ARAÚJO, HARILTON ;FILHO, RAIMIR; RODRIGUES, JOEL ; RABELO, RICARDO ; SOUSA, NATANAEL ; FILHO, JOSÉ ; SOBRAL, JOSÉ . A Proposal for IoT Dynamic Routes Selection Based on Contextual Information. SENSORS, v. 18, p. 353, 2018.', '18.': 'ARAÚJO, PAULO RÉGIS C. ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. ; OLIVEIRA, JOÃO P. C. M. ; BRAGA, STEPHANIE A. . Middleware for integration of legacy electrical equipment into smart grid infrastructure using wireless sensor networks. INTERNATIONAL JOURNAL OF COMMUNICATION SYSTEMS, v. 31, p. e3380, 2018.', '19.': 'SOBRAL, JOSÉ V.V. ; RODRIGUES, JOEL J.P.C. ; RABELO, RICARDO A.L. ; LIMA FILHO, JOSÉ C. ; SOUSA, NATANAEL ; ARAUJO, HARILTON S. ;HOLANDA FILHO, RAIMIR. A framework for enhancing the performance of Internet of Things applications based on RFID and WSNs. JOURNAL OF NETWORK AND COMPUTER APPLICATIONS, v. 107, p. 56-68, 2018.', '20.': 'LEMOS, MARCUS ;FILHO, RAIMIR; RABÊLO, RICARDO ; DE CARVALHO, CARLOS ; MENDES, DOUGLAS ; COSTA, VALNEY . An Energy-Efficient Approach to Enhance Virtual Sensors Provisioning in Sensor Clouds Environments. SENSORS, v. 18, p. 689, 2018.', '21.': 'DE ARAÚJO, PAULO ;FILHO, RAIMIR; RODRIGUES, JOEL ; OLIVEIRA, JOÃO ; BRAGA, STEPHANIE . Infrastructure for Integration of Legacy Electrical Equipment into a Smart-Grid Using Wireless Sensor Networks. SENSORS, v. 18, p. 1312, 2018.', '22.': 'RAMOS, ALEX ; LAZAR, MARCELLA ;FILHO, RAIMIR HOLANDA; RODRIGUES, JOEL J. P. C. . Model-Based Quantitative Network Security Metrics: A Survey. IEEE Communications Surveys and Tutorials, v. 19, p. 2704-2734, 2017.', '23.': 'MARINHO, R. ;HOLANDA FILHO, RAIMIR. Exploring a P2P Transient Botnet - From Discovery to Enumeration. The Journal on Cybercrime & Digital Investigations, v. 3, p. 30-39, 2017.', '24.': 'LEMOS, MARCUS ; CARVALHO, CARLOS ; LOPES, DOUGLAS ;HOLANDA, RAIMIR; RABÊLO, RICARDO . UMA ABORDAGEM PARA PROVISIONAMENTO AUTOMÁTICO DE SENSORES EM NUVENS DE SENSORES. LEARNING AND NONLINEAR MODELS, v. 14, p. 4-14, 2016.', '25.': 'RAMOS, A. L. ;HOLANDA FILHO, R.. Sensor Data Security Level Estimation Scheme for Wireless Sensor Networks. SENSORS, v. 15, p. 2104-2136, 2015.', '26.': 'ARAUJO, Paulo Regis C. ;HOLANDA FILHO, R.; RODRIGUES, Antonio. Wendell. O. ; ARAUJO, Andre. Luiz. C. ; MORAES FILHO, Jose. A. ; OLIVEIRA, Joao. Paolo. C. M. . A Middleware for the Integration of Smart Grid Elements with WSN Based Solutions. International Journal of Distributed Sensor Networks, v. 2014, p. 1-15, 2014.', '27.': 'SOBRAL, J. ; SOUSA, A. ; ARAUJO, H. S. ; BALUZ, R. ;HOLANDA FILHO, R.; SOUSA, M. V. ; Rabelo, Ricardo A. L. . A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks. JOURNAL OF COMMUNICATION AND COMPUTER, v. 10, p. 702-712, 2013.', '28.': 'Rabelo, Ricardo A. L. ; LEAL, L. B. ; SOUSA, M. V. ;HOLANDA FILHO, R.; Borges, Fabio A. S. . Uma Abordagem baseada em Sistemas de Inferência Fuzzy de Mamdani e Algoritmos Genéticos para Estimação da Qualidade de Rotas em redes de Sensores sem Fio Multi-Sink. Learning and Nonlinear Models, v. 10, p. 4-18, 2012.', '29.': 'Rabelo, Ricardo A. L. ; LEAL, L. B. ; SOUSA, M. V. ;HOLANDA FILHO, R.; Borges, Fabio A. S. . An integration of fuzzy inference systems and Genetic Algorithms for Wireless Sensor Networks. INTERNATIONAL JOURNAL OF HYBRID INTELLIGENT SYSTEMS, v. 9, p. 61-74, 2012.', '30.': 'BRAYNER, A. ; Coelho, André Luis Carvalho ; Marinho, Karine ;HOLANDA FILHO, R.; Castro, Wagner L. T. . On Query Processing in Wireless Sensor Networks Using Classes of Quality of Queries. Information Fusion (Print), v. 15, p. 44-55, 2012.', '31.': 'HOLANDA FILHO, R.; OLIVEIRA, J. A. . Controle Externo da Governança de Tecnologia da Informação. Revista Controle, v. X, p. 65-85, 2012.', '32.': 'HOLANDA FILHO, R.; MAIA, J. E. B. ; CARMO, M. F. F. . Detecting computer network attacks using statistical discriminators and cluster analysis. Revista Tecnologia (UNIFOR), v. 28, p. 33-41, 2007.', '33.': 'HOLANDA FILHO, R.; MAIA, J. E. B. ; PAULINO, G. . Broadband network traffic characterization and classification using a multivariate statistical method. Revista Tecnologia (UNIFOR), v. 27, p. 113-122, 2006.', '34.': 'FURTADO, E. ; LINCOLN, F. ; FURTADO, V. ;HOLANDA FILHO, R.. Um Sistema de Aprendizagem Colaborativa de Didática Utilizando Cenários. Revista Brasileira de Informática na Educação, v. 8o., 2001.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count=1\n",
    "# for i in dict_articles.values():\n",
    "#     print(count, i)\n",
    "#     article_info = ArticleHandler.extract_article_info(i)\n",
    "#     pprint(article_info)\n",
    "#     count+=1\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(?P<authors>[^.]*?): Este trecho corresponde aos autores do artigo. Ele começa procurando qualquer sequência de caracteres que não contenha um ponto (.). A parte (?P<authors> ...) nomeia esse grupo de captura como \"authors\", o que significa que os autores correspondentes serão acessíveis pelo nome \"authors\" após a correspondência bem-sucedida.\n",
    "\n",
    "\\. : Isso corresponde a um ponto e um espaço em branco que separam os autores do restante das informações do artigo.\n",
    "\n",
    "(?P<title>.*?)(?<=\\w): Isso corresponde ao título do artigo. Ele procura por qualquer sequência de caracteres (.*?) até encontrar um ponto de parada. A parte (?P<title> ...) nomeia este grupo de captura como \"title\".\n",
    "\n",
    "\\. : Mais uma vez, isso corresponde a um ponto e um espaço em branco após o título.\n",
    "\n",
    "(?P<journal>.*?)(,|;|.): Isso corresponde ao nome da revista ou periódico onde o artigo foi publicado. Ele procura por qualquer sequência de caracteres (.*?) até encontrar uma vírgula, ponto e vírgula ou ponto. A parte (?P<journal> ...) nomeia este grupo de captura como \"journal\".\n",
    "\n",
    "\\s*v\\.\\s*: Isso corresponde à string \"v.\" (indicando volume) com espaços em branco opcionais em ambos os lados.\n",
    "\n",
    "(?P<volume>\\d+): Isso corresponde ao número do volume do periódico. Ele procura por uma sequência de dígitos (\\d+). A parte (?P<volume> ...) nomeia este grupo de captura como \"volume\".\n",
    "\n",
    ",|;|.: Isso corresponde a uma vírgula, ponto e vírgula ou ponto, que podem ser usados para separar o volume do restante das informações.\n",
    "\n",
    "\\s*p\\.\\s*: Isso corresponde à string \"p.\" (indicando páginas) com espaços em branco opcionais em ambos os lados.\n",
    "\n",
    "(?P<page>.*?): Isso corresponde às informações das páginas do artigo. Ele procura por qualquer sequência de caracteres (.*?) até encontrar outra vírgula, ponto e vírgula ou ponto. A parte (?P<page> ...) nomeia este grupo de captura como \"page\".\n",
    "\n",
    ",|;|.: Mais uma vez, isso corresponde a uma vírgula, ponto e vírgula ou ponto, que podem ser usados para separar as informações das páginas do ano.\n",
    "\n",
    "(?P<year>\\d{4})\\.: Isso corresponde ao ano de publicação do artigo, que é representado por quatro dígitos (\\d{4}) seguidos de um ponto final. A parte (?P<year> ...) nomeia este grupo de captura como \"year\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exibição dos dataframes por seção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict = generate_dataframe_and_neo4j_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(neo4j_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Identificação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Endereço']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Formação acadêmica/titulação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Formação Complementar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Linhas de pesquisa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Projetos de desenvolvimento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Áreas de atuação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Idiomas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Atuação Profissional_Fundação Oswaldo Cruz, FIOCRUZ, Brasil.'\n",
    " 'Produções_Produção bibliográfica',\n",
    " 'Bancas_Participação em bancas de trabalhos de conclusão',\n",
    " 'Eventos_Participação em eventos, congressos, exposições e feiras',\n",
    " 'Orientações_Orientações e supervisões concluídas',\n",
    " 'Inovação_Projeto de desenvolvimento tecnológico']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa estruturação funcionou bem para as seções que tem anos como chaves, mas falha para em seções com subníveis de detalhamento, como nas produções e no resumo, onde os dados de valores não foram populados no dicionário embora as chaves tenham sido extraídas com sucesso.\n",
    "\n",
    "Pode ser por falta de algum elemento, classe ou marcador não apontada ou não hierarquizada corretamente na função de extração.\n",
    "\n",
    "Deve-se escolher outros marcadores para complementar a extração, ou usar o atual somente para seção 'Formação acadêmica/titulação'.\n",
    "\n",
    "Falhas não capturou corretamente:\n",
    "- subchaves de Atuação Profissional\n",
    "- subchaves de Orientações\n",
    "- subchaves de Produções pra Livros e Capítulos de livros\n",
    "\n",
    "Falhas não capturoud e forma alguma:\n",
    "- Projetos de Pesquisa\n",
    "- Inovação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não funcionou tão bem para a Atuação profissional pois não capturou chaves corretas\n",
    "atuaprof_df = neo4j_dict['Atuação Profissional_Fundação Oswaldo Cruz, FIOCRUZ, Brasil.']\n",
    "atuaprof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_df = neo4j_dict['Linhas de pesquisa']\n",
    "linhas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projdesv_df = neo4j_dict['Projetos de desenvolvimento']\n",
    "projdesv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projpesq_df = neo4j_dict['Áreas de atuação']\n",
    "projpesq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Produção bibliográfica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair marcadores CSS específicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_classes_content(soup_element, target_classes):\n",
    "    result_dict = defaultdict(list)\n",
    "    \n",
    "    for t_class in target_classes:\n",
    "        for element in soup_element.find_all(class_=t_class):\n",
    "            text_content = element.text.strip()\n",
    "            result_dict[t_class].append(text_content)\n",
    "            \n",
    "    return dict(result_dict)\n",
    "\n",
    "target_classes = [\n",
    "        # 'infpessoa', \n",
    "        'nome', \n",
    "        'resumo', \n",
    "        # 'artigo-completo', \n",
    "        # 'cita-artigos', \n",
    "        # 'citacoes', \n",
    "        # 'detalhes', \n",
    "        # 'fator', \n",
    "        # 'foto', \n",
    "        # 'informacao-artigo', \n",
    "        # 'informacoes-autor', \n",
    "        # 'rodape-cv', \n",
    "        # 'science_cont', \n",
    "        # 'texto',\n",
    "        #  \n",
    "        # 'cita', \n",
    "        # 'trab'\n",
    "        ]\n",
    "\n",
    "result_dict = extract_classes_content(soup.body, target_classes)\n",
    "from pprint import pprint\n",
    "pprint(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções montar dicionários de partes específicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_infopessoa(soup):\n",
    "#     # Localiza a seção com a classe 'infpessoa'\n",
    "#     section = soup.find('div', class_='infpessoa')\n",
    "\n",
    "#     # Inicializa um dicionário para armazenar os dados extraídos\n",
    "#     extracted_data = {}\n",
    "\n",
    "#     # Extrai e armazena o nome\n",
    "#     name = section.find('h2', class_='nome').text.strip() if section.find('h2', class_='nome') else None\n",
    "#     extracted_data['Nome'] = re.sub(r'\\s+', ' ', name) if name else None\n",
    "\n",
    "#     # Extrai e armazena o título ou posição\n",
    "#     title = section.find('span', class_='texto').text.strip() if section.find('span', class_='texto') else None\n",
    "#     extracted_data['Bolsa'] = re.sub(r'\\s+', ' ', title) if title else None\n",
    "\n",
    "#     # Extrai e armazena as informações adicionais\n",
    "#     info_list = section.find('ul', class_='informacoes-autor')\n",
    "#     if info_list:\n",
    "#         for li in info_list.find_all('li'):\n",
    "#             text = re.sub(r'\\s+', ' ', li.text.strip())\n",
    "#             if 'Endereço para acessar este CV:' in text:\n",
    "#                 extracted_data['Endereço para acessar este CV'] = text.replace('Endereço para acessar este CV:', '').strip()\n",
    "#             elif 'ID Lattes:' in text:\n",
    "#                 extracted_data['ID Lattes'] = text.replace('ID Lattes:', '').strip()\n",
    "#             elif 'Última atualização do currículo em' in text:\n",
    "#                 extracted_data['Última atualização do currículo em'] = text.replace('Última atualização do currículo em', '').strip()\n",
    "\n",
    "#     extracted_data['Resumo'] = soup.find('p', class_='resumo').text.strip()\n",
    "    \n",
    "#     return extracted_data\n",
    "\n",
    "\n",
    "# def extract_academic(soup):\n",
    "#     \"\"\"\n",
    "#     Extrai dados da página do Lattes, nas divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "#     Parâmetros:\n",
    "#     - Bs4 object (soup): Objeto dod Beaultiful Soap com o conteúdo bruto da página HTML.\n",
    "    \n",
    "#     Retorno:\n",
    "#     - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "#     \"\"\"\n",
    "#     # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "#     extracted_data = {}\n",
    "    \n",
    "#     # Localiza todas as divs com a classe 'title-wrapper'\n",
    "#     divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "#     for div_key in divs_key:\n",
    "#         # Extrai o conteúdo da tag para formar a chave do dicionário\n",
    "#         find_div = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "#         if find_div:\n",
    "#             key = find_div.text.strip('\\n')\n",
    "        \n",
    "#         # Encontra a div que segue imediatamente para o valor\n",
    "#         div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "#         # Extrai o conteúdo da div para o valor\n",
    "#         value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "#         # Armazena no dicionário se ambas chave e valor existirem\n",
    "#         if key and value:\n",
    "#             extracted_data[key] = value\n",
    "    \n",
    "#     return extracted_data\n",
    "\n",
    "\n",
    "# def mount_articles(extracted_content):\n",
    "#     \"\"\"\n",
    "#     Organiza os dados extraídos pelo web scraping em um DataFrame do pandas, melhorando a detecção de '\\n'.\n",
    "    \n",
    "#     Parâmetros:\n",
    "#     - extracted_content (dict): Um dicionário contendo os conteúdos extraídos organizados por classe.\n",
    "    \n",
    "#     Retorno:\n",
    "#     - Um DataFrame do pandas contendo os dados organizados.\n",
    "#     \"\"\"\n",
    "#     # Localiza a lista de artigos completos\n",
    "#     artigos_completos = extracted_content.get('artigo-completo', [])\n",
    "#     print(f'Total de artigos detectados: {len(artigos_completos)}')\n",
    "    \n",
    "#     # Concatena todos os artigos em uma única string\n",
    "#     artigos_concatenados = \" \".join(artigos_completos)\n",
    "    \n",
    "#     # Remove múltiplos espaços e substitui por um único espaço\n",
    "#     artigos_concatenados = re.sub(r'\\s+', ' ', artigos_concatenados)\n",
    "    \n",
    "#     # Divide os artigos com base em um padrão específico (exemplo: números seguidos de pontos)\n",
    "#     artigos_divididos = re.split(r'\\s\\d+\\.\\s', artigos_concatenados)\n",
    "    \n",
    "#     # Remove entradas vazias\n",
    "#     artigos_divididos = [artigo.strip() for artigo in artigos_divididos if artigo.strip()]\n",
    "    \n",
    "#     # Lista para armazenar os registros para o DataFrame\n",
    "#     records = []\n",
    "#     ordens = []\n",
    "    \n",
    "#     for artigo in artigos_divididos:\n",
    "#         # Encontra o primeiro ano mencionado no artigo\n",
    "#         match_ano = re.search(r'20\\d{2}', artigo)\n",
    "        \n",
    "#         if match_ano:\n",
    "#             ano = match_ano.group(0)\n",
    "#             indice, resto = artigo.split(ano, 1)\n",
    "            \n",
    "#             # Armazena o registro em formato de dicionário\n",
    "#             record = {\n",
    "#                 'Índice': indice.split('. ')[0],\n",
    "#                 'Ano': ano,\n",
    "#                 'Título e Autores': resto.strip()\n",
    "#             }\n",
    "#             records.append(record)\n",
    "    \n",
    "#     # Cria um DataFrame do pandas com os registros\n",
    "#     df = pd.DataFrame(records)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# def extract_research_project(soup):\n",
    "#     project_list = []\n",
    "#     projects_section = soup.find('h1', string='Projetos de pesquisa')\n",
    "#     periods=[]\n",
    "#     titles=[]\n",
    "#     descriptions=[]\n",
    "#     if projects_section:\n",
    "#         project_divs = projects_section.find_all_next('div', {'class': 'layout-cell layout-cell-3 text-align-right'})\n",
    "        \n",
    "#         for div in project_divs:\n",
    "#             project_dict = {}\n",
    "#             period_div = div.find('b')\n",
    "#             if period_div:\n",
    "#                 periods.append(period_div.text.strip())\n",
    "            \n",
    "#             title_div_container = div.find_next_sibling('div')\n",
    "#             if title_div_container:\n",
    "#                 title_div = title_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "#                 if title_div:\n",
    "#                     titles.append(title_div.text.strip())\n",
    "            \n",
    "#             # Locate the div that contains the project description\n",
    "#             parent_div = div.find_parent('div')\n",
    "#             if parent_div:\n",
    "#                 description_div_container = parent_div.find_next_sibling('div')\n",
    "#                 if description_div_container:\n",
    "#                     description_div_container = description_div_container.find_next_sibling('div')\n",
    "#                     if description_div_container:\n",
    "#                         description_div = description_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "#                         if description_div:\n",
    "#                             full_text = description_div.text\n",
    "#                             description_start_index = full_text.find('Descrição:')\n",
    "#                             if description_start_index != -1:\n",
    "#                                 descriptions.append(full_text[description_start_index + len('Descrição:'):].strip())\n",
    "\n",
    "#     df =pd.DataFrame({\n",
    "#         'PERIODO': pd.Series(periods),\n",
    "#         'TITULO': pd.Series(titles),\n",
    "#         'DESCRICAO': pd.Series(descriptions),\n",
    "#             })                \n",
    "    \n",
    "#     descricoes = df[df['PERIODO']==\"\"]['TITULO'].values\n",
    "#     df = df[df['PERIODO']!=\"\"]\n",
    "#     df = df[:len(descricoes)]\n",
    "#     df['DESCRICAO']=descricoes\n",
    "#     df\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extrair_normal(soup, verbose=False):\n",
    "#     elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "\n",
    "#     info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "#     name = info_list[0]\n",
    "\n",
    "#     data_dict = {\"labels\": \"Person\",\"name\": name,\"InfPes\": [], \"Resumo\": []}\n",
    "#     data_dict['InfPes'] = info_list\n",
    "#     summary_text  = elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()\n",
    "#     data_dict['Resumo'].append(summary_text)\n",
    "\n",
    "#     tit1 = ['Identificação','Endereço','Formação acadêmica/titulação','Pós-doutorado','Formação Complementar','Atuação Profissional','Linhas de pesquisa',\n",
    "#             'Projetos de pesquisa','Projetos de extensão','Projetos de desenvolvimento','Revisor de periódico','Revisor de projeto de fomento',\n",
    "#             'Áreas de atuação','Idiomas','Inovação']\n",
    "#     tit2 = ['Produções','Bancas','Orientações']\n",
    "#     # Não extraindo ainda, semelhante ao tit2 mas não há subseções dados direto nas seções \"inst_back\"\n",
    "#     tit3 = ['Eventos']\n",
    "\n",
    "#     for div_title_wrapper in divs_title_wrapper:\n",
    "#         data_cells_12 = div_title_wrapper.findChildren('div', class_='data-cell')\n",
    "\n",
    "#         # print(len(data_cells_12), 'conteiners de dados')\n",
    "#         titulo = div_title_wrapper.find('h1').text.strip()\n",
    "#         if titulo not in data_dict:\n",
    "#             if titulo == '':\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 data_dict[titulo] = {}\n",
    "#         if verbose:\n",
    "#             print(f'{titulo}')\n",
    "\n",
    "#         for data_cell in data_cells_12:\n",
    "#             # as divs data_cells_12 englobam div class=infopessoa e várias divs class=title-wrapper\n",
    "\n",
    "#             # Formações e demais seções que não tem subseções\n",
    "#             if titulo in tit1:\n",
    "#                 ## Listagem das divisões e subdivisões\n",
    "#                 divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "#                 # print(len(divs_inst_back))\n",
    "                \n",
    "#                 for div_inst_back_cell in divs_inst_back:                                        \n",
    "#                     inst_back = div_inst_back_cell.findChild('b')\n",
    "#                     if inst_back:\n",
    "#                         instback_text = inst_back.get_text().strip()\n",
    "#                         data_dict[titulo][instback_text] = []\n",
    "                        \n",
    "#                         if verbose:                            \n",
    "#                             print(f'  {instback_text}') # nomes de seção com tipos de produção\n",
    "\n",
    "#                     divs_subsection = div_inst_back_cell.find_next_siblings('div', class_='cita-artigos')\n",
    "#                     for subsection in divs_subsection:\n",
    "#                         subsection_text = subsection.get_text().strip()\n",
    "#                         if subsection_text:\n",
    "#                             data_dict[titulo][instback_text][subsection_text] = []\n",
    "#                             if verbose:\n",
    "#                                 print(f'      {subsection_text}')   \n",
    "\n",
    "#                 elementos_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "#                 elementos_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "#                 keys=[]\n",
    "#                 vals=[]\n",
    "#                 for i,j in zip(elementos_layout_cell_3, elementos_layout_cell_9):\n",
    "#                     if elementos_layout_cell_3 and elementos_layout_cell_9:\n",
    "#                         key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#                         key_text = key.get_text().strip().replace('\\n',' ').replace('\\t','')\n",
    "#                         keys.append(key_text)\n",
    "#                         val = j.find('div', class_='layout-cell-pad-5')\n",
    "#                         val_text = val.get_text().strip().replace('\\n',' ').replace('\\t','')\n",
    "#                         vals.append(val_text)\n",
    "#                         if verbose:\n",
    "#                             print(f'      {key_text:>3}: {val_text}') # impressão dos dados chave e valor\n",
    "                \n",
    "#                 agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "#                 if titulo not in data_dict:\n",
    "#                     data_dict[titulo] = {}\n",
    "#                 data_dict[titulo].update(agg_dict)                \n",
    "\n",
    "#             # Produções, Orientações e Bancas (que contém subseções)\n",
    "#             elif titulo in tit2:\n",
    "#                 ## Listagem das divisões e subdivisões\n",
    "#                 divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "#                 # print(len(divs_inst_back))\n",
    "                \n",
    "#                 for div_inst_back_cell in divs_inst_back:                                        \n",
    "#                     inst_back = div_inst_back_cell.findChild('b')\n",
    "#                     if inst_back:\n",
    "#                         instback_text = inst_back.get_text().strip()\n",
    "#                         if verbose:\n",
    "#                             print(f'  {instback_text}') # nomes de seção com tipos de produção\n",
    "\n",
    "#                         div_citacoes = div_inst_back_cell.find_next_sibling('div', class_='cita-artigos')\n",
    "#                         if div_citacoes:\n",
    "#                             cita_artigos_text = div_citacoes.findChild('b').get_text().strip()\n",
    "#                             # print(f'    {cita_artigos_text}') # nomes de subseção como ocorrências \n",
    "                            \n",
    "#                             if cita_artigos_text == 'Citações':\n",
    "#                                 ## Extrair dados das citações dos artigos publicados em bases indexadas\n",
    "#                                 sub_section_list = []\n",
    "                                    \n",
    "#                                 ## Extrair quantidade de citações e fator H através das divs de subseção que contém a classe lyout-cell-12\n",
    "#                                 next_siblings = div_citacoes.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "\n",
    "#                                 sub_section_list = []  # Inicialize a lista de dicionários fora do loop\n",
    "\n",
    "#                                 for i in next_siblings:\n",
    "#                                     citation_counts = i.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que contém os Valores de Citações\n",
    "#                                     if citation_counts:\n",
    "#                                         for i in citation_counts:\n",
    "#                                             database = i.get_text()\n",
    "#                                             total_trab = i.find_next_sibling(\"div\", class_=\"trab\").get_text().split(\"Total de trabalhos:\")[1]\n",
    "#                                             total_cite = i.find_next_sibling(\"div\", class_=\"cita\").get_text().split(\"Total de citações:\")[1]\n",
    "#                                             fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "#                                             num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "#                                             data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\").get_text().split(\"Data:\")[1].strip()\n",
    "\n",
    "#                                             # Converta os valores para tipos de dados adequados\n",
    "#                                             total_trab = int(total_trab)\n",
    "#                                             total_cite = int(total_cite)\n",
    "\n",
    "#                                             citation_numbers = {\n",
    "#                                                 \"Database\": database,\n",
    "#                                                 \"Total de trabalhos\": total_trab,\n",
    "#                                                 \"Total de citações\": total_cite,\n",
    "#                                                 \"Índice_H\": num_fator_h,\n",
    "#                                                 \"Data\": data_wos\n",
    "#                                             }\n",
    "\n",
    "#                                             sub_section_list.append(citation_numbers)\n",
    "\n",
    "#                                             if verbose:\n",
    "#                                                 print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')\n",
    "\n",
    "#                         # Extração dos Artigos publicados em periódicos\n",
    "#                         div_artigos = div_inst_back_cell.find_next_siblings('div', id='artigos-completos')\n",
    "#                         for div_artigo in div_artigos:\n",
    "#                             if div_artigo:\n",
    "#                                 divs_cita_artigos = div_artigo.find_all('div', class_='cita-artigos')\n",
    "#                                 for cita_artigos in divs_cita_artigos:\n",
    "#                                     if cita_artigos:\n",
    "#                                         cita_artigos_text = cita_artigos.findChild('b').get_text().strip()\n",
    "#                                         if verbose:\n",
    "#                                             print(f'    {cita_artigos_text}') # nomes de subseção como ocorrências          \n",
    "                                        \n",
    "#                                         elementos_layout_cell_1 = div_artigo.find_all('div', class_='layout-cell-1')\n",
    "#                                         elementos_layout_cell_11 = div_artigo.find_all('div', class_='layout-cell-11')\n",
    "                                    \n",
    "#                                         # extract(instback_text, elementos_layout_cell_1,elementos_layout_cell_11)\n",
    "#                                         vals_jcr=[]\n",
    "#                                         keys=[]\n",
    "#                                         vals=[]\n",
    "#                                         for i,j in zip(elementos_layout_cell_1, elementos_layout_cell_11):\n",
    "#                                             if elementos_layout_cell_1 and elementos_layout_cell_11:\n",
    "#                                                 key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#                                                 key_text = key.get_text().strip().replace('\\n',' ').replace('\\t','')\n",
    "#                                                 val = j.find('div', class_='layout-cell-pad-5')\n",
    "#                                                 info_dict = {\n",
    "#                                                     'data-issn': 'NULL',\n",
    "#                                                     'impact-factor': 'NULL',  \n",
    "#                                                     'jcr-year': 'NULL',\n",
    "#                                                     'journal': 'NULL',\n",
    "#                                                     'raw_data': 'NULL',\n",
    "#                                                 }\n",
    "#                                                 # Remova as tags span da div\n",
    "#                                                 for span in val.find_all('span'):\n",
    "#                                                     span.extract()\n",
    "                                                \n",
    "#                                                 val_text = val.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "\n",
    "#                                                 keys.append(key_text)\n",
    "#                                                 vals.append(val_text)\n",
    "#                                                 if verbose:\n",
    "#                                                     print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "#                                                 if cita_artigos_text == 'Artigos completos publicados em periódicos':\n",
    "#                                                     sup_element = j.find('sup')\n",
    "#                                                     raw_jcr_data = sup_element.get_text()\n",
    "#                                                     # print('sup_element:',sup_element)\n",
    "#                                                     img_element = sup_element.find('img')\n",
    "#                                                     # print('img_element:',img_element)\n",
    "\n",
    "#                                                     if sup_element:\n",
    "#                                                         if img_element:\n",
    "#                                                             original_title = img_element.get('original-title')\n",
    "#                                                             raw_jcr_data = img_element.get_text()\n",
    "#                                                             if original_title:\n",
    "#                                                                 info_list = original_title.split('<br />') if original_title.split('<br />') else original_title\n",
    "                                                                \n",
    "#                                                                 if info_list != 'NULL':\n",
    "#                                                                     info_dict = {\n",
    "#                                                                         'data-issn': img_element.get('data-issn'),\n",
    "#                                                                         'impact-factor': info_list[1].split(': ')[1],\n",
    "#                                                                         'jcr-year': info_list[1].split(': ')[0].replace('Fator de impacto ',''),\n",
    "#                                                                         'journal': info_list[0],\n",
    "#                                                                         'raw_data': raw_jcr_data,\n",
    "#                                                                     }\n",
    "#                                                             else:\n",
    "#                                                                 info_dict = {\n",
    "#                                                                     'data-issn': img_element.get('data-issn'),\n",
    "#                                                                     'impact-factor': 'NULL',\n",
    "#                                                                     'jcr-year': 'NULL',\n",
    "#                                                                     'journal': img_element.get('original-title'),\n",
    "#                                                                     'raw_data': raw_jcr_data,\n",
    "#                                                                 }\n",
    "#                                                     else:\n",
    "#                                                         info_dict = {\n",
    "#                                                             'data-issn': 'NULL',\n",
    "#                                                             'original-title': 'NULL',\n",
    "#                                                             'Fator de impacto': 'NULL',\n",
    "#                                                             'ano_apuração': 'NULL',\n",
    "#                                                             'raw_data': raw_jcr_data, \n",
    "#                                                         }                                                                \n",
    "                                                        \n",
    "#                                                     vals_jcr.append(info_dict)\n",
    "#                                                     if verbose:\n",
    "#                                                         print(f'         {info_dict}')\n",
    "\n",
    "#                                         agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "#                                         if titulo not in data_dict:\n",
    "#                                             data_dict[titulo] = {}\n",
    "#                                         data_dict[titulo].update(agg_dict)\n",
    "                                        \n",
    "#                                         if 'JCR' not in data_dict:\n",
    "#                                             data_dict['JCR'] = []\n",
    "#                                         data_dict['JCR'].extend(vals_jcr)\n",
    "\n",
    "#                     extracted = ['Citações', 'Artigos completos publicados em periódicos']\n",
    "#                     divs = div_inst_back_cell.find_next_siblings('div', class_='cita-artigos')\n",
    "#                     # print('subsections', len(divs))\n",
    "#                     for div_publicacoes in divs:\n",
    "#                         tags_b = div_publicacoes.find('b')\n",
    "#                         if tags_b:\n",
    "#                             current_section = div_publicacoes.find_previous_sibling('div', class_='inst_back')\n",
    "#                             current_section_text = current_section.find('b').get_text()\n",
    "#                             # print(current_section_text)\n",
    "#                             if current_section_text not in data_dict[titulo]:\n",
    "#                                 data_dict[titulo][current_section_text] = {}  # Inicializa dicionário vazio se não existir para seções\n",
    "#                             current_subection_text = tags_b.get_text().strip() if tags_b.get_text().strip() else None\n",
    "#                             if current_subection_text not in data_dict[titulo][current_section_text]:\n",
    "#                                 data_dict[titulo][current_section_text][current_subection_text] = {}  # Inicializa dicionário vazio se não existir para subseções\n",
    "                            \n",
    "#                             if current_subection_text not in extracted:\n",
    "#                                 if verbose:\n",
    "#                                     print(f'    {current_subection_text}')\n",
    "#                                 elementos_layout_cell_1 = div_publicacoes.find_next_siblings('div', class_='layout-cell-1')\n",
    "#                                 elementos_layout_cell_11 = div_publicacoes.find_next_siblings('div', class_='layout-cell-11')\n",
    "                                \n",
    "#                                 keys=[]\n",
    "#                                 vals=[]\n",
    "#                                 for i, j in zip(elementos_layout_cell_1, elementos_layout_cell_11):                                            \n",
    "#                                     key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#                                     key_text = key.get_text().strip() if key else \"\"\n",
    "#                                     val = j.find('div', class_='layout-cell-pad-5')\n",
    "#                                     val_text = val.get_text().strip().replace('\\n',' ').replace('\\t','') if val else \"\"\n",
    "#                                     keys.append(key_text)\n",
    "#                                     vals.append(val_text) \n",
    "#                                     if verbose:\n",
    "#                                         print(f'      {key_text:>3}: {val_text}')  # dados chave e valor  \n",
    "\n",
    "#                                     #condição de parada\n",
    "#                                     try:\n",
    "#                                         # next_class = j.find_next_sibling().get('class', [])[0]\n",
    "#                                         # print(f'      {next_class}')\n",
    "#                                         next_next_class = j.find_next_sibling().find_next_sibling().get('class', [])[0]\n",
    "#                                         # print(f'      {next_next_class}')\n",
    "#                                         if next_next_class:\n",
    "#                                             if next_next_class == 'cita-artigos' or next_next_class == 'clear':                                                \n",
    "#                                                 extracted.append(current_subection_text)\n",
    "#                                                 break\n",
    "#                                     except:\n",
    "#                                         break\n",
    "                        \n",
    "#                                     agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "#                                     data_dict[titulo][current_section_text][current_subection_text] = agg_dict\n",
    "#                                     data_dict['Produções']['Produção bibliográfica']['Citações'] = sub_section_list\n",
    "\n",
    "#             # Eventos (não contém subseções)\n",
    "#             elif titulo in tit3:\n",
    "#                 secoes = ['Participação em eventos, congressos, exposições e feiras','Organização de eventos, congressos, exposições e feiras']\n",
    "#                 ## Listagem das divisões e subdivisões\n",
    "#                 divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "#                 # print(len(divs_inst_back))\n",
    "                          \n",
    "#     return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extrair_normal(soup, verbose=False):\n",
    "#     elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "\n",
    "#     info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "#     name = info_list[0]\n",
    "\n",
    "#     data_dict = {\"labels\": \"Person\",\"name\": name,\"InfPes\": [], \"Resumo\": []}\n",
    "#     data_dict['InfPes'] = info_list\n",
    "#     summary_text  = elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()\n",
    "#     data_dict['Resumo'].append(summary_text)\n",
    "\n",
    "#     tit1 = ['Identificação','Endereço','Formação acadêmica/titulação','Pós-doutorado','Formação Complementar','Atuação Profissional','Linhas de pesquisa',\n",
    "#             'Projetos de pesquisa','Projetos de extensão','Projetos de desenvolvimento','Revisor de periódico','Revisor de projeto de fomento',\n",
    "#             'Áreas de atuação','Idiomas','Inovação']\n",
    "#     tit2 = ['Produções','Bancas','Orientações']\n",
    "#     tit3 = ['Eventos']\n",
    "\n",
    "#     for div_title_wrapper in divs_title_wrapper:\n",
    "#         data_cells_12 = div_title_wrapper.findChildren('div', class_='data-cell')\n",
    "\n",
    "#         titulo = div_title_wrapper.find('h1').text.strip()\n",
    "#         if titulo not in data_dict:\n",
    "#             if titulo == '':\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 data_dict[titulo] = {}\n",
    "#         if verbose:\n",
    "#             print(f'{titulo}')\n",
    "\n",
    "#         for data_cell in data_cells_12:\n",
    "\n",
    "#             # Formações e demais seções que não têm subseções\n",
    "#             if titulo in tit1:\n",
    "#                 divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "                \n",
    "#                 for div_inst_back_cell in divs_inst_back:                                        \n",
    "#                     inst_back = div_inst_back_cell.findChild('b')\n",
    "#                     if inst_back:\n",
    "#                         instback_text = inst_back.get_text().strip()\n",
    "#                         data_dict[titulo][instback_text] = []\n",
    "                        \n",
    "#                         if verbose:                            \n",
    "#                             print(f'  {instback_text}')\n",
    "\n",
    "#                     divs_subsection = div_inst_back_cell.find_next_siblings('div', class_='cita-artigos')\n",
    "#                     for subsection in divs_subsection:\n",
    "#                         subsection_text = subsection.get_text().strip()\n",
    "#                         if subsection_text:\n",
    "#                             data_dict[titulo][instback_text].append(subsection_text)\n",
    "#                             if verbose:\n",
    "#                                 print(f'      {subsection_text}')\n",
    "\n",
    "#                 elementos_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "#                 elementos_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "#                 keys=[]\n",
    "#                 vals=[]\n",
    "#                 for i,j in zip(elementos_layout_cell_3, elementos_layout_cell_9):\n",
    "#                     if elementos_layout_cell_3 and elementos_layout_cell_9:\n",
    "#                         key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#                         key_text = key.get_text().strip().replace('\\n',' ').replace('\\t','')\n",
    "#                         keys.append(key_text)\n",
    "#                         val = j.find('div', class_='layout-cell-pad-5')\n",
    "#                         val_text = val.get_text().strip().replace('\\n',' ').replace('\\t','')\n",
    "#                         vals.append(val_text)\n",
    "#                         if verbose:\n",
    "#                             print(f'      {key_text:>3}: {val_text}')\n",
    "                \n",
    "#                 agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "#                 if titulo not in data_dict:\n",
    "#                     data_dict[titulo] = {}\n",
    "#                 data_dict[titulo].update(agg_dict)                \n",
    "\n",
    "#             # Produções, Orientações e Bancas (que contém subseções)\n",
    "#             elif titulo in tit2:\n",
    "#                 divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "#                 for div_inst_back_cell in divs_inst_back:\n",
    "#                     inst_back = div_inst_back_cell.findChild('b')\n",
    "#                     if inst_back:\n",
    "#                         instback_text = inst_back.get_text().strip()\n",
    "#                         if instback_text not in data_dict[titulo]:\n",
    "#                             data_dict[titulo][instback_text] = {}\n",
    "#                         if verbose:\n",
    "#                             print(f'  {instback_text}')\n",
    "\n",
    "#                     divs_cita_artigos = div_inst_back_cell.find_next_siblings('div', class_='cita-artigos')\n",
    "#                     for div_cita in divs_cita_artigos:\n",
    "#                         cita_artigos_text = div_cita.findChild('b').get_text().strip() if div_cita.findChild('b') else None\n",
    "#                         if cita_artigos_text:\n",
    "#                             data_dict[titulo][instback_text][cita_artigos_text] = []\n",
    "#                             if verbose:\n",
    "#                                 print(f'      {cita_artigos_text}')\n",
    "                            \n",
    "#                             # Aqui você pode adicionar a extração de informações específicas para cada subseção.\n",
    "\n",
    "#                     elementos_layout_cell_1 = div_inst_back_cell.find_all('div', class_='layout-cell-1')\n",
    "#                     elementos_layout_cell_11 = div_inst_back_cell.find_all('div', class_='layout-cell-11')\n",
    "#                     keys = []\n",
    "#                     vals = []\n",
    "#                     for i, j in zip(elementos_layout_cell_1, elementos_layout_cell_11):\n",
    "#                         key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#                         key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '') if key else None\n",
    "#                         val = j.find('div', class_='layout-cell-pad-5')\n",
    "#                         val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '') if val else None\n",
    "#                         if key_text and val_text:\n",
    "#                             keys.append(key_text)\n",
    "#                             vals.append(val_text)\n",
    "#                             if verbose:\n",
    "#                                 print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "#                     agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "#                     if instback_text and agg_dict:\n",
    "#                         data_dict[titulo][instback_text].update(agg_dict)\n",
    "\n",
    "#             # Seção \"Eventos\"\n",
    "#             elif titulo in tit3:\n",
    "#                 # Coleta de dados para a seção \"Eventos\" conforme a lógica original\n",
    "#                 divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "#                 for div_inst_back_cell in divs_inst_back:                                        \n",
    "#                     inst_back = div_inst_back_cell.findChild('b')\n",
    "#                     if inst_back:\n",
    "#                         instback_text = inst_back.get_text().strip()\n",
    "#                         if verbose:\n",
    "#                             print(f'  {instback_text}')\n",
    "\n",
    "#     return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_tit1_soup(soup, verbose=False):\n",
    "#     data_dict = {}\n",
    "\n",
    "#     elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "    \n",
    "#     # Títulos contendo subseções\n",
    "#     tit1 = ['Identificação', 'Endereço', 'Formação acadêmica/titulação', 'Pós-doutorado', 'Formação Complementar',\n",
    "#             'Linhas de pesquisa', 'Projetos de pesquisa', 'Projetos de extensão',\n",
    "#             'Projetos de desenvolvimento', 'Revisor de periódico', 'Revisor de projeto de fomento', 'Áreas de atuação',\n",
    "#             'Idiomas', 'Inovação']\n",
    "\n",
    "#     tit2 = ['Atuação Profissional'] # dados com subseções\n",
    "#     for div_title_wrapper in divs_title_wrapper:\n",
    "#         # Encontre o título do bloco\n",
    "#         titulo = div_title_wrapper.find('h1').text.strip()\n",
    "#         data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        \n",
    "#         # Verifique se o título está na lista 'tit1'\n",
    "#         if titulo in tit1:\n",
    "#             data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "#             if verbose:\n",
    "#                 print(titulo)\n",
    "            \n",
    "#             for data_cell in data_cells:\n",
    "#                 divs_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "#                 divs_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "#                 keys = []\n",
    "#                 vals = []\n",
    "\n",
    "#                 for i, j in zip(divs_layout_cell_3, divs_layout_cell_9):\n",
    "#                     if divs_layout_cell_3 and divs_layout_cell_9:\n",
    "#                         key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#                         key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "#                         keys.append(key_text)\n",
    "#                         val = j.find('div', class_='layout-cell-pad-5')\n",
    "#                         val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "#                         vals.append(val_text)\n",
    "#                         if verbose:\n",
    "#                             print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "#                 agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "#                 data_dict[titulo] = agg_dict\n",
    "        \n",
    "#         if titulo in tit2:\n",
    "#             data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "#             if verbose:\n",
    "#                 print(titulo)\n",
    "            \n",
    "#             for data_cell in data_cells:\n",
    "#                 sections = data_cell.find_all(\"div\", class_=\"inst_back\")               \n",
    "#                 if verbose:\n",
    "#                     print(len(sections), 'seções')\n",
    "\n",
    "#                 for section in sections:\n",
    "#                     section_name = section.find('b').get_text().strip()\n",
    "#                     data_dict[titulo][section_name] = []\n",
    "#                     if verbose:\n",
    "#                         print(section_name)\n",
    "\n",
    "#                     sibling = section.find_next_sibling()\n",
    "#                     current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "#                     while sibling:\n",
    "#                         classes = sibling.get('class', [])\n",
    "\n",
    "#                         if 'layout-cell-3' in classes:  # Data key\n",
    "#                             key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "#                             sibling = sibling.find_next_sibling()\n",
    "\n",
    "#                             if sibling and 'layout-cell-9' in sibling.get('class', []):  # Check if value is present\n",
    "#                                 val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "#                                 current_data[key] = val\n",
    "#                                 if verbose:\n",
    "#                                     print(len(current_data.values()), key, val)\n",
    "\n",
    "#                         elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "#                             next_sibling = sibling.find_next_sibling()\n",
    "#                             if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "#                                 sibling = None\n",
    "#                             else:\n",
    "#                                 if current_data:\n",
    "#                                     data_dict[titulo][section_name] = current_data  # Armazenamos os dados em uma lista\n",
    "\n",
    "#                         if sibling:\n",
    "#                             sibling = sibling.find_next_sibling()\n",
    "#     return data_dict\n",
    "\n",
    "\n",
    "# def extract_tit2_soup(soup, verbose=False):\n",
    "#     data_dict = {}\n",
    "                \n",
    "#     elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')    \n",
    "    \n",
    "#     tit2 = ['Produções', 'Bancas', 'Orientações']\n",
    "\n",
    "#     for div_title_wrapper in divs_title_wrapper:\n",
    "#         # Encontre o título do bloco\n",
    "#         titulo = div_title_wrapper.find('h1').text.strip()\n",
    "#         data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        \n",
    "#         # Verifique se o título está na lista 'tit2'\n",
    "#         if titulo in tit2:\n",
    "#             data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "#             if verbose:\n",
    "#                 print(titulo)\n",
    "\n",
    "#             for data_cell in data_cells:\n",
    "#                 sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "#                 if verbose:\n",
    "#                     print(len(sections), 'seções')\n",
    "\n",
    "#                 for section in sections:\n",
    "#                     section_name = section.find('b').get_text().strip()\n",
    "#                     data_dict[titulo][section_name] = {}\n",
    "\n",
    "#                     sibling = section.find_next_sibling()\n",
    "#                     current_subsection = None\n",
    "#                     current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "#                     div_citacoes = section.find_next_sibling('div', class_='cita-artigos')\n",
    "#                     if verbose:\n",
    "#                         print(len(div_citacoes), 'próxima cita-artigos')\n",
    "#                     if div_citacoes:\n",
    "#                         cita_artigos_text = div_citacoes.findChild('b').get_text().strip()\n",
    "#                         if verbose:\n",
    "#                             print(f'    {cita_artigos_text}') # nomes de subseção como ocorrências \n",
    "                        \n",
    "#                         if cita_artigos_text == 'Citações':\n",
    "#                             current_subsection = cita_artigos_text\n",
    "#                             data_dict[titulo][section_name]['Citações'] = {}\n",
    "#                             ## Extrair dados das citações dos artigos publicados em bases indexadas\n",
    "#                             sub_section_list = []\n",
    "                                \n",
    "#                             ## Extrair quantidade de citações e fator H das divs de subseção com classe lyout-cell-12\n",
    "#                             next_siblings = div_citacoes.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "\n",
    "#                             for i in next_siblings:\n",
    "#                                 citation_counts = i.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que contém os Valores de Citações\n",
    "#                                 if citation_counts:\n",
    "#                                     for i in citation_counts:\n",
    "#                                         database = i.get_text()\n",
    "#                                         total_trab = i.find_next_sibling(\"div\", class_=\"trab\").get_text().split(\"Total de trabalhos:\")[1]\n",
    "#                                         total_cite = i.find_next_sibling(\"div\", class_=\"cita\").get_text().split(\"Total de citações:\")[1]\n",
    "#                                         fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "#                                         num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "#                                         data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\").get_text().split(\"Data:\")[1].strip()\n",
    "\n",
    "#                                         # Converta os valores para tipos de dados adequados\n",
    "#                                         total_trab = int(total_trab)\n",
    "#                                         total_cite = int(total_cite)\n",
    "\n",
    "#                                         citation_numbers = {\n",
    "#                                             \"Database\": database,\n",
    "#                                             \"Total de trabalhos\": total_trab,\n",
    "#                                             \"Total de citações\": total_cite,\n",
    "#                                             \"Índice_H\": num_fator_h,\n",
    "#                                             \"Data\": data_wos\n",
    "#                                         }\n",
    "\n",
    "#                                         # Verifique se a subseção atual já existe no dicionário\n",
    "#                                         if 'Citações' not in data_dict[titulo][section_name]:\n",
    "#                                             data_dict[titulo][section_name]['Citações'] = {}  # Inicialize como uma lista vazia\n",
    "\n",
    "#                                         data_dict[titulo][section_name]['Citações'] = citation_numbers\n",
    "\n",
    "#                                         if verbose:\n",
    "#                                             print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')\n",
    "#                         else:\n",
    "#                             while sibling:\n",
    "#                                 classes = sibling.get('class', [])\n",
    "\n",
    "#                                 if 'cita-artigos' in classes:  # Subsection start\n",
    "#                                     subsection_name = sibling.find('b').get_text().strip()\n",
    "#                                     current_subsection = subsection_name\n",
    "#                                     data_dict[titulo][section_name][current_subsection] = {}\n",
    "#                                     current_data = {}  # Inicializamos o dicionário de dados da subseção atual\n",
    "\n",
    "#                                 elif 'layout-cell-1' in classes:  # Data key\n",
    "#                                     key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "#                                     sibling = sibling.find_next_sibling()\n",
    "\n",
    "#                                     if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "#                                         val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "#                                         current_data[key] = val\n",
    "\n",
    "#                                 elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Subsection or section end\n",
    "#                                     next_sibling = sibling.find_next_sibling()\n",
    "#                                     if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "#                                         sibling = None\n",
    "#                                     else:\n",
    "#                                         if current_subsection:\n",
    "#                                             data_dict[titulo][section_name][current_subsection] = current_data  # Armazenamos os dados da subseção atual\n",
    "\n",
    "#                                 if sibling:\n",
    "#                                     sibling = sibling.find_next_sibling()\n",
    "\n",
    "#     return data_dict\n",
    "\n",
    "\n",
    "# def extract_tit3_soup(soup, verbose=False):\n",
    "#     data_dict = {}\n",
    "\n",
    "#     elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "    \n",
    "#     # Títulos da seção 'Eventos'\n",
    "#     tit3 = ['Eventos']\n",
    "\n",
    "#     for div_title_wrapper in divs_title_wrapper:\n",
    "#         # Encontre o título do bloco\n",
    "#         titulo = div_title_wrapper.find('h1').text.strip()\n",
    "#         data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "#         # Verifique se o título está na lista 'tit3'\n",
    "#         if titulo in tit3:\n",
    "#             data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "#             if verbose:\n",
    "#                 print(titulo)\n",
    "            \n",
    "#             for data_cell in data_cells:\n",
    "#                 sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "#                 if verbose:\n",
    "#                     print(len(sections), 'seções')\n",
    "\n",
    "#                 for section in sections:\n",
    "#                     section_name = section.find('b').get_text().strip()\n",
    "#                     data_dict[titulo][section_name] = []\n",
    "#                     if verbose:\n",
    "#                         print(section_name)\n",
    "\n",
    "#                     sibling = section.find_next_sibling()\n",
    "#                     current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "#                     while sibling:\n",
    "#                         classes = sibling.get('class', [])\n",
    "\n",
    "#                         if 'layout-cell-1' in classes:  # Data key\n",
    "#                             key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "#                             sibling = sibling.find_next_sibling()\n",
    "\n",
    "#                             if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "#                                 val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "#                                 current_data[key] = val\n",
    "#                                 if verbose:\n",
    "#                                     print(len(current_data.values()), key, val)\n",
    "\n",
    "#                         elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "#                             next_sibling = sibling.find_next_sibling()\n",
    "#                             if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "#                                 sibling = None\n",
    "#                             else:\n",
    "#                                 if current_data:\n",
    "#                                     data_dict[titulo][section_name] = current_data  # Armazenamos os dados em uma lista\n",
    "\n",
    "#                         if sibling:\n",
    "#                             sibling = sibling.find_next_sibling()\n",
    "\n",
    "#     return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_full_data(soup, verbose=False):\n",
    "#     data_dict = {}\n",
    "    \n",
    "#     elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "\n",
    "#     info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "#     name = info_list[0]\n",
    "\n",
    "#     data_dict = {\"labels\": \"Person\",\"name\": name,\"InfPes\": [], \"Resumo\": []}\n",
    "#     data_dict['InfPes'] = info_list\n",
    "#     summary_text  = elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()\n",
    "#     data_dict['Resumo'].append(summary_text)\n",
    "\n",
    "#     tit1 = ['Identificação','Endereço','Formação acadêmica/titulação','Pós-doutorado','Formação Complementar','Atuação Profissional','Linhas de pesquisa',\n",
    "#             'Projetos de pesquisa','Projetos de extensão','Projetos de desenvolvimento','Revisor de periódico','Revisor de projeto de fomento',\n",
    "#             'Áreas de atuação','Idiomas','Inovação']\n",
    "#     tit2 = ['Produções','Bancas','Orientações']\n",
    "#     # Não extraindo ainda, semelhante ao tit2 mas não há subseções dados direto nas seções \"inst_back\"\n",
    "#     tit3 = ['Eventos']\n",
    "\n",
    "#     for div_title_wrapper in divs_title_wrapper:\n",
    "#         data_cells_12 = div_title_wrapper.findChildren('div', class_='data-cell')\n",
    "\n",
    "#         titulo = div_title_wrapper.find('h1').text.strip()\n",
    "#         if titulo not in data_dict:\n",
    "#             if titulo == '':\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 data_dict[titulo] = {}\n",
    "#         if verbose:\n",
    "#             print(f'{titulo}')\n",
    "\n",
    "#         for data_cell in data_cells_12:\n",
    "#             if titulo in tit1 or titulo in tit3:\n",
    "#                 divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "\n",
    "#                 for div_inst_back_cell in divs_inst_back:\n",
    "#                     inst_back = div_inst_back_cell.findChild('b')\n",
    "#                     if inst_back:\n",
    "#                         instback_text = inst_back.get_text().strip()\n",
    "#                         data_dict.setdefault(titulo, {}).setdefault(instback_text, {})\n",
    "\n",
    "#                         divs_subsection = div_inst_back_cell.find_next_siblings('div', class_='cita-artigos')\n",
    "#                         for subsection in divs_subsection:\n",
    "#                             subsection_text = subsection.get_text().strip()\n",
    "#                             if subsection_text:\n",
    "#                                 data_dict[titulo][instback_text][subsection_text] = extract_partial_data(subsection, verbose=verbose)\n",
    "\n",
    "#                 elementos_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "#                 elementos_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "#                 keys = []\n",
    "#                 vals = []\n",
    "#                 for i, j in zip(elementos_layout_cell_3, elementos_layout_cell_9):\n",
    "#                     if elementos_layout_cell_3 and elementos_layout_cell_9:\n",
    "#                         key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#                         key_text = key.get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "#                         keys.append(key_text)\n",
    "#                         val = j.find('div', class_='layout-cell-pad-5')\n",
    "#                         val_text = val.get_text().strip().replace('\\n', '').replace('\\t', '')\n",
    "#                         vals.append(val_text)\n",
    "#                         if verbose:\n",
    "#                             print(f'      {key_text:>3}: {val_text}')\n",
    "                \n",
    "#                 agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "#                 if titulo not in data_dict:\n",
    "#                     data_dict[titulo] = {}\n",
    "#                 data_dict[titulo].update(agg_dict)\n",
    "\n",
    "#             elif titulo in tit2:\n",
    "#                 divs_inst_back = data_cell.find_all('div', class_='inst_back')\n",
    "\n",
    "#                 for div_inst_back_cell in divs_inst_back:\n",
    "#                     inst_back = div_inst_back_cell.findChild('b')\n",
    "#                     if inst_back:\n",
    "#                         instback_text = inst_back.get_text().strip()\n",
    "#                         if verbose:\n",
    "#                             print(f'  {instback_text}')\n",
    "\n",
    "#                         div_citacoes = div_inst_back_cell.find_next_sibling('div', class_='cita-artigos')\n",
    "#                         if div_citacoes:\n",
    "#                             cita_artigos_text = div_citacoes.findChild('b').get_text().strip()\n",
    "#                             if cita_artigos_text == 'Citações':\n",
    "#                                 partial_data = extract_tit2(div_citacoes, verbose=verbose)\n",
    "#                                 data_dict.setdefault(titulo, {}).setdefault(instback_text, {})[cita_artigos_text] = partial_data\n",
    "\n",
    "#                         div_artigos = div_inst_back_cell.find_next_siblings('div', id='artigos-completos')\n",
    "#                         for div_artigo in div_artigos:\n",
    "#                             if div_artigo:\n",
    "#                                 divs_cita_artigos = div_artigo.find_all('div', class_='cita-artigos')\n",
    "#                                 for cita_artigos in divs_cita_artigos:\n",
    "#                                     if cita_artigos:\n",
    "#                                         cita_artigos_text = cita_artigos.findChild('b').get_text().strip()\n",
    "#                                         if verbose:\n",
    "#                                             print(f'    {cita_artigos_text}')\n",
    "\n",
    "#                                         elementos_layout_cell_1 = div_artigo.find_all('div', class_='layout-cell-1')\n",
    "#                                         elementos_layout_cell_11 = div_artigo.find_all('div', class_='layout-cell-11')\n",
    "                                        \n",
    "#                                         keys = []\n",
    "#                                         vals = []\n",
    "#                                         for i, j in zip(elementos_layout_cell_1, elementos_layout_cell_11):\n",
    "#                                             key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#                                             key_text = key.get_text().strip().replace('\\n', '').replace('\\t', '')\n",
    "#                                             val = j.find('div', class_='layout-cell-pad-5')\n",
    "#                                             val_text = val.get_text().strip().replace('\\n', '').replace('\\t', '')\n",
    "#                                             keys.append(key_text)\n",
    "#                                             vals.append(val_text)\n",
    "#                                             if verbose:\n",
    "#                                                 print(f'      {key_text:>3}: {val_text}')\n",
    "                                            \n",
    "#                                             # Condição de parada\n",
    "#                                             try:\n",
    "#                                                 next_next_class = j.find_next_sibling().find_next_sibling().get('class', [])[0]\n",
    "#                                                 if next_next_class:\n",
    "#                                                     if next_next_class == 'cita-artigos' or next_next_class == 'clear':\n",
    "#                                                         break\n",
    "#                                             except:\n",
    "#                                                 break\n",
    "\n",
    "#                                         partial_data = extract_tit1(cita_artigos, verbose=verbose)\n",
    "#                                         data_dict[titulo][instback_text][cita_artigos_text] = partial_data\n",
    "\n",
    "#     return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_tooltip_data(driver, element):\n",
    "#     # Use a biblioteca ActionChains para simular uma ação de mouse hover\n",
    "#     hover = ActionChains(driver).move_to_element(element)\n",
    "#     hover.perform()\n",
    "\n",
    "#     # Aguarde até que o tooltip seja exibido (ajuste o tempo limite conforme necessário)\n",
    "#     tooltip = WebDriverWait(driver, 10).until(\n",
    "#         EC.presence_of_element_located((By.CLASS_NAME, 'jcrTip'))\n",
    "#     )\n",
    "\n",
    "#     # Extraia o texto do tooltip\n",
    "#     tooltip_text = tooltip.text\n",
    "\n",
    "#     # Faça o parsing do texto do tooltip conforme necessário\n",
    "#     # Por exemplo, você pode dividir as informações usando '\\n' ou ':'\n",
    "#     info_list = tooltip_text.split('\\n')\n",
    "#     info_dict = {\n",
    "#         'data-issn': element.get_attribute('data-issn'),\n",
    "#         'original-title': info_list[0],\n",
    "#         'Fator de impacto (JCR 2022)': info_list[1].split(': ')[1]\n",
    "#     }\n",
    "\n",
    "#     return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_jcr_data(soup, result_list = []):\n",
    "#     from pprint import pprint\n",
    "\n",
    "#     sup_elements = soup.find_all('sup')\n",
    "#     print(len(sup_elements))\n",
    "\n",
    "#     for sup_element in sup_elements:\n",
    "#         img_element = sup_element.find('img')\n",
    "#         if img_element:\n",
    "#             data_issn = img_element.get('data-issn')\n",
    "#             original_title = img_element.get('original-title', '')\n",
    "            \n",
    "#             # Inicialize as informações como em branco\n",
    "#             data_issn_value = ''\n",
    "#             fator_de_impacto_value = ''\n",
    "#             apuracao_jcr_value = ''\n",
    "            \n",
    "#             # Verifique se as informações desejadas estão presentes e extraia-as\n",
    "#             if 'Fator de impacto (JCR 2021):' in original_title:\n",
    "#                 fator_de_impacto_start = original_title.find('Fator de impacto (JCR 2021): ') + len('Fator de impacto (JCR 2021): ')\n",
    "#                 fator_de_impacto_value = original_title[fator_de_impacto_start:]\n",
    "                \n",
    "#                 # Verifique se existe uma quebra de linha para separar a apuração do JCR\n",
    "#                 if '<br />' in fator_de_impacto_value:\n",
    "#                     fator_de_impacto_parts = fator_de_impacto_value.split('<br />')\n",
    "#                     fator_de_impacto_value = fator_de_impacto_parts[0]\n",
    "#                     apuracao_jcr_value = fator_de_impacto_parts[1]\n",
    "            \n",
    "#             # Crie um dicionário com as informações extraídas\n",
    "#             info_dict = {\n",
    "#                 'data-issn': data_issn,\n",
    "#                 'original-title': original_title,\n",
    "#                 'Fator de impacto': fator_de_impacto_value,\n",
    "#                 'Apuração JCR': apuracao_jcr_value\n",
    "#             }\n",
    "\n",
    "#             # Adicione o dicionário à lista de resultados\n",
    "#             result_list.append(info_dict)\n",
    "\n",
    "#     # Exiba a lista de dicionários\n",
    "#     return result_list\n",
    "\n",
    "\n",
    "# def extract_jcr_tag(sup_element, result_list = []):\n",
    "\n",
    "#     img_element = sup_element.find('img')\n",
    "#     if img_element:\n",
    "#         data_issn = img_element.get('data-issn')\n",
    "#         original_title = img_element.get('original-title', '')\n",
    "        \n",
    "#         # Inicialize as informações como em branco\n",
    "#         data_issn_value = ''\n",
    "#         fator_de_impacto_value = ''\n",
    "#         apuracao_jcr_value = ''\n",
    "        \n",
    "#         # Verifique se as informações desejadas estão presentes e extraia-as\n",
    "#         if 'Fator de impacto (JCR 2021):' in original_title:\n",
    "#             fator_de_impacto_start = original_title.find('Fator de impacto (JCR 2021): ') + len('Fator de impacto (JCR 2021): ')\n",
    "#             fator_de_impacto_value = original_title[fator_de_impacto_start:]\n",
    "            \n",
    "#             # Verifique se existe uma quebra de linha para separar a apuração do JCR\n",
    "#             if '<br />' in fator_de_impacto_value:\n",
    "#                 fator_de_impacto_parts = fator_de_impacto_value.split('<br />')\n",
    "#                 fator_de_impacto_value = fator_de_impacto_parts[0]\n",
    "#                 apuracao_jcr_value = fator_de_impacto_parts[1]\n",
    "        \n",
    "#         # Crie um dicionário com as informações extraídas\n",
    "#         info_dict = {\n",
    "#             'data-issn': data_issn,\n",
    "#             'original-title': original_title,\n",
    "#             'Fator de impacto': fator_de_impacto_value,\n",
    "#             'Apuração JCR': apuracao_jcr_value\n",
    "#         }\n",
    "\n",
    "#         # Adicione o dicionário à lista de resultados\n",
    "#         # result_list.append(info_dict)\n",
    "\n",
    "#     return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract(instback_text, elementos_layout_cell_1,elementos_layout_cell_11):\n",
    "#     for i,j in zip(elementos_layout_cell_1, elementos_layout_cell_11):\n",
    "#         if elementos_layout_cell_1 and elementos_layout_cell_11:\n",
    "#             key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#             key_text = key.get_text().strip().replace('\\n',' ').replace('\\t','')\n",
    "    \n",
    "#             val = j.find('div', class_='layout-cell-pad-5')\n",
    "#             val_text = val.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "#             print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "#             if instback_text == 'Produção bibliográfica':\n",
    "#                 sup_element = j.find('sup')\n",
    "\n",
    "#                 if sup_element:\n",
    "#                     img_element = sup_element.find('img')\n",
    "#                     if img_element:\n",
    "#                         data_issn = img_element.get('data-issn')\n",
    "#                         original_title = img_element.get('original-title', '')\n",
    "                        \n",
    "#                         # Inicialize as informações como em branco\n",
    "#                         data_issn_value = ''\n",
    "#                         fator_de_impacto_value = ''\n",
    "#                         apuracao_jcr_value = ''\n",
    "                        \n",
    "#                         # Verifique se as informações desejadas estão presentes e extraia-as\n",
    "#                         if 'Fator de impacto (JCR 2021):' in original_title:\n",
    "#                             fator_de_impacto_start = original_title.find('Fator de impacto (JCR 2021): ') + len('Fator de impacto (JCR 2021): ')\n",
    "#                             fator_de_impacto_value = original_title[fator_de_impacto_start:]\n",
    "                            \n",
    "#                             # Verifique se existe uma quebra de linha para separar a apuração do JCR\n",
    "#                             if '<br />' in fator_de_impacto_value:\n",
    "#                                 fator_de_impacto_parts = fator_de_impacto_value.split('<br />')\n",
    "#                                 fator_de_impacto_value = fator_de_impacto_parts[0]\n",
    "#                                 apuracao_jcr_value = fator_de_impacto_parts[1]\n",
    "                        \n",
    "#                         # Crie um dicionário com as informações extraídas\n",
    "#                         detail_dict = {\n",
    "#                             'data-issn': data_issn,\n",
    "#                             'original-title': original_title,\n",
    "#                             'Fator de impacto': fator_de_impacto_value,\n",
    "#                             'Apuração JCR': apuracao_jcr_value\n",
    "#                         }\n",
    "#                         print(f'          {detail_dict}')\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_pairs(section, looking_class_key, looking_class_val, classes_to_ignore, aggregated_data_dict, verbose=False):\n",
    "#     \"\"\"Extraction of key-value pairs from HTML subsections for aggregation into a dictionary object.\n",
    "    \n",
    "#     Parameters:\n",
    "#         section (bs4.element.Tag): Root HTML tag from which traversal begins.\n",
    "#         looking_class_key (str): Class name to identify HTML elements containing keys.\n",
    "#         looking_class_val (str): Class name to identify HTML elements containing values.\n",
    "#         classes_to_ignore (str): Class name to identify HTML elements that mark the end of a subsection.\n",
    "#         aggregated_data_dict (dict): Aggregated data in dictionary format.\n",
    "    \n",
    "#     Returns:\n",
    "#         aggregated_data_dict (dict): Dictionary populated with key-value pairs from the HTML subsection.\n",
    "#     \"\"\"\n",
    "#     initial_class = section.find_next()\n",
    "#     class_found_0 = initial_class.get('class', [])\n",
    "#     class_found_1 = initial_class.find_next_sibling().get('class', [])\n",
    "#     class_found_2 = initial_class.find_next_sibling().find_next_sibling().get('class', [])\n",
    "#     class_found_3 = initial_class.find_next_sibling().find_next_sibling().find_next_sibling()\n",
    "    \n",
    "#     iteracao=0\n",
    "\n",
    "#     next_node = initial_class\n",
    "#     while True:\n",
    "#         next_node = next_node.find_next_sibling() if next_node.find_next_sibling() else None\n",
    "#         iteracao+=1\n",
    "#         # if verbose:\n",
    "#             # print('    \\nBuscando:',looking_class_key)\n",
    "#             # print('    Achados 0:',class_found_0)\n",
    "#             # print('    Achados 1:',class_found_1)\n",
    "#             # print('    Achados 2:',class_found_2)   \n",
    "#             # print('    Achados 3:',class_found_3)    \n",
    "#             # print(f'    Iteração: {iteracao:02}')\n",
    "\n",
    "#         if next_node is None:\n",
    "#             break \n",
    "#         if verbose:\n",
    "#             align_right = next_node.find('div', class_=\"text-align-right\") if next_node else None\n",
    "#             if align_right:\n",
    "#                 b_text = align_right.findChild('b') if align_right.findChild('b') else None\n",
    "#                 print('\\nClasses da próxima div:', b_text.get_text())\n",
    "                \n",
    "#         next_node_child = next_node.findChild() if next_node.findChild() else None\n",
    "#         if verbose:\n",
    "#             if next_node_child:\n",
    "#                 div_val  = next_node_child\n",
    "#                 cellpad5 = next_node_child.get('class', []) if next_node else None\n",
    "\n",
    "#                 if cellpad5:\n",
    "#                     # print('Classes div Filho da próxima div:',cellpad5)\n",
    "\n",
    "#                     # Remova as tags span da div\n",
    "#                     for span in div_val.find_all('span'):\n",
    "#                         span.extract()\n",
    "\n",
    "#                     # Obtenha o texto restante na div\n",
    "#                     texto = div_val.get_text(strip=True) \n",
    "\n",
    "#                     print('Valores div:',texto.strip().replace('\\n', ''))   \n",
    "#                     print(extract_jcr_tag(div_val.findChild('sup')))\n",
    "#                     print('-'*125)\n",
    "    \n",
    "#         if next_node_child is None:\n",
    "#             break\n",
    "    \n",
    "#         if next_node_child.has_attr('class') and classes_to_ignore in next_node['class']:\n",
    "#             classes1=next_node_child.get('class', [])\n",
    "#             print(f'Continuar: {classes1}')\n",
    "#             continue\n",
    "        \n",
    "#         if next_node_child.has_attr('class') and looking_class_key in next_node['class']:\n",
    "#             if verbose:\n",
    "#                 print('Entrou na condição de extração')\n",
    "#             key_node = next_node.findChildren('div').find('b')\n",
    "#             if verbose:\n",
    "#                 print('\\nKey:',key_node)        \n",
    "#             if key_node:\n",
    "#                 key_text = key_node.get_text().strip()\n",
    "#                 val_node = key_node.find_next_sibling('div', class_=looking_class_val)\n",
    "#                 if val_node:\n",
    "#                     val_text = val_node.get_text().strip()\n",
    "#                     if verbose:\n",
    "#                         print('\\Val:',val_text)   \n",
    "#                     aggregated_data_dict[key_text] = val_text\n",
    "    \n",
    "#     print(val_text)\n",
    "#     return aggregated_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "def extract_data_from_cell(cell):\n",
    "    \"\"\"\n",
    "    Extracts relevant text data from a layout cell.\n",
    "    \"\"\"\n",
    "    texts = cell.stripped_strings\n",
    "    list_texts = [x.replace('\\n\\t\\t\\t\\t\\t\\t\\t',' ') for x in texts]\n",
    "    list_texts = [x.replace('\\t\\t\\t\\t\\t\\t',' ') for x in list_texts]\n",
    "    list_texts = [x.replace('\\t\\t\\t\\t\\t',' ') for x in list_texts]\n",
    "    list_texts = [x.replace('\\n',' ') for x in list_texts]\n",
    "    \n",
    "    return list_texts\n",
    "\n",
    "def parse_resume(soup):\n",
    "    result_dict = {}\n",
    "    json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None \n",
    "    json_data['name'] = node_name\n",
    "\n",
    "    parag_elements = soup.find_all('p')\n",
    "    for elem in parag_elements:\n",
    "        # Retrieve the class attribute, returning None if not found\n",
    "        class_name = elem.get('class', [None])[0]\n",
    "        text_content = elem.text\n",
    "        \n",
    "        # Update the dictionary\n",
    "        if class_name:  # Only update if class_name is not None\n",
    "            result_dict[class_name] = text_content\n",
    "\n",
    "        json_data[\"Properties\"] = result_dict\n",
    "\n",
    "    return json_data\n",
    "\n",
    "def parse_cita_artigos(soup):\n",
    "    json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    json_data['name'] = node_name\n",
    "\n",
    "    # Step 3: Information extraction: Bibliographic Production Section\n",
    "    producao_bibliografica_div = soup.find('div', {'id': 'artigos-completos'})\n",
    "    div_cita = producao_bibliografica_div.find_patent()\n",
    "    producoes = []\n",
    "    for artigo_div in div_cita.find_all('div', {'class': 'cita-artigos'}):\n",
    "        artigo_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        \n",
    "        producoes.append(artigo_dict)\n",
    "        json_data[\"Properties\"]['OutrasProduções'] = producoes\n",
    "\n",
    "    return json_data\n",
    "\n",
    "def parse_jcr_articles(soup):\n",
    "    json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    json_data['name'] = node_name\n",
    "\n",
    "    # Step 3: Information extraction: Bibliographic Production Section\n",
    "    producao_bibliografica_div = soup.find('div', {'id': 'artigos-completos'})\n",
    "    producoes = []\n",
    "    for artigo_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        artigo_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = artigo_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = artigo_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = artigo_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = artigo_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        \n",
    "        dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        # str_autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        # autores = split_authors(str_autores)\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        artigo_dict['ano']     = ano\n",
    "        # artigo_dict['autores'] = autores\n",
    "        artigo_dict['revista'] = revista\n",
    "        artigo_dict['titulo']  = titulo\n",
    "        artigo_dict['jcr']     = jcr\n",
    "        artigo_dict['doi']     = doi\n",
    "        \n",
    "        producoes.append(artigo_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = producoes\n",
    "   \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sep_divs(soup):\n",
    "    titles_list=[]\n",
    "    h1 = soup.find_all('h1')\n",
    "    ignore=['','Currículo Lattes - Busca Textual - Visualização do Currículo']\n",
    "    for i in h1:\n",
    "        if i:\n",
    "            h1_text = i.get_text().strip()\n",
    "            if h1_text not in ignore:\n",
    "                titles_list.append(h1_text)\n",
    "\n",
    "    return titles_list\n",
    "\n",
    "def clean_texts(tag):\n",
    "    tag_text = tag.get_text().strip()\n",
    "    return tag_text\n",
    "\n",
    "def get_section_next(div):\n",
    "    inst_back = div.find('div', _class='inst_back')\n",
    "    inst_back_text = inst_back.get_text().strip()\n",
    "    return inst_back_text\n",
    "\n",
    "def get_section_prev(div):\n",
    "    inst_back = div.find_previous_sibling('div', _class='inst_back')\n",
    "    inst_back_text = inst_back.get_text().strip()\n",
    "    return inst_back_text\n",
    "\n",
    "def get_subsection_next(div):\n",
    "    cita_artigos = div.find('div', _class='cita-artigos')\n",
    "    cita_artigos_text = cita_artigos.get_text().strip()\n",
    "    return cita_artigos_text\n",
    "\n",
    "def get_subsection_prev(div):\n",
    "    cita_artigos = div.find_previous_sibling('div', _class='cita-artigos')\n",
    "    cita_artigos_text = cita_artigos.get_text().strip()\n",
    "    return cita_artigos_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_articledata(data_div):\n",
    "    import numpy as np\n",
    "    if data_div:\n",
    "        info_dict = {}\n",
    "        \n",
    "        # Extracting data from span tags\n",
    "        year = data_div.find('span', {'data-tipo-ordenacao': 'ano'})\n",
    "        if year:\n",
    "            info_dict['Year'] = year.text\n",
    "\n",
    "        jcr = data_div.find('span', {'data-tipo-ordenacao': 'jcr'})\n",
    "        if jcr:\n",
    "            info_dict['JCR'] = jcr.text\n",
    "        else:\n",
    "            info_dict['JCR'] = np.NaN\n",
    "\n",
    "        numero_citacao = data_div.find('span', {'class': 'numero-citacao'})\n",
    "        numero_citacao = numero_citacao.text if numero_citacao else np.NaN\n",
    "        if numero_citacao:\n",
    "            info_dict['Cites'] = numero_citacao\n",
    "\n",
    "        author = data_div.find('span', {'data-tipo-ordenacao': 'autor'})\n",
    "        if author:\n",
    "            info_dict['Author'] = author.text\n",
    "\n",
    "        bold_author = data_div.find('b')\n",
    "        if bold_author:\n",
    "            info_dict['Bold Author'] = bold_author.text\n",
    "\n",
    "        # Extracting additional data like DOIs and other authors\n",
    "        doi_link = data_div.find('a', {'class': 'icone-producao icone-doi'})\n",
    "        if doi_link:\n",
    "            info_dict['DOI Link'] = doi_link.get('href')\n",
    "\n",
    "        # importance = data_div.find('span', {'data-tipo-ordenacao': 'importancia'})\n",
    "        # if importance:\n",
    "        #     info_dict['Importance'] = importance.text\n",
    "\n",
    "        # Displaying the extracted data\n",
    "        return info_dict\n",
    "    \n",
    "# def extract_pairs(subsection_div, looking_class_key, looking_class_val):\n",
    "#     data_dict = {}\n",
    "#     divs_cell1 = subsection_div.find_next_sibling(\"div\", class_=looking_class_key)\n",
    "#     print(f'        Quantidade de livros: {len(divs_cell1)}')\n",
    "#     key_orders_div  = divs_cell1.find('b')\n",
    "#     key_orders_txt  = key_orders_div.get_text().strip()\n",
    "#     values_div      = divs_cell1.find_next_sibling(\"div\", class_=looking_class_val)\n",
    "#     values_txt      = values_div.findChild().get_text().strip()\n",
    "#     data_dict[key_orders_txt] = values_txt\n",
    "#     next_div       = values_div.find_next_sibling().find_next_sibling()\n",
    "#     next_div_class = next_div.get('class')[1]\n",
    "#     if next_div_class == looking_class_key:\n",
    "#         extract_pairs(next_div, looking_class_key, looking_class_val)\n",
    "#     else:\n",
    "#         return data_dict\n",
    "\n",
    "# def extract_pairs(subsection_div, looking_class_key, looking_class_val, data_dict=None):\n",
    "#     if data_dict is None:\n",
    "#         data_dict = {}\n",
    "    \n",
    "#     divs_cell1 = subsection_div.find_next_sibling(\"div\", class_=looking_class_key)\n",
    "#     if divs_cell1 is None:\n",
    "#         return data_dict, None\n",
    "    \n",
    "#     key_orders_div  = divs_cell1.find('b')\n",
    "#     if key_orders_div:\n",
    "#         key_orders_txt  = key_orders_div.get_text().strip()\n",
    "#         values_div      = divs_cell1.find_next_sibling(\"div\", class_=looking_class_val)\n",
    "        \n",
    "#         if values_div:\n",
    "#             values_txt = values_div.findChild().get_text().strip() if values_div.findChild() else \"\"\n",
    "#             data_dict[key_orders_txt] = values_txt\n",
    "    \n",
    "#     next_div = divs_cell1.find_next_sibling()\n",
    "#     if next_div:\n",
    "#         next_div_class = ' '.join(next_div.get('class', []))\n",
    "        \n",
    "#         if looking_class_key in next_div_class.split():\n",
    "#             return extract_pairs(next_div, looking_class_key, looking_class_val, data_dict)\n",
    "    \n",
    "#     return data_dict, next_div_class        \n",
    "\n",
    "\n",
    "# def extract_pairs(current_div, looking_class_key, looking_class_val, aggregated_data_dict):\n",
    "#     if current_div is None:\n",
    "#         return aggregated_data_dict\n",
    "    \n",
    "#     next_div = current_div.find_next_sibling(\"div\", class_=looking_class_key)    \n",
    "#     if next_div is None or looking_class_key not in next_div.get('class', []):\n",
    "#         return aggregated_data_dict\n",
    "\n",
    "#     key_orders_div = next_div.find('b')\n",
    "#     if key_orders_div:\n",
    "#         key_orders_txt = key_orders_div.get_text().strip()\n",
    "#         values_div = next_div.find_next_sibling(\"div\", class_=looking_class_val)\n",
    "#         if values_div:\n",
    "#             values_txt = values_div.findChild().get_text().strip()\n",
    "#             aggregated_data_dict[key_orders_txt] = values_txt\n",
    "\n",
    "#     next_sibling_div = next_div.find_next_sibling()\n",
    "#     while next_sibling_div and 'clear' in next_sibling_div.get('class', []):\n",
    "#         next_sibling_div = next_sibling_div.find_next_sibling()\n",
    "\n",
    "#     return extract_pairs(next_sibling_div, looking_class_key, looking_class_val, aggregated_data_dict)\n",
    "\n",
    "\n",
    "def extract_pairs(main_div, recurrent_classes, looking_class_key, looking_class_val, classes_to_ignore, aggregated_data_dict):\n",
    "    \"\"\"\n",
    "    Function to iteratively extract key-value pairs from sibling div elements in the DOM.\n",
    "    \n",
    "    Parameters:\n",
    "        recurrent_classes (str): The class which indicates a new recurrent section in the DOM.\n",
    "        looking_class_key (str): The class whose first child's 'b' tag content serves as dictionary key.\n",
    "        looking_class_val (str): The class whose text content serves as dictionary value.\n",
    "        classes_to_ignore (str): The class which should interrupt the sibling iteration.\n",
    "        aggregated_data_dict (dict): A pre-existing dictionary to append the extracted data.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The aggregated dictionary containing the extracted data.\n",
    "    \"\"\"\n",
    "    sections = main_div.find_all('div', class_=recurrent_classes)\n",
    "    \n",
    "    for section in sections:\n",
    "        next_node = section\n",
    "        while True:\n",
    "            next_node = next_node.find_next_sibling()\n",
    "            if next_node is None:\n",
    "                break\n",
    "            if next_node.has_attr('class') and classes_to_ignore in next_node['class']:\n",
    "                break\n",
    "            if next_node.has_attr('class') and looking_class_key in next_node['class']:\n",
    "                key_node = next_node.find('div').find('b')\n",
    "                if key_node:\n",
    "                    key_text = key_node.get_text().strip()\n",
    "                    val_node = next_node.find_next_sibling('div', class_=looking_class_val)\n",
    "                    if val_node:\n",
    "                        val_text = val_node.get_text().strip()\n",
    "                        aggregated_data_dict[key_text] = val_text\n",
    "    return aggregated_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4.element import Tag\n",
    "\n",
    "def traverse_up(soup, parent_dict, current_section=None, root_properties=None):\n",
    "    \"\"\"\n",
    "    Traverses through the soup object recursively and populates the dictionary.\n",
    "    root_properties: the 'Properties' dictionary at the root level, where all attributes should be stored.\n",
    "    current_section: the current section being processed, allows to set subsections.\n",
    "    \"\"\"\n",
    "    section = None\n",
    "    subsection = None\n",
    "    section_dict = {}\n",
    "    subsection_dict = {}\n",
    "    \n",
    "    # Existing code for name extraction\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    if node_name is not None:\n",
    "        parent_dict['name'] = node_name\n",
    "\n",
    "    # Existing code for paragraph extraction\n",
    "    parag_elements = soup.find_all('p')\n",
    "    for elem in parag_elements:\n",
    "        class_name = elem.get('class', [None])[0]\n",
    "        text_content = elem.get_text()\n",
    "        if class_name:\n",
    "            parent_dict[class_name] = text_content\n",
    "\n",
    "    # Initialization of root_properties\n",
    "    if root_properties is None:\n",
    "        root_properties = parent_dict.setdefault('Properties', {})\n",
    "    \n",
    "    for child in soup.children:\n",
    "        if isinstance(child, Tag):\n",
    "            ## Segmento para extrair os dados de JCR dos artigos completos em periódicos\n",
    "            # if child.get('id') == 'artigos-completos':\n",
    "            #     section_elem = child.find_previous_sibling(\"div\", class_=\"inst_back\")          \n",
    "            #     if section_elem:\n",
    "            #         section = section_elem.get_text().strip()\n",
    "            #         section_dict = root_properties.setdefault(section, {})\n",
    "\n",
    "            #     subsection_elem = child.find('b')\n",
    "            #     if subsection_elem:\n",
    "            #         subsection = subsection_elem.get_text().strip()\n",
    "            #         if section:  \n",
    "            #             subsection_dict = section_dict.setdefault(subsection, {})\n",
    "\n",
    "            #     jcr_articles_dict = parse_jcr_articles(soup)\n",
    "            #     if jcr_articles_dict:\n",
    "            #         if subsection:  \n",
    "            #             subsection_dict.update(jcr_articles_dict['Properties'])\n",
    "\n",
    "            ## Segmento para extrair até Produções (não tem subseções)\n",
    "            if \"layout-cell-3\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "                cell_key = extract_data_from_cell(child)\n",
    "                if cell_key:\n",
    "                    cell_key = ' '.join(cell_key)\n",
    "\n",
    "                    sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-9\")\n",
    "                    if sibling_cell:\n",
    "                        cell_values = extract_data_from_cell(sibling_cell)\n",
    "\n",
    "                        if current_section:\n",
    "                            root_properties[current_section][cell_key] = cell_values\n",
    "                        else:\n",
    "                            root_properties[cell_key] = cell_values\n",
    "\n",
    "            ## Segmento para extrair de Produções até o final\n",
    "            elif \"layout-cell-1\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "                section_elem = child.findParent().findParent().find_previous_sibling(\"div\", class_=\"inst_back\")\n",
    "                if section_elem:\n",
    "                    section = section_elem.get_text().strip()\n",
    "                    section_dict = root_properties.setdefault(section, {})\n",
    "                elif section_elem == None:\n",
    "                    list_sub = ['Dissertação de mestrado','Tese de doutorado']\n",
    "                    subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                    if subsection_elem:\n",
    "                        if subsection_elem.get_text().strip() in list_sub:\n",
    "                            section_elem = child.find_previous_sibling(\"div\", class_=\"inst_back\")\n",
    "                    if section_elem:\n",
    "                        section = section_elem.get_text().strip()\n",
    "                        section_dict = root_properties.setdefault(section, {})                \n",
    "                else:\n",
    "                    print('Erro ao seção')     \n",
    "\n",
    "                subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                if subsection_elem:\n",
    "                    subsection = subsection_elem.get_text().strip()\n",
    "                elif subsection_elem == None:\n",
    "                    subsection_elem = child.findParent().find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                    if subsection_elem:\n",
    "                        subsection = subsection_elem.get_text().strip()                \n",
    "                else:\n",
    "                    print('Erro ao definir subseção')\n",
    "                \n",
    "                cell_key = extract_data_from_cell(child)\n",
    "                if cell_key:\n",
    "                    cell_key = ' '.join(cell_key)    \n",
    "\n",
    "                if section:\n",
    "                    current_section_dict = section_dict.get(current_section, {})\n",
    "                else:\n",
    "                    current_section_dict = root_properties.get(current_section, {})\n",
    "                    \n",
    "                # Converte par dictionário se for lista\n",
    "                if isinstance(current_section_dict, list):\n",
    "                    current_section_dict = {i: item for i, item in enumerate(current_section_dict)}\n",
    "                    if section:\n",
    "                        section_dict[current_section] = current_section_dict\n",
    "                    else:\n",
    "                        root_properties[current_section] = current_section_dict\n",
    "                        \n",
    "                subsection_dict = current_section_dict.setdefault(subsection, {})\n",
    "                \n",
    "                # Retira valores da próxima div irmã que contém classe layout-cell-11\n",
    "                sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-11\")\n",
    "                if sibling_cell:\n",
    "                    cell_values = extract_data_from_cell(sibling_cell)\n",
    "                    subsection_dict[cell_key] = cell_values\n",
    "\n",
    "            # Code for headers\n",
    "            title = child.find(\"h1\")\n",
    "            if title:\n",
    "                current_section = title.get_text().strip()\n",
    "                new_dict = {}\n",
    "                if section:\n",
    "                    section_dict[current_section] = new_dict\n",
    "                else:\n",
    "                    root_properties[current_section] = new_dict\n",
    "                \n",
    "                traverse_up(child, new_dict, current_section, root_properties)\n",
    "            else:\n",
    "                traverse_up(child, parent_dict, current_section, root_properties)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4.element import Tag\n",
    "\n",
    "def traverse_bottom(soup, parent_dict, current_section=None, root_properties=None):\n",
    "    \"\"\"\n",
    "    Traverses through the soup object recursively and populates the dictionary.\n",
    "    root_properties: the 'Properties' dictionary at the root level, where all attributes should be stored.\n",
    "    current_section: the current section being processed, allows to set subsections.\n",
    "    \"\"\"\n",
    "    section = None\n",
    "    subsection = None\n",
    "    section_dict = {}\n",
    "    subsection_dict = {}\n",
    "    \n",
    "    # Existing code for name extraction\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    if node_name is not None:\n",
    "        parent_dict['name'] = node_name\n",
    "\n",
    "    # Existing code for paragraph extraction\n",
    "    parag_elements = soup.find_all('p')\n",
    "    for elem in parag_elements:\n",
    "        class_name = elem.get('class', [None])[0]\n",
    "        text_content = elem.get_text()\n",
    "        if class_name:\n",
    "            parent_dict[class_name] = text_content\n",
    "\n",
    "    # Initialization of root_properties\n",
    "    if root_properties is None:\n",
    "        root_properties = parent_dict.setdefault('Properties', {})\n",
    "    \n",
    "    for child in soup.children:\n",
    "        if isinstance(child, Tag):\n",
    "            ## Segmento para extrair os dados de JCR dos artigos completos em periódicos\n",
    "            # if child.get('id') == 'artigos-completos':\n",
    "            #     section_elem = child.find_previous_sibling(\"div\", class_=\"inst_back\")          \n",
    "            #     if section_elem:\n",
    "            #         section = section_elem.get_text().strip()\n",
    "            #         section_dict = root_properties.setdefault(section, {})\n",
    "\n",
    "            #     subsection_elem = child.find('b')\n",
    "            #     if subsection_elem:\n",
    "            #         subsection = subsection_elem.get_text().strip()\n",
    "            #         if section:  \n",
    "            #             subsection_dict = section_dict.setdefault(subsection, {})\n",
    "\n",
    "            #     jcr_articles_dict = parse_jcr_articles(soup)\n",
    "            #     if jcr_articles_dict:\n",
    "            #         if subsection:  \n",
    "            #             subsection_dict.update(jcr_articles_dict['Properties'])\n",
    "\n",
    "            ## Segmento para extrair até Produções (não tem subseções)\n",
    "            if \"layout-cell-3\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "                cell_key = extract_data_from_cell(child)\n",
    "                if cell_key:\n",
    "                    cell_key = ' '.join(cell_key)\n",
    "\n",
    "                    sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-9\")\n",
    "                    if sibling_cell:\n",
    "                        cell_values = extract_data_from_cell(sibling_cell)\n",
    "\n",
    "                        if current_section:\n",
    "                            root_properties[current_section][cell_key] = cell_values\n",
    "                        else:\n",
    "                            root_properties[cell_key] = cell_values\n",
    "\n",
    "            ## Segmento para extrair de Produções até o final\n",
    "            elif \"layout-cell-1\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "                section_elem = child.findParent().findParent().find_previous_sibling(\"div\", class_=\"inst_back\")\n",
    "                if section_elem:\n",
    "                    section = section_elem.get_text().strip()\n",
    "                    section_dict = root_properties.setdefault(section, {})\n",
    "                elif section_elem == None:\n",
    "                    list_sub = ['Dissertação de mestrado','Tese de doutorado']\n",
    "                    subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                    if subsection_elem:\n",
    "                        if subsection_elem.get_text().strip() in list_sub:\n",
    "                            section_elem = child.find_previous_sibling(\"div\", class_=\"inst_back\")\n",
    "                    if section_elem:\n",
    "                        section = section_elem.get_text().strip()\n",
    "                        section_dict = root_properties.setdefault(section, {})                \n",
    "                else:\n",
    "                    print('Erro ao seção')     \n",
    "\n",
    "                subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                if subsection_elem:\n",
    "                    subsection = subsection_elem.get_text().strip()\n",
    "                elif subsection_elem == None:\n",
    "                    subsection_elem = child.findParent().find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                    if subsection_elem:\n",
    "                        subsection = subsection_elem.get_text().strip()                \n",
    "                else:\n",
    "                    print('Erro ao definir subseção')\n",
    "                \n",
    "                cell_key = extract_data_from_cell(child)\n",
    "                if cell_key:\n",
    "                    cell_key = ' '.join(cell_key)    \n",
    "\n",
    "                if section:\n",
    "                    current_section_dict = section_dict.get(current_section, {})\n",
    "                else:\n",
    "                    current_section_dict = root_properties.get(current_section, {})\n",
    "                    \n",
    "                # Converte par dictionário se for lista\n",
    "                if isinstance(current_section_dict, list):\n",
    "                    current_section_dict = {i: item for i, item in enumerate(current_section_dict)}\n",
    "                    if section:\n",
    "                        section_dict[current_section] = current_section_dict\n",
    "                    else:\n",
    "                        root_properties[current_section] = current_section_dict\n",
    "                        \n",
    "                subsection_dict = current_section_dict.setdefault(subsection, {})\n",
    "                \n",
    "                # Retira valores da próxima div irmã que contém classe layout-cell-11\n",
    "                sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-11\")\n",
    "                if sibling_cell:\n",
    "                    cell_values = extract_data_from_cell(sibling_cell)\n",
    "                    subsection_dict[cell_key] = cell_values\n",
    "\n",
    "            # Code for headers\n",
    "            title = child.find(\"h1\")\n",
    "            if title:\n",
    "                current_section = title.get_text().strip()\n",
    "                new_dict = {}\n",
    "                if section:\n",
    "                    section_dict[current_section] = new_dict\n",
    "                else:\n",
    "                    root_properties[current_section] = new_dict\n",
    "                \n",
    "                traverse_bottom(child, new_dict, current_section, root_properties)\n",
    "            else:\n",
    "                traverse_bottom(child, parent_dict, current_section, root_properties)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_empty_list(value):\n",
    "#     return isinstance(value, list) and not value\n",
    "\n",
    "# def convert_to_hashable(value):\n",
    "#     if is_empty_list(value):\n",
    "#         return None\n",
    "#     elif isinstance(value, list):\n",
    "#         return tuple(value)\n",
    "#     return value\n",
    "\n",
    "# def print_dictionary(dictionary, indent=0):\n",
    "#     \"\"\"\n",
    "#     Recursively prints nested dictionaries.\n",
    "\n",
    "#     Parameters:\n",
    "#         dictionary (dict): The dictionary to print.\n",
    "#         indent (int): The current indentation level for nested dictionaries.\n",
    "#     \"\"\"\n",
    "#     for key, value in dictionary.items():\n",
    "#         if isinstance(value, dict):\n",
    "#             print('  ' * indent + str(key) + ': ')\n",
    "#             print_dictionary(value, indent + 1)\n",
    "#         else:\n",
    "#             print('  ' * indent + str(key) + ': ' + str(value))\n",
    "\n",
    "# def append_to_key(dictionary, key, value):\n",
    "#     \"\"\"Appends a value to a list associated with a given key in a dictionary.\n",
    "#     If the key does not exist, a new list is created and the value is appended to it.\n",
    "#     \"\"\"\n",
    "#     if key in dictionary:\n",
    "#         if isinstance(dictionary[key], list):\n",
    "#             dictionary[key].append(value)\n",
    "#         else:\n",
    "#             dictionary[key] = [dictionary[key], value]\n",
    "#     else:\n",
    "#         dictionary[key] = [value]\n",
    "\n",
    "# def find_sibling_cell(cell):\n",
    "#     # Your logic here to find sibling cells\n",
    "#     return None\n",
    "\n",
    "# def find_main_cell(soup):\n",
    "#     main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     return main_cell\n",
    "\n",
    "# def traverse(child, parent_dict, current_section=None, root_properties=None):\n",
    "#     if root_properties is None:\n",
    "#         root_properties = {}\n",
    "    \n",
    "#     new_dict = {\"Label\": None, \"Properties\": {}}\n",
    "#     cell_key = convert_to_hashable(extract_data_from_cell(child))\n",
    "    \n",
    "#     if cell_key is not None:\n",
    "#         cell_values = extract_data_from_cell(find_sibling_cell(child))\n",
    "#         parent_dict[cell_key] = convert_to_hashable(cell_values)\n",
    "        \n",
    "#     for sub_child in child.find_all_next():\n",
    "#         traverse(sub_child, new_dict, current_section, root_properties)\n",
    "        \n",
    "#     root_properties[current_section] = merge_properties_ordered(root_properties.get(current_section, {}), new_dict)\n",
    "#     return root_properties\n",
    "\n",
    "# def parse_soup(soup):\n",
    "#     main_cell = find_main_cell(soup)\n",
    "#     merged_dict = {\"Label\": \"Person\", \"Properties\": {}}\n",
    "#     if main_cell:\n",
    "#         parsed_data = traverse(main_cell, merged_dict)\n",
    "#         return merge_properties_ordered(merged_dict, parsed_data)\n",
    "#     return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_properties_ordered(dic1, dic2):\n",
    "    from collections import OrderedDict\n",
    "    \n",
    "    \"\"\"Merges the 'Properties' subdictionaries from dic1 and dic2 into a new ordered dictionary.\"\"\"\n",
    "    merged_dict = OrderedDict()\n",
    "    merged_dict['Label'] = 'Person'\n",
    "    merged_dict['name'] = dic1.get('name', '')\n",
    "    merged_dict['Properties'] = OrderedDict()\n",
    "\n",
    "    # Explicitly extract the 'Properties' subdictionaries from dic1 and dic2\n",
    "    prop1 = dic1.get('Properties', {})\n",
    "    prop2 = dic2.get('Properties', {})\n",
    "    \n",
    "    # Iterate over the keys of dic1 first to preserve their order\n",
    "    for key in prop1.keys():\n",
    "        merged_dict['Properties'][key] = prop1[key]\n",
    "\n",
    "    # Next, iterate over the keys of dic2, updating values and potentially inserting new keys\n",
    "    for key in prop2.keys():\n",
    "        val1 = merged_dict['Properties'].get(key)\n",
    "        val2 = prop2[key]\n",
    "        \n",
    "        # Merge logic: concatenate if both are lists, otherwise use the value from dic2\n",
    "        if isinstance(val1, list) and isinstance(val2, list):\n",
    "            merged_dict['Properties'][key] = val1 + val2\n",
    "        else:\n",
    "            merged_dict['Properties'][key] = val2\n",
    "\n",
    "    return merged_dict\n",
    "\n",
    "def parse_soup(soup):\n",
    "    resume_dict   = parse_resume(soup)\n",
    "    articles_dict = {\"Label\": \"Person\", \"name\":{}, \"Properties\": {}}\n",
    "    articles_dict = parse_jcr_articles(soup)\n",
    "    merged_dict   = merge_properties_ordered(resume_dict, articles_dict)\n",
    "\n",
    "    main_cell     = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    generic_dict = {\"Label\": \"Person\", \"Properties\": {}}\n",
    "    if main_cell:\n",
    "        traverse_bottom(main_cell, generic_dict)\n",
    "\n",
    "    merged_dict = merge_properties_ordered(merged_dict, generic_dict)\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = parse_soup(soup)\n",
    "print(data_dict.keys())\n",
    "print(data_dict['Properties'].keys())\n",
    "print(data_dict['Properties']['Orientações'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['Properties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_subsection(section_title, parent_dict, data):\n",
    "#     sub_key = f\"Produções-{section_title}\"\n",
    "#     parent_dict[sub_key] = data\n",
    "\n",
    "# def traverse(soup, parent_dict, current_section=None, root_properties=None):\n",
    "#     \"\"\"\n",
    "#     Traverses through the soup object recursively and populates the dictionary.\n",
    "#     root_properties: the 'Properties' dictionary at the root level, where all attributes should be stored.\n",
    "#     current_section: the current section being processed, allows to set subsections.\n",
    "#     \"\"\"\n",
    "#     subsections = ['Artigos completos publicados em periódicos',\n",
    "#                    'Livros publicados/organizados ou edições',\n",
    "#                    'Trabalhos completos publicados em anais de congressos',\n",
    "#                    'Trabalhos de conclusão de curso de graduação',\n",
    "#                    'Participação em eventos, congressos, exposições e feiras',\n",
    "#                    'Trabalho de conclusão de curso de graduação'\n",
    "#                    ]\n",
    "\n",
    "#     if root_properties is None:\n",
    "#         root_properties = parent_dict.setdefault('Properties', {})\n",
    "\n",
    "#     for child in soup.children:\n",
    "#         if isinstance(child, Tag):\n",
    "#             # Additional case for JCR articles extraction\n",
    "#             if child.get('id') == 'artigos-completos':\n",
    "#                 jcr_articles_dict = parse_jcr_articles(soup)\n",
    "#                 if jcr_articles_dict:\n",
    "#                     root_properties.update(jcr_articles_dict['Properties'])\n",
    "\n",
    "#             # Standard extraction for cells\n",
    "#             elif \"layout-cell-3\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "#                 cell_key = extract_data_from_cell(child)\n",
    "#                 if cell_key:\n",
    "#                     cell_key = ' '.join(cell_key)\n",
    "\n",
    "#                     sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-9\")\n",
    "#                     if sibling_cell:\n",
    "#                         cell_values = extract_data_from_cell(sibling_cell)\n",
    "#                         if current_section:\n",
    "#                             root_properties[current_section][cell_key] = cell_values\n",
    "#                         else:\n",
    "#                             root_properties[cell_key] = cell_values\n",
    "\n",
    "#             # Non-Standard extraction\n",
    "#             elif \"layout-cell-1\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "#                 cell_key = extract_data_from_cell(child)\n",
    "#                 if cell_key:\n",
    "#                     cell_key = ' '.join(cell_key)\n",
    "\n",
    "#                     sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-11\")\n",
    "#                     if sibling_cell:\n",
    "#                         cell_values = extract_data_from_cell(sibling_cell)\n",
    "#                         clean_values = [x.replace('\\n\\t\\t\\t\\t\\t\\t\\t',' ') for x in cell_values]\n",
    "\n",
    "#                         if current_section:\n",
    "#                             if isinstance(root_properties[current_section], list):\n",
    "#                                 converted_dict = {i: item for i, item in enumerate(root_properties[current_section])}\n",
    "#                                 root_properties[current_section] = converted_dict\n",
    "#                             root_properties[current_section][cell_key] = cell_values\n",
    "#                         else:\n",
    "#                             root_properties[cell_key] = clean_values\n",
    "\n",
    "#             # Handling headers to define sections\n",
    "#             title = child.find(\"h1\")\n",
    "#             if title:\n",
    "#                 current_section = title.get_text().strip()\n",
    "#                 new_dict = {}\n",
    "#                 root_properties[current_section] = new_dict\n",
    "#                 traverse(child, new_dict, current_section, root_properties)\n",
    "\n",
    "#             # Detecting the subsection\n",
    "#             subsection = child.find_previous(\"div\", class_=\"cita-artigos\").get_text().strip() if child.find_previous(\"div\", class_=\"cita-artigos\") else None\n",
    "#             if subsection in subsections:\n",
    "#                 print(subsection)\n",
    "\n",
    "#                 if isinstance(root_properties.get(current_section), list):\n",
    "#                     root_properties[current_section] = {i: item for i, item in enumerate(root_properties[current_section])}\n",
    "                    \n",
    "#                 new_key = f\"Produções-{subsection}\"\n",
    "\n",
    "#                 if current_section and isinstance(root_properties.get(current_section), dict):\n",
    "#                     # Check if subsection exists before attempting to pop it\n",
    "#                     if subsection in root_properties[current_section]:\n",
    "#                         root_properties[new_key] = root_properties[current_section].pop(subsection)\n",
    "#                 elif not current_section:\n",
    "#                     print(\"current_section is not defined.\")\n",
    "#                 else:\n",
    "#                     print(f\"Unexpected type for root_properties[{current_section}]: {type(root_properties[current_section])}\")\n",
    "            \n",
    "#             else:\n",
    "#                 traverse(child, parent_dict, current_section, root_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(soup): \n",
    "    # Extração de todas as classes no documento, armazenadas em uma lista\n",
    "    all_classes = [value for element in soup.find_all(class_=True) for value in element[\"class\"]]\n",
    "    \n",
    "    # Contagem de ocorrências de cada classe\n",
    "    class_count = Counter(all_classes)\n",
    "    \n",
    "    # Conversão do objeto Counter para um dicionário padrão para facilitar a manipulação subsequente\n",
    "    class_dict = dict(class_count)\n",
    "    \n",
    "    return class_dict\n",
    "\n",
    "def list_divs(soup):\n",
    "    div_elements = soup.find_all('div')\n",
    "    unique_classes = set()\n",
    "\n",
    "    for div in div_elements[1:]:\n",
    "        div_id = div.get('id', 'N/A')\n",
    "        class_list = div.get('class')\n",
    "        print(f'{div_id} {class_list}')\n",
    "        \n",
    "        if class_list:\n",
    "            for i in class_list:\n",
    "                cont = div.find('div',{'class': i})\n",
    "                try:\n",
    "                    text = cont.text.strip().replace('/n/n/n','/n').replace('/n/n','/n')\n",
    "                    print(f'  {text}')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            print('-'*50)\n",
    "            unique_classes.update(class_list)\n",
    "    \n",
    "    return unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_divs_with_hierarchy(tag, indent_level=0, verbose=False):\n",
    "    indent = \" \" * indent_level * 4  # Four spaces for each level of indentation\n",
    "    div_elements = tag.find_all('div', recursive=False)  # Find direct children only\n",
    "    unique_classes = set()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"{indent}Inspecting level {indent_level}, found {len(div_elements)} divs.\")\n",
    "\n",
    "    for div in div_elements:\n",
    "        class_list = div.get('class')\n",
    "        if class_list:\n",
    "            unique_classes.update(class_list)\n",
    "            print(f\"{indent} {', '.join(class_list)}\")\n",
    "        else:\n",
    "            print(f\"{indent} None\")\n",
    "\n",
    "        # Recursive call to explore the children of this div\n",
    "        list_divs_with_hierarchy(div, indent_level + 1)\n",
    "\n",
    "    if indent_level == 0:  # Print unique classes only once, at the end of the initial call\n",
    "        print(\"\\nUnique classes:\")\n",
    "        for i in unique_classes:\n",
    "            print(f'    {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contagem de classes únicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize BeautifulSoup with a sample HTML content\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_classes(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_divs(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achar atributos de classes e divs definidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# keys_dicts = soup.find_all('div', {'class':'layout-cell-pad-5'})\n",
    "# for i in keys_dicts:\n",
    "#     pprint(i.attrs)\n",
    "#     text = i.get_text().replace('\\n\\n\\n\\n','').replace('\\t\\t\\n\\t','').replace('\\t\\t\\t','').strip()\n",
    "#     pprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achar classe específica (dicionário class) em elemento (div)\n",
    "# from pprint import pprint\n",
    "\n",
    "# # query0 = soup.select_one('div', {'class':'layout-cell-1'}).attrs\n",
    "# keys_dicts = soup.select('div', {'class':'text-align-right'})\n",
    "# for i in keys_dicts:\n",
    "#     pprint(i.attrs)\n",
    "#     pprint(i.get_text().replace('\\n\\n\\n\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achar classe específica (dicionário class) em elemento (div)\n",
    "# from pprint import pprint\n",
    "\n",
    "# # query0 = soup.select_one('div', {'class':'layout-cell-1'}).attrs\n",
    "# keys_dicts = soup.find_all('div', {'class':'text-align-right'})\n",
    "# for i in keys_dicts:\n",
    "#     pprint(i.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar a hierarquia com identação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "# body_tag = soup.body\n",
    "\n",
    "# # Then try calling list_divs_with_hierarchy on the <body> tag\n",
    "# list_divs_with_hierarchy(body_tag, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicial_element = soup.select_one('div', {'class':'title-wrapper'})\n",
    "# inicial_element = soup.find('div', {'class':'layout-cell-pad-main'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_divs_with_hierarchy(inicial_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar atributos de elemento e classe especificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query1 = soup.select_one('img', {'class':'foto'}).attrs\n",
    "# pprint(query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Extrair dados e seções específicas\n",
    "\n",
    "- extrair_dados(soup, verbose=False)\n",
    "- extrair_artigos(soup, verbose=False)\n",
    "- extrair_orientacoes(soup, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume(soup):\n",
    "    # <h2 class=\"nome\" tabindex=\"0\">Antonio Marcos Aires Barbosa</h2>\n",
    "    name_element = soup.find('h2', {'class': 'nome'})\n",
    "    node_title = name_element.text if name_element else \"Unknown\"\n",
    "    found_list = []\n",
    "    nodes = {}\n",
    "    properties = {}\n",
    "    recurrent_div = soup.find_all('div', {'title-wrapper'})\n",
    "\n",
    "    for div in recurrent_div:\n",
    "        parag_element = div.find('p')\n",
    "\n",
    "        if parag_element:\n",
    "            key   = parag_element.get('class')[0]\n",
    "            value = parag_element.text.strip('\\n')\n",
    "            properties[f'{key}'] = value\n",
    "            found_list.append(properties)\n",
    "            \n",
    "    nodes[node_title] = properties\n",
    "                       \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_resume(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "node_name = node_name_element.text if node_name_element else None\n",
    "node_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair estrutura de árvore recursivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "import json\n",
    "\n",
    "def extract_tree_structure(soup, level=0):\n",
    "    \"\"\" \n",
    "    Extrai recursivamente a estrutura de árvore de um objeto soup.\n",
    "    \n",
    "    Parâmetros:\n",
    "        soup (bs4.BeautifulSoup | bs4.Tag): O objeto soup ou tag para explorar.\n",
    "        level (int): Nível atual da árvore para fins de aninhamento.\n",
    "        \n",
    "    Retorna:\n",
    "        dict: Dicionário que representa a estrutura da árvore.\n",
    "    \"\"\"\n",
    "    \n",
    "    if soup is None:\n",
    "        return None\n",
    "    \n",
    "    tree_structure = {}\n",
    "    \n",
    "    if isinstance(soup, Tag):\n",
    "        tree_structure['name'] = soup.name\n",
    "        tree_structure['classes'] = soup.get('class', [])\n",
    "    \n",
    "    children_structure = []\n",
    "    for child in soup.children:\n",
    "        if isinstance(child, Tag):\n",
    "            child_structure = extract_tree_structure(child, level + 1)\n",
    "            children_structure.append(child_structure)\n",
    "    \n",
    "    if children_structure:\n",
    "        tree_structure['children'] = children_structure\n",
    "    \n",
    "    return tree_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicial_element = soup.find('div',{'class': 'content-wrapper'})\n",
    "# tree_structure = extract_tree_structure(inicial_element)\n",
    "# print(json.dumps(tree_structure, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def extract_texts(soup):\n",
    "#     json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "    \n",
    "#     # Extract name from 'h2' tag as the first entry of the main dictionary\n",
    "#     name_tag = soup.find('h2', {'class': 'nome'})\n",
    "#     if name_tag:\n",
    "#         json_data['Properties']['name'] = name_tag.get_text().strip()\n",
    "    \n",
    "#     main_layout_cells = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "#     for main_cell in main_layout_cells:\n",
    "#         current_section = None\n",
    "#         current_subsection = None\n",
    "#         current_subtit = None\n",
    "\n",
    "#         # Handle 'h1' tags for sections\n",
    "#         h1_tags = main_cell.find_all('h1')\n",
    "#         for h1_tag in h1_tags:\n",
    "#             current_section = h1_tag.get_text().strip()\n",
    "#             json_data['Properties'].setdefault(current_section, {})\n",
    "        \n",
    "#         # Handle 'inst_back' tags for subsections\n",
    "#         inst_back_tags = main_cell.find_all('div', {'class': 'inst_back'})\n",
    "#         for inst_back_tag in inst_back_tags:\n",
    "#             current_subsection = inst_back_tag.get_text().strip()\n",
    "#             if current_section:\n",
    "#                 json_data['Properties'][current_section].setdefault(current_subsection, {})\n",
    "\n",
    "#         # # Handle 'subtit-1' tags for sub-subsections\n",
    "#         # subtit_tags = main_cell.find_all('div', {'class': 'subtit-1'})\n",
    "#         # for subtit_tag in subtit_tags:\n",
    "#         #     current_subtit = subtit_tag.get_text().strip()\n",
    "#         #     if current_section and current_subsection:\n",
    "#         #         json_data['Properties'][current_section][current_subsection].setdefault(current_subtit, {})\n",
    "        \n",
    "#         # Handle key-value pairs\n",
    "#         keys = main_cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "#         vals = main_cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "#         for key_elem, val_elem in zip(keys, vals):\n",
    "#             key_text = key_elem.get_text().strip()\n",
    "#             val_text = val_elem.get_text().strip()\n",
    "\n",
    "#             if current_subtit and current_subsection and current_section:\n",
    "#                 json_data['Properties'][current_section][current_subsection][current_subtit][key_text] = val_text\n",
    "#             elif current_subsection and current_section:\n",
    "#                 json_data['Properties'][current_section][current_subsection][key_text] = val_text\n",
    "#             elif current_section:\n",
    "#                 json_data['Properties'][current_section][key_text] = val_text\n",
    "\n",
    "#     return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_texts(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from py2neo import Node\n",
    "\n",
    "def list_secoes(soup, tag):\n",
    "    elements = soup.find_all(tag, {'tabindex': '0'})\n",
    "    for elem in elements:\n",
    "        text = elem.get_text().strip()\n",
    "        node = Node(\"H1Element\", text=text)\n",
    "        print(f'{text}')\n",
    "\n",
    "def list_subsection(soup, classe):\n",
    "    class_elements = soup.find_all('div', {classe})\n",
    "    for elem in class_elements:\n",
    "        class_text = elem.get_text().strip()\n",
    "        class_node = Node(\"Class_Element\", text=class_text)\n",
    "        print(f'{class_text}')\n",
    "\n",
    "def list_b(soup):\n",
    "    # Find all 'b' elements without filtering by tabindex\n",
    "    b_elements = soup.find_all('b')\n",
    "    for elem in b_elements:\n",
    "        # Retrieve the text content within the 'b' element and strip any leading/trailing white space\n",
    "        b_text = elem.get_text().strip()\n",
    "\n",
    "        # Locate the parent 'div' of the current 'b' element\n",
    "        parent_div = elem.find_parent('div')\n",
    "        \n",
    "        # Initialize variables to store class and id attributes\n",
    "        parent_div_class = None\n",
    "        parent_div_id = None\n",
    "\n",
    "        # Retrieve the class and id attributes if they exist\n",
    "        if parent_div is not None:\n",
    "            parent_div_class = parent_div.get('class', [None])[0]  # Retrieve the first class name if multiple are present\n",
    "            # parent_div_id = parent_div.get('id', None)  # Retrieve the id attribute\n",
    "\n",
    "        # The Node object creation step has been retained for further use\n",
    "        b_node = Node(\"BElement\", text=b_text)\n",
    "        \n",
    "        # Print the extracted text, along with the parent div's class and id\n",
    "        print(f'Text: {b_text:40} | Div: {parent_div_class}')\n",
    "\n",
    "def list_ascendants(soup, tag):\n",
    "    # Find all 'b' elements\n",
    "    b_elements = soup.find_all(tag)\n",
    "    for elem in b_elements:\n",
    "        # Retrieve the text content within the 'b' element, stripping any leading/trailing white space\n",
    "        b_text = elem.get_text().strip()\n",
    "\n",
    "        # Locate the parent element of the current 'b' element\n",
    "        parent_elem = elem.find_parent()\n",
    "        \n",
    "        # Locate the grandparent element of the current 'b' element\n",
    "        grandparent_elem = parent_elem.find_parent() if parent_elem is not None else None\n",
    "\n",
    "        # Retrieve the class names of parent and grandparent elements, returning None if not found\n",
    "        parent_class_name = parent_elem.get('class', [None])[0] if parent_elem is not None else None\n",
    "        grandparent_class_name = grandparent_elem.get('class', [None])[0] if grandparent_elem is not None else None\n",
    "\n",
    "        # Create a Node object for further use\n",
    "        b_node = Node(\"BElement\", text=b_text)\n",
    "\n",
    "        # Print the extracted text along with the class names of the parent and grandparent elements\n",
    "        print(f'{grandparent_class_name} | {parent_class_name} | Text: {b_text}')\n",
    "\n",
    "def parse_h1(soup, parent_node, graph):\n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    for elem in h1_elements:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <h2 class=\"nome\" tabindex=\"0\">Antonio Marcos Aires Barbosa</h2>\n",
    "name_element = soup.find('h2', {'class': 'nome'})\n",
    "node_title = name_element.text if name_element else \"Unknown\"\n",
    "print(node_title)\n",
    "found_list = []\n",
    "nodes = {}\n",
    "properties = {}\n",
    "recurrent_div = soup.find_all('div', {'title-wrapper'})\n",
    "# print(len(recurrent_div))\n",
    "\n",
    "print('Valores de p')\n",
    "for div in recurrent_div:\n",
    "    parag_element = div.find('p')\n",
    "    if parag_element:\n",
    "        key   = parag_element.get('class', [None])[0]\n",
    "        value = parag_element.get_text().split('\\n')        \n",
    "        print(f'{key:22}: {value}')\n",
    "\n",
    "    sections = div.find_all('inst_back')\n",
    "    for section in sections:\n",
    "        title_element = section.get_text().strip()\n",
    "        if title_element:\n",
    "            print('\\nValores de inst_back')\n",
    "            print(len(title_element))\n",
    "\n",
    "for n,div in enumerate(recurrent_div):\n",
    "    print('\\nValores de h1')            \n",
    "    section_element = div.find('h1')\n",
    "    if section_element and n>0:\n",
    "        key = section_element.get_text().strip()\n",
    "        value = section_element.find_next().get_text().split('\\n\\n')\n",
    "        print(f'{key:22}: {value}')\n",
    "\n",
    "print('\\nValores de b')\n",
    "for div in recurrent_div:\n",
    "    subsections = div.find_all('b')\n",
    "    for element in subsections:\n",
    "        if element:\n",
    "            key = element.get_text().strip()\n",
    "            filtered_list = [x.strip() for x in element.find_next().get_text().split('\\n') if x.strip()]\n",
    "            value = filtered_list\n",
    "            print(f'{key:22}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "def extract_sections(soup):\n",
    "    master_dict = {}  # Initialize an empty dictionary to store the hierarchical data\n",
    "    \n",
    "    # Loop through sections designated by 'title-wrapper'\n",
    "    for section_elem in soup.find_all('div', {'class': 'title-wrapper'}):\n",
    "        section_title_elem = section_elem.find('h1')\n",
    "        section_title = section_title_elem.get_text().strip() if section_title_elem else None\n",
    "        \n",
    "        if section_title:\n",
    "            master_dict[section_title] = {}\n",
    "       \n",
    "            layouts = soup.find_all('div', {'class': ['layout-cell']})\n",
    "            print(len(layouts), 'células de layout')\n",
    "            # Loop to extract keys and values from specific classes\n",
    "            for layout in layouts:\n",
    "                subsection_title = layout.get_text().strip()\n",
    "                master_dict[section_title][subsection_title] = {}                    \n",
    "                tags_b = layout.find_all('b')\n",
    "                sections = layout.find_all('div', {'inst_back'})\n",
    "                subsections = layout.find_all('h1')\n",
    "                tags_a = layout.find_all('a')\n",
    "                cell05 = layout.find_all('div', {'layout-cell-pad-5 text-align-right'})\n",
    "                rights = layout.find_all('div', {'text-align-right'})\n",
    "                cell12 = layout.find_all('div', {'layout-cell-12'})\n",
    "                cell03 = layout.find_all('div', {'layout-cell-3'})\n",
    "                cell09 = layout.find_all('div', {'layout-cell-9'})\n",
    "                # print(len(tags_b), len(rights), len(sections), len(subsections), len(tags_a), len(cell12), len(cell05), len(cell03), len(cell09))\n",
    "\n",
    "                content_key = layout.get_text().replace('\\n', '').strip()\n",
    "                print('Classe:',layout.get('class', [None]))\n",
    "                print(' Dados:',content_key)\n",
    "                content_value = {}\n",
    "                master_dict[section_title][subsection_title][content_key] = content_value\n",
    "    return master_dict\n",
    "\n",
    "result_dict = extract_sections(soup)\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts(soup, element):\n",
    "    json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "\n",
    "    # Find the parent element that contains relevant sections\n",
    "    parent_element = element.find_parent() if element.find_parent() else soup\n",
    "    \n",
    "    current_section = None\n",
    "    current_subsection = None\n",
    "\n",
    "    # Iterate over each child of the parent element\n",
    "    for child in parent_element.children:\n",
    "        # Handling section tags (Level 1)\n",
    "        if child.name == 'h1':\n",
    "            current_section = child.get_text().strip()\n",
    "            json_data[\"Properties\"][current_section] = {}\n",
    "            current_subsection = None\n",
    "        # Handling subsection tags (Level 2)\n",
    "        elif child.get('class') == ['inst_back']:\n",
    "            if current_section:\n",
    "                current_subsection = child.find('b').get_text().strip()\n",
    "                json_data[\"Properties\"][current_section][current_subsection] = {}\n",
    "        # Handling layout cells for keys and values (Level 3)\n",
    "        elif child.get('class') == ['layout-cell']:\n",
    "            keys = child.find_all('div', {'class': 'layout-cell-3'}, limit=1)\n",
    "            vals = child.find_all('div', {'class': 'layout-cell-9'}, limit=1)\n",
    "            for key_elem, value_elem in zip(keys, vals):\n",
    "                key_text = key_elem.get_text().strip()\n",
    "                val_text = value_elem.get_text().strip()\n",
    "                target_dict = json_data[\"Properties\"]\n",
    "                if current_section:\n",
    "                    target_dict = target_dict[current_section]\n",
    "                    if current_subsection:\n",
    "                        target_dict = target_dict[current_subsection]\n",
    "                target_dict[key_text] = val_text\n",
    "                \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaves Dicionários Nível01: Seções, Nível02: Subseções, Nível03: Títulos\n",
    "def find_class_ascendants(soup,classe):\n",
    "    div_sec = soup.find_all('div',{'class':classe})\n",
    "    for i in div_sec:\n",
    "        level0 = i.find_parent().find_parent().find_parent().find_parent()\n",
    "        level1 = i.find_parent().find_parent().find_parent()\n",
    "        level2 = i.find_parent().find_parent()\n",
    "        level3 = i.find_parent()\n",
    "        print(f'{level0.name} {level0.get(\"class\", [None])}')\n",
    "        print(f'  {level1.name} {level1.get(\"class\", [None])}')\n",
    "        print(f'    {level2.name} {level2.get(\"class\", [None])}')\n",
    "        print(f'      {level3.name} {level3.get(\"class\", [None])}')\n",
    "        print(f'        {i.name} {i.get(\"class\", [None])}')\n",
    "        print(f'        {i.get_text().strip()}')\n",
    "\n",
    "def find_tag_ascendants(soup,tag):\n",
    "    div_sec = soup.find_all(tag)\n",
    "    for i in div_sec:\n",
    "        level0 = i.find_parent().find_parent().find_parent().find_parent()\n",
    "        level1 = i.find_parent().find_parent().find_parent()\n",
    "        level2 = i.find_parent().find_parent()\n",
    "        level3 = i.find_parent()\n",
    "        print(f'{level0.name} {level0.get(\"class\", [None])}')\n",
    "        print(f'  {level1.name} {level1.get(\"class\", [None])}')\n",
    "        print(f'    {level2.name} {level2.get(\"class\", [None])}')\n",
    "        print(f'      {level3.name} {level3.get(\"class\", [None])}')\n",
    "        print(f'        {i.name} {i.get(\"class\", [None])}')\n",
    "        print(f'        {i.get_text().strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_class_ascendants(soup, 'subtit-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão as chave de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'layout-cell-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão os valores de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'layout-cell-pad-5 text-align-right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão os valores de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'layout-cell-9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_tag_ascendants(soup,'h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_tag_ascendants(soup,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão os valores de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'subtit-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão os valores de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'inst_back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.select_one('layout-cell layout-cell-12 layout-cell-pad-main'))\n",
    "# print(soup.select_one('div.layout-cell.layout-cell-12'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaves Dicionários Nível 02 - Subseções\n",
    "div_tit = soup.find_all('h1')\n",
    "for i in div_tit:\n",
    "    tit_text = i.get_text().strip()\n",
    "    print(tit_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaves Dicionários Nível 02 - Subseções\n",
    "div_tit = soup.find_all('div', 'inst_back')\n",
    "for i in div_tit:\n",
    "    tit_text = i.find('b').get_text().strip()\n",
    "    print(tit_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    master_dict = {}\n",
    "    \n",
    "    # Extract sections\n",
    "    section_elements = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    for section_elem in section_elements:\n",
    "        section_title_elem = section_elem.find('h1')\n",
    "        section_title = section_title_elem.get_text().strip() if section_title_elem else None\n",
    "        \n",
    "        if section_title:\n",
    "            master_dict[section_title] = {}\n",
    "            \n",
    "            # Extract subsections\n",
    "            subsection_elements = section_elem.find_all('div', {'class': 'inst_back'})\n",
    "            for subsection_elem in subsection_elements:\n",
    "                subsection_title = subsection_elem.get_text().strip()\n",
    "                master_dict[section_title][subsection_title] = {}\n",
    "                \n",
    "                # Extract key-value pairs using classes you've defined for the last level\n",
    "                layout_cells = subsection_elem.find_parent().find_all('div', {'layout-cell'})\n",
    "                \n",
    "                for layout_cell in layout_cells:\n",
    "                    data_dict = extract_texts(layout_cell)\n",
    "                    master_dict[section_title][subsection_title] = data_dict\n",
    "                        \n",
    "    return master_dict\n",
    "\n",
    "result_dict = extract_data(soup)\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_sections(soup):\n",
    "    # Initialize an empty dictionary to hold the hierarchical data\n",
    "    master_dict = {}\n",
    "    \n",
    "    # Find all elements that are considered as sections\n",
    "    section_elements = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    for section_elem in section_elements:\n",
    "        # Extract section title if available\n",
    "        section_title_elem = section_elem.find('h1')\n",
    "        section_title = section_title_elem.get_text().strip() if section_title_elem else None\n",
    "        \n",
    "        if section_title:\n",
    "            master_dict[section_title] = {}\n",
    "            \n",
    "            # Extract subsections within each section\n",
    "            subsection_elements = section_elem.find_all('div', {'class': 'inst_back'})\n",
    "            for subsection_elem in subsection_elements:\n",
    "                subsection_title = subsection_elem.get_text().strip()\n",
    "                master_dict[section_title][subsection_title] = {}\n",
    "                \n",
    "                # Extract content within each subsection and store as key-value pairs\n",
    "                content_elements_par = subsection_elem.find_all('p')\n",
    "                content_elements_b = subsection_elem.find_all('b')\n",
    "                for content_elem in content_elements_par:\n",
    "                    content_key = content_elem.get('class')[0] if content_elem.get('class') else None\n",
    "                    content_value = content_elem.get_text().strip()\n",
    "                    \n",
    "                    if content_key and content_value:\n",
    "                        master_dict[section_title][subsection_title][content_key] = content_value\n",
    "\n",
    "                for content_elem in content_elements_b:\n",
    "                    content_key = content_elem.get('class')[0] if content_elem.get('class') else None\n",
    "                    content_value = content_elem.get_text().strip()\n",
    "                    \n",
    "                    if content_key and content_value:\n",
    "                        master_dict[section_title][subsection_title][content_key] = content_value\n",
    "                        \n",
    "    return master_dict\n",
    "\n",
    "result_dict = extract_sections(soup)\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = soup.find_all('div', {'title-wrapper'})\n",
    "\n",
    "for elem in elements:\n",
    "    par = elem.find('p')\n",
    "    if par:\n",
    "        key = par.get('class')[0]\n",
    "        val = par.get_text().strip()\n",
    "        print(f\"{key}: {val}\")  \n",
    "    \n",
    "    tit = elem.find('h1')\n",
    "    if tit:\n",
    "        section = tit.get_text()\n",
    "        if section != '':\n",
    "            key = tit.get_text().strip()\n",
    "            if tit.find_next('b'):\n",
    "                for i in tit.find_next('b'):\n",
    "                    val = i.get_text()\n",
    "                    if val.strip() != '':\n",
    "                        print(val)\n",
    "            lin = tit.find('b')\n",
    "            print(f\"{key:35}: {val}\")\n",
    "        subsect = soup.find('inst_back')\n",
    "        if subsect:\n",
    "            print(len(subsect))\n",
    "            print(f\"{subsect.get_text().strip()}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_secoes(soup,'h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subsection(soup, 'inst_back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ascendants(soup,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alldata(soup):\n",
    "    name_element = soup.find('h2', {'class': 'nome'})\n",
    "    node_title = name_element.text if name_element else \"Unknown\"\n",
    "    found_list = []\n",
    "    nodes = {}\n",
    "    properties = {}\n",
    "    recurrent_div = soup.find_all('div', {'title-wrapper'})\n",
    "\n",
    "    for div in recurrent_div:\n",
    "        parag_element = div.find('p')\n",
    "        title_element = div.find_all('inst_back')\n",
    "\n",
    "        if parag_element:\n",
    "            key   = parag_element.get('class')[0]\n",
    "            value = parag_element.text.strip('\\n')\n",
    "            properties[f'{key}'] = value\n",
    "            found_list.append(properties)\n",
    "\n",
    "        if title_element:\n",
    "            key   = title_element.get('class')\n",
    "            value = title_element.text.strip('\\n')\n",
    "            properties[f'{key}'] = value\n",
    "            found_list.append(properties)\n",
    "            \n",
    "    nodes[node_title] = properties\n",
    "                       \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(get_alldata(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes de extração de seções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_dados(soup, verbose=False):\n",
    "    # Step 1: Identify Node Name\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Node: {node_name}\")\n",
    "\n",
    "    # Step 2: Initialize JSON object\n",
    "    json_data = {\"Node\": node_name, \"Properties\": {}}\n",
    "    \n",
    "    # Step 3: Traverse to Find Sections\n",
    "    json_data = extrair_wraper(soup, json_data)\n",
    "\n",
    "    # Step 4: Information extraction: Bibliographic Production Section\n",
    "    producao_bibliografica_div = soup.find('div', {'id': 'artigos-completos'})\n",
    "    producoes = []\n",
    "    for artigo_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        artigo_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = artigo_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = artigo_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = artigo_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = artigo_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        artigo_dict['ano']     = ano\n",
    "        artigo_dict['autores'] = autores\n",
    "        artigo_dict['revista'] = revista\n",
    "        artigo_dict['titulo']  = titulo\n",
    "        artigo_dict['jcr']     = jcr\n",
    "        artigo_dict['doi']     = doi\n",
    "        producoes.append(artigo_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = producoes\n",
    "\n",
    "    projdevtec=[]\n",
    "    for projdevtec_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        projdevtec_dict = {}\n",
    "        projdevtec.append(projdevtec_dict)\n",
    "        json_data[\"Properties\"][\"Inovação\"][\"Projeto de desenvolvimento tecnológico\"] = projdevtec\n",
    "\n",
    "    # projpesq=[]\n",
    "    # for projpesq_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "    #     projpesq_dict = {}\n",
    "    \n",
    "    \n",
    "    #     projpesq.append(projdevtec_dict)\n",
    "    #     json_data[\"Properties\"][\"Inovação\"][\"Projeto de desenvolvimento tecnológico\"] = projpesq\n",
    "\n",
    "    orientacoes=[]\n",
    "    for orientacoes_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        orientacoes_dict = {}   \n",
    "        orientacoes.append(orientacoes_dict)\n",
    "        json_data[\"Properties\"][\"Orientações\"] = orientacoes\n",
    "\n",
    "    bancas=[]\n",
    "    for bancas_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        bancas_dict = {}\n",
    "        bancas.append(bancas_dict)\n",
    "        json_data[\"Properties\"][\"Bancas\"] = bancas\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_orientacoes(soup, verbose=False):\n",
    "    # Step 1: Identify Node Name\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Node: {node_name}\")\n",
    "\n",
    "    # Step 2: Initialize JSON object\n",
    "    json_data = {\"Node\": node_name, \"Properties\": {}}\n",
    "\n",
    "    # Step 3: Information extraction: Bibliographic Production Section\n",
    "    general_div = soup.find('div', {'class': 'Orientações e supervisões concluídas'})\n",
    "    if verbose:\n",
    "        if general_div:\n",
    "            print(f\"Qte.Divs: {len(general_div)}\")\n",
    "        else:\n",
    "            print('Nenhuma div encontrada com essa hierarquia')\n",
    "            return\n",
    "    orientacoes=[]\n",
    "    for selected_div in general_div.find_all('div', {'class': 'identity'}):\n",
    "        orientacoes_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = selected_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = selected_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = selected_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = selected_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = selected_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        \n",
    "        dados   = selected_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        str_autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        autores = split_authors(str_autores)\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        orientacoes_dict['ano']     = ano\n",
    "        orientacoes_dict['autores'] = autores\n",
    "        orientacoes_dict['revista'] = revista\n",
    "        orientacoes_dict['titulo']  = titulo\n",
    "        orientacoes_dict['jcr']     = jcr\n",
    "        orientacoes_dict['doi']     = doi\n",
    "        orientacoes.append(orientacoes_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = orientacoes\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "node_name = node_name_element.text if node_name_element else 'Unknown'\n",
    "\n",
    "text_contents = []\n",
    "elements_list = soup.find_all('b')\n",
    "print(len(elements_list))\n",
    "for i in elements_list:\n",
    "    # Check if text content matches the targeted string\n",
    "    if i.text == 'Orientações e supervisões concluídas':\n",
    "        # Retrieve the parent element\n",
    "        parent_element = i.find_parent()\n",
    "        \n",
    "        # Retrieve the name of the parent element\n",
    "        parent_name = parent_element.name if parent_element else 'Unknown'\n",
    "        \n",
    "        # Retrieve and store the text contents of all child elements of the parent\n",
    "        for child in parent_element.find_all(True):\n",
    "            text_contents.append(child.text)\n",
    "        \n",
    "        print(f\"Found in parent element: {parent_name}\")\n",
    "        print(\"Texts of all child elements:\", text_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in elements_list:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrair_orientacoes(soup, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Extrair do DOM Dados Identificação\n",
    "\n",
    "- parse_personinfo(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Initialize the Neo4j driver\n",
    "uri = \"bolt://localhost:7687\"\n",
    "n4j_driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "def create_unique_composite_constraint(tx, label, properties):\n",
    "    # Build the updated query string for creating the unique composite constraint\n",
    "    # Note: The 'properties' parameter is expected to be a list of property names.\n",
    "    prop_string = \", \".join([f\"n.{prop}\" for prop in properties])\n",
    "    constraint_name = \"_\".join([label.lower()] + [prop.lower() for prop in properties])\n",
    "    \n",
    "    query = (\n",
    "        f\"CREATE CONSTRAINT {constraint_name} \"\n",
    "        f\"IF NOT EXISTS FOR (n:{label}) REQUIRE ({prop_string}) IS NODE KEY\"\n",
    "    )\n",
    "    # Execute the transaction to create the constraint\n",
    "    tx.run(query)\n",
    "\n",
    "def create_or_update_node(tx, node_label, properties):\n",
    "    find_query = f\"MATCH (n:{node_label} {{name: $name}}) RETURN n\"\n",
    "    existing_nodes = list(tx.run(find_query, name=properties['name']))\n",
    "\n",
    "    if existing_nodes:\n",
    "        existing_node = existing_nodes[0]['n']\n",
    "        if existing_node['Last_Updated'] < properties['Last_Updated']:\n",
    "            update_query = (\n",
    "                f\"MATCH (n:{node_label} {{name: $name}}) \"\n",
    "                f\"SET n += $properties\"\n",
    "            )\n",
    "            tx.run(update_query, properties=properties, name=properties['name'])\n",
    "    else:\n",
    "        create_query = f\"CREATE (n:{node_label} $properties)\"\n",
    "        tx.run(create_query, properties=properties)\n",
    "\n",
    "def update(n4j_driver, person_properties, verbose=False):\n",
    "    with n4j_driver.session() as session:\n",
    "        try:\n",
    "            session.write_transaction(create_or_update_node, 'Person', person_properties)\n",
    "            if verbose:\n",
    "                print(\"Operation completed:\")\n",
    "                print(f\"{person_properties.values()} has been evaluated for creation or update.\")\n",
    "        except Exception as e:\n",
    "            pprint(f\"An error occurred: {e}\")\n",
    "\n",
    "def augment_node(tx, node_label, node_name, properties_to_add):\n",
    "    query = (\n",
    "        f\"MATCH (n:{node_label} {{name: $node_name}}) \"\n",
    "        f\"SET n += $properties_to_add \"\n",
    "        f\"RETURN n\"\n",
    "    )\n",
    "    result = tx.run(query, node_name=node_name, properties_to_add=properties_to_add)\n",
    "    return [record['n'] for record in result]\n",
    "\n",
    "def update_node_with_new_properties(n4j_driver, node_label, node_name, properties_to_add, verbose=False):\n",
    "    with n4j_driver.session() as session:\n",
    "        try:\n",
    "            updated_nodes = session.write_transaction(augment_node, node_label, node_name, properties_to_add)\n",
    "            if verbose:\n",
    "                print(\"Operation completed:\")\n",
    "                for node in updated_nodes:\n",
    "                    print(f\"Updated Node: {node}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Function for parsing resume and returning JSON\n",
    "def parse_resume(soup, verbose=False):\n",
    "    # Existing parsing logic...\n",
    "    # Assuming json_data contains the parsed data\n",
    "    json_data = {\"Node\": \"node_name\", \"Properties\": {\"property_1\": \"value_1\"}}\n",
    "\n",
    "    # Update the Node with new properties\n",
    "    update_node_with_new_properties(\n",
    "        n4j_driver, \n",
    "        'Person', \n",
    "        json_data['Node'], \n",
    "        json_data['Properties'], \n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar a restrição nome-data_atualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session and Transaction for creating unique composite constraint\n",
    "with n4j_driver.session() as session:\n",
    "    session.write_transaction(create_unique_composite_constraint, 'Person', ['name', 'Last_Updated'])\n",
    "\n",
    "# Close the Neo4j driver\n",
    "n4j_driver.close()\n",
    "\n",
    "# Log the completion of constraint creation\n",
    "print(\"Unique composite constraint has been successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_properties = parse_personinfo(soup)\n",
    "person_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_to_add = parse_resume(soup)\n",
    "\n",
    "# Invoke the function to update the node\n",
    "update_node_with_new_properties(n4j_driver, 'Person', 'John Doe', properties_to_add, verbose=True)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "n4j_driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(n4j_driver, person_properties, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '12/09/2023'\n",
    "# Assuming that the date string is in the format 'DD/MM/YYYY', parse it into a datetime object\n",
    "# Modify the format string according to the actual format of date_str\n",
    "parsed_date = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "\n",
    "# Convert the datetime object to ISO 8601 format (YYYY-MM-DD)\n",
    "iso_date = parsed_date.strftime('%Y-%m-%d')\n",
    "person_properties['Last_Updated'] = iso_date\n",
    "person_properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução da persistência em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or update publications in Neo4j based on the dictionary\n",
    "create_or_update_publications(graph, dict_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create or update publications in Neo4j based on the dictionary\n",
    "create_or_update_publications(graph, dict_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar Grafo em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração e Persistência dos elementos H1\n",
    "try:\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "except Exception as e:\n",
    "    print('Erro ao conectar ao Neo4j')\n",
    "    print(e)\n",
    "try:    \n",
    "    header_node = persist_to_neo4j(header_data)\n",
    "    print({type(header_node)})\n",
    "    parse_h1_elements(soup, header_node, graph)\n",
    "    cv_node, properties = parse_personinfo(soup)\n",
    "except Exception as e:\n",
    "    print('Erro ao persistir nó')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node(\"Curriculum\", \n",
    "     title=header_data['title'].split('(')[1].strip(')'), \n",
    "     meta_keywords=header_data['meta_keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes em desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div', {'class': 'layout-cell-pad-main'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "def list_divs(soup_element, indent=0):\n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return\n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):  # check if the child is an instance of Tag\n",
    "            # Check for 'div' tags and list classes\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')  # Default to 'None' if class is not present\n",
    "                print('  ' * indent + f\"div {class_name}\")\n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            # # Check for 'h1' tags\n",
    "            # elif child.name == 'h1':\n",
    "            #     print('  ' * indent + f\"h1 {child.text.strip()}\")\n",
    "            #     list_divs(child, indent)\n",
    "\n",
    "            # Check for 'h2' tags\n",
    "            elif child.name == 'h2':\n",
    "                print('  ' * indent + f\"h2 {child.text.strip()}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            # Check for 'a' tags\n",
    "            elif child.name == 'a':\n",
    "                print('  ' * indent + f\"a {child.text.strip()}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            # Check for 'ul' tags\n",
    "            elif child.name == 'ul':\n",
    "                print('  ' * indent + \"ul\")\n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            # Check for 'li' tags\n",
    "            elif child.name == 'li':\n",
    "                print('  ' * indent + \"li: \" + child.text.strip())\n",
    "                list_divs(child, indent + 1)\n",
    "                            \n",
    "            # Check for 'inst_back' as class in any tag\n",
    "            elif child.has_attr('class') and 'inst_back' in child['class']:\n",
    "                print('  ' * indent + f\"{child.name} inst_back\")\n",
    "                list_divs(child, indent + 1)\n",
    "            \n",
    "            # Check for 'b' tags\n",
    "            elif child.name == 'b':\n",
    "                print('  ' * indent + f\"b {child.text.strip()}\")\n",
    "                list_divs(child, indent)\n",
    "        else:\n",
    "            list_divs(child, indent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Inicia a travessia a partir da div encontrada\n",
    "list_divs(starting_div, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrando dados extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, indent=0):\n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return\n",
    "    \n",
    "    ignorar = [None, ' ','Baixar Currículo','Imprimir Currículo']\n",
    "    \n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')\n",
    "\n",
    "                # Checar classe 'layout-cell-pad-5' com os valores dos dicionários\n",
    "                if class_name == ['layout-cell-pad-5']:\n",
    "                    print('  ' * (indent + 1) + f\" val: {dividir(child.get_text())}\")                    \n",
    "                \n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            elif child.name == 'h2':\n",
    "                print('  ' * indent + f\"Node Name: {child.string}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'h1':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    print('  ' * indent + f\"Properties: {rotulo}\")\n",
    "                    list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'inst_back':\n",
    "                print('  ' * indent + f\"Subtitulo: {child.get_text()}\")\n",
    "                list_divs(child, indent + 1)\n",
    "                \n",
    "            elif child.name == 'inst_back':\n",
    "                print('  ' * indent + f\"Subtitulo: {child.get_text()}\")\n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            elif child.name == 'a':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    # print('  ' * indent + f\"Group: {rotulo}\")\n",
    "                    list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'b':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                print('  ' * indent + f\" key: {rotulo}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'p':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                print('  ' * indent + f\" val: {rotulo}\")\n",
    "                list_divs(child, indent)                \n",
    "            \n",
    "            # elif child.name == 'span':\n",
    "            #     rotulo = dividir(child.get_text())[0]\n",
    "            #     print('  ' * indent + f\" val: {rotulo}\")\n",
    "            #     list_divs(child, indent)   \n",
    "\n",
    "        else:\n",
    "            list_divs(child, indent)\n",
    "\n",
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Inicia a travessia a partir da div encontrada\n",
    "list_divs(starting_div, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ainda com problemas..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo chaves corretamente mas não valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_data(element, parent_dict, current_section=None):\n",
    "    key = None\n",
    "    \n",
    "    if isinstance(element, Tag):\n",
    "        if element.name == 'a' or element.name == 'h2':\n",
    "            current_section = element.text.strip()\n",
    "            parent_dict[current_section] = {}\n",
    "        \n",
    "        elif 'text-align-right' in element.get('class', []):\n",
    "            key = element.find('b').text.strip() if element.find('b') else None\n",
    "            if key and current_section:\n",
    "                parent_dict[current_section][key] = ''\n",
    "        \n",
    "        elif 'layout-cell-9 layout-cell-pad-5' in element.get('class', []):\n",
    "            value = element.find('a').text.strip() if element.find('a') else element.text.strip()\n",
    "            if value and current_section and key:\n",
    "                parent_dict[current_section][key] = value\n",
    "\n",
    "    if isinstance(element, Tag):\n",
    "        for child in element.children:\n",
    "            extract_data(child, parent_dict, current_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "initial_dict = {}\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Invoke the function\n",
    "extract_data(starting_div, initial_dict)\n",
    "initial_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, json_output):\n",
    "    stack = deque([json_output])\n",
    "    \n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            current_dict = stack[-1]\n",
    "            \n",
    "            if child.name == 'h1':\n",
    "                key = dividir(child.get_text())[0]\n",
    "                current_dict[key] = {}\n",
    "                stack.append(current_dict[key])\n",
    "                \n",
    "            elif child.name == 'b':\n",
    "                key = dividir(child.get_text())[0]\n",
    "                current_dict[key] = {}\n",
    "                stack.append(current_dict[key])\n",
    "                \n",
    "            elif child.name == 'div':\n",
    "                class_name = child.get('class', [])\n",
    "                \n",
    "                # Ensure the class matches exactly\n",
    "                if 'layout-cell-pad-5' in class_name:\n",
    "                    value = dividir(child.get_text())\n",
    "                    \n",
    "                    # Additional safety check\n",
    "                    if len(stack) > 1:\n",
    "                        parent_dict = stack[-2]\n",
    "                        parent_key = list(parent_dict.keys())[-1]\n",
    "                        parent_dict[parent_key] = value\n",
    "                        stack.pop()\n",
    "                    else:\n",
    "                        print(\"Warning: Stack size insufficient.\")\n",
    "\n",
    "            # Debugging information\n",
    "            print(f\"Stack size: {len(stack)}, Current dict keys: {current_dict.keys()}\")\n",
    "\n",
    "            # Continue recursion\n",
    "            list_divs(child, current_dict)\n",
    "            \n",
    "    if len(stack) > 1:\n",
    "        stack.pop()\n",
    "\n",
    "# Initialize empty dictionary\n",
    "json_output = {}\n",
    "\n",
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Assuming 'soup' contains your BeautifulSoup object\n",
    "list_divs(starting_div, json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Não monta os dicionários no Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, parent_dict):\n",
    "    stack = deque([(soup_element, parent_dict)])  # Initialize the stack with parent element and parent dictionary\n",
    "\n",
    "    while stack:\n",
    "        element, current_dict = stack.pop()  # Get the last tuple (element, dictionary) from the stack\n",
    "\n",
    "        for child in element.children:\n",
    "            if isinstance(child, Tag):\n",
    "                \n",
    "                if child.name == 'h1':\n",
    "                    key = dividir(child.get_text())[0]\n",
    "                    current_dict[key] = {}\n",
    "                    stack.append((child, current_dict[key]))  # Append child element and its dictionary to the stack\n",
    "                    \n",
    "                elif child.name == 'b':\n",
    "                    key = dividir(child.get_text())[0]\n",
    "                    current_dict[key] = {}\n",
    "                    stack.append((child, current_dict[key]))  # Append child element and its dictionary to the stack\n",
    "                \n",
    "                elif child.name == 'div':\n",
    "                    class_name = child.get('class', [])\n",
    "                    \n",
    "                    if 'layout-cell-pad-5' in class_name:\n",
    "                        value = dividir(child.get_text())\n",
    "                        if value:  # Only populate if value is non-empty\n",
    "                            current_dict.update(value)  # Add the value to the current dictionary\n",
    "                            \n",
    "                else:\n",
    "                    stack.append((child, current_dict))  # Append child element and current dictionary for other cases\n",
    "\n",
    "\n",
    "# Initialize empty dictionary\n",
    "json_output = {}\n",
    "\n",
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Assuming 'soup' contains your BeautifulSoup object\n",
    "list_divs(starting_div, json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, dados, indent=0):\n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return\n",
    "    \n",
    "    ignorar = [None, ' ', 'Baixar Currículo', 'Imprimir Currículo']\n",
    "    \n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')\n",
    "                if class_name == ['layout-cell-pad-5']:\n",
    "                    valor = dividir(child.get_text())\n",
    "                    # Adicionar valor ao dicionário\n",
    "                    dados[f'Value_Level_{indent}'] = valor\n",
    "                \n",
    "                list_divs(child, dados, indent + 1)\n",
    "            \n",
    "            elif child.name == 'h2':\n",
    "                # Adicionar nome do nó ao dicionário\n",
    "                dados[f'Node_Name'] = child.string\n",
    "                list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == \"foto\":\n",
    "                # Adicionar nome do nó ao dicionário\n",
    "                dados[f'Informações'] = child.string\n",
    "                list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == 'h1':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    # Adicionar chave ao dicionário de primeiro nível\n",
    "                    dados[f'Properties'] = rotulo\n",
    "                    list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == 'inst_back':\n",
    "                # Adicionar subtítulo ao dicionário\n",
    "                dados[f'Subtitle_Level_{indent}'] = child.get_text()\n",
    "                list_divs(child, dados, indent + 1)\n",
    "\n",
    "            elif child.name == 'a':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    # Adicionar grupo ao dicionário\n",
    "                    dados[f'Group_Level_{indent}'] = rotulo\n",
    "                    list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == 'b':\n",
    "                # Adicionar chave ao dicionário\n",
    "                chave = dividir(child.get_text())[0]\n",
    "                dados[f'Group_Level_{indent}'] = chave\n",
    "                list_divs(child, dados, indent)\n",
    "\n",
    "        else:\n",
    "            list_divs(child, dados, indent)\n",
    "\n",
    "# Iniciar a função principal\n",
    "if __name__ == '__main__':\n",
    "    # soup é uma variável que contém o objeto BeautifulSoup do seu HTML\n",
    "    dados = {}\n",
    "\n",
    "    # Encontra a div específica a partir da qual iniciar a travessia\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "    # Chame a função list_divs passando o objeto BeautifulSoup e o dicionário vazio\n",
    "    list_divs(starting_div, dados)\n",
    "    \n",
    "    # Convertendo o dicionário para JSON\n",
    "    dados_json = json.dumps(dados, indent=4, ensure_ascii=False)\n",
    "    print(dados_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dados_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\", \"\")\n",
    "    string = string.replace(\"\\t\", '')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, graph_dict=None, indent=0, verbose=False):\n",
    "    if graph_dict is None:\n",
    "        graph_dict = {}\n",
    "    \n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return graph_dict\n",
    "    \n",
    "    ignorar = [None, ' ', 'Baixar Currículo', 'Imprimir Currículo']\n",
    "\n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            if verbose:\n",
    "                conteudo = dividir(child.get_text())[0]\n",
    "                print(f\"Tag encontrada: {child.name}, Classe: {child.get('class', 'None')}, {conteudo}\")\n",
    "\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')\n",
    "\n",
    "                if class_name == ['layout-cell-pad-5']:\n",
    "                    conteudo = dividir(child.get_text())[0]\n",
    "                    graph_dict[\"values\"] = conteudo\n",
    "                \n",
    "                list_divs(child, graph_dict, indent + 1, verbose)\n",
    "\n",
    "            elif child.name == 'h2':\n",
    "                node_name = child.string\n",
    "                if \"nodes\" not in graph_dict:\n",
    "                    graph_dict[\"nodes\"] = []\n",
    "                graph_dict[\"nodes\"].append({\"name\": node_name})\n",
    "\n",
    "                list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "            elif child.name == 'h1':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    graph_dict[\"properties\"] = {\"label\": rotulo}\n",
    "\n",
    "                list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "            elif child.name == 'inst_back':\n",
    "                graph_dict[\"subtitles\"] = child.get_text()\n",
    "\n",
    "            elif child.name == 'a':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    if \"groups\" not in graph_dict:\n",
    "                        graph_dict[\"groups\"] = []\n",
    "                    graph_dict[\"groups\"].append({\"label\": rotulo})\n",
    "\n",
    "                list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "            elif child.name == 'b':\n",
    "                graph_dict[\"keys\"] = dividir(child.get_text())[0]\n",
    "\n",
    "        else:\n",
    "            list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Inicia a travessia a partir da div encontrada\n",
    "result = list_divs(starting_div, verbose=True)\n",
    "json.dumps(result, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    return string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "def extract_content(element, keys, key_classes, val_classes, verbose=False):\n",
    "    result = {}\n",
    "    try:\n",
    "        if verbose is True:\n",
    "            print(f\"Debug: Processing element of type {type(element)} with attributes {element.attrs}\")\n",
    "\n",
    "        for key_class in key_classes:\n",
    "            key_elements = element.find_all(class_=key_class, recursive=True)\n",
    "            for key_elem in key_elements:\n",
    "                key_text = key_elem.text.strip()\n",
    "                if verbose is True:\n",
    "                    print(f\"Debug: Found key element with text: {key_text}\")\n",
    "\n",
    "                value_elem = key_elem.find_next_sibling(class_=val_classes)\n",
    "                if value_elem:\n",
    "                    value_text = rep(value_elem.text.strip())\n",
    "                    if verbose is True:\n",
    "                        print(f\"Debug: Found value element with text: {value_text}\")\n",
    "                    result[key_text] = value_text\n",
    "                else:\n",
    "                    if verbose is True:\n",
    "                        print(f\"Debug: No value element found for the key '{key_text}' with attributes {key_elem.attrs}\")\n",
    "\n",
    "        for section_class in keys:\n",
    "            section_elements = element.find_all('div', {'class': section_class}, recursive=True)\n",
    "            for idx, section_elem in enumerate(section_elements):\n",
    "               if verbose is True:\n",
    "                print(f\"Debug: Found section element with attributes: {section_elem.attrs}\")\n",
    "\n",
    "                section_dict = extract_content(section_elem, keys, key_classes, val_classes)\n",
    "                section_key = section_elem.get('id', f'UnnamedSection{idx}')\n",
    "                result[section_key] = section_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in extract_content: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "    # keys = ['infpessoa', 'id','title_wraper','infpessoa','artigo-completo']\n",
    "    # key_classes = ['title_wraper','infpessoa','inst_back', 'id','layout-cell layout-cell-3 text-align-right','artigo-completo', 'layout-cell-1','cita-artigos']\n",
    "    # val_classes = ['layout-cell-9','data-cell','layout-cell-11']\n",
    "\n",
    "try:\n",
    "    # Initialize BeautifulSoup object (Assuming that 'soup' has been initialized)\n",
    "    keys = [\n",
    "            'b',\n",
    "            'id',\n",
    "            'infpessoa',\n",
    "            'title_wraper',\n",
    "            'artigo-completo',\n",
    "            'title_wraper',\n",
    "            'infpessoa',\n",
    "            'id',\n",
    "            'artigo-completo',\n",
    "            ]\n",
    "    key_classes = [\n",
    "                   'text-align-right',\n",
    "                   'title_wraper',        \n",
    "                   'inst_back',\n",
    "                #    'layout-cell-1', # Trabalhos em eventos\n",
    "                #    'cita-artigos', # Trabalhos completos e conclusão de curso de graduação\n",
    "                   ]\n",
    "    val_classes = [\n",
    "        'layout-cell-9',\n",
    "        'data-cell',\n",
    "        'layout-cell-11',\n",
    "        'title_wraper',\n",
    "        # 'infpessoa',\n",
    "        'id',\n",
    "        'artigo-completo',        \n",
    "        ]\n",
    "\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "    master_dict = {}\n",
    "    \n",
    "    if starting_div:\n",
    "        master_dict = extract_content(starting_div, keys, key_classes, val_classes)\n",
    "        print(json.dumps(master_dict, indent=4, ensure_ascii=False))\n",
    "    else:\n",
    "        print(\"Starting div not found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred in main: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    return string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "def extract_content(element, keys, key_classes, val_classes):\n",
    "    result = {}\n",
    "    \n",
    "    if isinstance(element, Tag):\n",
    "        for key in keys:\n",
    "            occurrences = element.find_all(key)\n",
    "            for c in occurrences:\n",
    "                if c is not None:\n",
    "                    result[key] = rep(c.text).strip()\n",
    "\n",
    "        for key_class in key_classes:\n",
    "            occurrences = element.find_all('div', {'class': key_class})\n",
    "            for idx, c in enumerate(occurrences):\n",
    "                if c is not None:\n",
    "                    key = c.text.strip() if c.text.strip() else f\"Unnamed-{idx}\"\n",
    "                    result[key] = {}\n",
    "                    \n",
    "                    sibling = c.find_next_sibling('div', {'class': val_classes})\n",
    "                    if sibling:\n",
    "                        result[key] = rep(sibling.text).strip()\n",
    "\n",
    "    return result\n",
    "\n",
    "try:\n",
    "    # Initialize BeautifulSoup object (Assuming that 'soup' has been initialized)\n",
    "    keys = ['infpessoa', 'id','title_wraper','infpessoa','artigo-completo']\n",
    "    key_classes = ['title_wraper','infpessoa','inst_back', 'id','layout-cell layout-cell-3 text-align-right','artigo-completo', 'layout-cell-1','cita-artigos']\n",
    "    val_classes = ['layout-cell-9','data-cell','layout-cell-11']\n",
    "\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "    master_dict = {}\n",
    "    for i in starting_div.children:\n",
    "        if isinstance(i, Tag):\n",
    "            master_dict.update(extract_content(i, keys, key_classes, val_classes))\n",
    "\n",
    "    print(json.dumps(master_dict, indent=4, ensure_ascii=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "def rep(string):\n",
    "    return string.replace(\"\\n\\n\\n\",\"\\n\").replace(\"\\n\\n\",\"\\n\")\n",
    "\n",
    "keys = ['infpessoa','h2','a', 'b', 'ul', 'id', 'inst_back','layout-cell-3','layout-cell-9','layout-cell-12','layout-cell-pad-5']\n",
    "\n",
    "try:\n",
    "    # Assuming 'soup' has been defined and initialized with the HTML document\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})        \n",
    "\n",
    "    for i in starting_div.children:\n",
    "        if isinstance(i, Tag):  # Confirming that the child is an HTML Tag\n",
    "            for key in keys:\n",
    "                # Use find_all to get a list of all occurrences\n",
    "                occurrences = i.find_all(key)\n",
    "                \n",
    "                for c in occurrences:  # Iterate over each occurrence\n",
    "                    if c is not None:\n",
    "                        print(f'{key:10} {rep(c.text).strip()}')\n",
    "                        print('-' * 75)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(element):\n",
    "    \"\"\"Recursively extracts content and organizes it into nested dictionaries.\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Check for 'title-wrapper' class as the first-level dictionary key\n",
    "    if 'title-wrapper' in element.attrs.get('class', []):\n",
    "        key = element.text.strip()\n",
    "        result[key] = {}\n",
    "        \n",
    "        for child in element.findChildren('div', recursive=False):\n",
    "            if 'layout-cell-12' in child.attrs.get('class', []):\n",
    "                second_level_key = child.text.strip()\n",
    "                result[key][second_level_key] = {}\n",
    "                \n",
    "                for grandchild in child.findChildren('div', recursive=False):\n",
    "                    if 'layout-cell-3' in grandchild.attrs.get('class', []):\n",
    "                        third_level_key = grandchild.text.strip()\n",
    "                        \n",
    "                        for great_grandchild in grandchild.findChildren('div', recursive=False):\n",
    "                            if 'layout-cell-9' in great_grandchild.attrs.get('class', []):\n",
    "                                value = great_grandchild.text.strip()\n",
    "                                result[key][second_level_key][third_level_key] = value\n",
    "    \n",
    "    # Recursively process child elements\n",
    "    for child in element.findChildren('div', recursive=False):\n",
    "        result.update(extract_content(child))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "# print(layout_cell_main.text.strip())\n",
    "print(len(layout_cell_main.find_all('div', 'title-wrapper')))\n",
    "for title_wrapper in layout_cell_main.find_all('div', 'title-wrapper'):\n",
    "    for data in title_wrapper.find_all('div', 'layout-cell-3'):\n",
    "        key = data.find('div', 'layout-cell-3')\n",
    "        val = data.find('div', 'layout-cell-9')\n",
    "        if key is not None:\n",
    "            print(f'{key.text.strip()}: {val.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_structure = extract_content(soup)\n",
    "json_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_scrape(tag, result_dict):\n",
    "    classes = tag.get(\"class\", [])\n",
    "    \n",
    "    if \"title-wrapper\" in classes:\n",
    "        section_name = tag.text.strip()\n",
    "        result_dict[section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[section_name])\n",
    "\n",
    "    elif \"text-align-right\" in classes or \"subtit-1\" in classes:\n",
    "        sub_section_name = tag.text.strip()\n",
    "        result_dict[sub_section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[sub_section_name])\n",
    "            \n",
    "    elif \"layout-cell-9\" in classes or \"layout-cell-pad-5\" in classes:\n",
    "        value = tag.text.strip()\n",
    "        result_dict[\"value\"] = value\n",
    "    \n",
    "    elif \"layout-cell\" in classes:\n",
    "        for child in tag.children:\n",
    "            if child.name:\n",
    "                recursive_scrape(child, result_dict)\n",
    "\n",
    "def main(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    # Initial recursive call\n",
    "    for child in soup.find_all('div'):\n",
    "        recursive_scrape(child, result_dict)\n",
    "        \n",
    "    # Convert the result to JSON\n",
    "    result_json = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(result_json)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_scrape(tag, result_dict):\n",
    "    classes = tag.get(\"class\", [])\n",
    "    \n",
    "    if \"title-wrapper\" in classes:\n",
    "        section_name = tag.text.strip()\n",
    "        result_dict[section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[section_name])\n",
    "\n",
    "    elif \"text-align-right\" in classes or \"subtit-1\" in classes:\n",
    "        sub_section_name = tag.text.strip()\n",
    "        result_dict[sub_section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[sub_section_name])\n",
    "            \n",
    "    elif \"layout-cell-9\" in classes or \"layout-cell-pad-5\" in classes:\n",
    "        value = tag.text.strip()\n",
    "        result_dict[\"value\"] = value\n",
    "    \n",
    "    elif \"layout-cell\" in classes:\n",
    "        for child in tag.children:\n",
    "            if child.name:\n",
    "                recursive_scrape(child, result_dict)\n",
    "\n",
    "def main(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    # Initial recursive call\n",
    "    for child in soup.find_all('div'):\n",
    "        recursive_scrape(child, result_dict)\n",
    "        \n",
    "    # Convert the result to JSON\n",
    "    result_json = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(result_json)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados hierárquicos de um Soup de documento HTML e retorna um dicionário aninhado.\n",
    "\n",
    "    Parâmetros:\n",
    "        Objeto Soup do html_content (object): O conteúdo HTML como uma string.\n",
    "\n",
    "    Retorna:\n",
    "        dict: Um dicionário aninhado contendo os dados extraídos.\n",
    "    \"\"\"\n",
    "\n",
    "    def recursive_extraction(element):\n",
    "        children_data = {}\n",
    "        children = element.find_all(recursive=False)\n",
    "        \n",
    "        for child in children:\n",
    "            if child.has_attr('class') and 'text-align-right' in child['class']:\n",
    "                key = child.get_text().strip()\n",
    "                \n",
    "                value_container = child.find_next_sibling(class_='layout-cell-9')\n",
    "                if value_container:\n",
    "                    value = recursive_extraction(value_container)\n",
    "                    children_data[key] = value if value else value_container.get_text().strip()\n",
    "        \n",
    "        return children_data\n",
    "\n",
    "    # Instanciar um objeto BeautifulSoup\n",
    "    # soup = BeautifulSoup(html_content, 'html.parser', from_encoding='utf-8')\n",
    "  \n",
    "    # Iniciar a extração recursiva a partir do elemento raiz\n",
    "    # root = soup.body\n",
    "    # result_dict = recursive_extraction(root)\n",
    "\n",
    "    # Encontra a div específica a partir da qual iniciar a travessia\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "    # Inicia a travessia a partir da div encontrada\n",
    "    result_dict = recursive_extraction(starting_div)\n",
    "\n",
    "    # Converter o dicionário em um objeto JSON\n",
    "    result_json = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "  \n",
    "    # Salvar o JSON em um arquivo, especificando o encoding como UTF-8\n",
    "    with open('output_nested.json', 'w', encoding='utf-8') as file:\n",
    "        file.write(result_json)\n",
    "  \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = extrair_dados(soup, True)\n",
    "nome_no = 'Antonio Marcos Aires Barbosa'\n",
    "imprimir_informacoes({nome_no: json_data['Properties']}, nome_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "for title_wrapper in layout_cell_main.find_all('div.title-wrapper'):\n",
    "    index = title_wrapper.select_one('ul, a, b').text.strip()\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_from_html(soup):\n",
    "    \"\"\"\n",
    "    Generate a JSON object from an Soup object for a Neo4j integration.\n",
    "\n",
    "    Parameters:\n",
    "        soup (object): Soup object from a HTML content.\n",
    "\n",
    "    Returns:\n",
    "        json_data (dict): A dictionary representing the JSON structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract node name\n",
    "    node_name = soup.select_one('div.infpessoa h2.nome').text.strip()\n",
    "\n",
    "    # Initialize the master dictionary\n",
    "    json_data = {node_name: {}}\n",
    "\n",
    "    # Locate the main layout cell\n",
    "    layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "\n",
    "    # Iterate over title-wrapper elements\n",
    "    for title_wrapper in layout_cell_main.select('div.title-wrapper'):\n",
    "        index = title_wrapper.select_one('ul, a, b').text.strip()\n",
    "        \n",
    "        # Initialize the child dictionary\n",
    "        json_data[node_name][index] = {}\n",
    "\n",
    "        # Iterate over layout cells\n",
    "        for layout_cell_3 in title_wrapper.select('div.layout-cell.layout-cell-3.text-align-right'):\n",
    "            grandchild_index = layout_cell_3.select_one('div.layout-cell-pad-5.text-align-right').text.strip()\n",
    "            \n",
    "            # Find the corresponding layout cell for values\n",
    "            layout_cell_9 = layout_cell_3.find_next_sibling('div', class_='layout-cell.layout-cell-9')\n",
    "            \n",
    "            values_text = layout_cell_9.select_one('div.layout-cell-pad-5').text\n",
    "            \n",
    "            # Create a list of values\n",
    "            values = values_text.split('<br class=\"clear\">' or '\\n\\n\\n')\n",
    "\n",
    "            # Add the grandchild dictionary\n",
    "            json_data[node_name][index][grandchild_index] = values\n",
    "\n",
    "    return json_data\n",
    "\n",
    "# Generate JSON data\n",
    "json_data = generate_json_from_html(soup)\n",
    "\n",
    "# Print or persist the generated JSON data\n",
    "print(json.dumps(json_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lattes = extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pprint(json_lattes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dict_to_dataframe(json_data, parent_key='', separator='.'):\n",
    "    \"\"\"\n",
    "    Convert a nested dictionary into a flat dictionary, suitable for DataFrame conversion.\n",
    "    \n",
    "    Parameters:\n",
    "    - json_data (dict): The nested dictionary to flatten.\n",
    "    - parent_key (str, optional): The concatenated key used to represent nesting.\n",
    "    - separator (str, optional): The character to use for separating nested keys.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame representing the flattened dictionary.\n",
    "    \"\"\"\n",
    "    flat_dict = {}\n",
    "    \n",
    "    for k, v in json_data.items():\n",
    "        new_key = f\"{parent_key}{separator}{k}\" if parent_key else k\n",
    "        \n",
    "        if isinstance(v, dict):\n",
    "            flat_dict.update(dict_to_dataframe(v, new_key, separator=separator))\n",
    "        else:\n",
    "            flat_dict[new_key] = v\n",
    "            \n",
    "    return pd.DataFrame([flat_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming json_lattes is a JSON-formatted string\n",
    "json_lattes_dict = json.loads(json_lattes)\n",
    "\n",
    "# Then call the dict_to_dataframe function\n",
    "df = dict_to_dataframe(json_lattes_dict)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração com BeaultifulSoap\n",
    "def parse_header(soup):\n",
    "    splpt = 'Currículo do Sistema de Currículos Lattes ('\n",
    "    string_title = soup.title.string if soup.title else \"Unknown\"\n",
    "    title = string_title.split(splpt)[1].strip(')')\n",
    "    \n",
    "    meta_keywords = soup.find(\"meta\", {\"http-equiv\": \"keywords\"})[\"content\"] if soup.find(\"meta\", {\"http-equiv\": \"keywords\"}) else \"Unknown\"\n",
    "    return {\"title\": title, \"meta_keywords\": meta_keywords}\n",
    "\n",
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "    from py2neo import Graph, Node, Relationship\n",
    "    \n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    secoes = []\n",
    "    \n",
    "    print(f'{len(h1_elements[2:])} elementos encontrados')\n",
    "    for n,i in enumerate(h1_elements):\n",
    "        if n>1:\n",
    "            secao = i.text\n",
    "            print(f'    {secao}')\n",
    "            secoes.append(secao)\n",
    "    \n",
    "    for elem in h1_elements[2:]:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        \n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_parsoninfo(soup):\n",
    "    # Localizar o elemento de link que contém o título do currículo\n",
    "    link_element = soup.find(\"a\", {\"href\": lambda x: x and \"abreDetalhe\" in x})\n",
    "\n",
    "    # Extrair o texto do link para usar como título do nó\n",
    "    node_title = link_element.text if link_element else \"Unknown\"\n",
    "    print(f'Título do Nó: {node_title}')\n",
    "\n",
    "    # Localizar o elemento div contendo as propriedades\n",
    "    properties_div = soup.find(\"div\", {\"class\": \"resultado\"})\n",
    "    if properties_div:\n",
    "        print(f'Resultado: {properties_div.text}')\n",
    "    else:\n",
    "        print('Resultados não encontrados')\n",
    "\n",
    "    # Inicializar um dicionário para armazenar as propriedades\n",
    "    properties = {}\n",
    "    \n",
    "    # Localizar o elemento li que contém as informações do idlattes\n",
    "    li_element = soup.find(\"li\")\n",
    "    for i in li_element:\n",
    "        if 'http://lattes.cnpq.br/' in i:\n",
    "            idlattes = i.split('http://lattes.cnpq.br/')[1]\n",
    "            properties['Idlattes'] = idlattes\n",
    "            print(idlattes)\n",
    "\n",
    "    # Extrair e armazenar as propriedades relevantes\n",
    "    if properties_div:\n",
    "        properties['Nacionalidade'] = 'Brasil'\n",
    "        properties['Cargo'] = properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}).text if properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}) else 'Desconhecido'\n",
    "        properties['Titulação'] = properties_div.contents[-4] if len(properties_div.contents) > 4 else 'Desconhecido'\n",
    "\n",
    "        # Extração de nome e identificador único\n",
    "        a_element = li_element.find(\"a\")\n",
    "        properties[\"Nome\"] = a_element.text\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Títulos Acadêmicos e outras informações\n",
    "\n",
    "    return node_title, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_annotation_in_db(uri, user, password, annot_html, cv_id):\n",
    "    \"\"\"\n",
    "    Store the annotation HTML in a Neo4j database.\n",
    "\n",
    "    Parameters:\n",
    "        uri (str): URI of the Neo4j database\n",
    "        user (str): Username for the Neo4j database\n",
    "        password (str): Password for the Neo4j database\n",
    "        annot_html (str): The HTML string containing annotations\n",
    "        cv_id (str): The unique identifier for the annotated CV\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize Neo4j driver\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    # Define Cypher query for adding an annotation\n",
    "    add_annotation_query = '''\n",
    "    MERGE (cv:CV {id: $cv_id})\n",
    "    CREATE (a:Annotation {html: $annot_html})\n",
    "    MERGE (cv)-[:HAS]->(a)\n",
    "    '''\n",
    "\n",
    "    # Execute query\n",
    "    with driver.session() as session:\n",
    "        session.run(add_annotation_query, cv_id=cv_id, annot_html=annot_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# from collections import Counter\n",
    "\n",
    "# def enumerate_tags(soup):\n",
    "#     # Extração de todos os marcadores (tags) no documento\n",
    "#     all_tags = [tag.name for tag in soup.find_all(True)]\n",
    "    \n",
    "#     # Contagem de ocorrências de cada marcador\n",
    "#     tag_count = Counter(all_tags)\n",
    "    \n",
    "#     # Conversão do objeto Counter para um dicionário padrão para facilitar a manipulação subsequente\n",
    "#     tag_dict = dict(tag_count)\n",
    "    \n",
    "#     return tag_dict\n",
    "\n",
    "# # Exemplo de uso\n",
    "# tag_dictionary = enumerate_tags(soup)\n",
    "# print(tag_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# div_elements = soup.find_all('div')\n",
    "# div_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def extract_div_data(soup):\n",
    "#     \"\"\"\n",
    "#     Extrai dados das divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "#     Parâmetros:\n",
    "#     - html_document (str): String contendo o documento HTML.\n",
    "    \n",
    "#     Retorno:\n",
    "#     - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "#     \"\"\"\n",
    "#     # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "#     extracted_data = {}\n",
    "    \n",
    "#     # Localiza todas as divs com a classe 'layout-cell-pad-5 text-align-right'\n",
    "# #     divs_key = soup.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "#     divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "#     for div_key in divs_key:\n",
    "#         # Extrai o conteúdo da tag <b> dentro da div\n",
    "#         key = div_key.find('b').text if div_key.find('b') else None\n",
    "        \n",
    "#         # Encontra a div que segue imediatamente\n",
    "#         div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5'})\n",
    "        \n",
    "#         # Extrai o conteúdo da div\n",
    "#         value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "#         # Armazena no dicionário se ambas chave e valor existirem\n",
    "#         if key and value:\n",
    "#             extracted_data[key] = value\n",
    "    \n",
    "#     return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_div_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(extracted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    title_elements = soup.find_all('h1')\n",
    "    for title_elem in title_elements:\n",
    "        title_text = title_elem.text.strip()\n",
    "        \n",
    "        data_dict = {}\n",
    "        \n",
    "        parent_div = title_elem.find_parent('div', {'class': 'title-wrapper'})\n",
    "        if parent_div:\n",
    "            data_cells = parent_div.find_all('div', {'class': 'layout-cell layout-cell-12 data-cell'})\n",
    "            \n",
    "            current_higher_order_dict = None  # Initialize a variable to store the current higher-order dictionary\n",
    "            \n",
    "            for cell in data_cells:\n",
    "                inst_back_elem = cell.find('div', {'class': 'inst_back'})\n",
    "                \n",
    "                if inst_back_elem:\n",
    "                    higher_order_key = inst_back_elem.text.strip()\n",
    "                    current_higher_order_dict = {}  # Create a new dictionary for this higher-order key\n",
    "                    data_dict[higher_order_key] = current_higher_order_dict  # Associate the new dictionary with the higher-order key\n",
    "                    \n",
    "                year_elems = cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                details_elems = cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "                \n",
    "                for year_elem, details_elem in zip(year_elems, details_elems):\n",
    "                    year_text = year_elem.text.strip() if year_elem else None\n",
    "                    details_text = details_elem.text.strip() if details_elem else None\n",
    "                    \n",
    "                    if current_higher_order_dict is not None:\n",
    "                        # Insert the year-details pair into the current higher-order dictionary\n",
    "                        current_higher_order_dict[year_text] = details_text\n",
    "                    else:\n",
    "                        # If no higher-order key is present, associate the year-details pair directly with the title\n",
    "                        data_dict[year_text] = details_text\n",
    "                \n",
    "            result_dict[title_text] = data_dict\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dict_to_dataframe(input_dict):\n",
    "    def recursive_descent(current_dict, parent_key='', separator='.'):\n",
    "        nonlocal flattened_dict\n",
    "        for k, v in current_dict.items():\n",
    "            new_key = f\"{parent_key}{separator}{k}\" if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                recursive_descent(v, new_key, separator=separator)\n",
    "            else:\n",
    "                flattened_dict[new_key] = v\n",
    "                \n",
    "    flattened_dict = {}\n",
    "    recursive_descent(input_dict)\n",
    "    \n",
    "    # Create DataFrame from the flattened dictionary\n",
    "    df = pd.DataFrame([flattened_dict])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_dict = extract_data(soup)\n",
    "df = dict_to_dataframe(nested_dict)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag, NavigableString\n",
    "\n",
    "html_content = '''\n",
    "<div class=\"layout-cell-pad-5\">\n",
    "    Doutorado em andamento em Informática Aplicada.\n",
    "    <br class=\"clear\">Universidade de Fortaleza, UNIFOR, Brasil.\n",
    "    <br class=\"clear\">Título: Processamento de Linguagem Natural no Desenvolvimento de Bioprodutos para Saúde\n",
    "    <br class=\"clear\">Orientador: <a class=\"icone-lattes\" target=\"_blank\" href=\"http://lattes.cnpq.br/2607811863279622\" tabindex=\"80\"><img src=\"images/curriculo/logolattes.gif\"></a>Raimir Holanda Filho.\n",
    "</div>\n",
    "'''\n",
    "\n",
    "soup_sample = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "def extract_text_by_br_class(soup):\n",
    "    result_list = []\n",
    "    elements = soup.select('div.layout-cell-pad-5')\n",
    "    for element in elements:\n",
    "        text_segments = []\n",
    "        current_dict = {}\n",
    "        for content in element.contents:\n",
    "            if isinstance(content, Tag):\n",
    "                if content.get('class') == ['clear']:\n",
    "                    joined_text = ' '.join(text_segments).strip()\n",
    "                    if \"Orientador:\" in joined_text:\n",
    "                        key, value = joined_text.split(\"Orientador:\")\n",
    "                        current_dict[key.strip()] = {'name': value.strip(), 'url': current_dict.pop('url', None)}\n",
    "                    else:\n",
    "                        current_dict[joined_text] = None\n",
    "                    text_segments = []\n",
    "                elif content.name == 'a':\n",
    "                    current_dict['url'] = content.get('href')\n",
    "            elif isinstance(content, NavigableString):\n",
    "                text_segments.append(str(content).strip())\n",
    "        # Capture any remaining text\n",
    "        if text_segments:\n",
    "            joined_text = ' '.join(text_segments).strip()\n",
    "            current_dict[joined_text] = None\n",
    "        result_list.append(current_dict)\n",
    "    return result_list\n",
    "\n",
    "# Execução da função\n",
    "extracted_texts = extract_text_by_br_class(soup_sample)\n",
    "extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def extract_text_from_selectors(soup,select_path):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for title_wrapper in soup.select('div.title-wrapper'):\n",
    "        elements = title_wrapper.select(select_path)\n",
    "#         print(len(elements))\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for element in elements:\n",
    "            text_content = element.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4.element import Tag, NavigableString\n",
    "def extract_text_by_br_class(soup):\n",
    "    result_list = []\n",
    "    elements = soup.select('div.layout-cell-pad-5')\n",
    "    for element in elements:\n",
    "        text_segments = []\n",
    "        current_dict = {}\n",
    "        for content in element.contents:\n",
    "            if isinstance(content, Tag):\n",
    "                if content.get('class') == ['clear']:\n",
    "                    joined_text = ' '.join(text_segments).strip()\n",
    "                    if \"Orientador:\" in joined_text:\n",
    "                        key, value = joined_text.split(\"Orientador:\")\n",
    "                        current_dict[key.strip()] = {'name': value.strip(), 'url': current_dict.pop('url', None)}\n",
    "                    else:\n",
    "                        current_dict[joined_text] = None\n",
    "                    text_segments = []\n",
    "                elif content.name == 'a':\n",
    "                    current_dict['url'] = content.get('href')\n",
    "            elif isinstance(content, NavigableString):\n",
    "                text_segments.append(str(content).strip())\n",
    "        # Capture any remaining text\n",
    "        if text_segments:\n",
    "            joined_text = ' '.join(text_segments).strip()\n",
    "            current_dict[joined_text] = None\n",
    "        result_list.append(current_dict)\n",
    "    return result_list\n",
    "\n",
    "# Execução da função\n",
    "extracted_texts = extract_text_by_br_class(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=\"layout-cell.layout-cell-3.text-align-right\"\n",
    "vals=\"layout-cell layout-cell-9\"\n",
    "# <div class=\"layout-cell layout-cell-9\">\n",
    "# <div class=\"layout-cell-pad-5\">Doutorado em andamento em Informática Aplicada. <br class=\"clear\">Universidade de Fortaleza, UNIFOR, Brasil.\n",
    "\t\t\n",
    "# \t<br class=\"clear\">Título: Processamento de Linguagem Natural no Desenvolvimento de Bioprodutos para Saúde<br class=\"clear\">Orientador: <a class=\"icone-lattes\" target=\"_blank\" href=\"http://lattes.cnpq.br/2607811863279622\" tabindex=\"80\"><img src=\"images/curriculo/logolattes.gif\"></a>Raimir Holanda Filho. </div>\n",
    "# </div>\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "father     = 'div.title-wrapper'\n",
    "sons       = 'h1'\n",
    "grandchild = '' \n",
    "def extract_text_titles(soup,father,sons):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for father_element in soup.select(father):\n",
    "        son_elements = father_element.select(sons)\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for grandchild in son_elements:\n",
    "            text_content = grandchild.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_titles(soup,father,sons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "father = 'div.title-wrapper'\n",
    "sons   = 'h1'\n",
    "def extract_text_titles(soup,father,sons):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for father_element in soup.select(father):\n",
    "        elements = father_element.select(sons)\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for element in elements:\n",
    "            text_content = element.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "# <div class=\"layout-cell-pad-5 text-align-right\">\n",
    "# <b>2017 - 2019</b>\n",
    "# </div>\n",
    "select_path='div.layout-cell-pad-12.data-cell'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "# <div class=\"layout-cell-pad-5 text-align-right\">\n",
    "# <b>2017 - 2019</b>\n",
    "# </div>\n",
    "select_path='div.layout-cell-pad-5.text-align-right'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "select_path='layout-cell-pad-5.text-align-right'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_to_dict(soup):\n",
    "    result_list = []\n",
    "    \n",
    "    for title_wrapper in soup.select('div.title-wrapper'):\n",
    "        temp_dict = defaultdict(dict)\n",
    "        first_level_key = title_wrapper.select_one('div.layout-cell.layout-cell-12.data-cell').text.strip()\n",
    "        \n",
    "        for cell in title_wrapper.select('div.layout-cell.layout-cell-12.data-cell'):\n",
    "            second_level_keys = cell.select('div.layout-cell-pad-5.text-align-right')\n",
    "            values = cell.select('div.layout-cell.layout-cell-3.text-align-right')\n",
    "            \n",
    "            if len(second_level_keys) == len(values):\n",
    "                for key, value in zip(second_level_keys, values):\n",
    "                    second_level_key = key.text.strip()\n",
    "                    value_text = value.text.strip()\n",
    "                    temp_dict[first_level_key][second_level_key] = value_text\n",
    "        \n",
    "        result_list.append(temp_dict)\n",
    "    \n",
    "    return result_list\n",
    "\n",
    "# Execute the function\n",
    "result_list = extract_to_dict(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['layout-cell', 'layout-cell-3', 'text-align-right']\n",
    "\n",
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "# 'text-align-right': 152,\n",
    "# 'layout-cell-pad-5': 152,\n",
    "\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-pad-main'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'text-align-right'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-9'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-3'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'data-cell'},\n",
    "#                                     {'class': 'layout-cell'},\n",
    "#                                     {'class': 'layout-cell-9},                                    \n",
    "#                                     {'class': 'layout-cell-12'},\n",
    "#                                     {'class': 'layout-cell-pad-5'},\n",
    "#                                     {'class': 'data-cell'}\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_text_from_hierarchy(tag, result_dict, parent_key=\"root\"):\n",
    "    div_elements = tag.find_all('div', recursive=False)\n",
    "    \n",
    "    for div in div_elements:\n",
    "        class_list = div.get('class')\n",
    "        class_key = ', '.join(class_list) if class_list else \"None\"\n",
    "        \n",
    "        # Criando uma chave única que incorpora o caminho da raiz até esta div\n",
    "        full_key = f\"{parent_key} -> {class_key}\"\n",
    "\n",
    "        # Coleta o texto contido no elemento div atual\n",
    "        text_content = div.get_text(strip=True)\n",
    "\n",
    "        # Armazenar o conteúdo textual sob esta chave única\n",
    "        if full_key not in result_dict:\n",
    "            result_dict[full_key] = []\n",
    "        result_dict[full_key].append(text_content)\n",
    "\n",
    "        # Chamada recursiva para extrair textos dos filhos deste div\n",
    "        extract_text_from_hierarchy(div, result_dict, full_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(list)\n",
    "\n",
    "extract_text_from_hierarchy(soup.body, result_dict, 'text-align-right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.keys():\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.values():\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes, key_stack=None):\n",
    "    if key_stack is None:\n",
    "        key_stack = []\n",
    "    \n",
    "    popped_key = None\n",
    "    if soup_element.name == \"div\":\n",
    "        class_list = soup_element.get('class', [])\n",
    "        \n",
    "        if any(k in class_list for k in key_classes):\n",
    "            new_key = soup_element.text.strip()\n",
    "#             print(f'Key found: {new_key}')  # Debugging line\n",
    "            key_stack.append(new_key)\n",
    "        \n",
    "        elif any(v in class_list for v in value_classes) and key_stack:\n",
    "            value = soup_element.text.strip()\n",
    "#             print(f'Value found: {value}')  # Debugging line\n",
    "            current_key = key_stack[-1]\n",
    "            if current_key in result_dict:\n",
    "                result_dict[current_key].append(value)\n",
    "            else:\n",
    "                result_dict[current_key] = [value]\n",
    "                \n",
    "#     print(f\"Current key_stack: {key_stack}\")  # Debugging line\n",
    "\n",
    "    for child in soup_element.find_all(\"div\", recursive=False):\n",
    "        extract_key_value_pairs(child, result_dict, key_classes, value_classes, key_stack)\n",
    "\n",
    "    if popped_key:\n",
    "        key_stack.append(popped_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_element  = soup.find('div', {'class': 'text-align-right'})\n",
    "# key_classes   = ['data-cell']\n",
    "# value_classes = ['text-align-right']\n",
    "\n",
    "key_classes   = ['layout-cell-3']\n",
    "value_classes = ['layout-cell-9']\n",
    "\n",
    "extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.keys():\n",
    "    pprint(i)\n",
    "\n",
    "print()\n",
    "\n",
    "for i in result_dict.values():\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_element  = soup.find('div', {'class': 'layout-cell-pad-5'})\n",
    "key_classes   = ['layout-cell-3']\n",
    "value_classes = ['layout-cell-9']\n",
    "\n",
    "extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)\n",
    "pprint(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = 'infpessoa'\n",
    "\n",
    "def extract_target_classes(soup,classe):\n",
    "    soup_element  = soup.find('div', {'class': classe})\n",
    "#     key_classes   = ['data-cell']\n",
    "#     value_classes = ['text-align-right']\n",
    "    key_classes   = ['layout-cell-9']\n",
    "    value_classes = ['layout-cell-pad-5']\n",
    "    \n",
    "    target_classes = [\n",
    "        'Produção '\n",
    "        'Bibliográfica',\n",
    "        'Produção '\n",
    "        'Técnica',\n",
    "        'Produção '\n",
    "        'Artística/Cultural'\n",
    "        'nome', \n",
    "        'resumo', \n",
    "        'artigo-completo', \n",
    "        'cita', \n",
    "        'cita-artigos', \n",
    "        'citacoes', \n",
    "        'detalhes', \n",
    "        'fator', \n",
    "        'foto', \n",
    "        'informacao-artigo', \n",
    "        'informacoes-autor', \n",
    "        'infpessoa', \n",
    "        'rodape-cv', \n",
    "        'science_cont', \n",
    "        'texto', \n",
    "        'trab'\n",
    "        ]\n",
    "    try:\n",
    "        extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)\n",
    "        for x,y in zip(result_dict.keys(),result_dict.values()):\n",
    "            if x in target_classes:\n",
    "                try:\n",
    "                    print(f\"{x:>12} | {y[0]}\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        filtered_dict = {k: v for k, v in result_dict.items() if k in target_classes}\n",
    "\n",
    "    except:\n",
    "        print(f'Classe \"{classe}\" não encontrada')\n",
    "    return filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = 'infpessoa'\n",
    "extract_target_classes(soup,classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_target_classes(soup,classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outras formas de persistir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Para criar parâmetros dinâmicos e persistir níveis de detales usa-se APOC\n",
    "# def flatten_dict(d, parent_key='', sep='.'):\n",
    "#     items = {}\n",
    "#     for k, v in d.items():\n",
    "#         new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "#         if isinstance(v, dict):\n",
    "#             items.update(flatten_dict(v, new_key, sep=sep))\n",
    "#         else:\n",
    "#             items[new_key] = v\n",
    "#     return items\n",
    "\n",
    "# def persist_to_neo4j(extracted_data):\n",
    "#     driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     try:\n",
    "#         with driver.session() as session:\n",
    "#             name = extracted_data.get('name', 'UNKNOWN')\n",
    "#             session.write_transaction(lambda tx: tx.run(\n",
    "#                 \"MERGE (n:Person {name: $name}) RETURN n\",\n",
    "#                 name=name,\n",
    "#             ))\n",
    "            \n",
    "#             # Flatten the dictionary\n",
    "#             flat_data = flatten_dict(extracted_data)\n",
    "\n",
    "#             # Persist each flattened key-value pair\n",
    "#             for key, value in flat_data.items():\n",
    "#                 if key is None or key == '':\n",
    "#                     continue\n",
    "                \n",
    "#                 query = f\"MATCH (n:Person {{name: $name}}) SET n.`{key}` = $value RETURN n\"\n",
    "#                 session.write_transaction(lambda tx, query=query, value=value: tx.run(query, name=name, value=value))\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {e}\")\n",
    "#     finally:\n",
    "#         driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist_to_neo4j(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neo4j import GraphDatabase\n",
    "\n",
    "# def recursive_persist(tx, parent_node_id, properties):\n",
    "#     for key, value in properties.items():\n",
    "#         if key is None or key == '':\n",
    "#             continue\n",
    "\n",
    "#         if isinstance(value, dict):\n",
    "#             # Create a new node for the nested dictionary and link it to the parent\n",
    "#             result = tx.run(\n",
    "#                 \"MATCH (p) WHERE id(p) = $parent_id \"\n",
    "#                 \"CREATE (p)-[:HAS]->(m:SubNode {name: $key}) \"\n",
    "#                 \"RETURN id(m)\",\n",
    "#                 parent_id=parent_node_id,\n",
    "#                 key=key\n",
    "#             )\n",
    "#             new_node_id = result.single()[0]\n",
    "\n",
    "#             # Recursive call to set properties for the new node\n",
    "#             recursive_persist(tx, new_node_id, value)\n",
    "#         else:\n",
    "#             # Directly set the primitive property on the parent node\n",
    "#             tx.run(\n",
    "#                 \"MATCH (n) WHERE id(n) = $id \"\n",
    "#                 f\"SET n.`{key}` = $value\",\n",
    "#                 id=parent_node_id,\n",
    "#                 value=value\n",
    "#             )\n",
    "\n",
    "# def persist_to_neo4j(extracted_data):\n",
    "#     driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     try:\n",
    "#         with driver.session() as session:\n",
    "#             def transactional_work(tx):\n",
    "#                 name = extracted_data.get('name', 'UNKNOWN')\n",
    "#                 properties = {k: v for k, v in extracted_data.items() if k != 'name'}\n",
    "#                 result = tx.run(\n",
    "#                     \"MERGE (n:MyLabel {name: $name}) \"\n",
    "#                     \"RETURN id(n)\",\n",
    "#                     name=name\n",
    "#                 )\n",
    "#                 root_node_id = result.single()[0]\n",
    "#                 recursive_persist(tx, root_node_id, properties)\n",
    "\n",
    "#             session.write_transaction(transactional_work)\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {e}\")\n",
    "#     finally:\n",
    "#         driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with your `extracted_data`\n",
    "# persist_to_neo4j(extracted_data)\n",
    "# # Sample invocation\n",
    "# extracted_data = {\n",
    "#     'name': 'João Doé',\n",
    "#     'age': 30,\n",
    "#     None: 'This will be ignored',\n",
    "#     '': 'This will be ignored',\n",
    "#     'nested_dict': {\n",
    "#         'key1': 'value1',\n",
    "#         'key2': None\n",
    "#     },\n",
    "#     'nested_list': [1, 2, 3]  # This will now be converted to a JSON string\n",
    "# }\n",
    "# persist_to_neo4j(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teste funcional\n",
    "# def main():\n",
    "#     uri = \"bolt://localhost:7687\"\n",
    "#     driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "#     production_dict = {\n",
    "#         'name': 'John Doe',\n",
    "#         'Properties': {\n",
    "#             'Produções': [\n",
    "#                 {'ano': '2023', 'revista': 'Journal A', 'jcr': '3.662', 'doi': 'some_doi'},\n",
    "#                 {'ano': '2022', 'revista': 'Journal B', 'jcr': None, 'doi': 'another_doi'}\n",
    "#             ]\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     with driver.session() as session:\n",
    "#         session.write_transaction(add_producao, production_dict)\n",
    "\n",
    "#     driver.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Persistir produções como nós secundários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neo4j import GraphDatabase\n",
    "\n",
    "# ## Persiste as produções como nós secundários ligados ao nó principal\n",
    "# def add_producao(tx, production_dict):\n",
    "#     # Extract person's name and properties sub-dictionary\n",
    "#     person_name = production_dict.get('name', 'UNKNOWN')\n",
    "#     properties_dict = production_dict.get('Properties', {})\n",
    "\n",
    "#     # First, merge the Person node based on the name\n",
    "#     tx.run(\"MERGE (a:Person {name: $person_name})\", person_name=person_name)\n",
    "\n",
    "#     # Initialize an empty dictionary to hold the cleaned production data\n",
    "#     producoes = properties_dict.get('Produções', [])\n",
    "    \n",
    "#     for producao in producoes:\n",
    "#         clean_producao = {}\n",
    "        \n",
    "#         # Replace None with 'NULL' and ignore empty strings\n",
    "#         for key, value in producao.items():\n",
    "#             if value == '':\n",
    "#                 continue\n",
    "#             if value is None:\n",
    "#                 value = 'NULL'\n",
    "#             clean_producao[key] = value\n",
    "\n",
    "#         # Create or merge the Producao node based on unique keys ('ano' and 'revista' here)\n",
    "#         tx.run(\n",
    "#             \"MERGE (p:Producao {ano: $ano, revista: $revista}) \"\n",
    "#             \"SET p += $props\",\n",
    "#             ano=clean_producao.get('ano'),\n",
    "#             revista=clean_producao.get('revista'),\n",
    "#             props=clean_producao\n",
    "#         )\n",
    "\n",
    "#         # Connect the Person node to the Producao node\n",
    "#         tx.run(\n",
    "#             \"MATCH (a:Person {name: $person_name}), (p:Producao {ano: $ano, revista: $revista}) \"\n",
    "#             \"MERGE (a)-[r:HAS_PRODUCOES]->(p)\",\n",
    "#             person_name=person_name,\n",
    "#             ano=clean_producao.get('ano'),\n",
    "#             revista=clean_producao.get('revista')\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles_dict = {\"Label\": \"Person\", \"name\": {}, \"Properties\": {}}\n",
    "# articles_dict = extrair_normal(soup)\n",
    "# # articles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate driver\n",
    "# uri = \"bolt://localhost:7687\"\n",
    "# driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# # Create a session\n",
    "# session = driver.session()\n",
    "\n",
    "# # Call the function within a transaction\n",
    "# session.write_transaction(add_producao, articles_dict)\n",
    "\n",
    "# # Optionally, close the session and driver\n",
    "# session.close()\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Persiste todod dados como propriedades de um nó\n",
    "# main_cell     = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "# generic_dict = {\"Label\": \"Person\", \"Properties\": {}}\n",
    "# if main_cell:\n",
    "#     traverse(main_cell, generic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist_to_neo4j(generic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variações das funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funciona mas achata o dicionário de Propriedades em um único e não está em UTF-8\n",
    "# from neo4j import GraphDatabase\n",
    "# import json\n",
    "\n",
    "# class InvalidPropertyError(ValueError):\n",
    "#     \"\"\"Custom Exception for Invalid Properties\"\"\"\n",
    "#     pass\n",
    "\n",
    "# def serialize_properties(input_dict):\n",
    "#     \"\"\"\n",
    "#     Serializes the properties in the dictionary that are not of primitive types.\n",
    "    \n",
    "#     :param input_dict: The input dictionary with properties.\n",
    "#     :return: A dictionary with properties serialized if necessary.\n",
    "#     \"\"\"\n",
    "#     return {k: json.dumps(v) if not isinstance(v, (str, int, float, bool, list)) else v for k, v in input_dict.items()}\n",
    "\n",
    "# def ensure_string_keys(input_dict):\n",
    "#     \"\"\"\n",
    "#     Ensures all keys in the dictionary are of string type.\n",
    "    \n",
    "#     :param input_dict: The input dictionary.\n",
    "#     :return: A dictionary with string keys.\n",
    "#     \"\"\"\n",
    "#     return {str(k): v for k, v in input_dict.items()}\n",
    "\n",
    "# def create_or_merge_node(tx, label, properties):\n",
    "#     \"\"\"\n",
    "#     Creates or merges a node in the Neo4j database.\n",
    "    \n",
    "#     :param tx: The transaction object.\n",
    "#     :param label: The label for the node.\n",
    "#     :param properties: The properties for the node.\n",
    "#     :return: The created or merged node.\n",
    "#     \"\"\"\n",
    "#     query = f\"MERGE (n:{label} {{name: $name}}) SET n += $properties RETURN n\"\n",
    "#     return tx.run(query, name=properties.get('name', 'UNKNOWN'), properties=properties).single()\n",
    "\n",
    "# def are_properties_primitive(input_dict):\n",
    "#     \"\"\"\n",
    "#     Validates if all properties in the dictionary are of primitive types or lists thereof.\n",
    "    \n",
    "#     :param input_dict: The input dictionary.\n",
    "#     :return: Boolean indicating the validation result.\n",
    "#     \"\"\"\n",
    "#     for key, value in input_dict.items():\n",
    "#         if not (isinstance(value, (str, int, float, bool)) or isinstance(value, list)):\n",
    "#             print(f\"Invalid property: {key} -> {value}\")\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "# def persist_to_neo4j(uri, user, password, node_data, label):\n",
    "#     \"\"\"\n",
    "#     Persists data to Neo4j database.\n",
    "    \n",
    "#     :param uri: The URI of the Neo4j database.\n",
    "#     :param user: The username for the Neo4j database.\n",
    "#     :param password: The password for the Neo4j database.\n",
    "#     :param node_data: The dictionary containing the properties for the node.\n",
    "#     :param label: The label for the node.\n",
    "#     \"\"\"\n",
    "#     driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "#     with driver.session() as session:\n",
    "#         converted_dict = ensure_string_keys(node_data)\n",
    "        \n",
    "#         # Serialize non-primitive properties\n",
    "#         serialized_dict = serialize_properties(converted_dict)\n",
    "        \n",
    "#         # Now validate the properties\n",
    "#         if not are_properties_primitive(serialized_dict):\n",
    "#             raise InvalidPropertyError(\"Properties should be of primitive types or arrays of primitive types.\")\n",
    "        \n",
    "#         session.write_transaction(create_or_merge_node, label, serialized_dict)\n",
    "\n",
    "#     driver.close()\n",
    "\n",
    "# # Example usage\n",
    "# uri = \"bolt://localhost:7687\"  # Replace with your Neo4j URI\n",
    "# user = \"neo4j\"  # Replace with your username\n",
    "# password = \"password\"  # Replace with your password\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample invocation\n",
    "# extracted_data = {\n",
    "#     'name': 'John Doe',\n",
    "#     'age': 30,\n",
    "#     None: 'This will be skipped',\n",
    "#     '': 'This key will be skipped',\n",
    "#     'Properties': {\n",
    "#         'Endereço Profissional': {\n",
    "#             'Rua': '123 Main St',\n",
    "#             'Cidade': 'City',\n",
    "#             'CEP': '12345'\n",
    "#         },\n",
    "#         'Outra Propriedade': {\n",
    "#             'Chave1': 'Valor1',\n",
    "#             'Chave2': 'Valor2'\n",
    "#         }\n",
    "#     },\n",
    "#     'nested_list': [1, 2, 3]\n",
    "# }\n",
    "\n",
    "# persist_to_neo4j(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# pprint(node_raimir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data = node_raimir\n",
    "\n",
    "# try:\n",
    "#     persist_to_neo4j(extracted_data)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para anotação de dados em HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_published_articles(soup, qualis_data):\n",
    "    # Localiza elementos contendo os artigos publicados\n",
    "    articles_elements = soup.find_all('div', class_='PUBLICOU-article')\n",
    "    \n",
    "    # Inicializa lista para conter dados dos artigos publicados\n",
    "    annotated_articles = []\n",
    "    \n",
    "    # Itera sobre cada elemento e anotando as informações necessárias\n",
    "    for article in articles_elements:\n",
    "        title = article.find('div', class_='article-title').text\n",
    "        issn = article.find('div', class_='article-issn').text\n",
    "        qualis = qualis_data.get(issn, 'N/A')  # Buscando o Qualis correspondente\n",
    "        \n",
    "        # Adiciona ao conjunto de artigos anotados\n",
    "        annotated_articles.append({\n",
    "            'title': title,\n",
    "            'issn': issn,\n",
    "            'qualis': qualis\n",
    "        })\n",
    "    \n",
    "    # Persiste em SQLite\n",
    "    conn = sqlite3.connect(\"lattes_data.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Cria tabela de artigos publicados, se não existir\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS PUBLICOU_articles (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        title TEXT,\n",
    "        issn TEXT,\n",
    "        qualis TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insere artigos anotados na tabela\n",
    "    for article in annotated_articles:\n",
    "        cursor.execute(\"INSERT INTO PUBLICOU_articles (title, issn, qualis) VALUES (?, ?, ?)\",\n",
    "                       (article['title'], article['issn'], article['qualis']))\n",
    "    \n",
    "    # Commit e fecha a conexão\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "async def annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info):\n",
    "    print(\"Procurando por publicações de periódicos...\")\n",
    "    \n",
    "    # Inicializa BeautifulSoup para analisar o conteúdo HTML da página\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extrai e anota informações dos artigos publicados\n",
    "    qualis_info = await annotate_published_articles(driver, images_urls, qualis_data, qualis_data_cache, data_source_info, soup)\n",
    "    \n",
    "    return qualis_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_annotation_message(images_urls, visualization_url, recent_updates_url, pub_count):\n",
    "    annot_header_html = ''\n",
    "    annot_buttons_html = ''\n",
    "    pub_count_string = ''\n",
    "    \n",
    "    if pub_count > 0:\n",
    "        s_char = 's' if pub_count > 1 else ''\n",
    "        pub_count_string = f'anotou o Qualis de {pub_count} artigo{s_char} em periódico{s_char} neste CV.'\n",
    "        annot_buttons_html = render_template_string(\"\"\"\n",
    "            <a href=\"#artigos-completos\">\n",
    "                <button> Ver anotações </button>\n",
    "            </a>\n",
    "        \"\"\")\n",
    "    else:\n",
    "        pub_count_string = 'não anotou nenhum artigo em periódico neste CV.'\n",
    "\n",
    "    annot_header_html = render_template_string(\"\"\"\n",
    "        <a href=\"{{visualization_url}}\" target=\"_blank\" id=\"qlattes-logo\">\n",
    "            <img src=\"{{images_urls['qlattesLogoURL']}}\" width=\"70\">\n",
    "        </a>{{pub_count_string}}\n",
    "        </br>\n",
    "    \"\"\", visualization_url=visualization_url, images_urls=images_urls, pub_count_string=pub_count_string)\n",
    "\n",
    "    # Aqui a informação seria armazenada em um banco de dados em vez de ser injetada em um elemento HTML\n",
    "    store_annotation_in_db(annot_header_html + annot_buttons_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_qualis_data(qualis_info):\n",
    "    pub_data = []\n",
    "    pub_data_year = []\n",
    "    curr_year = 0\n",
    "    \n",
    "    for i in range(len(qualis_info)):\n",
    "        if curr_year != qualis_info[i]['year']:\n",
    "            if curr_year > 0:\n",
    "                pub_data.append({'year': curr_year, 'pubList': pub_data_year})\n",
    "                pub_data_year = []\n",
    "            \n",
    "            curr_year = qualis_info[i]['year']\n",
    "        \n",
    "        pub_data_item = {\n",
    "            'issn': qualis_info[i]['issn'],\n",
    "            'title': qualis_info[i]['title'],\n",
    "            'pubName': qualis_info[i]['pubName'],\n",
    "            'qualis': qualis_info[i]['qualisLabels']['qualis'],\n",
    "            'baseYear': qualis_info[i]['qualisLabels']['baseYear'],\n",
    "            'jcr': qualis_info[i]['jcrData']['jcr'] if 'jcrData' in qualis_info[i] and qualis_info[i]['jcrData'] else 0,\n",
    "            'jcrYear': qualis_info[i]['jcrData']['baseYear'] if 'jcrData' in qualis_info[i] and qualis_info[i]['jcrData'] else ''\n",
    "        }\n",
    "        \n",
    "        pub_data_year.append(pub_data_item)\n",
    "        \n",
    "    if len(qualis_info) > len(pub_data):\n",
    "        pub_data.append({'year': curr_year, 'pubList': pub_data_year})\n",
    "    \n",
    "    return pub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_lattes_page(name_link, images_urls, recent_updates_url, qualis_data, data_source_info):\n",
    "    print(qualis_data)\n",
    "    \n",
    "    # Inicializando o WebDriver\n",
    "    driver_service = Service('path/to/chromedriver')\n",
    "    driver = webdriver.Chrome(service=driver_service)\n",
    "    driver.get(name_link['link'])\n",
    "    \n",
    "    # URL para visualização (pode ser determinado conforme a necessidade)\n",
    "    visualization_url = 'path/to/visualization.html'\n",
    "    \n",
    "    # Limpar cache de dados de Qualis\n",
    "    qualis_data_cache = {}\n",
    "    \n",
    "    # Annotate Lattes page com informações de Qualis\n",
    "    lattes_info = await annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info)\n",
    "    \n",
    "    if lattes_info:\n",
    "        await inject_annotation_message(images_urls, visualization_url, recent_updates_url, len(lattes_info))\n",
    "    \n",
    "    # Consolidar dados de publicação a partir das informações de Lattes\n",
    "    pub_info = consolidate_qualis_data(lattes_info)\n",
    "    print(f\"pubInfo: {pub_info}\")\n",
    "    \n",
    "    # Tentativa de ler dados de Lattes do armazenamento local (neste caso, SQLite)\n",
    "    conn = sqlite3.connect('lattes_data.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM lattes_data WHERE name_link_link = ?\", (name_link['link'],))\n",
    "    existing_data = cursor.fetchall()\n",
    "    \n",
    "    lattes_data_array = []\n",
    "    \n",
    "    if existing_data:\n",
    "        # Filtrar dados existentes para evitar duplicatas\n",
    "        lattes_data_array = [elem for elem in existing_data if elem['name_link']['link'] != name_link['link']]\n",
    "    \n",
    "    # Adicionar dados de Lattes atuais ao array\n",
    "    lattes_data_array.append({'name_link': name_link, 'pub_info': pub_info})\n",
    "    \n",
    "    # Salvar array de dados de Lattes no armazenamento local (SQLite)\n",
    "    cursor.execute(\"INSERT INTO lattes_data (name_link, pub_info) VALUES (?, ?)\", (json.dumps(name_link), json.dumps(pub_info)))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Informações de nome, link e publicação de Lattes salvas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_link_html(text, tooltip, target_url):\n",
    "    return render_template_string(\"\"\"\n",
    "        <a href=\"{{target_url}}\" target=\"_blank\" title=\"{{tooltip}}\">{{text}}</a>\n",
    "    \"\"\", text=text, tooltip=tooltip, target_url=target_url)\n",
    "\n",
    "def create_icon_link_html(icon_url, icon_style, tooltip, target_url):\n",
    "    return render_template_string(\"\"\"\n",
    "        <a href=\"{{target_url}}\" target=\"_blank\" title=\"{{tooltip}}\">\n",
    "            <img src=\"{{icon_url}}\" style=\"{{icon_style}}\">\n",
    "        </a>\n",
    "    \"\"\", icon_url=icon_url, icon_style=icon_style, tooltip=tooltip, target_url=target_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_qualis_from_percentil(percentil):\n",
    "    if not percentil:\n",
    "        return 'N'\n",
    "\n",
    "    qualis_class_list     = ['A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4']\n",
    "    qualis_threshold_list = [87.5, 75, 62.5, 50, 37.5, 25, 12.5, 0]\n",
    "\n",
    "    for i in range(len(qualis_threshold_list)):\n",
    "        if percentil >= qualis_threshold_list[i]:\n",
    "            return qualis_class_list[i]\n",
    "\n",
    "    return 'N'\n",
    "\n",
    "\n",
    "def get_alternative_issn(issn, capes_alt_data, scopus_data):\n",
    "    # Pesquisar ISSN nos dados complementares da CAPES\n",
    "    match = next((elem for elem in capes_alt_data if elem['issn'] == issn or elem['alt_issn'] == issn), None)\n",
    "    \n",
    "    if match:\n",
    "        if 'alt_issn' in match and match['alt_issn'] != issn:\n",
    "            return match['alt_issn']\n",
    "        elif 'issn' in match and match['issn'] != issn:\n",
    "            return match['issn']\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        # Pesquisar ISSN nos dados do Scopus\n",
    "        match = next((elem for elem in scopus_data if elem['issn'] == issn or elem['e-issn'] == issn), None)\n",
    "        \n",
    "        if match:\n",
    "            if 'e-issn' in match and len(match['e-issn']) > 0 and match['e-issn'] != issn:\n",
    "                return match['e-issn']\n",
    "            elif 'issn' in match and len(match['issn']) > 0 and match['issn'] != issn:\n",
    "                return match['issn']\n",
    "            else:\n",
    "                return ''\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qualis_from_capes_data(issn, alt_issn, capes_data, scopus_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    # Procurar pelo ISSN na base de dados da CAPES\n",
    "    match = next((elem for elem in capes_data if elem['issn'] == issn), None)\n",
    "    \n",
    "    if match:\n",
    "        qualis_labels['source'] = 'capes'\n",
    "    elif alt_issn != '':\n",
    "        match = next((elem for elem in capes_data if elem['issn'] == alt_issn), None)\n",
    "        \n",
    "        if match:\n",
    "            qualis_labels['source'] = 'capes_alt'\n",
    "            \n",
    "    if match:\n",
    "        qualis_labels['qualis'] = match['qualis']\n",
    "        qualis_labels['pubName'] = match['title'].upper()\n",
    "        qualis_labels['baseYear'] = data_source_info[qualis_labels['source']]['baseYear']\n",
    "\n",
    "        qualis_labels_scopus = get_qualis_from_scopus_data(issn, alt_issn, scopus_data, data_source_info)\n",
    "        \n",
    "        if qualis_labels_scopus['qualis'] != 'N':\n",
    "            qualis_labels['linkScopus'] = qualis_labels_scopus['linkScopus']\n",
    "\n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "def get_qualis_from_pucrs_data(issn, alt_issn, pub_name, pucrs_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    labels_map = {\n",
    "        'pubName': 'periodico',\n",
    "        'qualis': 'Qualis_Final',\n",
    "        'percentil': 'percentil',\n",
    "        'linkScopus': 'link_scopus',\n",
    "        'adjusted': 'Ajuste_SBC'\n",
    "    }\n",
    "    \n",
    "    match = next((elem for elem in pucrs_data if elem['issn'] == issn or (alt_issn and elem['issn'] == alt_issn)), None)\n",
    "    \n",
    "    if match:\n",
    "        for key in labels_map.keys():\n",
    "            if labels_map[key] in match and match[labels_map[key]] != 'nulo':\n",
    "                qualis_labels[key] = match[labels_map[key]]\n",
    "                \n",
    "        qualis_labels['source'] = 'pucrs'\n",
    "        qualis_labels['baseYear'] = data_source_info['pucrs']['baseYear']\n",
    "\n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "def get_qualis_from_scopus_data(issn, alt_issn, scopus_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    # Procura pelo ISSN nos dados da Scopus\n",
    "    match = next((elem for elem in scopus_data if elem['issn'] == issn or elem['e-issn'] == issn), None)\n",
    "    \n",
    "    if not match and alt_issn != '':\n",
    "        match = next((elem for elem in scopus_data if elem['issn'] == alt_issn or elem['e-issn'] == alt_issn), None)\n",
    "        \n",
    "    if match:\n",
    "        qualis_labels['qualis'] = calculate_qualis_from_percentil(match['percentil'])\n",
    "        qualis_labels['pubName'] = match['title'].upper()\n",
    "        qualis_labels['percentil'] = match['percentil']\n",
    "        qualis_labels['linkScopus'] = match['source-id-url']\n",
    "        qualis_labels['source'] = 'scopus'\n",
    "        qualis_labels['baseYear'] = data_source_info['scopus']['baseYear']\n",
    "    \n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "async def get_qualis(issn, pub_name, qualis_data, qualis_data_cache, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    if not issn:\n",
    "        return qualis_labels\n",
    "\n",
    "    alt_issn = ''\n",
    "\n",
    "    # Verificar se o ISSN já está em cache\n",
    "    if issn in qualis_data_cache:\n",
    "        return qualis_data_cache[issn]\n",
    "    else:\n",
    "        # Verificar se um ISSN alternativo existe e está em cache\n",
    "        alt_issn = await get_alternative_issn(issn, qualis_data['capes_alt'], qualis_data['scopus'])\n",
    "        if alt_issn and alt_issn in qualis_data_cache:\n",
    "            return qualis_data_cache[alt_issn]\n",
    "\n",
    "    # Procurar pelo ISSN nos dados CAPES\n",
    "    qualis_labels = await get_qualis_from_capes_data(\n",
    "        issn, alt_issn, qualis_data['capes'], qualis_data['scopus'], data_source_info\n",
    "    )\n",
    "\n",
    "    # Se não encontrado\n",
    "    if qualis_labels['qualis'] == 'N':\n",
    "        # Procurar pelo ISSN nos dados PUC-RS\n",
    "        qualis_labels = await get_qualis_from_pucrs_data(\n",
    "            issn, alt_issn, pub_name, qualis_data['pucrs'], data_source_info\n",
    "        )\n",
    "\n",
    "        # Se ainda não encontrado\n",
    "        if qualis_labels['qualis'] == 'N':\n",
    "            # Procurar pelo ISSN nos dados Scopus\n",
    "            qualis_labels = await get_qualis_from_scopus_data(\n",
    "                issn, alt_issn, qualis_data['scopus'], data_source_info\n",
    "            )\n",
    "\n",
    "    # Adicionar rótulos ao cache Qualis\n",
    "    qualis_data_cache[issn] = qualis_labels\n",
    "\n",
    "    if alt_issn:\n",
    "        qualis_data_cache[alt_issn] = qualis_labels\n",
    "\n",
    "    return qualis_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_qualis_annotation(pub_info, images_URLs, data_source_info):\n",
    "    annotation_dict = {}\n",
    "    \n",
    "    # Create QLattes icon element\n",
    "    qlattes_img_elem = BeautifulSoup('<img>', 'html.parser')\n",
    "    qlattes_img_elem['src'] = images_URLs['qlattesIconURL']\n",
    "    qlattes_img_elem['style'] = 'margin-bottom:-4px'\n",
    "    \n",
    "    annotation_dict['qlattes_img_elem'] = str(qlattes_img_elem)\n",
    "    \n",
    "    # Create Qualis labels annotations\n",
    "    issn_label = f', ISSN {pub_info[\"issn\"]}' if pub_info.get('issn') else ''\n",
    "    \n",
    "    if pub_info['qualisLabels']['qualis'] == 'N':\n",
    "        qualis_annot = f' Não classificado{issn_label}'\n",
    "    else:\n",
    "        qualis_annot = f' {pub_info[\"qualisLabels\"][\"qualis\"]}{issn_label}'\n",
    "        \n",
    "        # add Data source and base year\n",
    "        source = pub_info['qualisLabels']['source']\n",
    "        source_info = data_source_info[source]\n",
    "        \n",
    "        data_source_label = f'{source_info[\"label\"]} ({source_info[\"baseYear\"]})'\n",
    "        qualis_annot += f', fonte {data_source_label}'\n",
    "    \n",
    "    annotation_dict['qualis_annot'] = qualis_annot\n",
    "    \n",
    "    # add icon with link to Google Scholar\n",
    "    base_url = 'https://scholar.google.com/scholar?q='\n",
    "    title_param = f'intitle%3A%22{pub_info[\"title\"].replace(\" \", \"+\")}%22'\n",
    "    link_scholar = f'{base_url}{title_param}'\n",
    "    \n",
    "    annotation_dict['link_scholar'] = link_scholar\n",
    "    \n",
    "    # add icon with link to Scopus (if available)\n",
    "    if pub_info['qualisLabels'].get('linkScopus'):\n",
    "        annotation_dict['link_scopus'] = pub_info['qualisLabels']['linkScopus']\n",
    "    \n",
    "    return annotation_dict\n",
    "\n",
    "def persist_annotation_div():\n",
    "    alert_div = BeautifulSoup('<div>', 'html.parser')\n",
    "    \n",
    "    alert_div['class'] = 'main-content max-width min-width'\n",
    "    alert_div['id'] = 'annot-div'\n",
    "    \n",
    "    print('Alert div persisted!')\n",
    "    \n",
    "    return str(alert_div)\n",
    "\n",
    "\n",
    "def persist_qualis_annotation(elem, pubInfo, imagesURLs, dataSourceInfo):\n",
    "    annot_dict = {}\n",
    "    \n",
    "    issnLabel = f\", ISSN {pubInfo['issn']}\" if pubInfo['issn'] else pubInfo['issn']\n",
    "    if pubInfo['qualisLabels']['qualis'] == 'N':\n",
    "        qualisAnnot = f\"Não classificado{issnLabel}\"\n",
    "    else:\n",
    "        qualisAnnot = f\"{pubInfo['qualisLabels']['qualis']}{issnLabel}\"\n",
    "        \n",
    "        dataSourceLabel = dataSourceInfo[pubInfo['qualisLabels']['source']]['label']\n",
    "        baseYear = dataSourceInfo[pubInfo['qualisLabels']['source']]['baseYear']\n",
    "        dataSourceLabel += f\" ({baseYear})\"\n",
    "        qualisAnnot += f\", fonte {dataSourceLabel}\"\n",
    "        \n",
    "    annot_dict['qualisAnnot'] = qualisAnnot\n",
    "    \n",
    "    titleParam = f\"intitle:\\\"{pubInfo['title']}\\\"\"\n",
    "    linkScholar = f\"https://scholar.google.com/scholar?q={titleParam}\"\n",
    "    annot_dict['linkScholar'] = linkScholar\n",
    "    \n",
    "    if pubInfo['qualisLabels'].get('linkScopus'):\n",
    "        annot_dict['linkScopus'] = pubInfo['qualisLabels']['linkScopus']\n",
    "    \n",
    "    elem['annotation'] = annot_dict\n",
    "\n",
    "def persist_annotation_div(imagesURLs, visualizationURL):\n",
    "    alert_div_dict = {}\n",
    "    alert_div_dict['id'] = \"annot-div\"\n",
    "    return alert_div_dict\n",
    "\n",
    "def persist_annotation_message(imagesURLs, visualizationURL, recentUpdatesURL, pubCount):\n",
    "    annot_dict = {}\n",
    "    if pubCount > 0:\n",
    "        sChar = 's' if pubCount > 1 else ''\n",
    "        pubCountString = f\"anotou o Qualis de {pubCount} artigo{sChar} em periódico{sChar}  neste CV.\"\n",
    "    else:\n",
    "        pubCountString = \"não anotou nenhum artigo em periódico neste CV.\"\n",
    "        \n",
    "    annot_dict['pubCountString'] = pubCountString\n",
    "    annot_dict['visualizationURL'] = visualizationURL\n",
    "    annot_dict['recentUpdatesURL'] = recentUpdatesURL\n",
    "    return annot_dict\n",
    "\n",
    "def set_attributes(elem, attrs):\n",
    "    for key, value in attrs.items():\n",
    "        elem[key] = value\n",
    "\n",
    "def consolidate_qualis_data(qualisInfo):\n",
    "    pubData = []\n",
    "    pubDataYear = []\n",
    "    currYear = 0\n",
    "    \n",
    "    for qInfo in qualisInfo:\n",
    "        if currYear != qInfo['year']:\n",
    "            if currYear > 0:\n",
    "                pubData.append({'year': currYear, 'pubList': pubDataYear})\n",
    "                pubDataYear = []\n",
    "            currYear = qInfo['year']\n",
    "        \n",
    "        pubDataItem = {\n",
    "            'issn': qInfo['issn'],\n",
    "            'title': qInfo['title'],\n",
    "            'pubName': qInfo['pubName'],\n",
    "            'qualis': qInfo['qualisLabels']['qualis'],\n",
    "            'baseYear': qInfo['qualisLabels']['baseYear'],\n",
    "            'jcr': qInfo['jcrData']['jcr'] if 'jcr' in qInfo['jcrData'] else 0,\n",
    "            'jcrYear': qInfo['jcrData']['baseYear'] if 'baseYear' in qInfo['jcrData'] else ''\n",
    "        }\n",
    "        pubDataYear.append(pubDataItem)\n",
    "    \n",
    "    if len(qualisInfo) > len(pubData):\n",
    "        pubData.append({'year': currYear, 'pubList': pubDataYear})\n",
    "        \n",
    "    return pubData\n",
    "\n",
    "# Suponhamos que 'html_content' seja o conteúdo HTML em que as anotações serão inseridas.\n",
    "# html_content = ...\n",
    "\n",
    "# Criamos um objeto BeautifulSoup\n",
    "# soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Aqui você pode utilizar os métodos acima para persistir as informações.\n",
    "# Exemplo:\n",
    "# elem = {}\n",
    "# persist_qualis_annotation(elem, pubInfo, imagesURLs, dataSourceInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML annotation and CV ID\n",
    "annot_html = \"<a href='some_url'>Annotated Data</a>\"\n",
    "cv_id = \"cv_123\"\n",
    "\n",
    "# Store annotation\n",
    "store_annotation_in_db(uri, user, password, annot_html, cv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def process_lattes_page_v1(name_link, images_urls, recent_updates_url, qualis_data, data_source_info): \n",
    "#     print(qualis_data)\n",
    "    \n",
    "#     # Inicializando o WebDriver\n",
    "#     driver_service = Service('path/to/chromedriver')\n",
    "#     driver = webdriver.Chrome(service=driver_service)\n",
    "#     driver.get(name_link['link'])\n",
    "    \n",
    "#     # URL para visualização (pode ser determinado conforme a necessidade)\n",
    "#     visualization_url = 'path/to/visualization.html'\n",
    "    \n",
    "#     # Limpar cache de dados de Qualis\n",
    "#     qualis_data_cache = {}\n",
    "    \n",
    "#     # Annotate Lattes page com informações de Qualis\n",
    "#     lattes_info = await annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info)\n",
    "    \n",
    "#     if lattes_info:\n",
    "#         await inject_annotation_message(images_urls, visualization_url, recent_updates_url, len(lattes_info))\n",
    "    \n",
    "#     # Consolidar dados de publicação a partir das informações de Lattes\n",
    "#     pub_info = consolidate_qualis_data(lattes_info)\n",
    "#     print(f\"pubInfo: {pub_info}\")\n",
    "    \n",
    "#     # Tentativa de ler dados de Lattes do armazenamento local (neste caso, SQLite)\n",
    "#     conn = sqlite3.connect('lattes_data.db')\n",
    "#     cursor = conn.cursor()\n",
    "#     cursor.execute(\"SELECT * FROM lattes_data WHERE name_link_link = ?\", (name_link['link'],))\n",
    "#     existing_data = cursor.fetchall()\n",
    "    \n",
    "#     lattes_data_array = []\n",
    "    \n",
    "#     if existing_data:\n",
    "#         # Filtrar dados existentes para evitar duplicatas\n",
    "#         lattes_data_array = [elem for elem in existing_data if elem['name_link']['link'] != name_link['link']]\n",
    "    \n",
    "#     # Adicionar dados de Lattes atuais ao array\n",
    "#     lattes_data_array.append({'name_link': name_link, 'pub_info': pub_info})\n",
    "    \n",
    "#     # Salvar array de dados de Lattes no armazenamento local (SQLite)\n",
    "#     cursor.execute(\"INSERT INTO lattes_data (name_link, pub_info) VALUES (?, ?)\", (json.dumps(name_link), json.dumps(pub_info)))\n",
    "#     conn.commit()\n",
    "#     conn.close()\n",
    "    \n",
    "#     print(\"Informações de nome, link e publicação de Lattes salvas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def annotate_published_articles_v2(driver, images_urls, qualis_data, qualis_data_cache, data_source_info, soup):\n",
    "#     # Localizar o primeiro elemento de artigo publicado\n",
    "#     start_elem = soup.find(\"div\", id=\"artigos-completos\")\n",
    "\n",
    "#     # Retornar uma lista vazia se não houver nenhum artigo publicado no CV\n",
    "#     if start_elem is None:\n",
    "#         return []\n",
    "\n",
    "#     qualis_info = []\n",
    "\n",
    "#     # Encontrar todos os artigos publicados\n",
    "#     pub_elems = start_elem.find_all(\"div\", class_=\"artigo-completo\")\n",
    "\n",
    "#     for pub_elem in pub_elems:\n",
    "#         qualis_pub_info = {\n",
    "#             'year': None,\n",
    "#             'issn': '',\n",
    "#             'title': '',\n",
    "#             'pubName': '',\n",
    "#             'qualisLabels': '',\n",
    "#             'jcrData': {}\n",
    "#         }\n",
    "#         # Obter o ano de publicação\n",
    "#         year_span = pub_elem.find(\"span\", class_=\"informacao-artigo\", attrs={\"data-tipo-ordenacao\": \"ano\"})\n",
    "#         if year_span:\n",
    "#             qualis_pub_info['year'] = int(year_span.text)\n",
    "        \n",
    "#         # Obter dados de publicação\n",
    "#         pub_elem_data = pub_elem.find(\"div\", attrs={\"cvuri\": True})\n",
    "\n",
    "#         if pub_elem_data:\n",
    "#             # Obter informações do periódico\n",
    "#             pub_info_string = pub_elem_data['cvuri']\n",
    "#             # Para fins de simplicidade, omitimos a função escapeHtml já que não é relevante para o BeautifulSoup\n",
    "\n",
    "#             # Obter ISSN, título e nome do periódico\n",
    "#             # Detalhes de implementação podem variar, pois o exemplo original usa JavaScript para manipular atributos DOM\n",
    "#             issn = ''  # Implemente a lógica para extrair o ISSN\n",
    "#             title = ''  # Implemente a lógica para extrair o título\n",
    "#             pub_name = ''  # Implemente a lógica para extrair o nome do periódico\n",
    "            \n",
    "#             qualis_pub_info['issn'] = issn\n",
    "#             qualis_pub_info['title'] = title\n",
    "#             qualis_pub_info['pubName'] = pub_name.upper()\n",
    "\n",
    "#             # Obter classificação Qualis do periódico\n",
    "#             qualis_labels = await get_qualis(issn, pub_name, qualis_data, qualis_data_cache, data_source_info)\n",
    "#             qualis_pub_info['qualisLabels'] = qualis_labels\n",
    "\n",
    "#             # Obter dados JCR (omitido neste exemplo; pode ser implementado conforme a necessidade)\n",
    "            \n",
    "#             # Adicionar informações ao vetor qualis_info\n",
    "#             qualis_info.append(qualis_pub_info)\n",
    "            \n",
    "#     return qualis_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def persist_to_neo4j(header_data):\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # Substitua pelo seu endpoint e credenciais\n",
    "#     header_node = Node(\"Curriculum\", title=header_data['title'], meta_keywords=header_data['meta_keywords'])\n",
    "#     graph.create(header_node)\n",
    "#     return header_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # Exemplo de entrada HTML\n",
    "#     html_content = '''\n",
    "#     <html lang=\"pt-BR\">\n",
    "#         <head>\n",
    "#             <title>Currículo do Sistema de Currículos Lattes (Raimir Holanda Filho)</title>\n",
    "#             <meta content=\"currículo,curriculo,curriculum,cv,vitae,lattes,produção,producao,científica,cientifica,Brasil\" http-equiv=\"keywords\">\n",
    "#         </head>\n",
    "#         <body>\n",
    "#             <h1 tabindex=\"0\">Educação</h1>\n",
    "#             <h1 tabindex=\"0\">Publicações</h1>\n",
    "#             <!-- ... -->\n",
    "#         </body>\n",
    "#     </html>\n",
    "#     '''\n",
    "#     # Inicialização do BeautifulSoup\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     # Extração e Persistência do cabeçalho\n",
    "#     header_data = parse_header(soup)\n",
    "#     header_node = persist_to_neo4j(header_data)\n",
    "    \n",
    "#     # Extração e Persistência dos elementos H1\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     parse_h1_elements(soup, header_node, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VERSÃO 01 de extrair dados\n",
    "# def extrair_dados(soup, verbose=False):\n",
    "#     nome_no = soup.select_one('div.infpessoa h2.nome').text if soup.select_one('div.infpessoa h2.nome') else None\n",
    "    \n",
    "#     if not nome_no:\n",
    "#         logging.error(\"Nome do nó não encontrado. Abortando.\")\n",
    "#         return\n",
    "    \n",
    "#     dados_json = {nome_no: {}}\n",
    "#     celula_principal = soup.select_one('div.layout-cell-pad-main')\n",
    "\n",
    "#     title_wrappers = celula_principal.select('div.title-wrapper')\n",
    "#     if verbose:\n",
    "#         logging.info(f'{len(title_wrappers)} seções de dados lidas com sucesso.')\n",
    "    \n",
    "#     for title_wrapper in title_wrappers:\n",
    "#         nome_secao = extrair_secao(title_wrapper)\n",
    "        \n",
    "#         if nome_secao is None:\n",
    "#             continue\n",
    "        \n",
    "#         if verbose:\n",
    "#             logging.info(f'Marcador de Seção: \"{nome_secao.text}\"')\n",
    "\n",
    "#         titulo = extrair_titulo(title_wrapper)\n",
    "#         if verbose:\n",
    "#             logging.info(f'Marcador de Título: \"{titulo}\"')\n",
    "                    \n",
    "#         chave = nome_secao.text\n",
    "#         dados_json[nome_no][chave] = {}\n",
    "\n",
    "#         # A seleção agora ocorre dentro do contexto de title_wrapper, e não de celula_principal.\n",
    "#         celulas_layout = title_wrapper.select('div.layout-cell.layout-cell-12.data-cell')\n",
    "        \n",
    "#         for celula_layout in celulas_layout:\n",
    "#             if celula_layout.find_all('div'):\n",
    "#                 indice, valores = extrair_indices(celula_layout)\n",
    "                \n",
    "#                 if indice and valores:\n",
    "#                     dados_json[nome_no][chave][indice] = valores\n",
    "\n",
    "#     if verbose:\n",
    "#         logging.info(f\"Total de índices extraídos: {len(dados_json[nome_no].keys())}\")\n",
    "#         # Outros blocos de código para depuração e verbosidade\n",
    "#     return dados_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ainda sem separador de seção\n",
    "# from bs4.element import Tag\n",
    "\n",
    "# def traverse(soup, parent_dict, current_section=None, root_properties=None):\n",
    "#     \"\"\"\n",
    "#     Traverses through the soup object recursively and populates the dictionary.\n",
    "#     root_properties: the 'Properties' dictionary at the root level, where all attributes should be stored.\n",
    "#     current_section: the current section being processed, allows to set subsections.\n",
    "#     \"\"\"\n",
    "#     section = None  \n",
    "#     node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "#     node_name = node_name_element.text if node_name_element else None\n",
    "#     if node_name is not None:\n",
    "#         parent_dict['name'] = node_name\n",
    "\n",
    "#     parag_elements = soup.find_all('p')\n",
    "#     for elem in parag_elements:\n",
    "#         class_name = elem.get('class', [None])[0]\n",
    "#         text_content = elem.get_text()\n",
    "#         if class_name:\n",
    "#             parent_dict[class_name] = text_content\n",
    "\n",
    "#     if root_properties is None:\n",
    "#         root_properties = parent_dict.setdefault('Properties', {})\n",
    "\n",
    "#     for child in soup.children:\n",
    "#         if isinstance(child, Tag):\n",
    "\n",
    "#             if child.get('id') == 'artigos-completos':\n",
    "#                 section_elem = child.findChild(\"div\", class_=\"inst_back\")\n",
    "#                 if section_elem:\n",
    "#                     section = section_elem.get_text().strip()\n",
    "#                     print(section)\n",
    "#                 subsection_elem = child.findChild(\"div\", class_=\"cita-artigos\")\n",
    "#                 if subsection_elem:\n",
    "#                     subsection = subsection_elem.get_text().strip()\n",
    "#                 jcr_articles_dict = parse_jcr_articles(soup)\n",
    "#                 if jcr_articles_dict:\n",
    "#                     root_properties.update(jcr_articles_dict['Properties'])\n",
    "\n",
    "#             elif \"layout-cell-3\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "#                 cell_key = extract_data_from_cell(child)\n",
    "#                 if cell_key:\n",
    "#                     cell_key = ' '.join(cell_key)\n",
    "\n",
    "#                     sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-9\")\n",
    "#                     if sibling_cell:\n",
    "#                         cell_values = extract_data_from_cell(sibling_cell)\n",
    "\n",
    "#                         if current_section:\n",
    "#                             root_properties[current_section][cell_key] = cell_values\n",
    "#                         else:\n",
    "#                             root_properties[cell_key] = cell_values\n",
    "\n",
    "#             elif \"layout-cell-1\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "#                 cell_key = extract_data_from_cell(child)\n",
    "#                 if cell_key:\n",
    "#                     cell_key = ' '.join(cell_key)\n",
    "#                     subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "#                     if subsection_elem:\n",
    "#                         subsection = subsection_elem.get_text().strip()\n",
    "\n",
    "#                         current_section_dict = root_properties.get(current_section, {})\n",
    "#                         # Convert to dictionary if it is a list\n",
    "#                         if isinstance(current_section_dict, list):\n",
    "#                             current_section_dict = {i: item for i, item in enumerate(current_section_dict)}\n",
    "#                             root_properties[current_section] = current_section_dict\n",
    "#                         subsection_dict = current_section_dict.setdefault(subsection, {})\n",
    "                        \n",
    "#                         sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-11\")\n",
    "#                         if sibling_cell:\n",
    "#                             cell_values = extract_data_from_cell(sibling_cell)\n",
    "#                             subsection_dict[cell_key] = cell_values\n",
    "\n",
    "#             title = child.find(\"h1\")\n",
    "#             if title:\n",
    "#                 current_section = title.get_text().strip()\n",
    "#                 new_dict = {}\n",
    "#                 root_properties[current_section] = new_dict\n",
    "#                 traverse(child, new_dict, current_section, root_properties)\n",
    "#             else:\n",
    "#                 traverse(child, parent_dict, current_section, root_properties)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
