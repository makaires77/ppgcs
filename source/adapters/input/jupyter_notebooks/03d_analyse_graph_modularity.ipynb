{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Machine Learning para identificação dinâmica de comunidades em grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicação às comunidades de Produção Acadêmica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from py2neo import Graph\n",
    "\n",
    "class Neo4jConnection:\n",
    "\n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "        \n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "        \n",
    "    def query(self, query, parameters=None, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try:\n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session() \n",
    "            response = list(session.run(query, parameters))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally:\n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response\n",
    "\n",
    "class Neo4jMetricsExtractor:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def distinct_community_names(self):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (n:GrandeÁrea)\n",
    "            RETURN DISTINCT n.name AS community_name\n",
    "            \"\"\")\n",
    "            return [record[\"community_name\"] for record in result]\n",
    "    \n",
    "    def distinct_community_codes(self):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (n:GrandeÁrea)\n",
    "            RETURN DISTINCT n.code AS community_code\n",
    "            \"\"\")\n",
    "            return [record[\"community_code\"] for record in result]\n",
    "\n",
    "    def community_density(self, community_code):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (n:GrandeÁrea) WHERE n.code = $community_code\n",
    "            WITH COUNT(n) AS node_count\n",
    "            MATCH (n:GrandeÁrea)-[r]-() WHERE n.code = $community_code\n",
    "            WITH node_count, COUNT(r) AS rel_count\n",
    "            RETURN CASE WHEN node_count = 0 THEN 0 ELSE rel_count * 1.0 / node_count END AS density\n",
    "            \"\"\", community_code=community_code)\n",
    "            \n",
    "            record = result.single()\n",
    "            if record:\n",
    "                return record[\"density\"]\n",
    "            else:\n",
    "                raise ValueError(f\"No data found for community: {community_code}\")\n",
    "                \n",
    "    def average_semantic_cohesion(self, community_code):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (ga:GrandeÁrea {code: $community_code})-[:CONTÉM_ÁREA]->(:Área)-[:CONTÉM_SUBÁREA]->(:Subárea)-[:CONTÉM_ESPECIALIDADE]->(h)\n",
    "            WHERE (h:Especialidade OR h:Subárea)\n",
    "            WITH collect(h) AS hierarchies\n",
    "            UNWIND hierarchies AS hierarchy\n",
    "            MATCH (p:Publicacao)-[r:SIMILAR]->(hierarchy)\n",
    "            WITH COUNT(r) AS total_relations, SIZE(hierarchies) AS total_hierarchies\n",
    "            RETURN total_relations * 1.0 / total_hierarchies AS average_semantic_cohesion\n",
    "            \"\"\", community_code=community_code)\n",
    "            record = result.single()\n",
    "            return record[\"average_semantic_cohesion\"] if record else None\n",
    "\n",
    "    def predominant_hierarchy(self, community_code):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (n) WHERE n.code = $community_code\n",
    "            RETURN labels(n) AS node_labels, COUNT(*) AS frequency\n",
    "            ORDER BY frequency DESC\n",
    "            LIMIT 1\n",
    "            \"\"\", community_code=community_code)\n",
    "            record = result.single()\n",
    "            predominant_label = [label for label in record[\"node_labels\"] if label in [\"GrandeÁrea\", \"Área\", \"Subárea\", \"Especialidade\"][0]]\n",
    "            return predominant_label, record[\"frequency\"]\n",
    "\n",
    "\n",
    "    def community_name_density(self, community_name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (n:GrandeÁrea) WHERE n.name = $community_name\n",
    "            WITH COUNT(n) AS node_count\n",
    "            MATCH (n:GrandeÁrea)-[r]-() WHERE n.name = $community_name\n",
    "            WITH node_count, COUNT(r) AS rel_count\n",
    "            RETURN CASE WHEN node_count = 0 THEN 0 ELSE rel_count * 1.0 / node_count END AS density\n",
    "            \"\"\", community_name=community_name)\n",
    "            \n",
    "            record = result.single()\n",
    "            if record:\n",
    "                return record[\"density\"]\n",
    "            else:\n",
    "                raise ValueError(f\"No data found for community: {community_name}\")\n",
    "\n",
    "    def average_name_semantic_cohesion(self, community_name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (n1:GrandeÁrea)-[r]->(n2:GrandeÁrea) WHERE n1.name = $community_name AND n2.name = $community_name\n",
    "            RETURN AVG(r.similarity) AS average_semantic_cohesion\n",
    "            \"\"\", community_name=community_name)\n",
    "            return result.single()[\"average_semantic_cohesion\"]\n",
    "\n",
    "    def predominant_name_hierarchy(self, community_name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (n:GrandeÁrea) WHERE n.name = $community_name\n",
    "            RETURN n.hierarchy, COUNT(*) AS frequency\n",
    "            ORDER BY frequency DESC\n",
    "            LIMIT 1\n",
    "            \"\"\", community_name=community_name)\n",
    "            record = result.single()\n",
    "            return record[\"n.hierarchy\"], record[\"frequency\"]   \n",
    "\n",
    "    def count_all_nodes(self):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (n)\n",
    "            RETURN count(n) as qte_nodes\n",
    "            \"\"\")\n",
    "            record = result.single()\n",
    "            return record[\"qte_nodes\"] if record else 0\n",
    "    \n",
    "    def count_unconnected_publications(self):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (p:Publicacao)\n",
    "            WHERE NOT (p)--()\n",
    "            RETURN COUNT(p) AS unconnected_publications_count\n",
    "            \"\"\")\n",
    "            record = result.single()\n",
    "            return record[\"unconnected_publications_count\"] if record else 0\n",
    "    \n",
    "    def create_named_projection(self):\n",
    "        with self._driver.session() as session:\n",
    "            # Suponho que estamos considerando todas as relações e nodos e que a propriedade de score é relevante\n",
    "            query = \"\"\"\n",
    "            CALL gds.graph.create('similarity_graph', '*', \n",
    "            {\n",
    "                ALL: {\n",
    "                    type: '*', \n",
    "                    properties: 'score'\n",
    "                }\n",
    "            })\n",
    "            \"\"\"\n",
    "            session.run(query)\n",
    "\n",
    "    def delete_named_projection(self):\n",
    "        with self._driver.session() as session:\n",
    "            session.run(\"CALL gds.graph.drop('similarity_graph')\")\n",
    "\n",
    "    def query(self, query, parameters=None, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try:\n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session() \n",
    "            response = list(session.run(query, parameters))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally:\n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response\n",
    "    \n",
    "    def calculate_louvain(self, score_threshold):\n",
    "        with self._driver.session() as session:\n",
    "            # First, create the named graph projection\n",
    "            session.run(\"\"\"\n",
    "            CALL gds.graph.project.cypher(\n",
    "              'louvainGraph',\n",
    "              'MATCH (n) RETURN id(n) AS id',\n",
    "              'MATCH (n1)-[r:SIMILAR]->(n2) WHERE r.score > $scoreThreshold RETURN id(n1) AS source, id(n2) AS target, r.score AS weight',\n",
    "              {parameters: {scoreThreshold: $scoreThreshold}}\n",
    "            )\n",
    "            YIELD graphName\n",
    "            \"\"\", scoreThreshold=score_threshold)\n",
    "            \n",
    "            # Then, compute Louvain communities on the projection\n",
    "            result = session.run(\"\"\"\n",
    "            CALL gds.louvain.stream('louvainGraph')\n",
    "            YIELD nodeId, communityId\n",
    "            RETURN gds.util.asNode(nodeId).name AS nodeName, communityId\n",
    "            \"\"\")\n",
    "            \n",
    "            # Drop the graph projection after the analysis\n",
    "            session.run(\"CALL gds.graph.drop('louvainGraph')\")\n",
    "            \n",
    "            # Return the Louvain computation result\n",
    "            return [(record[\"nodeName\"], record[\"communityId\"]) for record in result]\n",
    "        \n",
    "    def calculate_louvain_and_modularity(self, score_threshold):\n",
    "        with self._driver.session() as session:\n",
    "            # Crie a projeção do grafo com base no limiar especificado\n",
    "            session.run(\"\"\"\n",
    "            CALL gds.graph.create.cypher(\n",
    "                'dynamic_similarity_graph',\n",
    "                'MATCH (n) RETURN id(n) AS id',\n",
    "                'MATCH (n1)-[r:SIMILAR]->(n2) WHERE r.score >= $score_threshold RETURN id(n1) AS source, id(n2) AS target, r.score AS weight',\n",
    "                { parameters: { score_threshold: $score_threshold } }\n",
    "            )\n",
    "            \"\"\", score_threshold=score_threshold)\n",
    "\n",
    "            # Execute o algoritmo Louvain na projeção do grafo criada\n",
    "            louvain_result = session.run(\"\"\"\n",
    "            CALL gds.louvain.stream('dynamic_similarity_graph', { includeIntermediateCommunities: false })\n",
    "            YIELD nodeId, communityId\n",
    "            RETURN gds.util.asNode(nodeId).name AS name, communityId\n",
    "            \"\"\")\n",
    "            \n",
    "            # Colete os resultados de Louvain\n",
    "            communities = list(louvain_result)\n",
    "\n",
    "            # Calcule a modularidade\n",
    "            modularity_result = session.run(\"\"\"\n",
    "            CALL gds.louvain.mutate('dynamic_similarity_graph', {\n",
    "                mutateProperty: 'louvain',\n",
    "                includeIntermediateCommunities: false\n",
    "            })\n",
    "            YIELD communityCount, modularities\n",
    "            RETURN communityCount, modularities[0] AS modularity\n",
    "            \"\"\")\n",
    "            modularity = modularity_result.single()\n",
    "\n",
    "            # Delete a projeção do grafo após o uso\n",
    "            session.run(\"CALL gds.graph.drop('dynamic_similarity_graph')\")\n",
    "\n",
    "            return communities, modularity        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2394\n"
     ]
    }
   ],
   "source": [
    "# Criar uma instância do extrator\n",
    "extractor = Neo4jMetricsExtractor(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n",
    "\n",
    "# Contagem de nós\n",
    "qte_nodes = extractor.count_all_nodes()\n",
    "print(qte_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ciências Exatas e da Terra', 0)\n",
      "('Matemática', 1)\n",
      "('Elderly', 2)\n",
      "('Set', 3)\n",
      "('Mathematics Logic', 4)\n"
     ]
    }
   ],
   "source": [
    "# Identificar comunidades com Algoritmo de Louvain\n",
    "\n",
    "extractor = Neo4jMetricsExtractor(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "communities_by_threshold = extractor.calculate_louvain(thresholds)\n",
    "for i in communities_by_threshold[:5]:\n",
    "    print(i)\n",
    "extractor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade total de nós Produção isolados: 26\n",
      "\n",
      "Code 1.00.00.00:\n",
      "- Density: 8.0\n",
      "- Cohesion: 24.494897959183675\n",
      "- Predominant Hierarchy: ['GrandeÁrea'] (Frequency: 1)\n",
      "\n",
      "Code 2.00.00.00:\n",
      "- Density: 13.0\n",
      "- Cohesion: 6.617647058823529\n",
      "- Predominant Hierarchy: ['GrandeÁrea'] (Frequency: 1)\n",
      "\n",
      "Code 3.00.00.00:\n",
      "- Density: 13.0\n",
      "- Cohesion: 11.45531914893617\n",
      "- Predominant Hierarchy: ['GrandeÁrea'] (Frequency: 1)\n",
      "\n",
      "Code 4.00.00.00:\n",
      "- Density: 9.0\n",
      "- Cohesion: 8.3125\n",
      "- Predominant Hierarchy: ['GrandeÁrea'] (Frequency: 1)\n",
      "\n",
      "Code 5.00.00.00:\n",
      "- Density: 7.0\n",
      "- Cohesion: 18.608695652173914\n",
      "- Predominant Hierarchy: ['GrandeÁrea'] (Frequency: 1)\n",
      "\n",
      "Code 6.00.00.00:\n",
      "- Density: 13.0\n",
      "- Cohesion: 28.8062015503876\n",
      "- Predominant Hierarchy: ['GrandeÁrea'] (Frequency: 1)\n",
      "\n",
      "Code 7.00.00.00:\n",
      "- Density: 10.0\n",
      "- Cohesion: 31.072164948453608\n",
      "- Predominant Hierarchy: ['GrandeÁrea'] (Frequency: 1)\n",
      "\n",
      "Code 8.00.00.00:\n",
      "- Density: 3.0\n",
      "- Cohesion: 0.8260869565217391\n",
      "- Predominant Hierarchy: ['GrandeÁrea'] (Frequency: 1)\n"
     ]
    }
   ],
   "source": [
    "extractor = Neo4jMetricsExtractor(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n",
    "\n",
    "isolated_production_nodes = extractor.count_unconnected_publications()\n",
    "print(f\"Quantidade total de nós Produção isolados: {isolated_production_nodes}\")\n",
    "\n",
    "# Get distinct community codes\n",
    "community_codes = extractor.distinct_community_codes()\n",
    "\n",
    "# For each community code, calculate and print the metrics\n",
    "for code in community_codes:\n",
    "    try:\n",
    "        density = extractor.community_density(code)\n",
    "        cohesion = extractor.average_semantic_cohesion(code)\n",
    "        hierarchy, frequency = extractor.predominant_hierarchy(code)\n",
    "        print(f\"\\nCode {code}:\")\n",
    "        print(f\"- Density: {density}\")\n",
    "        print(f\"- Cohesion: {cohesion}\")\n",
    "        print(f\"- Predominant Hierarchy: {hierarchy} (Frequency: {frequency})\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error for community {code}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Neo4jMetricsExtractor(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n",
    "\n",
    "# Get distinct community codes\n",
    "community_names = extractor.distinct_community_names()\n",
    "\n",
    "# For each community code, calculate and print the metrics\n",
    "for name in community_names:\n",
    "    try:\n",
    "        density = extractor.community_name_density(name)\n",
    "        cohesion = extractor.average_name_semantic_cohesion(name)\n",
    "        hierarchy, frequency = extractor.predominant_name_hierarchy(name)\n",
    "        print(f\"Community {name}:\")\n",
    "        print(f\"- Density: {density}\")\n",
    "        print(f\"- Cohesion: {cohesion}\")\n",
    "        print(f\"- Predominant Hierarchy: {hierarchy} (Frequency: {frequency})\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error for community {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import optuna\n",
    "import logging\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "from community import community_louvain\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from py2neo import Graph, ServiceUnavailable, Neo4jError\n",
    "\n",
    "class CosineSimilarityRelationship:\n",
    "    def __init__(self, uri, user, password, model_name=\"default_model\"):\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.connect()\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.graph = Graph(self.uri, auth=(self.user, self.password))\n",
    "        except ServiceUnavailable as e:\n",
    "            logging.error(f\"Failed to connect to the database. Reason: {e}\")\n",
    "            raise ServiceUnavailable(f\"Failed to connect to the database. Reason: {e}\")\n",
    "\n",
    "    def check_connection(self):\n",
    "        if not hasattr(self, 'graph') or not self.graph:\n",
    "            logging.error(\"No database connection\")\n",
    "            raise Exception(\"No database connection\")\n",
    "\n",
    "    def cosine_similarity(self, a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "    def normalize_similarity(self, similarity):\n",
    "        return (similarity + 1) / 2\n",
    "\n",
    "    def get_memory_usage(self):\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_MB = process.memory_info().rss / (1024 ** 2)\n",
    "        return memory_MB\n",
    "\n",
    "    def get_all_embeddings(self, label):\n",
    "        cursor = self.graph.run(f\"MATCH (n:{label}) RETURN id(n) AS id, n.embedding AS embedding\")\n",
    "        return [record for record in cursor]\n",
    "\n",
    "    def process_similarity_for_nodes(self, source_embeddings, target_embeddings, source_label, target_label, threshold, batch_size):\n",
    "        relationships_created_count = 0\n",
    "        node_pairs_count = 0\n",
    "        tx = self.graph.begin()\n",
    "        try:\n",
    "            for source in tqdm(source_embeddings, desc=f\"Processing Semantic Similarity {source_label}/{target_label}\"):\n",
    "                for target in target_embeddings:\n",
    "                    similarity = self.cosine_similarity(np.array(source[\"embedding\"]), np.array(target[\"embedding\"]))\n",
    "                    normalized_similarity = self.normalize_similarity(similarity)\n",
    "                    node_pairs_count += 1\n",
    "                    if similarity > threshold:\n",
    "                        query = f\"\"\"\n",
    "                        MATCH (source:{source_label}) WHERE id(source) = $source_id\n",
    "                        MATCH (target:{target_label}) WHERE id(target) = $target_id\n",
    "                        MERGE (source)-[:SIMILAR {{score: $similarity, weight: $normalized_similarity}}]->(target)\n",
    "                        \"\"\"\n",
    "                        tx.run(query, source_id=source[\"id\"], target_id=target[\"id\"], similarity=float(similarity), normalized_similarity=float(normalized_similarity))\n",
    "                        relationships_created_count += 1\n",
    "                        if relationships_created_count % batch_size == 0:\n",
    "                            tx.commit()\n",
    "                            logging.info(f\"Committed {batch_size} relationships for {source_label}/{target_label}.\")\n",
    "                            tx = self.graph.begin()\n",
    "            if tx:\n",
    "                tx.commit()\n",
    "                logging.info(f\"Committed remaining relationships for {source_label}/{target_label}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during transaction: {e}\")\n",
    "            tx.rollback()\n",
    "            if tx:\n",
    "                tx.rollback()\n",
    "        return node_pairs_count, relationships_created_count\n",
    "\n",
    "    def create_similarity_embeedings_relationships(self, threshold=0.7, batch_size=3000):\n",
    "        start_time = time.time()\n",
    "        self.check_connection()\n",
    "        initial_memory = self.get_memory_usage()\n",
    "\n",
    "        pub_embeddings = self.get_all_embeddings(\"Publicacao\")\n",
    "        esp_embeddings = self.get_all_embeddings(\"Especialidade\")\n",
    "        sub_embeddings = self.get_all_embeddings(\"Subárea\")\n",
    "\n",
    "        total_pub = len(pub_embeddings)\n",
    "        total_esp = len(esp_embeddings)\n",
    "        total_sub = len(sub_embeddings)\n",
    "\n",
    "        logging.info(f\"Nodes: Publicacao {total_pub}, Especialidade {total_esp}, Subárea {total_sub}\")\n",
    "\n",
    "        pub_sub_pairs, pub_sub_rels = self.process_similarity_for_nodes(pub_embeddings, sub_embeddings, \"Publicacao\", \"Subárea\", threshold, batch_size)\n",
    "        pub_esp_pairs, pub_esp_rels = self.process_similarity_for_nodes(pub_embeddings, esp_embeddings, \"Publicacao\", \"Especialidade\", threshold, batch_size)\n",
    "\n",
    "        final_memory = self.get_memory_usage()\n",
    "        end_time = time.time()\n",
    "        memory_difference = final_memory - initial_memory\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        logging.info(f\"RAM Consumption: {np.round(memory_difference,2)} MB\")\n",
    "        logging.info(f\"Processing Time: {np.round(processing_time,2)} seconds\")\n",
    "        logging.info(f\"Current Memory Usage: {np.round(final_memory,2)} MB\")\n",
    "        logging.info(f\"Execution time for similarity calculations and relationship creation: {np.round(processing_time, 2)} seconds\")\n",
    "        logging.info(f\"Similarity threshold: {threshold}\")\n",
    "        logging.info(f\"Total node pairs analyzed: {pub_sub_pairs + pub_esp_pairs}\")\n",
    "        logging.info(f\"Node pairs Publicacao/Subárea: {pub_sub_pairs}\")\n",
    "        logging.info(f\"Node pairs Publicacao/Especialidade: {pub_esp_pairs}\")\n",
    "        logging.info(f\"Total relationships created: {pub_sub_rels + pub_esp_rels}\")\n",
    "\n",
    "    def run_similarity_operations(self, threshold=0.7):\n",
    "        self.create_similarity_embeedings_relationships(threshold)\n",
    "\n",
    "class DatabaseConnectionManager:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "\n",
    "    def get_graph(self):\n",
    "        try:\n",
    "            return Graph(self.uri, auth=(self.user, self.password))\n",
    "        except ServiceUnavailable as e:\n",
    "            logging.error(f\"Failed to connect to the database. Reason: {e}\")\n",
    "            raise\n",
    "\n",
    "class GraphDataRetriever:\n",
    "    def __init__(self, uri, user, password):\n",
    "        try:\n",
    "            self.graph = Graph(uri, auth=(user, password))\n",
    "        except Neo4jError as e:\n",
    "            print(\"Error connecting to the Neo4j database:\", e)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def retrieve_graph_data(self, threshold):\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "            MATCH (p:Publicacao)-[s:SIMILAR]->(t)\n",
    "            WHERE s.score >= {}\n",
    "            RETURN p, s, t\n",
    "            \"\"\".format(threshold)\n",
    "            results = self.graph.run(query)\n",
    "\n",
    "            node_features, edge_features, edge_index = [], [], []\n",
    "\n",
    "            for record in results:\n",
    "                p, s, t = record['p'], record['s'], record['t']\n",
    "                degree = len(list(self.graph.match((p, None, None))))\n",
    "                node_features.append([degree])\n",
    "                edge_features.append([s['score']])\n",
    "                edge_index.append([p.id, t.id])\n",
    "\n",
    "            node_features = torch.tensor(node_features, dtype=torch.float).to(self.device)\n",
    "            edge_features = torch.tensor(edge_features, dtype=torch.float).to(self.device)\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous().to(self.device)\n",
    "\n",
    "            # Compute global graph features\n",
    "            # Centralidade dos Nós\n",
    "            degree_centrality_query = \"\"\"\n",
    "            MATCH (n:Publicacao)\n",
    "            RETURN n.name AS node, size((n)--()) AS degree\n",
    "            \"\"\"\n",
    "            degree_centrality = self.graph.run(degree_centrality_query).data()\n",
    "\n",
    "            # Coeficiente de Aglomeração (Clustering Coefficient)\n",
    "            clustering_coefficient_query = \"\"\"\n",
    "            MATCH (n:Publicacao)\n",
    "            WITH n, apoc.node.degree(n) AS degree, apoc.coll.toSet([neighbor IN apoc.neighbors.nodes(n) | neighbor]) AS neighbors\n",
    "            WHERE degree > 1\n",
    "            RETURN n.name AS node, toFloat(size(apoc.coll.pairsMin(neighbors)[index IN range(0, size(apoc.coll.pairsMin(neighbors))-2) | \n",
    "            CASE WHEN (neighbors[index])-->(neighbors[index+1]) THEN 1 ELSE 0 END]) + size(apoc.coll.pairsMin(neighbors)[index IN range(0, \n",
    "            size(apoc.coll.pairsMin(neighbors))-2) | CASE WHEN (neighbors[index+1])-->(neighbors[index]) THEN 1 ELSE 0 END]))/toFloat((degree*(degree-1))) AS clustering\n",
    "            \"\"\"\n",
    "            clustering_coefficient = self.graph.run(clustering_coefficient_query).data()\n",
    "\n",
    "            # Densidade do Grafo\n",
    "            graph_density_query = \"\"\"\n",
    "            MATCH (n:Publicacao), (m:Publicacao) WHERE id(n) < id(m)\n",
    "            RETURN toFloat(count(distinct(n, m))) / (toFloat(count(n)*count(n)-1)/2) AS graph_density\n",
    "            \"\"\"\n",
    "            graph_density = self.graph.run(graph_density_query).data()[0]['graph_density']\n",
    "\n",
    "            # Transitividade\n",
    "            graph_transitivity_query = \"\"\"\n",
    "            CALL apoc.metrics.clusteringCoefficient()\n",
    "            YIELD clusteringCoefficient\n",
    "            RETURN clusteringCoefficient\n",
    "            \"\"\"\n",
    "            graph_transitivity = self.graph.run(graph_transitivity_query).data()[0]['clusteringCoefficient']\n",
    "\n",
    "            # Número de Componentes Conexas\n",
    "            connected_components_query = \"\"\"\n",
    "            CALL apoc.algo.unionFind('Publicacao')\n",
    "            YIELD partition\n",
    "            RETURN count(distinct partition) AS number_of_components\n",
    "            \"\"\"\n",
    "            num_connected_components = self.graph.run(connected_components_query).data()[0]['number_of_components']\n",
    "\n",
    "            # Assortatividade\n",
    "            assortativity_query = \"\"\"\n",
    "            CALL apoc.metrics.assortativity.degree('Publicacao', 'INCOMING', 'INCOMING')\n",
    "            YIELD assortativityCoefficient\n",
    "            RETURN assortativityCoefficient\n",
    "            \"\"\"\n",
    "            assortativity = self.graph.run(assortativity_query).data()[0]['assortativityCoefficient']\n",
    "\n",
    "            # Integração com o objeto Data\n",
    "            data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)\n",
    "            data.degree_centrality = torch.tensor([node['degree'] for node in degree_centrality], dtype=torch.float)\n",
    "            data.clustering_coefficient = torch.tensor([node['clustering'] for node in clustering_coefficient], dtype=torch.float)\n",
    "            data.graph_density = torch.tensor([graph_density], dtype=torch.float)\n",
    "            data.graph_transitivity = torch.tensor([graph_transitivity], dtype=torch.float)\n",
    "            data.num_connected_components = torch.tensor([num_connected_components], dtype=torch.int)\n",
    "            data.assortativity = torch.tensor([assortativity], dtype=torch.float)\n",
    "\n",
    "            return data\n",
    "\n",
    "        except Neo4jError as e:\n",
    "            print(\"Error retrieving graph data:\", e)\n",
    "            return None\n",
    "\n",
    "    def create_dataset(self, thresholds):\n",
    "        dataset = []\n",
    "        for t in thresholds:\n",
    "            data = self.retrieve_graph_data(t)\n",
    "            dataset.append(data)\n",
    "        return dataset\n",
    "\n",
    "class DataLoaderExtension(CosineSimilarityRelationship):\n",
    "    def __init__(self, uri, user, password, model_name=\"default_model\"):\n",
    "        super().__init__(uri, user, password, model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _create_tensors_from_nodes_and_edges(self, nodes, edges):\n",
    "        \"\"\"Transforms nodes and edges into PyTorch tensors.\"\"\"\n",
    "        try:\n",
    "            x = torch.tensor([node['embedding'] for node in nodes], dtype=torch.float).to(self.device)\n",
    "            edge_index = torch.tensor([edge[:2] for edge in edges], dtype=torch.long).t().contiguous().to(self.device)\n",
    "            edge_weights = torch.tensor([edge[2] for edge in edges], dtype=torch.float).to(self.device)\n",
    "            return x, edge_index, edge_weights\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in _create_tensors_from_nodes_and_edges: {e}\")\n",
    "            raise\n",
    "\n",
    "    def graph_to_pytorch_data(self):\n",
    "        \"\"\"Fetches graph data from Neo4j and returns it as PyTorch geometric data.\"\"\"\n",
    "        try:\n",
    "            pub_cursor = self.graph.run(\"MATCH (p:Publicacao) RETURN id(p) AS id, p.embedding AS embedding\")\n",
    "            all_nodes = list(pub_cursor)\n",
    "\n",
    "            if not all_nodes:\n",
    "                logging.warning(\"No nodes were found in the database.\")\n",
    "                return None\n",
    "\n",
    "            rel_cursor = self.graph.run(\"\"\"\n",
    "            MATCH (p1:Publicacao)-[r:SIMILAR]->(p2)\n",
    "            RETURN id(p1) AS source, id(p2) AS target, r.score AS weight\n",
    "            \"\"\")\n",
    "            # RETURN id(p1) AS source, id(p2) AS target, COALESCE(r.score, 1) AS weight\n",
    "            # RETURN id(p1) AS source, id(p2) AS target, r.score AS weight\n",
    "            all_edges = [(record['source'], record['target'], record['weight']) for record in rel_cursor]\n",
    "\n",
    "            x, edge_index, edge_weights = self._create_tensors_from_nodes_and_edges(all_nodes, all_edges)\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_weights)\n",
    "            \n",
    "            return data\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in graph_to_pytorch_data: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_data_loader(self, batch_size=32):\n",
    "        \"\"\"Creates a DataLoader with the provided batch size.\"\"\"\n",
    "        dataset = [self.graph_to_pytorch_data()]\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True if self.device.type == \"cuda\" else False)\n",
    "        \n",
    "    def calculate_modularity_neo4j(self):\n",
    "        \"\"\"Computes modularity using Neo4j's Louvain algorithm implementation.\"\"\"\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "            CALL gds.louvain.stream({\n",
    "                nodeProjection: 'Publicacao',\n",
    "                relationshipProjection: {\n",
    "                    SIMILAR: {\n",
    "                        type: 'SIMILAR',\n",
    "                        properties: 'weight',\n",
    "                        orientation: 'UNDIRECTED'\n",
    "                    }\n",
    "                },\n",
    "                includeIntermediateCommunities: false\n",
    "            })\n",
    "            YIELD nodeId, communityId\n",
    "            RETURN gds.modularity(nodeId, communityId) as modularity\n",
    "            \"\"\"\n",
    "            \n",
    "            result = self.graph.run(query).single()\n",
    "            modularity = result['modularity']\n",
    "            \n",
    "            return modularity\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in calculate_modularity_neo4j: {e}\")\n",
    "            return None\n",
    "\n",
    "class GraphClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        \"\"\"Initialization of the Graph Classification Model.\"\"\"\n",
    "        super(GraphClassificationModel, self).__init__()\n",
    "        self.conv1 = GraphSAGE(in_channels, hidden_channels)\n",
    "        self.conv2 = GraphSAGE(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"Forward propagation logic.\"\"\"\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def create_graph_with_threshold(self, threshold):\n",
    "        \"\"\"\n",
    "        Retorna os dados do grafo utilizando DataLoaderExtension e aplica o threshold fornecido.\n",
    "        \n",
    "        Args:\n",
    "        - threshold (float): Limiar para determinar a relação entre os nós.\n",
    "        \n",
    "        Returns:\n",
    "        - data (PyTorch Data Object): Dados do grafo formatados para PyTorch.\n",
    "        \"\"\"\n",
    "        # Incorporando a classe CosineSimilarityRelationship\n",
    "        similarity_relationship = CosineSimilarityRelationship()\n",
    "        \n",
    "        # Utilizando a classe DataLoaderExtension para obter os dados do grafo\n",
    "        loader_extension = DataLoaderExtension(self.uri, self.user, self.password)\n",
    "        \n",
    "        # Gerando os dados do grafo com o threshold aplicado\n",
    "        data = loader_extension.graph_to_pytorch_data(threshold, similarity_relationship)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def objective(self, trial):\n",
    "        \"\"\"Objective function for hyperparameter tuning using Optuna.\"\"\"\n",
    "        threshold = trial.suggest_float('threshold', 0.1, 1.0)\n",
    "        data = self.create_graph_with_threshold(threshold)\n",
    "\n",
    "        loader = DataLoader([data], batch_size=32, shuffle=True)\n",
    "\n",
    "        num_features = data.x.size(1)\n",
    "        num_classes = len(set(data.y.tolist())) # Assuming data.y contains labels.\n",
    "\n",
    "        model = GraphClassificationModel(num_features, 128, num_classes)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(100):\n",
    "            for batch in loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch.x, batch.edge_index)\n",
    "                loss = criterion(out, batch.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        G = to_networkx(data)\n",
    "        partition = community_louvain.best_partition(G)\n",
    "        modularity = community_louvain.modularity(partition, G)\n",
    "\n",
    "        return modularity\n",
    "\n",
    "class CommunityDetection:\n",
    "    \"\"\"\n",
    "    This class is responsible for identifying and evaluating communities within graph-structured data.\n",
    "    It utilizes the DynamicGraphLearning and DataLoaderExtension classes to extract meaningful representations\n",
    "    of nodes and subsequently uses these representations for community detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, uri, user, password, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Constructor for the CommunityDetection class.\n",
    "\n",
    "        Parameters:\n",
    "        - uri (str): URI for the database connection.\n",
    "        - user (str): Username for the database connection.\n",
    "        - password (str): Password for the database connection.\n",
    "        - learning_rate (float): Learning rate for the graph learning model.\n",
    "        \"\"\"\n",
    "        self.dynamic_graph_learning = GraphClassificationModel(uri, user, password, learning_rate)\n",
    "        self.data_loader_extension = DataLoaderExtension(uri, user, password)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def detect_communities(self, output):\n",
    "        \"\"\"\n",
    "        Detects communities within a graph based on the provided node representations.\n",
    "\n",
    "        Parameters:\n",
    "        - output (Tensor): Node representations generated by a graph neural network.\n",
    "\n",
    "        Returns:\n",
    "        - dict: Mapping of node indices to their respective community.\n",
    "        \"\"\"\n",
    "        data = self.data_loader_extension.graph_to_pytorch_data()\n",
    "        G = to_networkx(data, to_undirected=True)\n",
    "        \n",
    "        # Using the embeddings as node attributes for community detection.\n",
    "        for i, node in enumerate(G.nodes()):\n",
    "            G.nodes[node]['embedding'] = output[i].cpu().numpy()\n",
    "        \n",
    "        partition = community_louvain.best_partition(G, weight='embedding')\n",
    "        \n",
    "        return partition\n",
    "\n",
    "    def evaluate_communities(self):\n",
    "        \"\"\"\n",
    "        Evaluates the quality of the detected communities by computing their modularity.\n",
    "\n",
    "        Returns:\n",
    "        - float: Modularity score of the detected communities.\n",
    "        \"\"\"\n",
    "        data = self.data_loader_extension.graph_to_pytorch_data()\n",
    "        model = GraphClassificationModel(data.x.size(1), 128, len(set(data.y.tolist())))\n",
    "        model.to(self.device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.dynamic_graph_learning.learning_rate)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(100):  # An optimal number of epochs can be used.\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x.to(self.device), data.edge_index.to(self.device))\n",
    "            loss = criterion(out, data.y.to(self.device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        output = model(data.x.to(self.device), data.edge_index.to(self.device))\n",
    "        communities = self.detect_communities(output)\n",
    "        \n",
    "        G = to_networkx(data, to_undirected=True)\n",
    "        modularity = community_louvain.modularity(communities, G)\n",
    "        \n",
    "        return modularity\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Executes the community detection and evaluation pipeline.\n",
    "\n",
    "        Returns:\n",
    "        - float: Modularity score of the detected communities.\n",
    "        \"\"\"\n",
    "        self.dynamic_graph_learning.objective(optuna.trial.FixedTrial({}))\n",
    "        modularity_score = self.evaluate_communities()\n",
    "        \n",
    "        return modularity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo de Louvain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No contexto do Graph Data Science (GDS) Library do Neo4j, a projeção de nó, especificada pela chave nodeProjection em uma consulta Cypher, define quais rótulos de nó ou tipos de nó devem ser considerados ao realizar algoritmos de análise de grafos. Esta projeção determina um subconjunto de nós do banco de dados que será utilizado durante a execução do algoritmo.\n",
    "\n",
    "A nodeProjection é uma parte crucial na preparação de um grafo em memória, onde os algoritmos do GDS operam. Quando você chama um procedimento do GDS, como o gds.louvain.stream, é frequentemente necessário informar ao algoritmo qual parte do grafo ele deve considerar. Isso é feito através das projeções de nó e de relação.\n",
    "\n",
    "Aqui está um detalhamento do termo nodeProjection: 'Node':\n",
    "\n",
    "    nodeProjection: É a chave utilizada na chamada do procedimento GDS para definir quais nós serão incluídos na projeção do grafo.\n",
    "\n",
    "    'Node': Este é o valor associado à chave e refere-se ao rótulo de nó específico no banco de dados Neo4j que você deseja incluir na projeção. Por exemplo, se o seu banco de dados Neo4j tem nós rotulados como Person, Company, Product, etc., ao especificar nodeProjection: 'Person', você está dizendo ao algoritmo para considerar apenas os nós que têm o rótulo Person.\n",
    "\n",
    "A projeção de nó pode ser simples, utilizando apenas um rótulo de nó, ou mais complexa, envolvendo múltiplos rótulos e propriedades que definem filtros ou pesos para os nós no grafo projetado. Além disso, a projeção pode ser especificada de forma mais detalhada usando uma estrutura de mapa (ou dicionário em termos de Python) para fornecer informações adicionais, como propriedades de nós específicas a serem incluídas ou excluídas na projeção do grafo.\n",
    "\n",
    "Assim, ao realizar análises de grafos no Neo4j utilizando a biblioteca GDS, é essencial especificar corretamente a nodeProjection para que o algoritmo opere sobre o conjunto correto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = Neo4jConnection(uri=\"bolt://localhost:7687\", user=\"neo4j\", pwd=\"password\")\n",
    "\n",
    "# # Define the query with placeholders for parameters\n",
    "# query_with_params = \"\"\"\n",
    "# CALL gds.graph.project.cypher(\n",
    "#   'graphName',\n",
    "#   'MATCH (n) RETURN id(n) AS id',\n",
    "#   'MATCH (n1)-[r:SIMILAR]->(n2) WHERE r.score > $scoreThreshold RETURN id(n1) AS source, id(n2) AS target, r.score AS weight',\n",
    "#   {parameters: {scoreThreshold: $scoreThreshold}}\n",
    "# )\n",
    "# YIELD graphName\n",
    "# CALL gds.louvain.stream('graphName')\n",
    "# YIELD nodeId, communityId\n",
    "# RETURN gds.util.asNode(nodeId).name AS name, communityId\n",
    "# \"\"\"\n",
    "\n",
    "# # Define the parameters as a dictionary\n",
    "# params = {\n",
    "#     'scoreThreshold': 0.7\n",
    "# }\n",
    "\n",
    "# # Execute the query with the parameters\n",
    "# conn.query(query_with_params, parameters=params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aproveitar a mesma projeção e calcular a modularidade para cada conjunto de comunidades geradas por diferentes limiares de similaridade (threshold), uma estratégia eficiente é criar projeções de grafos parametrizadas e utilizar o algoritmo de modularidade disponível no Neo4j Graph Data Science (GDS) Library. \n",
    "\n",
    "A modularidade é uma medida que quantifica a força da divisão de uma rede em módulos (também chamados de comunidades). Quanto maior a modularidade, mais precisa é a divisão em comunidades.\n",
    "\n",
    "Segue um exemplo de como estender a classe Neo4jMetricsExtractor para calcular a modularidade após a detecção de comunidades com o algoritmo Louvain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo da Modularidade junto com identificação de comunidades por Louvain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No método para cálculo Modularidade junto com a identificação de comunidades:\n",
    "\n",
    "    Cria projeção de grafo com base no limiar de score passado como argumento. A projeção é dinâmica e temporária, ideal para cálculos que dependem de parâmetros variáveis.\n",
    "    \n",
    "    Executa o algoritmo Louvain na projeção para detectar comunidades.\n",
    "    \n",
    "    Calcula a modularidade com a função gds.louvain.mutate e retornamos as comunidades detectadas e o valor da modularidade.\n",
    "    \n",
    "    Remove a projeção do grafo após seu uso para liberar recursos.\n",
    "    \n",
    "É importante notar que esta função assume que a biblioteca GDS está instalada e que você está utilizando a API correta conforme sua versão. As versões mais recentes do GDS podem ter métodos diferentes para calcular modularidades e trabalhar com projeções de grafos. Verifique sempre a documentação oficial para as funções mais atualizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GML com PyTorch Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporar as métricas do grafo no PyTorch Geometric (PyG) requer uma abordagem que converta esses dados em formatos compatíveis para o aprendizado profundo. O PyG, especificamente, espera que os dados estejam em uma forma particular, tipicamente usando a classe Data para representar gráficos.\n",
    "\n",
    "Aqui está um exemplo simplificado de como você pode fazer isso, tendo em conta as métricas discutidas anteriormente:\n",
    "\n",
    "Estrutura básica do objeto Data:\n",
    "A classe Data no PyTorch Geometric é um contêiner para gráficos. Para gráficos simples, ela contém atributos como edge_index (uma matriz 2xN de índices de arestas), x (uma matriz NxM de características dos nós) e y (uma matriz Nx1 de rótulos dos nós, embora isso possa ser omitido em configurações não supervisionadas).\n",
    "\n",
    "Incorporar as métricas no objeto Data:\n",
    "Para o aprendizado dinâmico, pode-se supor que os gráficos estejam mudando ao longo do tempo. As métricas do grafo poderiam, portanto, ser usadas para enriquecer as características dos nós em cada gráfico temporal.\n",
    "\n",
    "Uso no aprendizado dinâmico não supervisionado:\n",
    "Para aprendizado dinâmico não supervisionado com auto-aprendizagem, o processo geral seria:\n",
    "\n",
    "    Usar um encoder para obter embeddings dos nós.\n",
    "    Utilizar esses embeddings para realizar agrupamento (por exemplo, usando k-means).\n",
    "    Usar os clusters como pseudo-rótulos.\n",
    "    Treinar um modelo de classificação usando esses pseudo-rótulos.\n",
    "    Iterar, refinando os pseudo-rótulos e otimizando o modelo.\n",
    "\n",
    "As métricas do grafo podem ser usadas para enriquecer as características dos nós durante esse processo, potencialmente fornecendo informações contextuais que melhoram o desempenho do auto-aprendizagem.\n",
    "\n",
    "Embora a inclusão de métricas de gráfico como características possa enriquecer o modelo, é crucial realizar uma análise e validação adequadas para garantir que essas características realmente contribuam para a melhoria do desempenho do modelo.\n",
    "\n",
    "Um ponto crucial é que o PyTorch Geometric normalmente opera sobre características de nós e arestas. As métricas que estamos discutindo aqui são características de grafo (ou seja, uma única métrica para todo o grafo). Assim, ao usar essas métricas em um modelo PyG, certifique-se de que o modelo é adequado para características de grafo, ou considere formas de incorporar essas métricas em características de nível de nó ou aresta, conforme necessário.\n",
    "\n",
    "Além disso, ao trabalhar com aprendizado dinâmico, lembre-se de que as características do grafo podem mudar ao longo do tempo, então as métricas e características devem ser recalculadas conforme o grafo é atualizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada a complexidade e especificidade do seu problema, o preparo do objeto `Data` para ser ingerido no PyTorch Geometric (PyG) envolverá várias etapas. Vamos abordá-las em sequência:\n",
    "\n",
    "1. **Projeção Dinâmica do Grafo com Valores de Threshold**: \n",
    "   Comece por gerar várias projeções do grafo para diferentes valores de threshold. Cada projeção representará um grafo onde as arestas entre os nós `Publicacao` e `Subárea`/`Especialidade` têm um valor de similaridade semântica acima do threshold definido.\n",
    "\n",
    "2. **Características do Nó e Aresta**:\n",
    "   Para cada nó `Publicacao`, calcule a similaridade semântica como uma característica de nó. Para cada aresta entre `Publicacao` e `Subárea`/`Especialidade`, use o valor da similaridade como uma característica de aresta.\n",
    "\n",
    "3. **Separação de Comunidades**:\n",
    "   Você pode utilizar algoritmos como o Louvain para identificar comunidades no grafo. Calcule o número de comunidades e o número de nós `Publicacao` que não pertencem a nenhuma comunidade como características globais.\n",
    "\n",
    "4. **Características Globais**:\n",
    "   Como o objetivo é espelhar as oito `Grande Área`, você pode também incluir o número total de comunidades identificadas como uma característica global.\n",
    "\n",
    "5. **Construção do Objeto Data**:\n",
    "   Cada projeção do grafo (para cada valor de threshold) será representada por um objeto `Data` no PyG. Supondo que você tem matrizes de adjacência e características de nó para cada projeção:\n",
    "\n",
    "   ```python\n",
    "   from torch_geometric.data import Data\n",
    "   import torch\n",
    "\n",
    "   def create_data_object(edge_index, node_features, edge_features, global_features):\n",
    "       data = Data(x=torch.tensor(node_features, dtype=torch.float),\n",
    "                   edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "                   edge_attr=torch.tensor(edge_features, dtype=torch.float))\n",
    "       data.global_attr = torch.tensor(global_features, dtype=torch.float)\n",
    "       return data\n",
    "   ```\n",
    "\n",
    "6. **Dataset Dinâmico**:\n",
    "   Após construir um objeto `Data` para cada projeção, compile-os em um conjunto de dados (dataset). Em situações dinâmicas, você pode tratar cada projeção como um \"snapshot\" no tempo e treinar seu modelo para entender como as características do grafo evoluem à medida que o threshold muda.\n",
    "\n",
    "7. **Modelo**:\n",
    "   Utilize um modelo de aprendizado profundo adequado para aprendizado não supervisionado em grafos, como Graph Autoencoders ou Graph Neural Networks (GNN). A entrada seria as características do nó e aresta, e a saída pode ser a reconstituição do grafo ou alguma outra representação útil. Durante o treinamento, você pode usar uma função de perda que promova a identificação de oito comunidades e penalize a exclusão de nós `Publicacao`.\n",
    "\n",
    "Finalmente, este é um esboço inicial e, dada a complexidade do problema, é provável que ajustes e iterações sejam necessários conforme você avança. Lembre-se também de normalizar ou padronizar as características para facilitar o treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class Neo4jService:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def fetch_data(self, query):\n",
    "        with self._driver.session() as session:\n",
    "            return session.run(query).data()\n",
    "        \n",
    "class DataLoaderExtension:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.graph = Graph(uri, auth=(user, password))\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def fetch_nodes_and_edges(self):\n",
    "        try:\n",
    "            node_cursor = self.graph.run(\"MATCH (p:Publicacao) RETURN id(p) AS id, p.embedding AS embedding\")\n",
    "            rel_cursor = self.graph.run(\"\"\"\n",
    "            MATCH (p1:Publicacao)-[r:SIMILAR]->(p2)\n",
    "            RETURN id(p1) AS source, id(p2) AS target, r.score AS weight\n",
    "            \"\"\")\n",
    "\n",
    "            nodes = list(node_cursor)\n",
    "            edges = [(record['source'], record['target'], record['weight']) for record in rel_cursor]\n",
    "\n",
    "            return nodes, edges\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in fetch_nodes_and_edges: {e}\")\n",
    "            return [], []\n",
    "\n",
    "    def transform_to_tensors(self, nodes, edges):\n",
    "        try:\n",
    "            x = torch.tensor([node['embedding'] for node in nodes], dtype=torch.float).to(self.device)\n",
    "            edge_index = torch.tensor([edge[:2] for edge in edges], dtype=torch.long).t().contiguous().to(self.device)\n",
    "            edge_weights = torch.tensor([edge[2] for edge in edges], dtype=torch.float).to(self.device)\n",
    "            return x, edge_index, edge_weights\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in transform_to_tensors: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    def load_graph_data(self):\n",
    "        nodes, edges = self.fetch_nodes_and_edges()\n",
    "        if not nodes:\n",
    "            logging.warning(\"No nodes were found in the database.\")\n",
    "            return None\n",
    "\n",
    "        x, edge_index, edge_weights = self.transform_to_tensors(nodes, edges)\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_weights)\n",
    "        return data\n",
    "\n",
    "    def create_data_loader(self, batch_size=32):\n",
    "        dataset = [self.load_graph_data()]\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True if self.device.type == \"cuda\" else False)\n",
    "\n",
    "    def create_graph(self, thresholds):\n",
    "        \"\"\"\n",
    "        Função para criar grafos baseados em diferentes limiares de similaridade de cosseno.\n",
    "\n",
    "        Parâmetros:\n",
    "        - uri (str): URI do banco de dados Neo4j.\n",
    "        - user (str): Nome do usuário do banco de dados.\n",
    "        - password (str): Senha do usuário do banco de dados.\n",
    "        - model_name (str): Nome do modelo para gerar embeddings. Padrão é \"default_model\".\n",
    "        - thresholds (list of float): Lista de limiares para determinar similaridade. Padrão é [0.6, 0.7, 0.8, 0.9].\n",
    "        - batch_size (int): Tamanho do lote para inserção em lote no banco de dados. Padrão é 3000.\n",
    "\n",
    "        Retorna:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        cosine_sim_rel = CosineSimilarityRelationship(self.graph.uri, self.graph.user, self.graph.password)\n",
    "\n",
    "        all_graph_data = {}\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            # Utilizando o método 'run_similarity_operations' para criar relações de similaridade\n",
    "            cosine_sim_rel.run_similarity_operations(threshold)\n",
    "            \n",
    "            # Obtendo nós e arestas após a criação das relações\n",
    "            nodes, edges = self.fetch_nodes_and_edges()\n",
    "            \n",
    "            if not nodes:\n",
    "                logging.warning(f\"No nodes were found in the database for threshold: {threshold}.\")\n",
    "                continue\n",
    "            \n",
    "            x, edge_index, edge_weights = self.transform_to_tensors(nodes, edges)\n",
    "            \n",
    "            # Armazenando os dados do grafo para o threshold atual\n",
    "            all_graph_data[threshold] = Data(x=x, edge_index=edge_index, edge_attr=edge_weights)\n",
    "\n",
    "        return all_graph_data\n",
    "            \n",
    "class GraphService:\n",
    "    def __init__(self, graph_data):\n",
    "        self.graph_data = graph_data\n",
    "\n",
    "    def get_data(self):\n",
    "        # Esta função pega os dados do grafo e os transforma no formato adequado\n",
    "        edge_index = torch.tensor(self.graph_data['edges'], dtype=torch.long)\n",
    "        x = torch.tensor(self.graph_data['nodes'], dtype=torch.float)\n",
    "        y = torch.tensor(self.graph_data['labels'], dtype=torch.long)\n",
    "        return Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "\n",
    "    def split_data(self, data, train_percent=0.7):\n",
    "        # Esta função divide os dados em conjuntos de treinamento e teste\n",
    "        dataset = InMemoryDataset(root='./', transform=self.get_data)\n",
    "        train_size = int(train_percent * len(dataset))\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "        return DataLoader(train_dataset, batch_size=64, shuffle=True), DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class GraphEmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, dropout=0.5):\n",
    "        super(GraphEmbeddingModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Primeira camada de convolução\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Segunda camada de convolução\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Terceira camada de convolução\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GNNModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Definição do modelo Graph Neural Network usando PyTorch Geometric.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GraphModel:\n",
    "    \"\"\"\n",
    "    Classe representando o modelo de aprendizado profundo para grafos com PyTorch Geometric.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        self.model = GNNModel(input_dim, hidden_dim, output_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "\n",
    "    def train(self, train_data, epochs: int = 10):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for data in train_data:  # Assumindo que train_data é um DataLoader\n",
    "                self.optimizer.zero_grad()\n",
    "                out = self.model(data)\n",
    "                loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def predict(self, data):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(data)\n",
    "        return logits\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(data)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct = (pred == data.y).sum().item()\n",
    "            acc = correct / len(data.y)\n",
    "        return acc\n",
    "\n",
    "    def self_learning(self, unlabeled_data):\n",
    "        # Neste método, a ideia é usar o modelo para fazer previsões em dados não rotulados\n",
    "        # E então usar essas previsões como novas labels \"pseudo\" para treinar o modelo novamente\n",
    "        logits = self.predict(unlabeled_data)\n",
    "        pseudo_labels = logits.argmax(dim=1)\n",
    "        unlabeled_data.y = pseudo_labels  # Definir as pseudo labels\n",
    "        self.train(unlabeled_data)  # Treinar novamente usando as pseudo labels\n",
    "\n",
    "class GraphAnalysisController:\n",
    "    def __init__(self, neo4j_service, model_path=None):\n",
    "        self.neo4j_service = neo4j_service\n",
    "        # self.graph_data = self._load_data_from_source()\n",
    "        self.graph_service = GraphService(self.graph_data)\n",
    "        # Instancie o DataLoaderExtension para carregar dados do Neo4j.\n",
    "        self.data_loader = DataLoaderExtension(self.graph_data)\n",
    "        \n",
    "        # Se o modelo for fornecido, carregue-o.\n",
    "        if model_path:\n",
    "            self.graph_model = GraphModel.load(model_path)\n",
    "        else:\n",
    "            self.graph_model = GraphModel()\n",
    "\n",
    "    ## Para usar dados salvos para retreinar o modelo\n",
    "    # def _load_data_from_source(self):\n",
    "    #     query = \"SEU_QUERY_PARA_OBTENÇÃO_DE_DADOS\"\n",
    "    #     return self.neo4j_service.fetch_data(query)\n",
    "    \n",
    "    # def _load_data_from_source(self):\n",
    "    #     # Para este exemplo, vamos supor uma função simplificada de carregamento de dados\n",
    "    #     with open(self.data_source_path, 'r') as file:\n",
    "    #         return json.load(file)\n",
    "\n",
    "    # def train_model(self, epochs=10):\n",
    "    #     data = self.graph_service.get_data()\n",
    "    #     train_loader, test_loader = self.graph_service.split_data(data)\n",
    "    #     loss_function = torch.nn.CrossEntropyLoss()\n",
    "    #     optimizer = torch.optim.Adam(self.graph_model.parameters(), lr=0.01)\n",
    "        \n",
    "    #     for epoch in range(epochs):\n",
    "    #         for batch in train_loader:\n",
    "    #             optimizer.zero_grad()\n",
    "    #             out = self.graph_model(batch.x, batch.edge_index)\n",
    "    #             loss = loss_function(out, batch.y)\n",
    "    #             loss.backward()\n",
    "    #             optimizer.step()\n",
    "        \n",
    "    #     return \"Training complete.\"\n",
    "\n",
    "    ## Para treinamento não-supervisionado\n",
    "    def train_model(self, train_data, num_epochs=100):\n",
    "        optimizer = torch.optim.Adam(self.graph_model.parameters(), lr=0.001)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for batch in train_data:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.graph_model(batch.x, batch.edge_index)\n",
    "                loss = F.mse_loss(out, batch.edge_attr)  # Aqui usamos edge_attr (pesos de borda) em vez de y\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def evaluate_model(self, test_data):\n",
    "        self.graph_model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_data:\n",
    "                out = self.graph_model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                pred = out.argmax(dim=1)\n",
    "                correct += int((pred == batch.y).sum())\n",
    "        return correct / len(test_data.dataset)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.graph_model.save(path)\n",
    "\n",
    "    def execute_pipeline(self, thresholds=[0.6, 0.7, 0.8, 0.9]):\n",
    "        # Gere grafos para cada limiar.\n",
    "        all_graph_data = self.data_loader.create_graph(thresholds)\n",
    "\n",
    "        # Divida os dados em conjuntos de treinamento e teste.\n",
    "        graph_service = GraphService(all_graph_data[thresholds[0]])  # Usando o primeiro limiar como exemplo.\n",
    "        train_data, test_data = graph_service.split_data(all_graph_data[thresholds[0]])\n",
    "\n",
    "        # Treine o modelo.\n",
    "        self.train_model(train_data)\n",
    "\n",
    "        # Avalie o modelo.\n",
    "        accuracy = self.evaluate_model(test_data)\n",
    "        print(f\"Model accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Salve o modelo.\n",
    "        self.save_model(\"model_path.pt\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class GraphTrainingPipeline:\n",
    "    def __init__(self, data_source_path, uri, username, password, input_dim, hidden_dim, output_dim, batch_size=32, n_splits=5):\n",
    "        self.data_loader_ext = DataLoaderExtension(uri, username, password)\n",
    "        self.data_loader = self.data_loader_ext.create_data_loader(batch_size=batch_size)\n",
    "        self.controller = GraphAnalysisController(data_source_path)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_splits = n_splits\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "\n",
    "    def train_and_evaluate(self, epochs=10):\n",
    "        kf = KFold(n_splits=self.n_splits)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(self.data_loader.dataset)):\n",
    "            print(f\"Fold {fold + 1}/{self.n_splits}\")\n",
    "            train_subset = torch.utils.data.Subset(self.data_loader.dataset, train_idx)\n",
    "            val_subset = torch.utils.data.Subset(self.data_loader.dataset, val_idx)\n",
    "            \n",
    "            train_loader = DataLoader(train_subset, batch_size=len(train_subset))\n",
    "            val_loader = DataLoader(val_subset, batch_size=len(val_subset))\n",
    "            \n",
    "            self.controller.graph_model = GNNModel(self.input_dim, self.hidden_dim, self.output_dim)  # Reinitialize model for each fold\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                self.controller.train_model(epochs=1)  # Train for one epoch\n",
    "                \n",
    "                # Plot training loss and validation accuracy in real-time\n",
    "                self.plot_metrics()\n",
    "\n",
    "                # Evaluate on validation set and store accuracy\n",
    "                accuracy = self.controller.evaluate_model()\n",
    "                self.accuracies.append(accuracy)\n",
    "\n",
    "                clear_output(wait=True)\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # Plotting losses\n",
    "        ax[0].plot(self.losses, '-o', label='Training Loss')\n",
    "        ax[0].set_title('Training Loss')\n",
    "        ax[0].set_xlabel('Epochs')\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].legend()\n",
    "\n",
    "        # Plotting accuracies\n",
    "        ax[1].plot(self.accuracies, '-o', label='Validation Accuracy')\n",
    "        ax[1].set_title('Validation Accuracy')\n",
    "        ax[1].set_xlabel('Epochs')\n",
    "        ax[1].set_ylabel('Accuracy')\n",
    "        ax[1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.controller.save_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTrainingPipeline:\n",
    "    # def __init__(self, data_source_path, uri, username, password, input_dim, hidden_dim, output_dim, batch_size=32, n_splits=5):\n",
    "    def __init__(self, neo4j_uri, neo4j_user, neo4j_password, thresholds=[0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]):\n",
    "        self.data_loader_extension = DataLoaderExtension(neo4j_uri, neo4j_user, neo4j_password)\n",
    "        self.graph_data = self.data_loader_extension.create_graph(thresholds)\n",
    "        self.best_threshold = None\n",
    "        self.best_accuracy = 0.0\n",
    "    \n",
    "        self.graph_model.train() # Define o modelo para o modo de treinamento\n",
    "        for batch in train_data: # Itera sobre os lotes de dados no DataLoader\n",
    "            optimizer.zero_grad() # Zera os gradientes\n",
    "            output = self.graph_model(batch) # Realiza a passagem para a frente\n",
    "\n",
    "            # # Como o problema parece ser de clusterização, você pode usar uma perda adequada, \n",
    "            # # por exemplo, uma perda baseada em similaridade ou uma perda baseada em distância.\n",
    "            # # Aqui, estou assumindo uma perda genérica, que deve ser substituída pela sua perda específica.\n",
    "            # loss = some_loss_function(output, target) \n",
    "            \n",
    "            # loss.backward() # Realiza a passagem para trás\n",
    "            # optimizer.step() # Atualiza os pesos\n",
    "\n",
    "            # # Você pode querer imprimir o valor da perda após cada época para acompanhar o treinamento\n",
    "            # print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    def evaluate_model_threshold(self, thresholds, evaluation_metrics):\n",
    "        \"\"\"\n",
    "        Avalia o modelo para diferentes limiares e retorna o limiar que otimiza as métricas de avaliação.\n",
    "        \n",
    "        Parâmetros:\n",
    "        - thresholds (list of float): Lista de limiares para testar.\n",
    "        - evaluation_metrics (list of functions): Lista de métricas de avaliação a serem consideradas.\n",
    "        \n",
    "        Retorna:\n",
    "        - best_threshold (float): O limiar que otimizou as métricas de avaliação.\n",
    "        \"\"\"\n",
    "\n",
    "        best_threshold = None\n",
    "        best_score = float('-inf') \n",
    "\n",
    "        for threshold in thresholds:\n",
    "            current_data = self.data_loader.create_graph([threshold])[threshold]\n",
    "            score = 0\n",
    "            for metric in evaluation_metrics:\n",
    "                score += metric(self.graph_model, current_data)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = threshold\n",
    "\n",
    "        return best_threshold\n",
    "\n",
    "    def community_detection(self, best_threshold):\n",
    "        \"\"\"\n",
    "        Detecta comunidades nos nós Producao usando o limiar otimizado.\n",
    "        \n",
    "        Parâmetros:\n",
    "        - best_threshold (float): O limiar otimizado.\n",
    "        \n",
    "        Retorna:\n",
    "        - communities (list of list of int): Lista de comunidades detectadas.\n",
    "        \"\"\"\n",
    "        # Obter os dados do grafo para o limiar otimizado\n",
    "        data = self.data_loader.create_graph([best_threshold])[best_threshold]\n",
    "\n",
    "        # Aplicar o modelo para obter as embeddings dos nós\n",
    "        embeddings = self.graph_model(data)\n",
    "\n",
    "        # Use um algoritmo de detecção de comunidade para detectar comunidades nas embeddings\n",
    "        # Por exemplo, você pode usar o algoritmo Louvain, mas qualquer algoritmo de sua escolha funcionaria.\n",
    "        partition = community.best_partition(embeddings) # usando python-louvain\n",
    "\n",
    "        communities = []\n",
    "        for community_id in set(partition.values()):\n",
    "            community_nodes = [node for node, c_id in partition.items() if c_id == community_id]\n",
    "            communities.append(community_nodes)\n",
    "\n",
    "        return communities\n",
    "\n",
    "    def save_communities_to_neo4j(self, communities):\n",
    "        \"\"\"\n",
    "        Salva as comunidades detectadas de volta no banco de dados Neo4j.\n",
    "        \n",
    "        Parâmetros:\n",
    "        - communities (list of list of int): Lista de comunidades detectadas.\n",
    "        \n",
    "        Retorna:\n",
    "        - status (str): Status da operação.\n",
    "        \"\"\"\n",
    "\n",
    "        # Aqui, você precisaria escrever o código para salvar as comunidades no banco de dados.\n",
    "        # Isso pode envolver a criação de relações entre nós em uma comunidade ou a definição de propriedades dos nós com seus IDs de comunidade.\n",
    "\n",
    "        # Placeholder\n",
    "        return \"Communities saved successfully.\"\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        # Passo 1: Carregar dados\n",
    "        data = self.data_loader.load_graph_data()\n",
    "\n",
    "        # Passo 2: Treinar modelo\n",
    "        self.train_model(data)\n",
    "\n",
    "        # Passo 3: Avaliar modelo e otimizar o limiar\n",
    "        thresholds = [i/10 for i in range(5, 10)]\n",
    "        evaluation_metrics = [some_metric_function] # Adicione suas métricas aqui\n",
    "        best_threshold = self.evaluate_model_threshold(thresholds, evaluation_metrics)\n",
    "\n",
    "        # Passo 4: Detectar comunidades usando o limiar otimizado\n",
    "        communities = self.community_detection(best_threshold)\n",
    "\n",
    "        # Passo 5: Salvar comunidades no banco de dados Neo4j\n",
    "        status = self.save_communities_to_neo4j(communities)\n",
    "\n",
    "        return status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Instanciando a classe para carregamento dos dados\n",
    "data_loader_ext = DataLoaderExtension(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"your_password\")\n",
    "data_loader = data_loader_ext.create_data_loader(batch_size=BATCH_SIZE)\n",
    "\n",
    "# Parâmetros para o modelo\n",
    "INPUT_DIM = len(data_loader.dataset[0].x[0])  # Número de características (features) do nodo\n",
    "HIDDEN_DIM = 64  # Pode ser ajustado conforme necessidade\n",
    "OUTPUT_DIM = 10  # Número de classes ou categorias. Ajuste conforme seu dataset\n",
    "\n",
    "# Instantiate the training pipeline\n",
    "pipeline = GraphTrainingPipeline(\n",
    "    DATA_SOURCE_PATH, \n",
    "    URI, \n",
    "    USERNAME, \n",
    "    PASSWORD, \n",
    "    INPUT_DIM, \n",
    "    HIDDEN_DIM, \n",
    "    OUTPUT_DIM\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "pipeline.train_and_evaluate(epochs=10)\n",
    "\n",
    "# Save the trained model\n",
    "pipeline.save_model(\"trained_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = GraphAnalysisController(neo4j_uri=\"bolt://localhost:7687\", neo4j_user=\"neo4j\", neo4j_password=\"your_password\")\n",
    "status = controller.run_pipeline()\n",
    "print(status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
