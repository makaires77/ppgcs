{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <center><center><img src=\"https://raw.githubusercontent.com/makaires77/fioce/master/assets/logo_fioce.png\" \n",
    "style=\"height:150px\" alt=\"Logo_Fiocruz\"></center></center> -->\n",
    "\n",
    "<center><center><img src=\"https://user-images.githubusercontent.com/61051085/81343928-3ce9d500-908c-11ea-9850-0210b4e94ba0.jpg\" \n",
    "style=\"height:150px\" alt=\"Logo_Unifor\"></center></center>\n",
    "\n",
    "## <center>Classificar tópicos em dados de texto com base em palavras-chave<br /></center>\n",
    "\n",
    "    Antonio Marcos Aires Barbosa (Fiocruz Ceará)\n",
    "\n",
    "Nesta etapa do trabalho expomos a segmentação de tópicos, a partir dos títulos de artigos analisados por NMF (Non-negative Matrix Factorization) para classificar publicações acadêmicas quanto às Grande Áreas e Áreas na Árvore de Conhecimento do CNPq, tendo como fonte de dados extrações dos currículos Lattes dos colaboradores da unidade Fiocruz Ceará. Também testamos a classificação do mesmo corpus por LDA (Latent Dirichlet Aloccation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação automatizada de Tópicos em dados de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abordagens metodológicas:\n",
    "\n",
    "No contexto da modelagem de tópicos e análise de textos várias cada uma das abordagens desenvolvidas durante as últimas décadas tem suas particularidades em termos de complexidade, custos computacionais e desempenho, partindo do Latent Dirichlet Allocation, segue um breve comparativo das opções:\n",
    "\n",
    "1. Non-negative Matrix Factorization (NMF)\n",
    "Usado para modelagem de tópicos, identificando tópicos em grandes conjuntos de documentos.\n",
    "Princípio: O NMF utiliza fatorização de matrizes, uma técnica linear (diferente do LDA que é baseado em um modelo probabilístico), geralmente tem um custo computacional menor que o LDA, especialmente para grandes conjuntos de dados, pois envolve operações de álgebra linear menos complexas.\n",
    "\n",
    "2. O LDA modela documentos como misturas de tópicos, onde cada tópico é uma distribuição de palavras. Essa representação permite a conversão de documentos textuais em vetores de tópicos, que podem ser usados em sistemas de busca vetorial. Vetores de tópicos do LDA podem ser utilizados para calcular a similaridade entre documentos ou entre uma consulta de busca e documentos, melhorando a relevância dos resultados em sistemas de busca.\n",
    "\n",
    "3. Latent Semantic Analysis (LSA) ou Latent Semantic Indexing (LSI)\n",
    "O LSA é usado para identificar a estrutura latente em documentos textuais, baseado na decomposição de valores singulares (SVD) de matrizes termo-documento, focando na captura de padrões de co-ocorrência de palavras. O LSA pode ser computacionalmente mais eficiente em alguns cenários que o LDA, mas é menos robusto em lidar com a ambiguidade de palavras e não modela a distribuição tópico-palavra explicitamente como o LDA.\n",
    "\n",
    "4. Word Embedding Models (como Word2Vec ou GloVe)\n",
    "Embora não sejam técnicas de modelagem de tópicos per se, esses modelos capturam semânticas latentes em textos e podem ser usados para análise de tópicos. Estes modelos aprendem representações vetoriais de palavras em um espaço de características contínuo, focando na semântica das palavras e não diretamente em tópicos. Word2Vec e GloVe podem ser computacionalmente intensivos, especialmente para grandes conjuntos de dados, mas tendem a oferecer uma rica representação semântica.\n",
    "\n",
    "5. Deep Learning Approaches (como as Redes Neurais Autoencoder)\n",
    "Alguns modelos de deep learning podem ser adaptados para tarefas de modelagem de tópicos, usando redes neurais para capturar relações não lineares complexas nos dados. Geralmente, exigem um custo computacional significativamente maior devido à necessidade de treinamento de redes neurais, mas podem oferecer desempenho superior em termos de captura de nuances nos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geração de vetores de incorporação: A busca vetorial opera calculando a similaridade entre o vetor de uma consulta de busca e os vetores que representam os documentos ou itens no banco de dados. Isso é frequentemente feito através de métricas como a similaridade do cosseno. A principal vantagem da busca vetorial é a capacidade de encontrar itens que são semanticamente relacionados à consulta, mesmo que não compartilhem palavras-chave exatas.\n",
    "\n",
    "A operacionalização de buscas vetoriais em dados de texto pode envolver várias técnicas como LDA, NMF, LSA ou modelos de word embeddings para transformar dados textuais em representações vetoriais que podem ser efetivamente utilizadas em sistemas de busca vetorial. As representações vetoriais geradas por essas técnicas, visam, de uma forma geral, capturar significados semânticos e tópicos latentes, permitindo buscas mais precisas e relevantes em grandes conjuntos de dados.\n",
    "\n",
    "1. Latent Dirichlet Allocation (LDA)\n",
    "O LDA modela documentos como misturas de tópicos, onde cada tópico é uma distribuição de palavras. Essa representação permite a conversão de documentos textuais em vetores de tópicos, que podem ser usados em sistemas de busca vetorial. Vetores de tópicos do LDA podem ser utilizados para calcular a similaridade entre documentos ou entre uma consulta de busca e documentos, melhorando a relevância dos resultados em sistemas de busca.\n",
    "\n",
    "2. Non-negative Matrix Factorization (NMF) e Latent Semantic Analysis (LSA)\n",
    "Relação com Busca Vetorial: Tanto o NMF quanto o LSA reduzem a dimensionalidade de matrizes termo-documento, gerando representações vetoriais de documentos em espaços de tópicos ou conceitos latentes. Essas representações vetoriais podem ser usadas para calcular similaridades na busca vetorial, ajudando a encontrar documentos que são semânticamente próximos a uma consulta.\n",
    "\n",
    "3. Word Embedding Models (Word2Vec, GloVe)\n",
    "Relação com Busca Vetorial: Modelos de word embeddings geram representações vetoriais de palavras em um espaço contínuo, capturando nuances semânticas e relacionamentos entre palavras. Podem ser utilizados para representar documentos em um espaço vetorial ou enriquecer as consultas de busca baseando-se na agregação dos vetores das palavras contidas nos documentos, o que pode melhorar a precisão e a relevância das buscas.\n",
    "\n",
    "4. Deep Learning Approaches (Autoencoders)\n",
    "Autoencoders e outras abordagens de deep learning podem ser usados para aprender representações vetoriais de alta dimensão de documentos ou outros tipos de dados. Essas representações podem ser empregadas em sistemas de busca vetorial para encontrar itens similares ou para recomendações personalizadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O LDA é uma técnica de modelagem de tópicos para descobrir temas latentes em grandes conjuntos de documentos textuais. Ele é baseado em modelos probabilísticos e é utilizado principalmente para entender e organizar coleções de documentos com base nos tópicos que contêm.\n",
    "\n",
    "Baseado na distribuição de Dirichlet, que é uma distribuição de probabilidade multivariada, generalização da distribuição beta para dimensões superiores. Amplamente usada para modelar a variabilidade em proporções ou probabilidades e tem a característica de ser uma distribuição conjugada a priori para modelos multinomiais, o que facilita a inferência bayesiana. No contexto do Latent Dirichlet Allocation (LDA), a distribuição de Dirichlet é usada para modelar a distribuição de tópicos em documentos e a distribuição de palavras em tópicos. Em termos simples, cada documento é considerado como uma mistura de vários tópicos, e cada tópico, por sua vez, é uma mistura de palavras. As distribuições de Dirichlet permitem a atribuição de probabilidades a estas misturas de uma maneira que pode capturar alguma incerteza e variabilidade nas composições dos tópicos e dos documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnica, algoritmo ou abordagem metodológica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Latent Dirichlet Allocation (LDA) é mais adequadamente classificado como uma técnica ou abordagem no campo da aprendizagem de máquina e processamento de linguagem natural. Como técnica, o LDA oferece um método sistemático para descobrir tópicos latentes em grandes conjuntos de documentos ou textos. Ele utiliza um modelo estatístico baseado em distribuições probabilísticas para inferir a estrutura de tópicos em um conjunto de dados.\n",
    "\n",
    "Embora às vezes seja referido como um algoritmo, isso pode ser ligeiramente impreciso. O LDA em si define o modelo matemático e a estrutura conceitual para a modelagem de tópicos. Porém, a implementação prática do LDA depende de algoritmos específicos para realizar a inferência e a aprendizagem dos parâmetros do modelo, como o algoritmo de amostragem de Gibbs ou métodos de inferência variacional. Estes algoritmos são componentes essenciais para aplicar a técnica do LDA em problemas práticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação do LDA associado ao Raciocínio Causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificação de Tópicos para Análise Causal: O LDA pode ser útil para processar e organizar grandes quantidades de dados textuais (como artigos científicos ou relatórios) para identificar áreas de interesse ou tópicos que podem ser mais explorados através de análises causais.\n",
    "\n",
    "Geração de Hipóteses: Ao revelar tópicos predominantes em conjuntos de dados textuais, o LDA pode ajudar na geração de hipóteses para estudos causais. Por exemplo, a identificação de tópicos frequentemente discutidos em artigos sobre saúde pública pode sugerir áreas para investigação causal.\n",
    "\n",
    "Limitações\n",
    "Falta de Análise Causal Direta: O LDA não fornece insights sobre relações causais. Ele identifica tópicos com base na co-ocorrência de palavras, o que é uma abordagem correlacional e não causal.\n",
    "\n",
    "Integração com Métodos Causais: Qualquer insight causal derivado da aplicação do LDA requer subsequente análise utilizando métodos de raciocínio causal. O LDA por si só não pode estabelecer causalidade.\n",
    "\n",
    "Interpretação Cuidadosa: A interpretação dos tópicos identificados pelo LDA deve ser feita com cautela, especialmente quando usada como base para investigações causais, pois a presença de um tópico em textos não implica causalidade.\n",
    "\n",
    "Em resumo, enquanto o LDA pode ser uma ferramenta útil na fase preliminar de organização e geração de hipóteses em estudos causais, ele não é adequado para estabelecer relações de causa e efeito. Portanto, sua aplicação em raciocínio causal deve ser complementada por métodos específicos para análise causal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurar e importar bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de incompatibilidade: PyTorch X Python 3.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !C:\\Users\\marcos.aires\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --user --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --upgrade pip\n",
    "\n",
    "# !pip install ipykernel\n",
    "\n",
    "# import os\n",
    "# os.listdir('../../../../')\n",
    "\n",
    "## Criar um ambiente virtual já a partir do jupyter\n",
    "# !python -m venv py312\n",
    "\n",
    "## Ativar o ambiente virtual\n",
    "# !.\\py312\\Scripts\\activate\n",
    "\n",
    "## Instalar os pacotes do requirements.txt\n",
    "# %pip install -r ../../../../requirements.txt\n",
    "\n",
    "# %pip install --user -r ../../../../requirements.txt --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org\n",
    "\n",
    "\n",
    "## Base do diretório do repositório em caminho absoluto\n",
    "# base_repo_dir = None\n",
    "# if 'win' in sys.platform:\n",
    "#     base_repo_dir = 'C:\\\\Users\\\\marcos.aires\\\\gml_classifier-1'  # Caminho base no Windows\n",
    "# else:\n",
    "#     base_repo_dir = '/home/mak/gml_classifier-1'  # Caminho base em Linux\n",
    "## Base do diretório do repositório em caminho relativo\n",
    "# os.listdir('./../data/')\n",
    "# folder_data_dev = './../data/'\n",
    "# list(os.listdir(folder_data_dev))\n",
    "\n",
    "# Se os pacotes estão instalados, mas não são localizados, é possível que o kernel do Jupyter não esteja usando o ambiente de instalação correto. Para verificar o ambiente de instalação do kernel, você pode usar o seguinte comando:\n",
    "\n",
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relatório Interno Artigos Fiocruz Ceará"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mdev/gml_classifier/source/adapters/input/jupyter_notebooks\n",
      "/home/mdev/anaconda3/lib/python39.zip\n",
      "/home/mdev/anaconda3/lib/python3.9\n",
      "/home/mdev/anaconda3/lib/python3.9/lib-dynload\n",
      "\n",
      "/home/mdev/anaconda3/lib/python3.9/site-packages\n",
      "/home/mdev/anaconda3/lib/python3.9/site-packages/IPython/extensions\n",
      "/home/mdev/.ipython\n",
      "/home/mdev/gml_classifier\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "for i in sys.path:\n",
    "    print(i)\n",
    "\n",
    "def get_path_repo():\n",
    "    \"\"\"Retorna o caminho absoluto cinco níveis acima do diretório atual.\"\"\"\n",
    "    current_directory = os.getcwd()\n",
    "    # Construir o caminho para subir cinco níveis\n",
    "    path_five_levels_up = os.path.join(current_directory, '../../../../')\n",
    "    # Normalizar o caminho para o formato absoluto\n",
    "    absolute_path = os.path.abspath(path_five_levels_up)\n",
    "    return absolute_path\n",
    "\n",
    "path = get_path_repo()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "password = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)'))': /simple/subprocess/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement subprocess (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for subprocess\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip3 install --upgrade pip\n",
    "!pip3 install subprocess --trusted-host pipy.org\n",
    "# !apt list --installed | grep openssl\n",
    "# !pip3 list | grep openssl\n",
    "# !pip3 install python-openssl\n",
    "# !subprocess.call(['sudo', 'apt', 'update', '&&', 'sudo', 'apt', 'install', 'libssl-dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: tqdm in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: xlrd in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: Flask in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: neo4j in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 5)) (5.16.0)\n",
      "Requirement already satisfied: numpy in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 6)) (1.21.5)\n",
      "Requirement already satisfied: numba in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 7)) (0.56.2)\n",
      "Requirement already satisfied: torch in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 8)) (1.12.1)\n",
      "Requirement already satisfied: gensim in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 9)) (4.2.0)\n",
      "Requirement already satisfied: pdfkit in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: pandas in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: py2neo in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 12)) (2021.2.4)\n",
      "Requirement already satisfied: Pillow in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 13)) (9.2.0)\n",
      "Requirement already satisfied: PyPDF2 in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 14)) (3.0.1)\n",
      "Requirement already satisfied: plotly in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 15)) (5.10.0)\n",
      "Requirement already satisfied: urllib3 in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 16)) (1.26.11)\n",
      "Requirement already satisfied: IPython in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 17)) (7.33.0)\n",
      "Requirement already satisfied: seaborn in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 18)) (0.12.0)\n",
      "Requirement already satisfied: openpyxl in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 19)) (3.0.10)\n",
      "Requirement already satisfied: networkx in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 20)) (2.8.7)\n",
      "Requirement already satisfied: requests in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 21)) (2.31.0)\n",
      "Requirement already satisfied: xhtml2pdf in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 22)) (0.2.14)\n",
      "Requirement already satisfied: wordcloud in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 23)) (1.9.3)\n",
      "Requirement already satisfied: ipywidgets in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 24)) (8.0.2)\n",
      "Requirement already satisfied: langdetect in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 25)) (1.0.9)\n",
      "Requirement already satisfied: weasyprint in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 26)) (60.2)\n",
      "Requirement already satisfied: matplotlib in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 27)) (3.6.0)\n",
      "Requirement already satisfied: transformers in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 28)) (4.37.1)\n",
      "Requirement already satisfied: scikit-learn in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 29)) (1.0.2)\n",
      "Requirement already satisfied: pyjarowinkler in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 30)) (1.8)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/mdev/anaconda3/lib/python3.9/site-packages (from -r /home/mdev/gml_classifier/requirements.txt (line 31)) (4.11.1)\n",
      "Requirement already satisfied: click in /home/mdev/anaconda3/lib/python3.9/site-packages (from nltk->-r /home/mdev/gml_classifier/requirements.txt (line 1)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/mdev/anaconda3/lib/python3.9/site-packages (from nltk->-r /home/mdev/gml_classifier/requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/mdev/anaconda3/lib/python3.9/site-packages (from nltk->-r /home/mdev/gml_classifier/requirements.txt (line 1)) (2022.9.13)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /home/mdev/anaconda3/lib/python3.9/site-packages (from Flask->-r /home/mdev/gml_classifier/requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from Flask->-r /home/mdev/gml_classifier/requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from Flask->-r /home/mdev/gml_classifier/requirements.txt (line 4)) (2.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from Flask->-r /home/mdev/gml_classifier/requirements.txt (line 4)) (4.11.4)\n",
      "Requirement already satisfied: pytz in /home/mdev/anaconda3/lib/python3.9/site-packages (from neo4j->-r /home/mdev/gml_classifier/requirements.txt (line 5)) (2022.4)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from numba->-r /home/mdev/gml_classifier/requirements.txt (line 7)) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /home/mdev/anaconda3/lib/python3.9/site-packages (from numba->-r /home/mdev/gml_classifier/requirements.txt (line 7)) (65.4.1)\n",
      "Requirement already satisfied: typing-extensions in /home/mdev/anaconda3/lib/python3.9/site-packages (from torch->-r /home/mdev/gml_classifier/requirements.txt (line 8)) (4.3.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from gensim->-r /home/mdev/gml_classifier/requirements.txt (line 9)) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from gensim->-r /home/mdev/gml_classifier/requirements.txt (line 9)) (5.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from pandas->-r /home/mdev/gml_classifier/requirements.txt (line 11)) (2.8.2)\n",
      "Requirement already satisfied: certifi in /home/mdev/anaconda3/lib/python3.9/site-packages (from py2neo->-r /home/mdev/gml_classifier/requirements.txt (line 12)) (2022.9.24)\n",
      "Requirement already satisfied: interchange~=2021.0.4 in /home/mdev/anaconda3/lib/python3.9/site-packages (from py2neo->-r /home/mdev/gml_classifier/requirements.txt (line 12)) (2021.0.4)\n",
      "Requirement already satisfied: monotonic in /home/mdev/anaconda3/lib/python3.9/site-packages (from py2neo->-r /home/mdev/gml_classifier/requirements.txt (line 12)) (1.6)\n",
      "Requirement already satisfied: packaging in /home/mdev/anaconda3/lib/python3.9/site-packages (from py2neo->-r /home/mdev/gml_classifier/requirements.txt (line 12)) (21.3)\n",
      "Requirement already satisfied: pansi>=2020.7.3 in /home/mdev/anaconda3/lib/python3.9/site-packages (from py2neo->-r /home/mdev/gml_classifier/requirements.txt (line 12)) (2020.7.3)\n",
      "Requirement already satisfied: pygments>=2.0.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from py2neo->-r /home/mdev/gml_classifier/requirements.txt (line 12)) (2.13.0)\n",
      "Requirement already satisfied: six>=1.15.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from py2neo->-r /home/mdev/gml_classifier/requirements.txt (line 12)) (1.16.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from plotly->-r /home/mdev/gml_classifier/requirements.txt (line 15)) (8.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/mdev/anaconda3/lib/python3.9/site-packages (from IPython->-r /home/mdev/gml_classifier/requirements.txt (line 17)) (0.18.1)\n",
      "Requirement already satisfied: decorator in /home/mdev/anaconda3/lib/python3.9/site-packages (from IPython->-r /home/mdev/gml_classifier/requirements.txt (line 17)) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /home/mdev/anaconda3/lib/python3.9/site-packages (from IPython->-r /home/mdev/gml_classifier/requirements.txt (line 17)) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/mdev/anaconda3/lib/python3.9/site-packages (from IPython->-r /home/mdev/gml_classifier/requirements.txt (line 17)) (5.4.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from IPython->-r /home/mdev/gml_classifier/requirements.txt (line 17)) (3.0.31)\n",
      "Requirement already satisfied: backcall in /home/mdev/anaconda3/lib/python3.9/site-packages (from IPython->-r /home/mdev/gml_classifier/requirements.txt (line 17)) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/mdev/anaconda3/lib/python3.9/site-packages (from IPython->-r /home/mdev/gml_classifier/requirements.txt (line 17)) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/mdev/anaconda3/lib/python3.9/site-packages (from IPython->-r /home/mdev/gml_classifier/requirements.txt (line 17)) (4.8.0)\n",
      "Requirement already satisfied: et_xmlfile in /home/mdev/anaconda3/lib/python3.9/site-packages (from openpyxl->-r /home/mdev/gml_classifier/requirements.txt (line 19)) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mdev/anaconda3/lib/python3.9/site-packages (from requests->-r /home/mdev/gml_classifier/requirements.txt (line 21)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mdev/anaconda3/lib/python3.9/site-packages (from requests->-r /home/mdev/gml_classifier/requirements.txt (line 21)) (3.4)\n",
      "Requirement already satisfied: arabic-reshaper>=3.0.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (3.0.0)\n",
      "Requirement already satisfied: html5lib>=1.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (1.1)\n",
      "Requirement already satisfied: pyHanko>=0.12.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (0.21.0)\n",
      "Requirement already satisfied: pyhanko-certvalidator>=0.19.5 in /home/mdev/anaconda3/lib/python3.9/site-packages (from xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (0.26.3)\n",
      "Requirement already satisfied: pypdf>=3.1.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (4.0.0)\n",
      "Requirement already satisfied: python-bidi>=0.4.2 in /home/mdev/anaconda3/lib/python3.9/site-packages (from xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (0.4.2)\n",
      "Requirement already satisfied: reportlab>=4.0.4 in /home/mdev/anaconda3/lib/python3.9/site-packages (from xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (4.0.9)\n",
      "Requirement already satisfied: svglib>=1.2.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (1.5.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from ipywidgets->-r /home/mdev/gml_classifier/requirements.txt (line 24)) (6.16.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from ipywidgets->-r /home/mdev/gml_classifier/requirements.txt (line 24)) (4.0.3)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from ipywidgets->-r /home/mdev/gml_classifier/requirements.txt (line 24)) (3.0.3)\n",
      "Requirement already satisfied: pydyf>=0.8.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from weasyprint->-r /home/mdev/gml_classifier/requirements.txt (line 26)) (0.8.0)\n",
      "Requirement already satisfied: cffi>=0.6 in /home/mdev/anaconda3/lib/python3.9/site-packages (from weasyprint->-r /home/mdev/gml_classifier/requirements.txt (line 26)) (1.15.1)\n",
      "Requirement already satisfied: tinycss2>=1.0.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from weasyprint->-r /home/mdev/gml_classifier/requirements.txt (line 26)) (1.1.1)\n",
      "Requirement already satisfied: cssselect2>=0.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from weasyprint->-r /home/mdev/gml_classifier/requirements.txt (line 26)) (0.7.0)\n",
      "Requirement already satisfied: Pyphen>=0.9.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from weasyprint->-r /home/mdev/gml_classifier/requirements.txt (line 26)) (0.14.0)\n",
      "Requirement already satisfied: fonttools>=4.0.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from fonttools[woff]>=4.0.0->weasyprint->-r /home/mdev/gml_classifier/requirements.txt (line 26)) (4.37.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from matplotlib->-r /home/mdev/gml_classifier/requirements.txt (line 27)) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mdev/anaconda3/lib/python3.9/site-packages (from matplotlib->-r /home/mdev/gml_classifier/requirements.txt (line 27)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from matplotlib->-r /home/mdev/gml_classifier/requirements.txt (line 27)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from matplotlib->-r /home/mdev/gml_classifier/requirements.txt (line 27)) (3.0.9)\n",
      "Requirement already satisfied: filelock in /home/mdev/anaconda3/lib/python3.9/site-packages (from transformers->-r /home/mdev/gml_classifier/requirements.txt (line 28)) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/mdev/anaconda3/lib/python3.9/site-packages (from transformers->-r /home/mdev/gml_classifier/requirements.txt (line 28)) (0.20.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from transformers->-r /home/mdev/gml_classifier/requirements.txt (line 28)) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/mdev/anaconda3/lib/python3.9/site-packages (from transformers->-r /home/mdev/gml_classifier/requirements.txt (line 28)) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from transformers->-r /home/mdev/gml_classifier/requirements.txt (line 28)) (0.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from scikit-learn->-r /home/mdev/gml_classifier/requirements.txt (line 29)) (3.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/mdev/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->-r /home/mdev/gml_classifier/requirements.txt (line 31)) (2.3.2.post1)\n",
      "Requirement already satisfied: pycparser in /home/mdev/anaconda3/lib/python3.9/site-packages (from cffi>=0.6->weasyprint->-r /home/mdev/gml_classifier/requirements.txt (line 26)) (2.21)\n",
      "Requirement already satisfied: webencodings in /home/mdev/anaconda3/lib/python3.9/site-packages (from cssselect2>=0.1->weasyprint->-r /home/mdev/gml_classifier/requirements.txt (line 26)) (0.5.1)\n",
      "Requirement already satisfied: zopfli>=0.1.4 in /home/mdev/anaconda3/lib/python3.9/site-packages (from fonttools[woff]>=4.0.0->weasyprint->-r /home/mdev/gml_classifier/requirements.txt (line 26)) (0.2.3)\n",
      "Requirement already satisfied: brotli>=1.0.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from fonttools[woff]>=4.0.0->weasyprint->-r /home/mdev/gml_classifier/requirements.txt (line 26)) (1.1.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r /home/mdev/gml_classifier/requirements.txt (line 28)) (2023.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/mdev/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->Flask->-r /home/mdev/gml_classifier/requirements.txt (line 4)) (3.8.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r /home/mdev/gml_classifier/requirements.txt (line 24)) (1.6.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/mdev/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r /home/mdev/gml_classifier/requirements.txt (line 24)) (7.3.4)\n",
      "Requirement already satisfied: nest-asyncio in /home/mdev/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r /home/mdev/gml_classifier/requirements.txt (line 24)) (1.5.6)\n",
      "Requirement already satisfied: psutil in /home/mdev/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r /home/mdev/gml_classifier/requirements.txt (line 24)) (5.9.2)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/mdev/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r /home/mdev/gml_classifier/requirements.txt (line 24)) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r /home/mdev/gml_classifier/requirements.txt (line 24)) (6.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->IPython->-r /home/mdev/gml_classifier/requirements.txt (line 17)) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from Jinja2>=3.0->Flask->-r /home/mdev/gml_classifier/requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/mdev/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->IPython->-r /home/mdev/gml_classifier/requirements.txt (line 17)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/mdev/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->-r /home/mdev/gml_classifier/requirements.txt (line 17)) (0.2.5)\n",
      "Requirement already satisfied: asn1crypto>=1.5.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from pyHanko>=0.12.1->xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (1.5.1)\n",
      "Requirement already satisfied: qrcode>=7.3.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from pyHanko>=0.12.1->xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (7.4.2)\n",
      "Requirement already satisfied: tzlocal>=4.3 in /home/mdev/anaconda3/lib/python3.9/site-packages (from pyHanko>=0.12.1->xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (5.2)\n",
      "Requirement already satisfied: cryptography>=41.0.5 in /home/mdev/anaconda3/lib/python3.9/site-packages (from pyHanko>=0.12.1->xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (42.0.1)\n",
      "Requirement already satisfied: oscrypto>=1.1.0 in /home/mdev/anaconda3/lib/python3.9/site-packages (from pyhanko-certvalidator>=0.19.5->xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (1.3.0)\n",
      "Requirement already satisfied: uritools>=3.0.1 in /home/mdev/anaconda3/lib/python3.9/site-packages (from pyhanko-certvalidator>=0.19.5->xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (4.0.2)\n",
      "Requirement already satisfied: chardet in /home/mdev/anaconda3/lib/python3.9/site-packages (from reportlab>=4.0.4->xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (5.0.0)\n",
      "Requirement already satisfied: lxml in /home/mdev/anaconda3/lib/python3.9/site-packages (from svglib>=1.2.1->xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (4.9.1)\n",
      "Requirement already satisfied: entrypoints in /home/mdev/anaconda3/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->-r /home/mdev/gml_classifier/requirements.txt (line 24)) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /home/mdev/anaconda3/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->-r /home/mdev/gml_classifier/requirements.txt (line 24)) (4.11.1)\n",
      "Requirement already satisfied: pypng in /home/mdev/anaconda3/lib/python3.9/site-packages (from qrcode>=7.3.1->pyHanko>=0.12.1->xhtml2pdf->-r /home/mdev/gml_classifier/requirements.txt (line 22)) (0.20220715.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !python3 -m pip install --upgrade pip\n",
    "%pip install -r {os.path.join(path,'requirements.txt')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conteúdos da pasta utils\n",
      "  articles_counter.py\n",
      "  cnpq_classes.py\n",
      "  descriptive_statistics.py\n",
      "  json_fle_manager.py\n",
      "  prepare_testing_environment.py\n",
      "  pytorch_checks.py\n",
      "  settings_backup.json\n",
      "  tests_prepare.py\n",
      "  __init__.py\n",
      "  __pycache__\n",
      "\n",
      "Conteúdos da pasta domains\n",
      "  bert_embeedings_to_neo4j.py\n",
      "  cnpq_tree.py\n",
      "  cnpq_tree4j.py\n",
      "  curriculum_scrapper.py\n",
      "  dataset_articles_generator.go\n",
      "  dataset_articles_generator_linux\n",
      "  dataset_articles_generator_optim.go\n",
      "  dataset_articles_generator_optim_linux\n",
      "  dataset_articles_generator_optim_windows.exe\n",
      "  dataset_articles_generator_py.py\n",
      "  dataset_articles_generator_windows.exe\n",
      "  experiment_monitor.py\n",
      "  experiment_profiler.py\n",
      "  fill_missing_data_crossref.py\n",
      "  gml_cnpq_classifier.py\n",
      "  go.mod\n",
      "  go.sum\n",
      "  hierarquical_semantic_matcher.py\n",
      "  lda_extractor.py\n",
      "  Passo01_FormularQuestãoPesquisa.ipynb\n",
      "  pasteur_scraper.py\n",
      "  report_fioce.py\n",
      "  research_process_automation.py\n",
      "  translate_en_pt.py\n",
      "  __pycache__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'lib' has no attribute 'X509_V_FLAG_NOTIFY_POLICY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17356/107832827.py\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0marticles_counter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArticlesCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexperiment_profiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeProfiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlda_extractor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLDAExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreport_fioce\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReportHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcnpq_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCNPQtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gml_classifier/source/domain/lda_extractor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gensim/parsing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .preprocessing import (  # noqa:F401\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpreprocess_documents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpreprocess_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gensim/parsing/preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmart_open\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgensim_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/smart_open/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msmart_open\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msmart_open_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_compressor\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m _WARNING = \"\"\"smart_open.s3_iter_bucket is deprecated and will stop functioning\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mso_compression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmart_open\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdoctools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msmart_open\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/smart_open/doctools.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mPLACEHOLDER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'    smart_open/doctools.py magic goes here'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/smart_open/transport.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mregister_transport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'smart_open.hdfs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mregister_transport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'smart_open.http'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mregister_transport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'smart_open.s3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0mregister_transport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'smart_open.ssh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mregister_transport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'smart_open.webhdfs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/smart_open/transport.py\u001b[0m in \u001b[0;36mregister_transport\u001b[0;34m(submodule)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0msubmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/smart_open/s3.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/boto3/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_warn_deprecated_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0m__author__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Amazon Web Services'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/boto3/session.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownServiceError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/botocore/session.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxform_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientArgsCreator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUTH_TYPE_MAPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/botocore/waiter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjmespath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWaiterDocstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_service_module_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/botocore/docs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mServiceDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/botocore/docs/service.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# language governing permissions and limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbcdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocumentStructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientDocumenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClientExceptionsDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaginator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPaginatorDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWaiterDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/botocore/docs/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# language governing permissions and limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResponseExampleDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m from botocore.docs.method import (\n\u001b[1;32m     16\u001b[0m     \u001b[0mdocument_custom_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/botocore/docs/example.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# ANY KIND, either express or implied. See the License for the specific\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# language governing permissions and limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShapeDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpy_default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/botocore/docs/shape.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# inherited from a Documenter class with the appropriate methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# and attributes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_json_value_header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/botocore/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawsrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttpsession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# IP Regexes retained for backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Always import the original SSLContext, even if it has been patched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         from urllib3.contrib.pyopenssl import (\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0morig_util_SSLContext\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSSLContext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mx509\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhazmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenssl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopenssl_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/OpenSSL/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mOpenSSL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcrypto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSSL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from OpenSSL.version import (\n\u001b[1;32m     10\u001b[0m     \u001b[0m__author__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/OpenSSL/crypto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mX509StoreFlags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m     \"\"\"\n\u001b[1;32m   1572\u001b[0m     \u001b[0mFlags\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX509\u001b[0m \u001b[0mverification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mchange\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbehavior\u001b[0m \u001b[0mof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/OpenSSL/crypto.py\u001b[0m in \u001b[0;36mX509StoreFlags\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \u001b[0mEXPLICIT_POLICY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX509_V_FLAG_EXPLICIT_POLICY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m     \u001b[0mINHIBIT_MAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX509_V_FLAG_INHIBIT_MAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m     \u001b[0mNOTIFY_POLICY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX509_V_FLAG_NOTIFY_POLICY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m     \u001b[0mCHECK_SS_SIGNATURE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX509_V_FLAG_CHECK_SS_SIGNATURE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'lib' has no attribute 'X509_V_FLAG_NOTIFY_POLICY'"
     ]
    }
   ],
   "source": [
    "base_repo_dir = get_path_repo()\n",
    "\n",
    "# Construir caminhos usando os.path.join para evitar problemas entre Linux/Windows\n",
    "os.listdir(get_path_repo())\n",
    "os.listdir(os.path.join(get_path_repo(), 'source', 'domain'))\n",
    "folder_utils = os.path.join(get_path_repo(),'utils')\n",
    "folder_domain = os.path.join(get_path_repo(), 'source','domain/')\n",
    "folder_data_input = os.path.join(base_repo_dir, 'data', 'input')\n",
    "folder_data_output = os.path.join(base_repo_dir, 'data', 'output')\n",
    "# Para o caso de folder_data_prod, que parece ser exclusivo para ambientes Unix\n",
    "folder_data_prod = os.path.join(base_repo_dir, 'data') if not 'win' in sys.platform else None\n",
    "\n",
    "print('\\nConteúdos da pasta utils')\n",
    "for i in os.listdir(folder_utils):\n",
    "    print(f\"  {i}\")\n",
    "\n",
    "print('\\nConteúdos da pasta domains')\n",
    "for i in os.listdir(folder_domain):\n",
    "    print(f\"  {i}\")\n",
    "\n",
    "# Adicionar pastas locais ao sys.path para importar pacotes criados\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "from bert_embeedings_to_neo4j import BertEmbeddingsToNeo4j\n",
    "from json_fle_manager import JSONFileManager as jfm\n",
    "from experiment_monitor import ExperimentMonitor\n",
    "from articles_counter import ArticlesCounter\n",
    "from experiment_profiler import TimeProfiler\n",
    "from lda_extractor import LDAExtractor\n",
    "from report_fioce import ReportHTML\n",
    "from cnpq_tree import CNPQtree\n",
    "\n",
    "print(f\"\\nCaminho base do repositório: {base_repo_dir}\")\n",
    "print(f\"\\nArquivos de entrada de dados: {jfm.list_json_files(folder_data_input)}\")\n",
    "print(f\"\\nArquivos de dados processados:\")\n",
    "list(os.listdir(os.path.join(folder_data_output)))\n",
    "\n",
    "# Definir arquivo dados brutos a processar e gerar dataset\n",
    "# profiler = TimeProfiler()\n",
    "# monitor = ExperimentMonitor(base_repo_dir, profiler)\n",
    "dict_json = 'dict_list_fioce.json'\n",
    "dict_list = jfm.load_json(folder_data_input,dict_json)\n",
    "print(f\"{' '*30}{len(dict_list)} currículos carregados arquivo: '{dict_json}'\")\n",
    "\n",
    "# if monitor.is_gpu_available():\n",
    "#     print(f\"\\nGPU disponível para execução de código.\")\n",
    "# else:\n",
    "#     print(f\"\\nNão foi detectada nenhuma GPU configurada corretamente no ambiente.\")\n",
    "\n",
    "# print(f\"\\nVerificação dos pacotes de PLN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atualizador = ArticlesCounter(dict_list)\n",
    "dtf_atualizado = atualizador.extrair_data_atualizacao(dict_list)\n",
    "print(f\"\\nDataframe de ilustração da produção acadêmica em análise\")\n",
    "dtf_atualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_repo_dir = get_path_repo()\n",
    "report = ReportHTML(base_repo_dir)\n",
    "filename = 'output_py_gpu_multithreads.json'\n",
    "errors = report.check_keys(filename)\n",
    "report.generate_fioce_report_html(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fioce_title_list = report.generat_title_list_report(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abordagem Non-negative Matrix Factorization (NMF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Em modelagem de tópicos o NMF é uma técnica alternativa que pode fornecer resultados mais facilmente interpretáveis em alguns casos. Assim como o LDA, o NMF identifica tópicos em um conjunto de documentos, mas utiliza uma abordagem matemática diferente. Passos para aplicar o NMF:\n",
    "\n",
    "- Vetorizar Títulos: Utilizar TF-IDF para converter os títulos dos artigos em vetores numéricos.\n",
    "- Aplicar NMF: Usar o NMF para identificar tópicos nos dados vetorizados.\n",
    "- Analisar Tópicos: Examinar os tópicos identificados e suas principais palavras-chave.\n",
    "\n",
    "A seguir criamos uma função com nome 'modelar_topicos_nmf' para fazer a atribuição dos tópicos a cada um dos títulos de uma lista de modo que ela retorne um dicionário com a atribuição de um tópico a cada um dos títulos, você pode seguir estes passos:\n",
    "\n",
    "- Atribuir Títulos aos Tópicos: Após a aplicação do NMF, use a matriz W (que contém os pesos dos tópicos para cada documento) para determinar a qual tópico cada título pertence. Isso é feito encontrando o índice do componente com o maior peso para cada título.\n",
    "\n",
    "- Criar um Dicionário de Atribuições: Crie um dicionário onde cada chave é um título de artigo e o valor correspondente é o tópico atribuído.\n",
    "\n",
    "Para atribuir Grande Área e Área a cada um dos títulos de um determinado autor, utilizando a estrutura criada com a classificação do CNPq, podemos implementar uma função que faz a modelagem de tópicos com NMF e, em seguida, compara os tópicos resultantes com as descrições das áreas e subáreas para encontrar a melhor correspondência. Essa tarefa envolve várias etapas:\n",
    "\n",
    "- Modelar Tópicos dos Títulos: Utilizar a função NMF existente para obter os tópicos dos títulos dos artigos.\n",
    "\n",
    "- Preparar Descrições das Áreas e Subáreas do CNPq para Comparação: Vetorizar as descrições das áreas e subáreas do CNPq usando o mesmo vetorizador TF-IDF usado para os títulos dos artigos.\n",
    "\n",
    "- Comparar Tópicos dos Artigos com as Áreas e Subáreas: Calcular a similaridade entre os tópicos dos artigos e as descrições vetorizadas das áreas e subáreas para determinar a melhor correspondência.\n",
    "\n",
    "- Atribuir os Títulos às Áreas Correspondentes: Baseado na comparação, atribuir cada título à área e subárea mais próxima por afinidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar Árvore do Conhecimento do CNPq em JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando a classe e processando os dados\n",
    "report = ReportHTML(base_repo_dir)\n",
    "\n",
    "# Carregando o arquivo de dados\n",
    "filename = 'output_py_gpu_multithreads.json'\n",
    "filepath = os.path.join(report.folder_data_output, filename)\n",
    "\n",
    "# Gerando a lista de títulos e as áreas de expertise\n",
    "titulos, areas_of_expertise, title_to_researcher = report.generat_title_list_report(filepath)\n",
    "\n",
    "# Estrutura CNPq\n",
    "estrutura_cnpq_str = report.create_cnpq_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para coleta de palavras-chave do usuário\n",
    "palavras_chave_reconhecidas = {}\n",
    "palavras_chave_usuario = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerar palavras-chave com modelo NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Caso já existam palavras-chave de modelo treinado anteriormente combinar palavras-chave\n",
    "palavras_chave_combinadas = {**palavras_chave_reconhecidas, **palavras_chave_usuario}\n",
    "\n",
    "# Salvar as palavras-chave combinadas em um arquivo JSON\n",
    "caminho_arquivo = os.path.join(folder_data_input, 'palavras_chave_area_cnpq.json')\n",
    "report.salvar_palavras_chave_em_disco(palavras_chave_combinadas, caminho_arquivo)\n",
    "\n",
    "# Definindo o número de componentes (tópicos) para o NMF\n",
    "n_components = 8\n",
    "\n",
    "# Processamento de atribuição de áreas do CNPq\n",
    "nmf, W, vectorizer, atribuicoes = report.atribuir_grande_area_e_area(titulos, estrutura_cnpq_str, n_components, palavras_chave_combinadas)\n",
    "\n",
    "# Converter a estrutura CNPq em um formato achatado (flat) que será utilizado para buscar os nomes das áreas\n",
    "flat_structure = report.flatten(estrutura_cnpq_str)\n",
    "\n",
    "# Extrair palavras-chave a partir do nmf e vectorizer criados pelo modelo NMF + Palavras-chave de usuário\n",
    "palavras_chave_nmf = report.extrair_palavras_chave_nmf(nmf, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atribuir títulos às Áreas e gerar relatórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar atribuições com base nas áreas de atuação dos pesquisadores componentes do NMF para calcular a similaridade TF-IDF\n",
    "atribuicoes_ajustadas = report.ajustar_atribuicoes_com_areas_atuacao(\n",
    "    atribuicoes, areas_of_expertise, title_to_researcher, flat_structure, vectorizer, nmf, W\n",
    ")\n",
    "\n",
    "# Gerar o relatório final em HTML com as atribuições ajustadas\n",
    "fioce_title_list = report.criar_relatorio_classificacao_cnpq(atribuicoes_ajustadas, vectorizer.get_feature_names_out(), flat_structure, nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrada de palavras-chave do usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def interface_usuario():\n",
    "    report = ReportHTML(base_repo_dir)\n",
    "\n",
    "    estrutura_cnpq_str = report.create_cnpq_structure()\n",
    "\n",
    "    while True:\n",
    "        criterio = input(\"Insira o nome ou código da Área CNPq para adicionar palavras-chave (ou 'sair' para encerrar): \")\n",
    "        if criterio.lower() == 'sair':\n",
    "            break\n",
    "\n",
    "        codigo_area, nome_area = report.buscar_area(estrutura_cnpq_str, criterio)\n",
    "\n",
    "        if nome_area:\n",
    "            palavras_chave = input(f\"Insira palavras-chave para a Área '{nome_area}' ({codigo_area}) [separadas por vírgula]: \").split(',')\n",
    "            palavras_chave = [palavra.strip() for palavra in palavras_chave if palavra.strip()]\n",
    "            \n",
    "            # Salvar as palavras-chave no dicionário (isso pode ser adaptado para salvar em um arquivo ou banco de dados)\n",
    "            # Exemplo: salvar_palavras_chave(codigo_area, palavras_chave)\n",
    "            \n",
    "            print(f\"Palavras-chave adicionadas para a Área {nome_area}: {palavras_chave}\")\n",
    "        else:\n",
    "            print(\"Área não encontrada.\")\n",
    "\n",
    "def salvar_palavras_chave_em_disco(palavras_chave_por_area, caminho_arquivo):\n",
    "    try:\n",
    "        with open(caminho_arquivo, 'w', encoding='utf-8') as arquivo:\n",
    "            json.dump(palavras_chave_por_area, arquivo, ensure_ascii=False, indent=4)\n",
    "            print(f\"Palavras-chave salvas com sucesso em {caminho_arquivo}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Erro ao salvar o arquivo: {e}\")\n",
    "\n",
    "def interface_usuario():\n",
    "    report = ReportHTML(base_repo_dir)\n",
    "\n",
    "    estrutura_cnpq_str = report.create_cnpq_structure()\n",
    "    palavras_chave_por_area = {}\n",
    "\n",
    "    while True:\n",
    "        criterio = input(\"Insira o nome ou código da Área CNPq para adicionar palavras-chave (ou 'sair' para encerrar): \")\n",
    "        if criterio.lower() == 'sair':\n",
    "            break\n",
    "\n",
    "        codigo_area, nome_area = report.buscar_area(estrutura_cnpq_str, criterio)\n",
    "\n",
    "        if nome_area:\n",
    "            palavras_chave = input(f\"Insira palavras-chave para a Área '{nome_area}' ({codigo_area}) [separadas por vírgula]: \").split(',')\n",
    "            palavras_chave = [palavra.strip() for palavra in palavras_chave if palavra.strip()]\n",
    "\n",
    "            palavras_chave_por_area[codigo_area] = palavras_chave\n",
    "            \n",
    "            print(f\"Palavras-chave adicionadas para a Área {nome_area}: {palavras_chave}\")\n",
    "\n",
    "            # Perguntar ao usuário se deseja continuar\n",
    "            continuar = input(\"Deseja adicionar palavras-chave para outra área? (s/n): \").lower()\n",
    "            if continuar != 's':\n",
    "                break\n",
    "        else:\n",
    "            print(\"Área não encontrada.\")\n",
    "            continuar = input(\"Deseja tentar outra área? (s/n): \").lower()\n",
    "            if continuar != 's':\n",
    "                break\n",
    "\n",
    "    if palavras_chave_por_area:\n",
    "        caminho_arquivo = os.path.join(base_repo_dir, 'data', 'input', 'palavras_chave_areas_cnpq.json')\n",
    "        salvar_palavras_chave_em_disco(palavras_chave_por_area, caminho_arquivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface_usuario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Abordagem Latent Dirichlet Aloccation (LDA)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinar modelo em Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = jfm.load_json(folder_data_output,filename)\n",
    "lda_extractor = LDAExtractor(base_repo_dir, num_topics=8, passes=20)\n",
    "lda_model, dictionary, corpus = lda_extractor.fit_transform(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exibição das palavras por tópicos\n",
    "\n",
    "Obs.: o POS tagging (part-of-speech tagging) do NLTK, na sua implementação padrão, não suporta a língua portuguesa. Vamos ajustar a função extrair_palavras_chave para trabalhar com textos em português, utilizando outra biblioteca ou técnica que suporte o processamento de texto em português. Uma alternativa é usar a biblioteca spaCy, que oferece suporte para a língua portuguesa, incluindo tokenização e POS tagging. Para a função de identificar palavras-chave vamos ajustar a função para extrair substantivos, adjetivos e verbos que são mais prováveis de serem palavras-chave significativas. Além disso, vamos remover palavras que são muito curtas, pois elas podem ser menos significativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir os principais termos em cada tópico\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Tópico: {idx}\\nPalavras: {topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Preparar os dados para visualização\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Visualizar\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribuir cada documento a um tópico\n",
    "doc_topics = [lda_model.get_document_topics(bow) for bow in corpus]\n",
    "\n",
    "# Exemplo: Exibir o tópico mais provável para o primeiro documento\n",
    "print(doc_topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Função para atribuir tópico a área CNPq\n",
    "# def atribuir_topico_a_cnpq(topico, palavras_chave_cnpq):\n",
    "#     palavras_topico = [palavra for palavra, _ in topico]\n",
    "#     scores = {area: sum(palavra in palavras_topico for palavra in palavras) for area, palavras in palavras_chave_cnpq.items()}\n",
    "#     return max(scores, key=scores.get)\n",
    "\n",
    "# palavras_chave_cnpq = []\n",
    "# cat_grandeareas = {}\n",
    "\n",
    "# # Exemplo de atribuição\n",
    "# for idx, topico in lda_model.show_topics(formatted=False):\n",
    "#     cod_area_cnpq = atribuir_topico_a_cnpq(topico, palavras_chave_cnpq)\n",
    "#     print(f\"Tópico {idx} está mais relacionado com a área: {cat_grandeareas.get(cod_area_cnpq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Abordagem por Latent Semantic Analysis (LSA)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Análise Semântica Latente ou Latent Semantic Analysis (LSA), é uma técnica usada em processamento de linguagem natural (NLP) para analisar relações entre uma coleção de documentos e os termos neles contidos. O objetivo principal da LSA é descobrir padrões na utilização de palavras nos documentos, permitindo identificar a semelhança semântica entre eles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Exemplo de documentos\n",
    "documentos = [\n",
    "    \"Evaluation of the antitumor and trypanocidal activities and alkaloid profile in species of Brazilian Cactaceae\",\n",
    "    \"Physalis angulata L. antineoplasic activity, in vitro, evaluation fromit´s stems and fruit capsules\",\n",
    "    \"Autoreactive CD4 T cells In the pathogenesis of the chronic myocarditis found in the experimental Trypanosoma cruzi Infection\"\n",
    "]\n",
    "\n",
    "# Converter documentos em uma matriz TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "# Aplicar LSA usando SVD truncado\n",
    "lsa = TruncatedSVD(n_components=8)  # n_components é o número de dimensões/tópicos desejados\n",
    "X_lsa = lsa.fit_transform(X)\n",
    "\n",
    "# X_lsa agora contém a representação de cada documento no espaço de tópicos latentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def classificar_artigos_nmf(titulos, num_componentes):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(titulos)\n",
    "    nmf = NMF(n_components=num_componentes, random_state=42)\n",
    "    W = nmf.fit_transform(X)\n",
    "    return W.argmax(axis=1)\n",
    "\n",
    "def classificar_em_niveis(titulos, num_grande_area, num_areas, num_subareas):\n",
    "    # Primeiro nível: Grande Área\n",
    "    classificacao_grande_area = classificar_artigos_nmf(titulos, num_grande_area)\n",
    "\n",
    "    # Segundo nível: Área\n",
    "    classificacao_area = {}\n",
    "    for ga in range(num_grande_area):\n",
    "        titulos_ga = [titulos[i] for i in range(len(titulos)) if classificacao_grande_area[i] == ga]\n",
    "        classificacao_area[ga] = classificar_artigos_nmf(titulos_ga, num_areas)\n",
    "\n",
    "    # Terceiro nível: Subárea\n",
    "    classificacao_subarea = {}\n",
    "    for ga in classificacao_area:\n",
    "        classificacao_subarea[ga] = {}\n",
    "        for a in range(num_areas):\n",
    "            titulos_area = [titulos[i] for i, ga_class in enumerate(classificacao_grande_area) if ga_class == ga and classificacao_area[ga][i] == a]\n",
    "            classificacao_subarea[ga][a] = classificar_artigos_nmf(titulos_area, num_subareas)\n",
    "\n",
    "    return classificacao_grande_area, classificacao_area, classificacao_subarea\n",
    "\n",
    "# Exemplo de uso\n",
    "'''\n",
    "Claudia Stutz Zubieta\n",
    "\n",
    "ÁREAS CNPq:\n",
    "        GrandeÁrea: ['Ciências da Saúde']\n",
    "             Áreas: ['Saúde Coletiva']\n",
    "          Subáreas: ['Biologia Molecular', 'Histologia', 'Farmacologia Geral', 'Citologia e Biologia Celular']\n",
    "    Especialidades: []\n",
    "\n",
    "ARTIGOS:\n",
    "Evaluation of the antitumor and trypanocidal activities and alkaloid profile in species of Brazilian Cactaceae\n",
    "Physalis angulata L. antineoplasic activity, in vitro, evaluation fromit´s stems and fruit capsules\n",
    "Autoreactive CD4 T cells In the pathogenesis of the chronic myocarditis found in the experimental Trypanosoma cruzi Infection\n",
    "'''\n",
    "\n",
    "titulos_pesquisador = [\n",
    "    \"Evaluation of the antitumor and trypanocidal activities and alkaloid profile in species of Brazilian Cactaceae\",\n",
    "    \"Physalis angulata L. antineoplasic activity, in vitro, evaluation fromit´s stems and fruit capsules\",\n",
    "    \"Autoreactive CD4 T cells In the pathogenesis of the chronic myocarditis found in the experimental Trypanosoma cruzi Infection\"\n",
    "]\n",
    "\n",
    "GrandeÁrea = ['Ciências da Saúde']\n",
    "Áreas = ['Saúde Coletiva']\n",
    "Subáreas = ['Biologia Molecular', 'Histologia', 'Farmacologia Geral', 'Citologia e Biologia Celular']\n",
    "\n",
    "num_grande_area = len(GrandeÁrea)\n",
    "num_areas = len(Áreas)\n",
    "num_subareas = len(Subáreas)\n",
    "\n",
    "classificacao_ga, classificacao_a, classificacao_sa = classificar_em_niveis(titulos_pesquisador, num_grande_area, num_areas, num_subareas)\n",
    "\n",
    "for titulo, topico in zip(titulos_pesquisador, classificacao_sa):\n",
    "    print(f\"Título: {titulo}\\nClassificado no Tópico: {topico + 1}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistência em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"password\"\n",
    "bert_to_neo4j = BertEmbeddingsToNeo4j(uri, username, password)\n",
    "\n",
    "# Supondo que você tenha uma função que retorna pares de article_id e texto\n",
    "# for article_id, text in get_article_id_and_text():\n",
    "#     embedding = bert_to_neo4j.get_embedding(text)\n",
    "#     bert_to_neo4j.update_embedding(article_id, embedding)\n",
    "\n",
    "# bert_to_neo4j.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abordagem de tradução (Com problema reinicia o computador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data[0].get('processed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from translate_en_pt import TranslatorEnPt\n",
    "\n",
    "def extrair_e_traduzir(json_data):\n",
    "    translator = TranslatorEnPt()\n",
    "    dados_traduzidos = []\n",
    "\n",
    "    for researcher in tqdm(json_data, desc=\"Processando dados dos currículos\"):\n",
    "        curriculo = researcher.get('processed_data')\n",
    "        novo_researcher = {\n",
    "            \"idLattes\": curriculo['id_lattes'],\n",
    "            \"name\": curriculo['name'],\n",
    "            \"articles\": []\n",
    "        }\n",
    "        n=0\n",
    "        for article in tqdm(curriculo.get('articles', []), desc=\"Traduzindo artigos\", leave=False):\n",
    "            n+=1\n",
    "            titulo = article.get('subdict_titulos', '').get(str(n)+'.')\n",
    "            # resumo = article.get('subdict_resumos', '').get(str(n)+'.')\n",
    "\n",
    "            # Traduzindo título e resumo, se necessário\n",
    "            if titulo:\n",
    "                titulo = translator.translate(titulo)\n",
    "            # if resumo:\n",
    "            #     resumo = translator.translate(resumo)\n",
    "            index=str(n)+'.'\n",
    "            novo_researcher['articles'].append({\n",
    "                \"year\": article.get('subdict_years').get(index),\n",
    "                \"title\": article.get('subdict_titulos').get(index),\n",
    "                \"doi\": article.get('subdict_doi').get(n),\n",
    "                \"impactFactor\": article.get('subdict_jci').get(n),\n",
    "                # \"abstract\": resumo\n",
    "            })\n",
    "\n",
    "        dados_traduzidos.append(novo_researcher)\n",
    "\n",
    "    return dados_traduzidos\n",
    "\n",
    "# Carregar dataset JSON\n",
    "json_data = jfm.load_json(folder_data_output,filename)\n",
    "\n",
    "# Realizar a extração e a tradução\n",
    "dados_traduzidos = extrair_e_traduzir(json_data)\n",
    "\n",
    "# Salvar os dados traduzidos em um novo arquivo JSON\n",
    "filepath = os.path.join(folder_data_output,'translated_data.json')\n",
    "with open(filepath, 'w') as file:\n",
    "    json.dump(dados_traduzidos, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_traduzidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Carregar o modelo BERT multilíngue e o tokenizer\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Exemplo de uso\n",
    "text = \"This is a test sentence.\"\n",
    "embedding = get_embedding(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset JSON\n",
    "folder='/home/mak/gml_classifier-1/data/output/'\n",
    "json_file='output_go_cpu_multithread_optim.json'\n",
    "with open(folder+json_file, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Instanciar a classe LDAExtractor\n",
    "lda_extractor = LDAExtractor(num_topics=8, passes=2)\n",
    "\n",
    "# Executar a extração e o pré-processamento dos dados\n",
    "lda_model, dictionary, doc_term_matrix = lda_extractor.fit_transform(json_data)\n",
    "\n",
    "# Para visualizar os tópicos\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f'Topic: {idx} \\nWords: {topic}')\n",
    "\n",
    "# # Executar a extração e o pré-processamento dos dados em lotes\n",
    "# batch_results = lda_extractor.fit_transform_batch(json_data, batch_size=50)\n",
    "\n",
    "# # Para visualizar os tópicos de cada lote\n",
    "# for lda_model, dictionary in batch_results:\n",
    "#     for idx, topic in lda_model.print_topics(-1):\n",
    "#         print(f'Topic: {idx} \\nWords: {topic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes em pré-processar com PLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python3 -m spacy download pt_core_news_sm\n",
    "# !pip install nltk\n",
    "!python -m nltk.downloader stopwords\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('rslp')\n",
    "nltk.download('stopwords')\n",
    "stop_words_pt = set(stopwords.words('portuguese'))\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Inicializando o stemmer\n",
    "stemmer = RSLPStemmer()\n",
    "\n",
    "def remover_stopwords(texto):\n",
    "    # Tokeniza o texto e remove stopwords\n",
    "    palavras = [token.text for token in nlp(texto) if token.text.lower() not in stop_words_pt]\n",
    "    return ' '.join(palavras)\n",
    "\n",
    "def extrair_radicais(descricao):\n",
    "    # palavras = word_tokenize(descricao) ## sem remover stopwords\n",
    "    palavras = [token.text for token in nlp(descricao) if token.text.lower() not in stop_words_pt]\n",
    "    radicais = [stemmer.stem(palavra) for palavra in palavras]\n",
    "    return radicais\n",
    "\n",
    "# Carregar o modelo de linguagem portuguesa do spaCy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def classificar_palavras_chave(descricao):\n",
    "    # Processa a descrição com o spaCy\n",
    "    doc = nlp(descricao.lower())\n",
    "    \n",
    "    # Extrai palavras-chave que são substantivos, adjetivos ou verbos\n",
    "    # e filtra palavras muito curtas (com menos de 3 caracteres)\n",
    "    palavras_chave = [token.text for token in doc if token.pos_ in ['NOUN', 'ADJ', 'VERB'] and len(token.text) > 2]\n",
    "\n",
    "    return palavras_chave\n",
    "\n",
    "def lematizar_palavras_chave(descricao):\n",
    "    # Processa a descrição com o spaCy\n",
    "    doc = nlp(descricao.lower())\n",
    "    \n",
    "    # Extrai palavras-chave que são substantivos, adjetivos ou verbos\n",
    "    # e filtra palavras muito curtas (com menos de 3 caracteres)\n",
    "    palavras_chave = [token.lemma_ for token in doc if token.pos_ in ['NOUN', 'ADJ', 'VERB'] and len(token.text) > 2]\n",
    "\n",
    "    return palavras_chave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descricao_teste = \"Analisaram os Algoritmos e a Complexidade Big-O em Computação\"\n",
    "print(classificar_palavras_chave(descricao_teste))\n",
    "print(lematizar_palavras_chave(descricao_teste))\n",
    "print(extrair_radicais(descricao_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Exemplo de lista de documentos (cada documento é uma string)\n",
    "documentos = [\n",
    "    \"Análise de dados e processamento de linguagem natural\",\n",
    "    \"Estudo de algoritmos em computação quântica\",\n",
    "    \"Pesquisa em física quântica e suas aplicações\",\n",
    "    # Adicione mais documentos conforme necessário\n",
    "]\n",
    "\n",
    "# Inicializar TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajustar e transformar os documentos\n",
    "tfidf_matrix = vectorizer.fit_transform(documentos)\n",
    "\n",
    "# Obter palavras-chave com base na pontuação TF-IDF\n",
    "def obter_palavras_chave(tfidf_matrix, vectorizer, top_n=10):\n",
    "    palavras = vectorizer.get_feature_names_out()\n",
    "    palavras_chave_dict = {}\n",
    "\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        # Array de TF-IDF para o documento i\n",
    "        tfidf_array = tfidf_matrix[i].toarray().flatten()\n",
    "\n",
    "        # Obter índices dos top_n valores de TF-IDF\n",
    "        top_n_indices = tfidf_array.argsort()[-top_n:]\n",
    "\n",
    "        # Mapear índices para palavras correspondentes e pontuações TF-IDF\n",
    "        palavras_chave_dict[i] = [(palavras[j], tfidf_array[j]) for j in top_n_indices]\n",
    "\n",
    "    return palavras_chave_dict\n",
    "\n",
    "# Obter as palavras-chave de cada documento\n",
    "palavras_chave = obter_palavras_chave(tfidf_matrix, vectorizer)\n",
    "print(palavras_chave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando 'estrutura' como dicionário hierárquico\n",
    "palavras_chave_cnpq = {}\n",
    "estrutura = report.create_cnpq_structure()\n",
    "for ga_code, ga_data in estrutura.items():\n",
    "    palavras_chave_ga = set()  # Usando um conjunto para evitar palavras-chave duplicadas\n",
    "\n",
    "    for a_code, a_data in ga_data[\"areas\"].items():\n",
    "        palavras_chave_ga.update(classificar_palavras_chave(a_data[\"descricao\"]))\n",
    "\n",
    "        for sa_code, sa_data in a_data[\"subareas\"].items():\n",
    "            palavras_chave_ga.update(classificar_palavras_chave(sa_data[\"descricao\"]))\n",
    "\n",
    "            for e_code, e_desc in sa_data[\"especialidades\"].items():\n",
    "                palavras_chave_ga.update(classificar_palavras_chave(e_desc))\n",
    "\n",
    "    # Convertendo o conjunto de palavras-chave em uma lista\n",
    "    palavras_chave_cnpq[ga_code] = list(palavras_chave_ga)\n",
    "\n",
    "# Imprimindo o dicionário de palavras-chave\n",
    "print(palavras_chave_cnpq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidar_textos(estrutura, ga_code):\n",
    "    textos = []\n",
    "    for a_code, a_data in estrutura[ga_code][\"areas\"].items():\n",
    "        textos.append(a_data[\"descricao\"])\n",
    "        for sa_code, sa_data in a_data[\"subareas\"].items():\n",
    "            textos.append(sa_data[\"descricao\"])\n",
    "            for e_code, e_desc in sa_data[\"especialidades\"].items():\n",
    "                textos.append(e_desc)\n",
    "    return ' '.join(textos)\n",
    "\n",
    "# Preparar os textos para cada grande área\n",
    "textos_grandes_areas = [consolidar_textos(estrutura, ga) for ga in estrutura.keys()]\n",
    "\n",
    "# Inicializar TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajustar e transformar os textos das grandes áreas\n",
    "tfidf_matrix = vectorizer.fit_transform(textos_grandes_areas)\n",
    "\n",
    "# Obter palavras-chave para cada grande área\n",
    "palavras_chave_tfidf = obter_palavras_chave(tfidf_matrix, vectorizer)\n",
    "\n",
    "# Atualizar o dicionário de palavras-chave\n",
    "for i, ga in enumerate(estrutura.keys()):\n",
    "    palavras_chave_cnpq[ga] = [item[0] for item in palavras_chave_tfidf[i]]\n",
    "\n",
    "# Imprimir o dicionário atualizado de palavras-chave\n",
    "print(palavras_chave_cnpq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Carregar o modelo de linguagem portuguesa do spaCy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# Carregar stopwords em português\n",
    "nltk.download('stopwords')\n",
    "stop_words_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "def remover_stopwords(texto):\n",
    "    # Tokeniza o texto e remove stopwords\n",
    "    palavras = [token.text for token in nlp(texto) if token.text.lower() not in stop_words_pt]\n",
    "    return ' '.join(palavras)\n",
    "\n",
    "def consolidar_textos(estrutura, ga_code):\n",
    "    textos = []\n",
    "    for a_code, a_data in estrutura[ga_code][\"areas\"].items():\n",
    "        textos.append(remover_stopwords(a_data[\"descricao\"]))\n",
    "        for sa_code, sa_data in a_data[\"subareas\"].items():\n",
    "            textos.append(remover_stopwords(sa_data[\"descricao\"]))\n",
    "            for e_code, e_desc in sa_data[\"especialidades\"].items():\n",
    "                textos.append(remover_stopwords(e_desc))\n",
    "    return ' '.join(textos)\n",
    "\n",
    "# Preparar os textos para cada grande área\n",
    "textos_grandes_areas = [consolidar_textos(estrutura, ga) for ga in estrutura.keys()]\n",
    "\n",
    "# Inicializar TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajustar e transformar os textos das grandes áreas\n",
    "tfidf_matrix = vectorizer.fit_transform(textos_grandes_areas)\n",
    "\n",
    "# Obter palavras-chave para cada grande área\n",
    "palavras_chave_tfidf = obter_palavras_chave(tfidf_matrix, vectorizer)\n",
    "\n",
    "# Atualizar o dicionário de palavras-chave\n",
    "for i, ga in enumerate(estrutura.keys()):\n",
    "    palavras_chave_cnpq[ga] = [item[0] for item in palavras_chave_tfidf[i]]\n",
    "\n",
    "# Imprimir o dicionário atualizado de palavras-chave\n",
    "print(palavras_chave_cnpq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "for i,j in palavras_chave_cnpq.items():\n",
    "    print(i)\n",
    "    print(j)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import json\n",
    "# from cnpq_tree import CNPQtree\n",
    "\n",
    "# cnpq = CNPQtree(get_path_repo())\n",
    "# caminho = os.path.join(folder_data_input,'cnpq_tabela-areas-conhecimento.pdf')\n",
    "# df_areas = cnpq.extrair_areas(caminho)\n",
    "\n",
    "# def contar_marcadores(texto):\n",
    "#     padrao = r'\\.00'\n",
    "#     ocorrencias = re.findall(padrao, texto)\n",
    "#     return len(ocorrencias)\n",
    "\n",
    "# cat_grandeareas={}\n",
    "# cat_subareas={}\n",
    "# cat_areas={}\n",
    "# cat_especialidades={}\n",
    "\n",
    "# for cod,des in zip(df_areas['Codigo'],df_areas['Descricao']):\n",
    "#     k = contar_marcadores(cod)\n",
    "#     if k==3:\n",
    "#         cat_grandeareas[cod]= des\n",
    "#     elif k==2:\n",
    "#         cat_areas[cod]= des\n",
    "#     elif k==1:\n",
    "#         cat_subareas[cod]= des\n",
    "#     elif k==0:\n",
    "#         cat_especialidades[cod]= des\n",
    "#     else:\n",
    "#         print('Erro na separação')\n",
    "#         print(f'{k} {cod}{des}')\n",
    "\n",
    "# print(f'{len(cat_grandeareas):4} Grandes Áreas')\n",
    "# print(f'{len(cat_areas):4} Áreas')\n",
    "# print(f'{len(cat_subareas):4} Subáreas')\n",
    "# print(f'{len(cat_especialidades):4} Especialidades')\n",
    "\n",
    "# # Criar a estrutura hierárquica em json\n",
    "# estrutura = {}\n",
    "\n",
    "# # Montando as grandes áreas\n",
    "# for ga_code, ga_desc in cat_grandeareas.items():\n",
    "#     estrutura[ga_code] = {\"descricao\": ga_desc, \"areas\": {}}\n",
    "\n",
    "#     # Montando as áreas\n",
    "#     for a_code, a_desc in cat_areas.items():\n",
    "#         if a_code.startswith(ga_code.split('.')[0]):\n",
    "#             estrutura[ga_code][\"areas\"][a_code] = {\"descricao\": a_desc, \"subareas\": {}}\n",
    "\n",
    "#             # Montando as subáreas\n",
    "#             for sa_code, sa_desc in cat_subareas.items():\n",
    "#                 if sa_code.startswith(a_code.split('.')[0] + '.' + a_code.split('.')[1]):\n",
    "#                     estrutura[ga_code][\"areas\"][a_code][\"subareas\"][sa_code] = {\"descricao\": sa_desc, \"especialidades\": {}}\n",
    "\n",
    "#                     # Montando as especialidades\n",
    "#                     for e_code, e_desc in cat_especialidades.items():\n",
    "#                         if e_code.startswith(sa_code.split('.')[0] + '.' + sa_code.split('.')[1] + '.' + sa_code.split('.')[2]):\n",
    "#                             estrutura[ga_code][\"areas\"][a_code][\"subareas\"][sa_code][\"especialidades\"][e_code] = e_desc\n",
    "\n",
    "# # Convertendo para JSON\n",
    "# json_estrutura = json.dumps(estrutura, indent=4)\n",
    "\n",
    "# # Exibindo o JSON\n",
    "# # print(json_estrutura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_cnpq_structure():\n",
    "#     import re\n",
    "#     import json\n",
    "#     from cnpq_tree import CNPQtree\n",
    "\n",
    "#     cnpq = CNPQtree(get_path_repo())\n",
    "#     caminho = os.path.join(folder_data_input,'cnpq_tabela-areas-conhecimento.pdf')\n",
    "#     df_areas = cnpq.extrair_areas(caminho)\n",
    "\n",
    "#     def contar_marcadores(texto):\n",
    "#         padrao = r'\\.00'\n",
    "#         ocorrencias = re.findall(padrao, texto)\n",
    "#         return len(ocorrencias)\n",
    "\n",
    "#     cat_grandeareas={}\n",
    "#     cat_subareas={}\n",
    "#     cat_areas={}\n",
    "#     cat_especialidades={}\n",
    "\n",
    "#     for cod,des in zip(df_areas['Codigo'],df_areas['Descricao']):\n",
    "#         k = contar_marcadores(cod)\n",
    "#         if k==3:\n",
    "#             cat_grandeareas[cod]= des\n",
    "#         elif k==2:\n",
    "#             cat_areas[cod]= des\n",
    "#         elif k==1:\n",
    "#             cat_subareas[cod]= des\n",
    "#         elif k==0:\n",
    "#             cat_especialidades[cod]= des\n",
    "#         else:\n",
    "#             print('Erro na separação')\n",
    "#             print(f'{k} {cod}{des}')\n",
    "\n",
    "#     print(f'{len(cat_grandeareas):4} Grandes Áreas')\n",
    "#     print(f'{len(cat_areas):4} Áreas')\n",
    "#     print(f'{len(cat_subareas):4} Subáreas')\n",
    "#     print(f'{len(cat_especialidades):4} Especialidades')\n",
    "\n",
    "#     # Criar a estrutura hierárquica em json\n",
    "#     estrutura = {}\n",
    "\n",
    "#     # Montando as grandes áreas\n",
    "#     for ga_code, ga_desc in cat_grandeareas.items():\n",
    "#         estrutura[ga_code] = {\"descricao\": ga_desc, \"areas\": {}}\n",
    "\n",
    "#         # Montando as áreas\n",
    "#         for a_code, a_desc in cat_areas.items():\n",
    "#             if a_code.startswith(ga_code.split('.')[0]):\n",
    "#                 estrutura[ga_code][\"areas\"][a_code] = {\"descricao\": a_desc, \"subareas\": {}}\n",
    "\n",
    "#                 # Montando as subáreas\n",
    "#                 for sa_code, sa_desc in cat_subareas.items():\n",
    "#                     if sa_code.startswith(a_code.split('.')[0] + '.' + a_code.split('.')[1]):\n",
    "#                         estrutura[ga_code][\"areas\"][a_code][\"subareas\"][sa_code] = {\"descricao\": sa_desc, \"especialidades\": {}}\n",
    "\n",
    "#                         # Montando as especialidades\n",
    "#                         for e_code, e_desc in cat_especialidades.items():\n",
    "#                             if e_code.startswith(sa_code.split('.')[0] + '.' + sa_code.split('.')[1] + '.' + sa_code.split('.')[2]):\n",
    "#                                 estrutura[ga_code][\"areas\"][a_code][\"subareas\"][sa_code][\"especialidades\"][e_code] = e_desc\n",
    "\n",
    "#     # Convertendo para JSON\n",
    "#     json_estrutura = json.dumps(estrutura, indent=4)\n",
    "\n",
    "#     return json_estrutura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# def modelar_topicos_nmf(lista_titulos, n_components):\n",
    "#     from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#     from sklearn.decomposition import NMF\n",
    "\n",
    "#     # Combinar stopwords em inglês e português\n",
    "#     stop_words_pt = stopwords.words('portuguese')\n",
    "#     stop_words_en = stopwords.words('english')\n",
    "#     combined_stopwords = stop_words_pt + stop_words_en\n",
    "\n",
    "#     # Pré-processamento e vetorização com TF-IDF\n",
    "#     vectorizer = TfidfVectorizer(stop_words=combined_stopwords)\n",
    "#     X = vectorizer.fit_transform(lista_titulos)\n",
    "\n",
    "#     # Aplicar NMF para modelagem de tópicos\n",
    "#     nmf = NMF(n_components=n_components, random_state=42)\n",
    "#     W = nmf.fit_transform(X)  # W contém os pesos dos tópicos para os documentos\n",
    "\n",
    "#     # Exibir os tópicos e suas principais palavras\n",
    "#     words = vectorizer.get_feature_names_out()\n",
    "#     for i, topic in enumerate(nmf.components_):\n",
    "#         top_words_idx = topic.argsort()[-10:][::-1]\n",
    "#         top_words = [words[idx] for idx in top_words_idx]\n",
    "#         print(f\"Palavras-chave no Tópico {i+1}: {' '.join(top_words)}\")\n",
    "\n",
    "#     return nmf, W, vectorizer\n",
    "\n",
    "# def atribuir_grande_area_e_area(titulos, estrutura_cnpq, n_components):\n",
    "#     # Primeiro, modelar os tópicos dos títulos dos artigos e obter W\n",
    "#     classificacao_topicos, W = modelar_topicos_nmf(titulos, n_components)\n",
    "\n",
    "#     # Verificar se estrutura_cnpq é um dicionário válido\n",
    "#     if not isinstance(estrutura_cnpq, dict):\n",
    "#         raise ValueError(\"A estrutura CNPq deve ser um dicionário.\")\n",
    "\n",
    "#     # Vetorizar as descrições das áreas e subáreas do CNPq\n",
    "#     descricoes_areas = []\n",
    "#     for ga in estrutura_cnpq:\n",
    "#         if \"areas\" in estrutura_cnpq[ga] and isinstance(estrutura_cnpq[ga][\"areas\"], dict):\n",
    "#             descricao_area = \" \".join([a[\"descricao\"] for a in estrutura_cnpq[ga][\"areas\"].values() if \"descricao\" in a])\n",
    "#             descricoes_areas.append(descricao_area)\n",
    "#         else:\n",
    "#             raise ValueError(\"Estrutura de áreas inválida para a Grande Área:\", ga)\n",
    "\n",
    "#     X_areas = vectorizer.transform(descricoes_areas)\n",
    "\n",
    "#     # Calcular a similaridade entre os tópicos dos artigos e as áreas\n",
    "#     similaridades = cosine_similarity(W, X_areas)\n",
    "\n",
    "#     # Atribuir cada título à área mais próxima por afinidade\n",
    "#     atribuicoes = []\n",
    "#     for i, titulo in enumerate(titulos):\n",
    "#         indice_area_mais_proxima = similaridades[i].argmax()\n",
    "#         grande_area, area = list(estrutura_cnpq.keys())[indice_area_mais_proxima], list(estrutura_cnpq.values())[indice_area_mais_proxima][\"areas\"].keys()\n",
    "#         atribuicoes.append((titulo, grande_area, area))\n",
    "\n",
    "#     return atribuicoes\n",
    "\n",
    "# def atribuir_grande_area_e_area(titulos, estrutura_cnpq_json, n_components):\n",
    "#     # Converter a string JSON em um dicionário\n",
    "#     try:\n",
    "#         estrutura_cnpq = json.loads(estrutura_cnpq_json)\n",
    "#     except json.JSONDecodeError:\n",
    "#         raise ValueError(\"A string fornecida não é um JSON válido.\")\n",
    "\n",
    "#     # Verificar se estrutura_cnpq é um dicionário válido\n",
    "#     if not isinstance(estrutura_cnpq, dict):\n",
    "#         raise ValueError(\"A estrutura CNPq deve ser um dicionário.\")\n",
    "\n",
    "#     # Primeiro, modelar os tópicos dos títulos dos artigos e obter nmf, W e vectorizer\n",
    "#     nmf, W, vectorizer = modelar_topicos_nmf(titulos, n_components)\n",
    "\n",
    "#     # Vetorizar as descrições das áreas e subáreas do CNPq\n",
    "#     descricoes = []\n",
    "#     for ga in estrutura_cnpq.values():\n",
    "#         for area in ga[\"areas\"].values():\n",
    "#             descricao_area = area[\"descricao\"]\n",
    "#             for subarea in area[\"subareas\"].values():\n",
    "#                 descricao_area += \" \" + subarea[\"descricao\"]\n",
    "#             descricoes.append(descricao_area)\n",
    "\n",
    "#     X_descricoes = vectorizer.transform(descricoes)\n",
    "\n",
    "#     # Transformar os tópicos de volta ao espaço de palavras\n",
    "#     H = nmf.components_\n",
    "#     topic_space = W.dot(H)\n",
    "\n",
    "#     # Calcular a similaridade entre os tópicos dos artigos e as áreas\n",
    "#     similaridades = cosine_similarity(topic_space, X_descricoes)\n",
    "\n",
    "#     # Atribuir cada artigo à área e subárea mais próximas\n",
    "#     atribuicoes = []\n",
    "#     for i in range(len(titulos)):\n",
    "#         indice_mais_proximo = similaridades[i].argmax()\n",
    "#         total_areas = sum(len(ga[\"areas\"]) for ga in estrutura_cnpq.values())\n",
    "\n",
    "#         if indice_mais_proximo >= total_areas:\n",
    "#             raise IndexError(\"Índice de área calculado está fora do intervalo da estrutura do CNPq.\")\n",
    "\n",
    "#         # Calculando o índice para grande área e área\n",
    "#         for ga_index, ga in enumerate(estrutura_cnpq.values()):\n",
    "#             if indice_mais_proximo < len(ga[\"areas\"]):\n",
    "#                 grande_area = list(estrutura_cnpq.keys())[ga_index]\n",
    "#                 area = list(ga[\"areas\"].keys())[indice_mais_proximo]\n",
    "#                 break\n",
    "#             else:\n",
    "#                 indice_mais_proximo -= len(ga[\"areas\"])\n",
    "\n",
    "#         atribuicoes.append((titulos[i], grande_area, area))\n",
    "\n",
    "#     return atribuicoes\n",
    "\n",
    "# def criar_relatorio_html(atribuicoes):\n",
    "#     from collections import defaultdict\n",
    "#     import html\n",
    "\n",
    "#     # Organizar os títulos por Grande Área e Área\n",
    "#     organizacao = defaultdict(lambda: defaultdict(list))\n",
    "#     for titulo, grande_area, area in atribuicoes:\n",
    "#         organizacao[grande_area][area].append(titulo)\n",
    "\n",
    "#     # Iniciar a string HTML\n",
    "#     html_content = \"<html><head><title>Relatório de Publicações</title></head><body>\"\n",
    "#     html_content += \"<h1>Relatório de Publicações por Área</h1>\"\n",
    "\n",
    "#     for ga, areas in organizacao.items():\n",
    "#         html_content += f\"<h2>Grande Área: {html.escape(ga)}</h2>\"\n",
    "#         for a, titulos in areas.items():\n",
    "#             html_content += f\"<h3>Área: {html.escape(a)}</h3><ul>\"\n",
    "#             for titulo in titulos:\n",
    "#                 html_content += f\"<li>{html.escape(titulo)}</li>\"\n",
    "#             html_content += \"</ul>\"\n",
    "\n",
    "#     html_content += \"</body></html>\"\n",
    "\n",
    "#     return html_content\n",
    "\n",
    "# n_components = 5\n",
    "# atribuicoes = atribuir_grande_area_e_area(fioce_title_list, json_estrutura, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA)\n",
    "@article{blei2003latent,\n",
    "  title={Latent Dirichlet Allocation},\n",
    "  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},\n",
    "  journal={Journal of Machine Learning Research},\n",
    "  volume={3},\n",
    "  pages={993--1022},\n",
    "  year={2003},\n",
    "  publisher={JMLR.org},\n",
    "  url={https://jmlr.csail.mit.edu/papers/v3/blei03a.html}\n",
    "}\n",
    "\n",
    "Non-negative Matrix Factorization (NMF)\n",
    "@article{lee1999learning,\n",
    "  title={Learning the parts of objects by non-negative matrix factorization},\n",
    "  author={Lee, Daniel D and Seung, H. Sebastian},\n",
    "  journal={Nature},\n",
    "  volume={401},\n",
    "  pages={788--791},\n",
    "  year={1999},\n",
    "  publisher={Nature Publishing Group},\n",
    "  doi={10.1038/44565},\n",
    "  url={https://www.nature.com/articles/44565}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanessa de Arruda Jorge: vanessa.jorge@fiocruz.br\n",
    "\n",
    "Rafaela Grando: rafaela.grando@fiocruz.br\n",
    "\n",
    "VPPCB: <vppcb@fiocruz.br>\n",
    "\n",
    "Arca Dados Fiocruz | Documentos Operativos:\n",
    "https://arcadados.fiocruz.br/www/documentos_operativos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruções para problemas com instalação: Use linux ou WSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyodbc -U\n",
    "# !pip install transformers -U --user --trusted-host pypi.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se o kernel que você está usando não estiver usando o ambiente de instalação correto, você pode alterar o ambiente de instalação usando o seguinte comando:\n",
    "\n",
    "# !jupyter kernelspec install --user --name notebook  C:/ProgramData/Anaconda3/envs/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install plotly\n",
    "# %pip install numba --user --trusted-host pypi.org\n",
    "# %pip install ipywidgets --user --trusted-host pypi.org\n",
    "# %pip install plotly --user --trusted-host pypi.org\n",
    "# %pip install wordcloud --user --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pangoft2-1.0-0 --user --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://doc.courtbouillon.org/weasyprint/stable/first_steps.html#installation\n",
    "\n",
    "    Python ≥ 3.7.0\n",
    "    Pango ≥ 1.44.0\n",
    "    pydyf ≥ 0.6.0\n",
    "    CFFI ≥ 0.6\n",
    "    html5lib ≥ 1.1\n",
    "    tinycss2 ≥ 1.0.0\n",
    "    cssselect2 ≥ 0.1\n",
    "    Pyphen ≥ 0.9.1\n",
    "    Pillow ≥ 9.1.0\n",
    "    fontTools ≥ 4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "pkg_list = ['pango', 'pydyf', 'cffi', 'html5lib', 'tinycss2', 'cssselect2', 'pyphen', 'Pillow', 'fontTools']\n",
    "\n",
    "for package in pkg_list:\n",
    "    try:\n",
    "        print(f'{package}: {pkg_resources.get_distribution(package).version}')\n",
    "    except:\n",
    "        print(f\"Erro ao buscar {package}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pango --user --trusted-host pypi.org"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "O ambiente global do Python é o ambiente padrão do Python. Todos os pacotes que você instala no ambiente global estarão disponíveis para todos os scripts Python que você executar.\n",
    "\n",
    "O ambiente virtual do Jupyter Notebook é um espaço isolado onde você pode instalar pacotes sem afetar o ambiente global do Python. Isso é útil se você quiser trabalhar em um projeto que requer pacotes específicos que não estão instalados no ambiente global do Python.\n",
    "\n",
    "Ambiente virtual é espaço isolado para instalar pacotes sem afetar o ambiente global do Python.\n",
    "Kernel é o interpretador Python que o Jupyter Notebook usa para executar seus scripts. Na prática, o ambiente virtual é uma pasta na sua máquina local que contém uma cópia do Python e de todos os pacotes que você instalou os pacotes que compõem o ambiente. Essa pasta é isolada do ambiente global do Python, o que significa que os pacotes instalados no ambiente virtual não afetarão os pacotes instalados no ambiente global.\n",
    "\n",
    "Para instalar os pacotes de software que permitirão realizar as operações pode-se optar por duas formas, usar % e ! para instalar pacotes vale para qualquer sistema operacional, incluindo Windows, Linux e macOS. Também vale para qualquer ambiente Python, incluindo o Jupyter Notebook.\n",
    "\n",
    "A diferença entre os dois comandos é que o comando %pip install executa o pip no ambiente virtual do Jupyter Notebook, enquanto o comando !pip install executa o pip no ambiente global do Python.\n",
    "\n",
    "Exemplo:\n",
    "Para instalar o pacote pandas no ambiente virtual do Jupyter Notebook\n",
    "%pip install pandas\n",
    "\n",
    "Para instalar o pacote pandas no ambiente global do Python\n",
    "!pip install pandas\n",
    "\n",
    "Em geral, é melhor usar um kernel de um ambiente virtual do que trabalhar direto no ambiente global da máquina, principalmente se você estiver trabalhando em um projeto específico. Isso ajudará a garantir que seu projeto seja executado de forma consistente e sem problemas.\n",
    "'''\n",
    "\n",
    "# !pip3 install gensim\n",
    "# !pip3 install langdetect\n",
    "# !pip3 install pdfkit\n",
    "# !pip install weasyprint ## Deu errado em Windows\n",
    "# !pip3 install weasyprint>=61.0 ## Deu errado em Windows\n",
    "# !pip3 install --force-reinstall weasyprint[pangoft2] ## Deu errado em Windows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
