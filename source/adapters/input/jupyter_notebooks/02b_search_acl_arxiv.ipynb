{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACL Anthology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def buscar_artigo(query, base_dados):\n",
    "#     # TODO: integração com a API da base de dados\n",
    "#     # Por exemplo, usando requests.get() se a API for RESTful\n",
    "#     resposta = requests.get(f\"API_URL?query={query}&database={base_dados}\")\n",
    "    \n",
    "#     if resposta.status_code == 200:\n",
    "#         dados_artigo = resposta.json()  # Exemplo, depende do formato retornado pela API\n",
    "#         bib_entry = formatar_citacao_bib(dados_artigo)\n",
    "#         return bib_entry\n",
    "#     else:\n",
    "#         return \"Não foi possível encontrar o artigo.\"\n",
    "\n",
    "# def formatar_citacao_bib(dados_artigo):\n",
    "#     # Formatar os dados do artigo em uma entrada .bib\n",
    "#     # Isso é um exemplo, a estrutura real depende dos dados retornados pela API\n",
    "#     return f\"@InProceedings{{ {dados_artigo['id']},\\n\" \\\n",
    "#            f\"  title={{ {dados_artigo['title']} }},\\n\" \\\n",
    "#            f\"  author={{ {', '.join(dados_artigo['authors'])} }},\\n\" \\\n",
    "#            f\"  year={{ {dados_artigo['year']} }},\\n\" \\\n",
    "#            f\"  booktitle={{ {dados_artigo['booktitle']} }},\\n\" \\\n",
    "#            f\"  url_papwc={{ {dados_artigo.get('url_papwc', '')} }},\\n\" \\\n",
    "#            f\"  url_arxiv={{ {dados_artigo.get('url_arxiv', '')} }},\\n\" \\\n",
    "#            f\"}}\\n\\n\" \\\n",
    "#            f\"Abstract: \\n{dados_artigo['abstract']}\\n\"\n",
    "\n",
    "\n",
    "# # Exemplo de uso\n",
    "# # bib_entry = buscar_artigo(\"Exemplo de Query\", \"base_dados_exemplo\")\n",
    "# # print(bib_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "def contar_entradas_bib_no_arquivo_compactado(caminho_arquivo_bib_gz):\n",
    "    padrao_entrada_bib = re.compile(r'@[\\w]+{')\n",
    "    contador = 0\n",
    "\n",
    "    with gzip.open(caminho_arquivo_bib_gz, 'rt', encoding='utf-8') as arquivo:\n",
    "        for linha in arquivo:\n",
    "            if padrao_entrada_bib.match(linha):\n",
    "                contador += 1\n",
    "\n",
    "    return contador\n",
    "\n",
    "def extrair_e_buscar_no_bib(query, caminho_arquivo_bib):\n",
    "    with gzip.open(caminho_arquivo_bib, 'rt', encoding='utf-8') as arquivo:\n",
    "        conteudo = arquivo.read()\n",
    "\n",
    "        # Busca simplificada; pode ser necessário ajustar para ser mais específica\n",
    "        padrao = re.compile(r'@inproceedings{.*?'+ re.escape(query) + r'.*?}', re.DOTALL)\n",
    "        resultado = padrao.findall(conteudo)\n",
    "\n",
    "        return resultado\n",
    "\n",
    "def busca_simples(query, caminho_arquivo_bib_gz, ano_inicio=None, ano_fim=None):\n",
    "    padrao = re.compile(r'@[\\w]+\\{.*?\\}', re.DOTALL)\n",
    "    padrao_query = re.compile(re.escape(query), re.IGNORECASE)\n",
    "    padrao_ano = re.compile(r'year = \"(\\d{4})\"')\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    with gzip.open(caminho_arquivo_bib_gz, 'rt', encoding='utf-8') as arquivo:\n",
    "        conteudo = arquivo.read()\n",
    "        entradas = padrao.findall(conteudo)\n",
    "\n",
    "        for entrada in entradas:\n",
    "            if padrao_query.search(entrada):\n",
    "                ano_match = padrao_ano.search(entrada)\n",
    "                if ano_match:\n",
    "                    ano = int(ano_match.group(1))\n",
    "                    if (ano_inicio is None or ano >= ano_inicio) and (ano_fim is None or ano <= ano_fim):\n",
    "                        resultados.append(entrada)\n",
    "\n",
    "    return resultados\n",
    "\n",
    "def busca_avancada(query, caminho_arquivo_bib_gz, ano_inicio=None, ano_fim=None):\n",
    "    padrao = re.compile(r'@[\\w]+\\{.*?\\}', re.DOTALL)\n",
    "    padrao_ano = re.compile(r'year = \"(\\d{4})\"')\n",
    "\n",
    "    # Divide a query em termos e operadores\n",
    "    termos_operadores = re.split(r'\\s+(AND|OR|NOT)\\s+', query)\n",
    "    termos = [termos_operadores[i] for i in range(len(termos_operadores)) if i % 2 == 0]\n",
    "    operadores = [termos_operadores[i] for i in range(len(termos_operadores)) if i % 2 != 0]\n",
    "    \n",
    "    # Compila regex para cada termo\n",
    "    padroes_termos = [re.compile(re.escape(termo), re.IGNORECASE) for termo in termos]\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    with gzip.open(caminho_arquivo_bib_gz, 'rt', encoding='utf-8') as arquivo:\n",
    "        conteudo = arquivo.read()\n",
    "        entradas = padrao.findall(conteudo)\n",
    "\n",
    "        for entrada in entradas:\n",
    "            ano_match = padrao_ano.search(entrada)\n",
    "            ano = int(ano_match.group(1)) if ano_match else None\n",
    "            if ano_inicio and ano and ano < ano_inicio:\n",
    "                continue\n",
    "            if ano_fim and ano and ano > ano_fim:\n",
    "                continue\n",
    "\n",
    "            # Avalia se a entrada corresponde aos termos e operadores\n",
    "            match = True\n",
    "            for i, padrao_termo in enumerate(padroes_termos):\n",
    "                termo_presente = padrao_termo.search(entrada) is not None\n",
    "                if operadores and i < len(operadores):\n",
    "                    operador = operadores[i]\n",
    "                    if operador == \"AND\":\n",
    "                        match = match and termo_presente\n",
    "                    elif operador == \"OR\":\n",
    "                        match = match or termo_presente\n",
    "                    elif operador == \"NOT\":\n",
    "                        match = match and not termo_presente\n",
    "                else:\n",
    "                    match = match and termo_presente\n",
    "\n",
    "            if match:\n",
    "                resultados.append(entrada)\n",
    "\n",
    "    return resultados\n",
    "\n",
    "def listar_titulos(entradas_bib):\n",
    "    padrao_titulo = re.compile(r'title = {(.+?)}')\n",
    "    titulos = []\n",
    "\n",
    "    for entrada in entradas_bib:\n",
    "        titulo_match = padrao_titulo.search(entrada)\n",
    "        if titulo_match:\n",
    "            titulos.append(titulo_match.group(1))\n",
    "\n",
    "    return titulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89047 publicações no acervo da ACL Anthology\n"
     ]
    }
   ],
   "source": [
    "folder  = '../data/'\n",
    "acl_bib = 'anthology+abstracts.bib'\n",
    "acl_bib_gz = 'anthology+abstracts.bib.gz'\n",
    "\n",
    "total_entradas = contar_entradas_bib_no_arquivo_compactado(folder+acl_bib_gz)\n",
    "print(f\"{total_entradas} publicações no acervo da ACL Anthology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 publicações no acervo da ACL Anthology com termo 'Innovation' no período 1990-2024\n"
     ]
    }
   ],
   "source": [
    "termo_simples = \"Innovation\"\n",
    "ano_inicio = 1990\n",
    "ano_fim = 2024\n",
    "resultados_simples = busca_simples(termo_simples, folder+acl_bib_gz, ano_inicio, ano_fim)\n",
    "print(f\"{len(resultados_simples)} publicações no acervo da ACL Anthology com termo '{termo_simples}' no período {ano_inicio}-{ano_fim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# for resultado in resultados_simples:\n",
    "#     pprint(resultado, width=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 publicações no acervo da ACL Anthology com termo 'innovation AND model'\n"
     ]
    }
   ],
   "source": [
    "query_booleana = \"innovation AND model\"\n",
    "resultados = busca_simples(query_booleana, folder+acl_bib_gz, ano_inicio=1990, ano_fim=2024)\n",
    "print(f\"{len(resultados)} publicações no acervo da ACL Anthology com termo '{query_booleana}'\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. Joulin, P. Bojanowski, T. Mikolov, H. Jegou, E. Grave, Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion\n",
    "\n",
    "@InProceedings{joulin2018loss,\n",
    "  title={Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion},\n",
    "  author={Joulin, Armand and Bojanowski, Piotr and Mikolov, Tomas and Jégou, Hervé and Grave, Edouard},\n",
    "  year={2018},\n",
    "  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n",
    "  url_papwc={https://paperswithcode.com/paper/loss-in-translation-learning-bilingual-word},\n",
    "  url_arxiv={},\n",
    "\n",
    "}\n",
    "\n",
    "https://aclanthology.org/D18-1330/\n",
    "\n",
    "Abstract: \n",
    "Continuous word representations learned separately on distinct languages can be aligned so that their words become comparable in a common space. Existing works typically solve a quadratic problem to learn a orthogonal matrix aligning a bilingual lexicon, and use a retrieval criterion for inference. In this paper, we propose an unified formulation that directly optimizes a retrieval criterion in an end-to-end fashion. Our experiments on standard benchmarks show that our approach outperforms the state of the art on word translation, with the biggest improvements observed for distant language pairs such as English-Chinese.\n",
    "\n",
    "\n",
    "@article{bojanowski2017enriching,\n",
    "  title={Enriching Word Vectors with Subword Information},\n",
    "  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n",
    "  journal={Transactions of the Association for Computational Linguistics},\n",
    "  volume={5},\n",
    "  year={2017},\n",
    "  issn={2307-387X},\n",
    "  pages={135--146}\n",
    "}\n",
    "\n",
    "https://aclanthology.org/Q17-1010/\n",
    "\n",
    "Abstract\n",
    "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-print Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subject_areas(url):\n",
    "    # Enviar uma solicitação GET para a página\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Dicionário para armazenar as áreas de assunto\n",
    "    subject_areas_dict = {}\n",
    "\n",
    "    # Verifique se a solicitação foi bem-sucedida\n",
    "    if response.status_code == 200:\n",
    "        # Analise o conteúdo da página com BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Encontre o elemento div que contém as áreas de assunto\n",
    "        subject_areas_div = soup.find('div', class_='item-list highwire-views-col-list highwire-views-cols-3 clearfix')\n",
    "\n",
    "        # Verifique se o elemento foi encontrado\n",
    "        if subject_areas_div:\n",
    "            # Encontre todas as tags 'a' dentro do div\n",
    "            subject_links = subject_areas_div.find_all('a')\n",
    "            \n",
    "            # Itere sobre as tags 'a' para extrair as áreas de assunto e seus URLs\n",
    "            for link in subject_links:\n",
    "                area_name = link.text.strip()\n",
    "                area_url = link['href']\n",
    "                subject_areas_dict[area_name] = area_url\n",
    "\n",
    "        # Exiba o dicionário resultante\n",
    "        # print(subject_areas_dict)\n",
    "\n",
    "    return subject_areas_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bioRxiv = 'https://www.biorxiv.org/'\n",
    "areas_dict = scrape_subject_areas(url_bioRxiv)\n",
    "print(f'{len(areas_dict)} coleções de áreas')\n",
    "\n",
    "if areas_dict:\n",
    "    for area, area_url in areas_dict.items():\n",
    "        print(f'   Área: {area:40} URL: {area_url}')\n",
    "else:\n",
    "    print('Seção de áreas de assunto não encontrada.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_content_summary_statistics(interval='m', format_='json'):\n",
    "    # Construa a URL do endpoint\n",
    "    url = f'https://api.biorxiv.org/sum/{interval}/{format_}'\n",
    "    \n",
    "    # Faça a solicitação GET\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Verifique se a solicitação foi bem-sucedida\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Erro ao buscar estatísticas de resumo de conteúdo. Código de status: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def fetch_usage_summary_statistics(interval='m', format_='json'):\n",
    "    # Construa a URL do endpoint\n",
    "    url = f'https://api.biorxiv.org/usage/{interval}/{format_}'\n",
    "    \n",
    "    # Faça a solicitação GET\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Verifique se a solicitação foi bem-sucedida\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Erro ao buscar estatísticas de uso. Código de status: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso das funções\n",
    "content_summary_data = fetch_content_summary_statistics()\n",
    "usage_summary_data   = fetch_usage_summary_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exibir os resultados\n",
    "# print(\"Estatísticas de Resumo de Conteúdo:\")\n",
    "# pprint(content_summary_data, width=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Estatísticas de Uso:\")\n",
    "# pprint(usage_summary_data, width=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_usage_statistics(data, width=1800, height=800):\n",
    "    # Extrair os valores relevantes dos dados\n",
    "    months = [entry['month'] for entry in data['bioRxiv content statistics']]\n",
    "    abstract_cumulative = [np.round(int(entry['abstract_cumulative'])/1000000, 2) for entry in data['bioRxiv content statistics']]\n",
    "    abstract_views = [np.round(int(entry['abstract_views'])/1000000, 2) for entry in data['bioRxiv content statistics']]\n",
    "\n",
    "    # Criar um DataFrame com os dados\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({'Mês': months, 'Resumo Cumulativo': abstract_cumulative, 'Visualizações de Resumo': abstract_views})\n",
    "\n",
    "    # Criar um gráfico com dois eixos y\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    \n",
    "    # Adicionar barras para 'Resumo Cumulativo'\n",
    "    fig.add_trace(go.Bar(x=df['Mês'], y=df['Resumo Cumulativo'], name='Resumo Cumulativo', text=df['Resumo Cumulativo'], textposition='outside'))\n",
    "\n",
    "    # Adicionar linhas para 'Visualizações de Resumo'\n",
    "    fig.add_trace(go.Scatter(x=df['Mês'], y=df['Visualizações de Resumo'], mode='lines+markers+text', name='Visualizações de Resumo', textposition='top center', yaxis='y2'))\n",
    "\n",
    "    # Configurar eixos\n",
    "    max_value_primary = max(df['Resumo Cumulativo'].max()+30, df['Visualizações de Resumo'].max())\n",
    "    ratio = 50  # Proporção desejada\n",
    "    max_value_secondary = max_value_primary / ratio\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Estatísticas de uso do bioRxiv',\n",
    "        xaxis_title='Mês',\n",
    "        yaxis_title='Milhões de resumos acumulados',\n",
    "        yaxis2_title='Milhões de visualizações por mês',\n",
    "        legend=dict(x=0.02, y=0.96),\n",
    "        width=width,\n",
    "        height=height,\n",
    "        yaxis=dict(range=[0, max_value_primary]),\n",
    "        yaxis2=dict(range=[0, max_value_secondary])\n",
    "    )\n",
    "\n",
    "    # Configurar o tamanho da fonte para os rótulos de dados\n",
    "    fig.update_traces(\n",
    "        textfont=dict(size=24),  # Tamanho da fonte para os rótulos de dados\n",
    "        selector=dict(type='bar', name='Resumo Cumulativo')  # Seletor para as barras\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        textfont=dict(size=40),  # Tamanho da fonte para os rótulos de dados\n",
    "        selector=dict(type='scatter', name='Visualizações de Resumo')  # Seletor para as linhas\n",
    "    )\n",
    "\n",
    "    # Exibir o gráfico\n",
    "    fig.show()\n",
    "\n",
    "def plot_content_statistics(content_data, width=1800, height=800):\n",
    "    # Extrair os valores relevantes dos dados\n",
    "    months = [entry['month'] for entry in content_data['bioRxiv content statistics']]\n",
    "    new_papers = [entry['new_papers'] for entry in content_data['bioRxiv content statistics']]\n",
    "    revised_papers = [entry['revised_papers'] for entry in content_data['bioRxiv content statistics']]\n",
    "    new_papers_cumulative = [entry['new_papers_cumulative'] for entry in content_data['bioRxiv content statistics']]\n",
    "    revised_papers_cumulative = [entry['revised_papers_cumulative'] for entry in content_data['bioRxiv content statistics']]\n",
    "\n",
    "    # Criar um DataFrame com os dados\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({'Mês': months,\n",
    "                        'Artigos novos no mês': new_papers, \n",
    "                        'Artigos revisados no mês':revised_papers, \n",
    "                        'Total de Artigos acumulado':new_papers_cumulative , \n",
    "                        'Total de Artigos revisados':revised_papers_cumulative})\n",
    "\n",
    "    # Criar um gráfico com dois eixos y\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    \n",
    "    # Adicionar barras para total acumulado de artigos\n",
    "    fig.add_trace(go.Bar(x=df['Mês'], y=df['Total de Artigos acumulado'], name='Total de Artigos acumulado', text=df['Total de Artigos acumulado'], textposition='outside'))\n",
    "    fig.add_trace(go.Bar(x=df['Mês'], y=df['Total de Artigos revisados'], name='Total de Artigos revisados', text=df['Total de Artigos revisados'], textposition='outside'))\n",
    "\n",
    "    # Configurar eixo principal\n",
    "    fig.update_yaxes(title_text='Quantidade de Artigos', secondary_y=False)\n",
    "\n",
    "    # Adicionar linhas para versões acumuladas no eixo secundário\n",
    "    fig.add_trace(go.Scatter(x=df['Mês'], y=df['Artigos novos no mês'], mode='lines+markers+text', name='Artigos novos no mês', textposition='top center', yaxis='y2'))\n",
    "    fig.add_trace(go.Scatter(x=df['Mês'], y=df['Artigos revisados no mês'], mode='lines+markers+text', name='Artigos revisados no mês', textposition='top center', yaxis='y2'))\n",
    "\n",
    "    # Configurar eixos\n",
    "    max_value_primary = max(df['Total de Artigos acumulado'].max()+30, df['Total de Artigos revisados'].max())\n",
    "    ratio = 50  # Proporção desejada\n",
    "    max_value_secondary = max_value_primary / ratio\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Estatísticas de conteúdo no bioRxiv',\n",
    "        xaxis_title='Mês',\n",
    "        yaxis_title='Quantidade acumulada de artigos',\n",
    "        yaxis2_title='Quantidades de artigos em cada mês',\n",
    "        legend=dict(x=0.02, y=0.96),\n",
    "        width=width,\n",
    "        height=height,\n",
    "        yaxis=dict(range=[0, max_value_primary]),\n",
    "        yaxis2=dict(range=[0, max_value_secondary])\n",
    "    )\n",
    "\n",
    "    # Configurar o tamanho da fonte para os rótulos de dados\n",
    "    fig.update_traces(\n",
    "        textfont=dict(size=24),\n",
    "        selector=dict(type='bar', name='Quantidade de novos papers')  # Seletor para as barras\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        textfont=dict(size=40),\n",
    "        selector=dict(type='scatter', name='Quantidade de papers revisados')  # Seletor para as linhas\n",
    "    )\n",
    "    \n",
    "    # fig.update_yaxes(overlaying='y2', side='right')\n",
    "\n",
    "    # Exibir o gráfico\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_content_statistics(content_summary_data)\n",
    "plot_usage_statistics(usage_summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matplotlib\n",
    "# def plot_usage_statistics_simple(data):\n",
    "#     # Extrair os valores relevantes dos dados\n",
    "#     months = [entry['month'] for entry in data['bioRxiv content statistics']]\n",
    "#     abstract_views = [int(entry['abstract_views']) for entry in data['bioRxiv content statistics']]\n",
    "#     pdf_downloads = [int(entry['pdf_downloads']) for entry in data['bioRxiv content statistics']]\n",
    "\n",
    "#     # Criar um gráfico de linha\n",
    "#     plt.figure(figsize=(18, 9))\n",
    "#     plt.plot(months, abstract_views, label='Visualizações de Resumo de Conteúdo', marker='o', linestyle='-', color='b')\n",
    "#     plt.plot(months, pdf_downloads, label='Downloads de PDF', marker='s', linestyle='--', color='r')\n",
    "\n",
    "#     # Configurar rótulos e título\n",
    "#     plt.xlabel('Mês')\n",
    "#     plt.ylabel('Contagem')\n",
    "#     plt.title('Estatísticas de Uso do bioRxiv')\n",
    "#     plt.xticks(rotation=45, fontsize=8)  # Rotação dos rótulos do eixo x para melhor legibilidade\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     # Adicionar legenda\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Exibir o gráfico\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Exemplo de uso com os dados fornecidos\n",
    "# usage_data = fetch_usage_summary_statistics()\n",
    "\n",
    "# plot_usage_statistics(usage_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
