{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <center><center><img src=\"https://raw.githubusercontent.com/makaires77/fioce/master/assets/logo_fioce.png\" \n",
    "style=\"height:150px\" alt=\"Logo_Fiocruz\"></center></center> -->\n",
    "\n",
    "<center><center><img src=\"https://user-images.githubusercontent.com/61051085/81343928-3ce9d500-908c-11ea-9850-0210b4e94ba0.jpg\" \n",
    "style=\"height:150px\" alt=\"Logo_Unifor\"></center></center>\n",
    "\n",
    "# Levantamento sistemático da Literatura\n",
    "### Buscas integradas por palavras-chave com Protocolo PRISMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detalhamentos sobre as fonte de dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principais Editoras e Agregador em Open Access, escolhidas pelo nível das APIs e disponibilização de conteúdo aberto\n",
    "\n",
    "Bases Indexadas de Editoras\n",
    "- API IEEE Xplore\n",
    "- API Springer\n",
    "\n",
    "Agregadores de outras fontes\n",
    "- API CORE UK\n",
    "- API CrossRef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORE Principal API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CORE</b> é um agregador e o nome de uma coleção de 280 milhões de artigos de pesquisa, sendo mais de 37 milhões com textos completos disponibilizados. Em quesito de Open Access é o agregador mais abrangente que coleta dados institucionais, repositórios de assuntos e pré-impressões, bem como periódicos de acesso aberto nível ouro e híbrido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpringerNature API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <b>Springer Nature</b> é uma editora científica líder global de livros e periódicos, que oferece conteúdo de qualidade por meio de produtos e serviços de informação inovadores. Publica cerca de 500 revistas acadêmicas e profissionais da sociedade. A Springer Nature tem operações em cerca de 20 países na Europa, nos EUA e na Ásia, e mais de 13.000 funcionários. No setor de ciência, tecnologia e medicina (STM), o grupo publica cerca de 3.000 periódicos e 13.000 novos livros por ano, bem como a maior coleção de e-books STM do mundo. \n",
    "\n",
    "Springer Nature oferta APIs para que os desenvolvedores com conteúdo disponível gratuitamente para uso não comercial, com saídas incluindo XML e JSON.:\n",
    "\n",
    "- <b>API Springer Metadata</b> Fornece metadados para mais de 13 milhões de documentos on-line (por exemplo, artigos de periódicos, capítulos de livros, protocolos).\n",
    "Springer Meta API - Fornece metadados com novas versões para mais de 13 milhões de documentos on-line (por exemplo, artigos de periódicos, capítulos de livros, protocolos).\n",
    "\n",
    "- <b>API Springer Open Access</b> Fornece metadados e conteúdo de texto completo, quando disponível, para mais de 649.000 artigos de acesso aberto de periódicos BioMed Central e SpringerOpen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operações RESTful\n",
    "\n",
    "A API Springer Nature oferece suporte a diversas operações e formatos de retorno diferentes. Eles podem ser controlados emitindo solicitações RESTful para o serviço de API em http://api.springernature.com</span>.\n",
    "\n",
    "As solicitações sempre devem incluir uma Coleção e um ResultFormat. A Collection identifica qual repositório deve ser pesquisado para obter resultados e o ResultFormat indica como os dados devem ser retornados pela API. Por exemplo, nesta solicitação:\n",
    "\n",
    "http://api.springernature.com/metadata/pam/doi/10.1007/s11276-008-0131-4?api_key=yourKeyHere\n",
    "\n",
    "Um elemento (metadata) no caminho designa a Collection e outro (pam) o formato desejado para o resultado, o ResultFormat. Neste caso, o repositório de metadados está sendo pesquisado e os metadados resultantes devem ser retornados em formato pam. Os valores permitidos para esses parâmetros estão listados na tabela abaixo:\n",
    "\n",
    "#### Parâmetros válidos de coleção e ResultFormat\n",
    "\n",
    "Nos exemplos abaixo, a terceira parte do caminho URI doi é um método abreviado de solicitação de metadados para um único artigo. É equivalente a fazer uma consulta para um único DOI usando o parâmetro q, por exemplo,\n",
    "\n",
    "http://api.springernature.com/meta/v2/jats?q=doi:10.1007/s11276-008-0131-4&api_key=yourKeyHere\n",
    "\n",
    "http://api.springernature.com/metadata/pam?q=doi:10.1007/s11276-008-0131-4&api_key=yourKeyHere\n",
    "\n",
    "O padrão JATS foi adicionado recentemente como formato de dados para meta/v2 (além de pam e json). \"jats\" é usado no caminho da solicitação para JATS (artigos de periódicos) e BITS (capítulos de livros).\n",
    "\n",
    "Existem diversas APIs que agora podem retornar resultados de \"metadata\":\n",
    "\n",
    "    metadata: Metadados XML originais e gerais usando o padrão PAM (formato Prism Aggregate) quando \"pam\" o formato é escolhido; usa namespaces e elementos prisma e dc (dublincore). Também disponível em json. Em uso, mas corrigido (ou seja, nenhuma alteração será feita).\n",
    "\n",
    "    meta/v2: Baseado na API de metadados. Possui padrão PAM para \"pam\" formato, mas elementos/campos adicionais adicionados ao longo do tempo. API fluida na qual alterações podem ser feitas. Disponível em pam, json e agora jats.\n",
    "\n",
    "Fonte: https://dev.springernature.com/restfuloperations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEEEXplore API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <b>IEEE</b> dispõe atualmente de 6.173.186 artefatos em sua biblioteca digital, entre os tipos:\n",
    "\n",
    "- Conferências (4.454.635)\n",
    "\n",
    "- Jornals (1.399.945)\n",
    "\n",
    "- Revistas (209.873)\n",
    "\n",
    "- Livros (68.877)\n",
    "\n",
    "- Artigos de acesso antecipado (24.775)\n",
    "\n",
    "- Padrões (14.466)\n",
    "\n",
    "- Cursos (591)\n",
    "\n",
    "Para o seguimento de Publicações em Periódicos são atualmente 410 Journals indexados, com publicadores como, além da própria IEEE, MIT Press, IBM, Ericsson, Nokia Bell Labs, dentre outros, cobrindo tópicos como: \n",
    "- Computação e Processamento\n",
    "- Componentes, circuitos, dispositivos e sistemas\n",
    "- Tecnologias de comunicação, rede e transmissão\n",
    "- Processamento e análise de sinais\n",
    "- Aplicações de energia, energia e indústria\n",
    "- Tópicos gerais para engenheiros\n",
    "- Campos, ondas e eletromagnetismo\n",
    "- Robótica e sistemas de controle\n",
    "- Transporte\n",
    "- Materiais projetados, dielétricos e plasmas\n",
    "- Aeroespacial\n",
    "- Bioengenharia\n",
    "- Profissão de engenharia\n",
    "- Geociências\n",
    "- Fotônica e Eletroóptica\n",
    "- Engenharia Nuclear\n",
    "\n",
    "A IEEE disponibiliza APIs de pesquisa nas quais é possível consultar e recuperar registros de metadados, incluindo resumos de mais de 5 milhões de documentos no IEEE Xplore®, incluindo periódicos, anais de conferências, livros, cursos e padrões. A API de metadados suporta pesquisas simples e booleanas.\n",
    "\n",
    "- <b>API de acesso aberto IEEE</b>: Consulta e recupera artigos de texto completo designados como Acesso Aberto, bem como Texto Completo pago. Permite consultar até 25 números DOI (Digital Object Identifier) e recuperar registros de metadados, incluindo resumos.\n",
    "\n",
    "- <b>API de acesso de texto completo IEEE</b>: API de identificador de objeto digital (DOI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A IEEE demora um ou dois dias úteis para habilitar a API Key\n",
    "- Antes de habilitada a API key as requisições dão erro de acesso 403\n",
    "- Chamada para iniciar o processo de busca e confirmação de execução para IEEE Xplore\n",
    "- researcher_ieee.confirm_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Informações da API IEEEXplore\n",
    "Content Discovery and Indexing:\n",
    "    For organizations looking to extract and index IEEE metadata into their internal search tools.\n",
    "    \n",
    "Open Access Articles:\n",
    "    For organizations looking to review full-text articles designated as Open Access.\n",
    "    \n",
    "Text and Data Mining (TDM):\n",
    "TDM is permitted for non-commercial research purposes only and requires an active IEEE Xplore institutional subscription. Please contact us for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montado query dinâmica da documentação IEEE no browser\n",
    "\n",
    "https://ieeexploreapi.ieee.org/api/v1/search/articles?apikey=n7z9zut2vzf68nmkack8p3yu&format=json&max_records=100&start_record=1&sort_order=asc&sort_field=article_number&article_title=graph+OR+ontology+AND+%22innovation+model%22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação do ambiente e classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Atualizar gerenciador de pacotes Pip pelo Terminal:\n",
    "# !python.exe -m pip install --upgrade pip\n",
    "\n",
    "## Em Linux\n",
    "# !pip3 install --upgrade pip\n",
    "# sudo apt-get update\n",
    "# sudo apt-get upgrade\n",
    "\n",
    "## Atualizar Chromedriver pelo Terminal:\n",
    "# !/usr/local/bin/chromedriver --version\n",
    "# !curl -s https://chromedriver.storage.googleapis.com/LATEST_RELEASE\n",
    "# sudo apt-get install libcurl4-openssl-dev\n",
    "# sudo apt-get install graphviz graphviz-dev\n",
    "\n",
    "## Instalar todos pacotes listados no arquivo requirements.txt do direto raiz:\n",
    "# %pip install -r ../../../../requirements.txt --user --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org\n",
    "\n",
    "## Instalar um a um os pacotes auxiliares faltantes no ambiente:\n",
    "# %pip install pycurl\n",
    "# %pip install pygraphviz\n",
    "\n",
    "## Instalar ou atualizar alguns pacotes:\n",
    "# !pip3 install --upgrade notebook jupyterlab ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd, os, re, sys, time, json, subprocess\n",
    "\n",
    "## Configurar exibição do pandas para melhor visualizar os dados\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "def find_repo_root(path='.', depth=10):\n",
    "    ''' \n",
    "    Busca o arquivo .git e retorna string com a pasta raiz do repositório\n",
    "    '''\n",
    "    # Prevent infinite recursion by limiting depth\n",
    "    if depth < 0:\n",
    "        return None\n",
    "    path = Path(path).absolute()\n",
    "    if (path / '.git').is_dir():\n",
    "        return path\n",
    "    return find_repo_root(path.parent, depth-1)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "## Definir a pasta de base do repositório local\n",
    "base_repo_dir = find_repo_root()\n",
    "\n",
    "## Sempre construir os caminhos usando os.path.join para compatibilidade WxL\n",
    "folder_utils = os.path.join(base_repo_dir, 'utils')\n",
    "folder_domain = os.path.join(base_repo_dir, 'source', 'domain')\n",
    "folder_data_input = os.path.join(base_repo_dir, 'data', 'input')\n",
    "folder_data_output = os.path.join(base_repo_dir, 'data', 'output')\n",
    "\n",
    "## Adicionar pastas locais ao sys.path para importar pacotes criados localmente\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "from scraper_pasteur import PasteurScraper\n",
    "from environment_setup import EnvironmentSetup\n",
    "from scraper_sucupira import SucupiraScraper\n",
    "from scraper_sucupira_edge import SucupiraScraperEdge\n",
    "from chromedriver_manager import ChromeDriverManager\n",
    "from lattes_scrapper import JSONFileManager, LattesScraper, HTMLParser, SoupParser, GetQualis, ArticlesCounter, DictToHDF5, attribute_to_be_non_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versões 123 Chrome e 123 Chromedriver estão compatíveis\n"
     ]
    }
   ],
   "source": [
    "# Cria instância da classe ChromeDriverManager e verifica compatibilidade entre versões do Chrome e Chromedriver\n",
    "actualizer = ChromeDriverManager()\n",
    "actualizer.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe para IEEEXplore API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ieee_sdk import XPLORE\n",
    "import sys, time, math, json, pycurl\n",
    "import certifi, pathlib, requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "\n",
    "class XPLORE:\n",
    "    # Endpoint default da API (maioria dos casos de queries que buscam artigos)\n",
    "    endPoint = \"https://ieeexploreapi.ieee.org/api/v1/search/articles\"\n",
    "\n",
    "    # Documento de acesso aberto, documento de texto completo, endpoint de citações\n",
    "    openAccessEndPoint = \"https://ieeexploreapi.ieee.org/api/v1/search/document/\"\n",
    "\n",
    "    # Citações e informações sobre autores\n",
    "    bioEndPoint = \"https://ieeexploreapi.ieee.org/api/v1/author/\"\n",
    "\n",
    "    # Requisições de utilização\n",
    "    usageEndPoint = \"https://ieeexploreapi.ieee.org/api/v1/user/\"\n",
    "\n",
    "    # requisição com token\n",
    "    authTokenEndPoint = \"https://ieeexploreapi.ieee.org/api/v1/auth/token\"\n",
    "\n",
    "    def __init__(self, apiKey):\n",
    "    \t# API key\n",
    "        self.apiKey = apiKey\n",
    "\n",
    "        # auth token\n",
    "        self.authToken = ''\n",
    "\n",
    "    \t# flag that some search criteria has been provided\n",
    "        self.queryProvided = False\n",
    "\n",
    "        # flag that chargeable Full Text request is occurring\n",
    "        self.requestingFullText = False\n",
    "\n",
    "        # flag that chargeable Paper Cites/Author Bio request is occurring\n",
    "        self.requestingBio = False\n",
    "\n",
    "        # flag that usage reque## Currently Supported IEEE Xplore API Use Casesst is occurring\n",
    "        self.requestingUsage = False\n",
    "\n",
    "        # flag for Open Access, which changes endpoint in use and limits results to just Open Access\n",
    "        self.usingOpenAccess = False\n",
    "\n",
    "        # flag that article number has been provided, which overrides all other search criteria\n",
    "        self.usingArticleNumber = False\n",
    "\n",
    "        # flag that a boolean method is in use\n",
    "        self.usingBoolean = False\n",
    "\n",
    "        # flag that a facet is in use\n",
    "        self.usingFacet = False\n",
    "\n",
    "        # flag that a facet has been applied, in the event that multiple facets are passed\n",
    "        self.facetApplied = False\n",
    "\n",
    "        # flag for Citations, which changes endpoint in use\n",
    "        self.citationLookup = False\n",
    "\n",
    "        # data type for results; default is json (other option is xml)\n",
    "        self.outputType = 'json'\n",
    "\n",
    "        # data format for results; default is raw (returned string); other option is object\n",
    "        self.outputDataFormat = 'raw'\n",
    "\n",
    "        # default of 25 results returned\n",
    "        self.resultSetMax = 25\n",
    "\n",
    "        # maximum of 200 results returned\n",
    "        self.resultSetMaxCap = 200\n",
    "\n",
    "        # records returned default to position 1 in result set\n",
    "        self.startRecord = 1\n",
    "\n",
    "        # default sort order is ascending; could also be 'desc' for descending\n",
    "        self.sortOrder = 'asc'\n",
    "\n",
    "        # field name that is being used for sorting\n",
    "        self.sortField = 'article_title'\n",
    "\n",
    "        # array of permitted search fields for searchField() method\n",
    "        self.allowedSearchFields = ['abstract', 'affiliation', 'article_number', 'article_title', 'author', 'boolean_text', 'content_type', 'd-au', 'd-pubtype', 'd-publisher', 'd-year', 'doi', 'end_year', 'facet', 'index_terms', 'isbn', 'issn', 'is_number', 'meta_data', 'open_access', 'publication_number', 'publication_title', 'publication_year', 'publisher', 'querytext', 'start_year', 'thesaurus_terms', 'start_date', 'end_date']\n",
    "\n",
    "        # dictionary of all search parameters in use and their values\n",
    "        self.parameters = {}\n",
    "\n",
    "        # dictionary of all filters in use and their values\n",
    "        self.filters = {}\n",
    "\n",
    "\n",
    "    # ensuring == can be used reliably\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.__dict__ == other.__dict__\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    # ensuring != can be used reliably\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "\n",
    "    # set the data type for the API output\n",
    "    # string outputType   Format for the returned result (JSON, XML)\n",
    "    # return void\n",
    "    def dataType(self, outputType):\n",
    "\n",
    "        outputType = outputType.strip().lower()\n",
    "        self.outputType = outputType\n",
    "\n",
    "\n",
    "    # set the data format for the API output\n",
    "    # string outputDataFormat   Data structure for the returned result (raw string or object)\n",
    "    # return void\n",
    "    def dataFormat(self, outputDataFormat):\n",
    "\n",
    "        outputDataFormat = outputDataFormat.strip().lower()\n",
    "        self.outputDataFormat = outputDataFormat\n",
    "\n",
    "\n",
    "    # set the start position in the returned data\n",
    "    # string start   Start position in the returned data\n",
    "    # return void\n",
    "    def startingResult(self, start):\n",
    "\n",
    "        self.startRecord = math.ceil(start) if (start > 0) else 1\n",
    "\n",
    "\n",
    "    # set the maximum number of results\n",
    "    # string maximum   Max number of results to return\n",
    "    # return void\n",
    "    def maximumResults(self, maximum):\n",
    "\n",
    "        self.resultSetMax = math.ceil(maximum) if (maximum > 0) else 25\n",
    "        if self.resultSetMax > self.resultSetMaxCap:\n",
    "            self.resultSetMax = self.resultSetMaxCap\n",
    "\n",
    "\n",
    "    # setting a filter on results\n",
    "    # string filterParam   Field used for filtering\n",
    "    # string value         Text to filter on\n",
    "    # return void\n",
    "    def resultsFilter(self, filterParam, value):\n",
    "\n",
    "        filterParam = filterParam.strip().lower()\n",
    "        value = value.strip()\n",
    "\n",
    "        if len(value) > 0:\n",
    "            self.filters[filterParam] = value\n",
    "            self.queryProvided = True\n",
    "\n",
    "            # Standards do not have article titles, so switch to sorting by article number\n",
    "            if (filterParam == 'content_type' and value == 'Standards'):\n",
    "                self.resultsSorting('publication_year', 'asc')\n",
    "\n",
    "\n",
    "    # setting sort order for results\n",
    "    # string field   Data field used for sorting\n",
    "    # string order   Sort order for results (ascending or descending)\n",
    "    # return void\n",
    "    def resultsSorting(self, field, order):\n",
    "\n",
    "        field = field.strip().lower()\n",
    "        order = order.strip()\n",
    "        self.sortField = field\n",
    "        self.sortOrder = order\n",
    "\n",
    "\n",
    "    # shortcut method for assigning search parameters and values\n",
    "    # string field   Field used for searching\n",
    "    # string value   Text to query\n",
    "    # return void\n",
    "    def searchField(self, field, value):\n",
    "\n",
    "        field = field.strip().lower()\n",
    "        if field in self.allowedSearchFields:\n",
    "            self.addParameter(field, value)\n",
    "        else:\n",
    "            print(\"Searches against field \" + field + \" are not supported\")\n",
    "\n",
    "\n",
    "    # string value   Abstract text to query\n",
    "    # return void\n",
    "    def abstractText(self, value):\n",
    "\n",
    "        self.addParameter('abstract', value)\n",
    "\n",
    "\n",
    "    # string value   Affiliation text to query\n",
    "    # return void\n",
    "    def affiliationText(self, value):\n",
    "\n",
    "        self.addParameter('affiliation', value)\n",
    "\n",
    "\n",
    "    # string value   Article number to query\n",
    "    # return void\n",
    "    def articleNumber(self, value):\n",
    "\n",
    "        self.addParameter('article_number', value)\n",
    "\n",
    "\n",
    "    # string value   Article title to query\n",
    "    # return void\n",
    "    def articleTitle(self, value):\n",
    "\n",
    "        self.addParameter('article_title', value)\n",
    "\n",
    "\n",
    "    # string value   Author to query\n",
    "    # return void\n",
    "    def authorText(self, value):\n",
    "\n",
    "        self.addParameter('author', value)\n",
    "\n",
    "\n",
    "    # string value   Author Facet text to query\n",
    "    # return void\n",
    "    def authorFacetText(self, value):\n",
    "\n",
    "        self.addParameter('d-au', value)\n",
    "\n",
    "\n",
    "    # string value   Value(s) to use in the boolean query\n",
    "    # return void\n",
    "    def booleanText(self, value):\n",
    "\n",
    "        self.addParameter('boolean_text', value)\n",
    "\n",
    "\n",
    "    # string value   Content Type Facet text to query\n",
    "    # return void\n",
    "    def citationType(self, value):\n",
    "\n",
    "        value = value.strip()\n",
    "        value = value.replace(' ', '')\n",
    "        value = value.replace('-', '_')\n",
    "\n",
    "        self.addParameter('citation_type', value)\n",
    "\n",
    "\n",
    "    # string value   Content Type Facet text to query\n",
    "    # return void\n",
    "    def contentTypeFacetText(self, value):\n",
    "\n",
    "        self.addParameter('d-pubtype', value)\n",
    "\n",
    "\n",
    "    # string value   Customer ID for usage query\n",
    "    # return void\n",
    "    def customerID(self, value):\n",
    "\n",
    "        self.addParameter('customer_id', value)\n",
    "\n",
    "\n",
    "    # string value   DOI (Digital Object Identifier) to query\n",
    "    # return void\n",
    "    def doi(self, value):\n",
    "\n",
    "        self.addParameter('doi', value)\n",
    "\n",
    "\n",
    "    # string value   Facet text to query\n",
    "    # return void\n",
    "    def facetText(self, value):\n",
    "\n",
    "        self.addParameter('facet', value)\n",
    "\n",
    "\n",
    "    # string value   Author Keywords, IEEE Terms, and Mesh Terms to query\n",
    "    # return void\n",
    "    def indexTerms(self, value):\n",
    "\n",
    "        self.addParameter('index_terms', value)\n",
    "\n",
    "\n",
    "    # string value   Start date (YYYYMMDD format) of publication insertion\n",
    "    # return void\n",
    "    def insertionStartDate(self, value):\n",
    "\n",
    "        self.addParameter('start_date', value)\n",
    "\n",
    "\n",
    "    # string value   End date (YYYYMMDD format) of publication insertion\n",
    "    # return void\n",
    "    def insertionEndDate(self, value):\n",
    "\n",
    "        self.addParameter('end_date', value)\n",
    "\n",
    "\n",
    "    # string value   ISBN (International Standard Book Number) to query\n",
    "    # return void\n",
    "    def isbn(self, value):\n",
    "\n",
    "        self.addParameter('isbn', value)\n",
    "\n",
    "\n",
    "    # string value   ISSN (International Standard Serial number) to query\n",
    "    # return void\n",
    "    def issn(self, value):\n",
    "\n",
    "        self.addParameter('issn', value)\n",
    "\n",
    "\n",
    "    # string value   Issue number to query\n",
    "    # return void\n",
    "    def issueNumber(self, value):\n",
    "\n",
    "        self.addParameter('is_number', value)\n",
    "\n",
    "\n",
    "    # string value   Text to query across metadata fields and the abstract\n",
    "    # return void\n",
    "    def metaDataText(self, value):\n",
    "\n",
    "        self.addParameter('meta_data', value)\n",
    "\n",
    "\n",
    "    # string value   Publication Facet text to query\n",
    "    # return void\n",
    "    def publicationFacetText(self, value):\n",
    "\n",
    "        self.addParameter('d-year', value)\n",
    "\n",
    "\n",
    "    # string value   Publisher Facet text to query\n",
    "    # return void\n",
    "    def publisherFacetText(self, value):\n",
    "\n",
    "        self.addParameter('d-publisher', value)\n",
    "\n",
    "\n",
    "    # string value   Publication title to query\n",
    "    # return void\n",
    "    def publicationTitle(self, value):\n",
    "\n",
    "        self.addParameter('publication_title', value)\n",
    "\n",
    "\n",
    "    # string or number value   Publication year to query\n",
    "    # return void\n",
    "    def publicationYear(self, value):\n",
    "\n",
    "        self.addParameter('publication_year', value)\n",
    "\n",
    "\n",
    "    # string value   Text to query across metadata fields, abstract and document text\n",
    "    # return void\n",
    "    def queryText(self, value):\n",
    "\n",
    "        self.addParameter('querytext', value)\n",
    "\n",
    "\n",
    "    # string value   Thesaurus terms (IEEE Terms) to query\n",
    "    # return void\n",
    "    def thesaurusTerms(self, value):\n",
    "\n",
    "        self.addParameter('thesaurus_terms', value)\n",
    "\n",
    "\n",
    "    # add query parameter\n",
    "    # string parameter   Data field to query\n",
    "    # string value       Text to use in query\n",
    "    # return void\n",
    "    def addParameter(self, parameter, value):\n",
    "      \n",
    "        value = value.strip()\n",
    "\n",
    "        if (len(value) > 0):\n",
    "\n",
    "            self.parameters[parameter]= value\n",
    "        \n",
    "            # viable query criteria provided\n",
    "            self.queryProvided = True\n",
    "\n",
    "            # set flags based on parameter\n",
    "            if (parameter == 'article_number'):\n",
    "\n",
    "                self.usingArticleNumber = True\n",
    "\n",
    "            if (parameter == 'boolean_text'):\n",
    "\n",
    "                self.usingBoolean = True\n",
    "\n",
    "            if (parameter == 'facet' or parameter == 'd-au' or parameter == 'd-year' or parameter == 'd-pubtype' or parameter == 'd-publisher'):\n",
    "\n",
    "                self.usingFacet = True\n",
    "\n",
    "    \n",
    "    # Open Access document\n",
    "    # string article   Article number to query\n",
    "    # return void\n",
    "    def openAccess(self, article):\n",
    "      \n",
    "        self.usingOpenAccess = True\n",
    "        self.queryProvided = True\n",
    "        self.articleNumber(article)\n",
    "\n",
    "\n",
    "    # Citations query\n",
    "    # string article    Article number to query\n",
    "    # string citeType   Citation type\n",
    "    # return void\n",
    "    def citations(self, article='0', citeType='ieee'):\n",
    "      \n",
    "        self.citationLookup = True\n",
    "        self.articleNumber(article)\n",
    "        self.citationType(citeType)\n",
    "\n",
    "\n",
    "    # Full Text token request\n",
    "    # string token   Authorization token for Full Text request\n",
    "    # return void\n",
    "    def setAuthToken(self, token):\n",
    "\n",
    "        self.authToken = token.strip()\n",
    "\n",
    "\n",
    "    # Full Text article request\n",
    "    # string article   Article number to query\n",
    "    # return void\n",
    "    def fullTextRequest(self, article):\n",
    "      \n",
    "        self.requestingFullText = True\n",
    "        self.queryProvided = True\n",
    "        self.articleNumber(article)\n",
    "\n",
    "\n",
    "    # Paper Cites / Author Bio request\n",
    "    # string author   Author ID to query\n",
    "    # return void\n",
    "    def authorBio(self, author):\n",
    "      \n",
    "        self.requestingBio = True\n",
    "        self.queryProvided = True\n",
    "        self.addParameter('author_number', author)\n",
    "\n",
    "\n",
    "    # Usage request\n",
    "    # string startingDate   Usage start date in M-D-YYYY format\n",
    "    # string endingDate     Usage end date in M-D-YYYY format\n",
    "    # return void\n",
    "    def usageRequest(self, startingDate='', endingDate=''):\n",
    "      \n",
    "        self.addParameter('usage_start_date', startingDate)\n",
    "        self.addParameter('usage_end_date', endingDate)\n",
    "        self.requestingUsage = True\n",
    "        self.queryProvided = True\n",
    "\n",
    "\n",
    "    # Checking for token expiration response\n",
    "    # string response             Response from API\n",
    "    # return boolean tokenValid   Whether token remains valid\n",
    "    def checkForTokenExpiration(self, response):\n",
    "\n",
    "        tokenValid = True\n",
    "        errorXML = '<ApiResponse><error>Token Expired</error></ApiResponse>'\n",
    "        errorJSON = '{\"error\":\"Token Expired\"}'\n",
    "        if response == errorXML or response == errorJSON: \n",
    "            tokenValid = False\n",
    "\n",
    "        return tokenValid\n",
    "\n",
    "\n",
    "    # calls the API\n",
    "    # string debugMode  If this mode is on (True) then output query and not data\n",
    "    # return either raw result string, XML or JSON object, or array\n",
    "    def callAPI(self, debugModeOff=True):\n",
    "\n",
    "        if self.requestingFullText is True:\n",
    "\n",
    "            apiQry = self.buildFullTextRequestQuery()\n",
    "\n",
    "        elif self.requestingBio is True:\n",
    "\n",
    "            apiQry = self.buildBioRequestQuery()\n",
    "\n",
    "        elif self.requestingUsage is True:\n",
    "\n",
    "            apiQry = self.buildUsageRequestQuery()\n",
    "        \n",
    "        elif self.usingOpenAccess is True:\n",
    "\n",
    "            apiQry = self.buildOpenAccessQuery()\n",
    "\n",
    "        elif self.citationLookup is True:\n",
    "\n",
    "            apiQry = self.buildCitationsQuery()\n",
    "\n",
    "        else:\n",
    "\n",
    "            apiQry = self.buildQuery()\n",
    "\n",
    "        if debugModeOff is False:\n",
    "        \n",
    "            return apiQry\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            if self.queryProvided is False:\n",
    "                print(\"No search criteria provided\")\n",
    "        \n",
    "            data = self.queryAPI(apiQry)\n",
    "\n",
    "        # does API response indicate an expired token?\n",
    "        if self.requestingFullText is True or self.requestingUsage is True:\n",
    "                \n",
    "            tokenValid = self.checkForTokenExpiration(data)\n",
    "                \n",
    "            # request new auth token\n",
    "            if tokenValid is False:\n",
    "                if self.requestingFullText is True:\n",
    "                    apiQry = self.buildFullTextRequestQuery(True)\n",
    "                elif self.requestingUsage is True:\n",
    "                   apiQry = self.buildUsageRequestQuery(True)\n",
    "                data = self.queryAPI(apiQry)\n",
    "            \n",
    "            formattedData = self.formatData(data)\n",
    "\n",
    "        else:\n",
    "            formattedData = self.formatData(data)            \n",
    "            \n",
    "        return formattedData\n",
    "\n",
    "\n",
    "    # creates the URL for the Open Access Document API call\n",
    "    # return string: full URL for querying the API\n",
    "    def buildOpenAccessQuery(self):\n",
    "\n",
    "        url = self.openAccessEndPoint;\n",
    "        url += str(self.parameters['article_number']) + '/fulltext'\n",
    "        url += '?apikey=' + str(self.apiKey)\n",
    "        url += '&format=' + str(self.outputType)\n",
    "\n",
    "        return url\n",
    "\n",
    "\n",
    "    # creates the URL for the Citations API call\n",
    "    # return string: full URL for querying the API\n",
    "    def buildCitationsQuery(self):\n",
    "\n",
    "        url = self.openAccessEndPoint;\n",
    "        url += str(self.parameters['article_number']) + '/citation'\n",
    "        url += '?apikey=' + str(self.apiKey)\n",
    "        url += '&format=' + str(self.outputType)\n",
    "        url += '&type=' + str(self.parameters['citation_type'])\n",
    "        url += '&max_records=' + str(self.resultSetMax)\n",
    "        url += '&start_record=' + str(self.startRecord)\n",
    "\n",
    "        return url\n",
    "\n",
    "\n",
    "    # boolean refresh            Whether to force a new token retrieval\n",
    "    # return string tokenValue   Token for requesting full text article or usage\n",
    "    def retrieveAuthToken(self, refresh=False):\n",
    "\n",
    "        # authentication token from user must be provided\n",
    "        if not self.authToken:\n",
    "            print(\"Authorization token not provided\")\n",
    "        \n",
    "        else:\n",
    "            # file name for persistence\n",
    "            fileName = str(self.apiKey) + '_token.txt'\n",
    "\n",
    "            # whether a new full text token needs to be requested\n",
    "            requestNewToken = False\n",
    "\n",
    "            # check for existing full text token\n",
    "            file = pathlib.Path(fileName)\n",
    "\n",
    "            # current time as timestamp\n",
    "            currentTime = time.time()\n",
    "\n",
    "            # no text file saved or cannot be accessed\n",
    "            if file.exists() is False:\n",
    "                requestNewToken = True\n",
    "\n",
    "            else:\n",
    "                storedValue = file.read_text()\n",
    "                storedValuesArr = storedValue.split('--////--')\n",
    "                tokenValue = storedValuesArr[0]\n",
    "                expiresAt = storedValuesArr[1]\n",
    "\n",
    "                if storedValue is False or not tokenValue or float(expiresAt) <= float(currentTime):\n",
    "                    requestNewToken = True\n",
    "\n",
    "            # request a new full text token\n",
    "            if requestNewToken is True or refresh is True:\n",
    "                data = self.getAuthTokenFromEndpoint()\n",
    "                obj = json.loads(data)\n",
    "                if 'token' not in obj:\n",
    "                    print(\"Token cannot be retrieved\")\n",
    "                else:\n",
    "                    tokenValue = obj['token']\n",
    "                    expiresAt = currentTime + 600\n",
    "                    valueToSave = str(tokenValue) + '--////--' + str(expiresAt)\n",
    "                    file.write_text(valueToSave)\n",
    "        \n",
    "            return tokenValue\n",
    "\n",
    "\n",
    "    # creates the URL for the non-Open Access Document API call\n",
    "    # return string: full URL for querying the API\n",
    "    def buildQuery(self):\n",
    "\n",
    "        url = self.endPoint;\n",
    "\n",
    "        url += '?apikey=' + str(self.apiKey)\n",
    "        url += '&format=' + str(self.outputType)\n",
    "        url += '&max_records=' + str(self.resultSetMax)\n",
    "        url += '&start_record=' + str(self.startRecord)\n",
    "        url += '&sort_order=' + str(self.sortOrder)\n",
    "        url += '&sort_field=' + str(self.sortField)\n",
    "\n",
    "        # add in search criteria\n",
    "        # article number query takes priority over all others\n",
    "        if (self.usingArticleNumber):\n",
    "\n",
    "            url += '&article_number=' + str(self.parameters['article_number'])\n",
    "\n",
    "        # boolean query\n",
    "        elif (self.usingBoolean):\n",
    "\n",
    "             url += '&querytext=(' + urllib.parse.quote_plus(self.parameters['boolean_text']) + ')'\n",
    "\n",
    "        else:\n",
    "\n",
    "            for key in self.parameters:\n",
    "\n",
    "                if (self.usingFacet and self.facetApplied is False):\n",
    "\n",
    "                    url += '&querytext=' + urllib.parse.quote_plus(self.parameters[key]) + '&facet=' + key\n",
    "                    self.facetApplied = True\n",
    "\n",
    "                else:\n",
    "\n",
    "                    url += '&' + key + '=' + urllib.parse.quote_plus(self.parameters[key])\n",
    "\n",
    "\n",
    "        # add in filters\n",
    "        for key in self.filters:\n",
    "\n",
    "            url += '&' + key + '=' + str(self.filters[key])\n",
    " \n",
    "        return url\n",
    "\n",
    "    # creates the URL for the API call\n",
    "    # string url  Full URL to pass to API\n",
    "    # return string: Results from API\n",
    "    def queryAPI(self, url):\n",
    "\n",
    "        buffer_obj = BytesIO()\n",
    "        qry_obj = pycurl.Curl()\n",
    "        qry_obj.setopt(qry_obj.URL, url)\n",
    "        qry_obj.setopt(qry_obj.WRITEDATA, buffer_obj)\n",
    "        qry_obj.setopt(qry_obj.CAINFO, certifi.where())\n",
    "        qry_obj.perform()\n",
    "        qry_obj.close()\n",
    "        response = buffer_obj.getvalue()\n",
    "        return response.decode('utf-8')\n",
    "\n",
    "    # request chargeable full text token\n",
    "    # return string: Full text token from API\n",
    "    def getAuthTokenFromEndpoint(self):\n",
    "\n",
    "        url = str(self.authTokenEndPoint)\n",
    "        post = { 'auth-token': self.authToken, 'apikey': self.apiKey }\n",
    "        post = urllib.parse.urlencode(post)\n",
    "\n",
    "        buffer_obj = BytesIO()\n",
    "        qry_obj = pycurl.Curl()\n",
    "        qry_obj.setopt(qry_obj.URL, url)\n",
    "        qry_obj.setopt(qry_obj.WRITEDATA, buffer_obj)\n",
    "        qry_obj.setopt(qry_obj.CAINFO, certifi.where())\n",
    "        qry_obj.setopt(qry_obj.POST, 1)\n",
    "        qry_obj.setopt(qry_obj.POSTFIELDS, post)\n",
    "        qry_obj.perform()\n",
    "        qry_obj.close()\n",
    "        response = buffer_obj.getvalue()\n",
    "        return response.decode('utf-8')\n",
    "\n",
    "    # creates the URL for the chargeable full text API call\n",
    "    # boolean refresh  Whether to force a new token retrieval\n",
    "    # return url: Full URL for querying the API\n",
    "    def buildFullTextRequestQuery(self, refresh=False):\n",
    "\n",
    "        clToken = self.retrieveAuthToken(refresh)\n",
    "\n",
    "        url = self.openAccessEndPoint;\n",
    "        url += self.parameters['article_number'] + '/fulltext'\n",
    "        url += '?apikey=' + str(self.apiKey)\n",
    "        url += '&format=' + str(self.outputType)\n",
    "        url += '&cltoken=' + str(clToken);\n",
    "\n",
    "        return url\n",
    "\n",
    "    # creates the URL for the chargeable paper cites / author Bio API call\n",
    "    # return url: Full URL for querying the API\n",
    "    def buildBioRequestQuery(self):\n",
    "\n",
    "        url = self.bioEndPoint;\n",
    "        url += self.parameters['author_number']\n",
    "        url += '?apikey=' + str(self.apiKey)\n",
    "        url += '&format=' + str(self.outputType)\n",
    "\n",
    "        return url\n",
    "    \n",
    "    # creates the URL for the usage API call\n",
    "    # boolean refresh  Whether to force a new token retrieval\n",
    "    # return url: Full URL for querying the API\n",
    "    def buildUsageRequestQuery(self, refresh=False):\n",
    "\n",
    "        clToken = self.retrieveAuthToken(refresh)\n",
    "\n",
    "        url = self.usageEndPoint;\n",
    "        url += self.parameters['customer_id']\n",
    "        url += '/samlreport'\n",
    "        url += '?apikey=' + str(self.apiKey)\n",
    "        url += '&includeTerms=true'\n",
    "        url += '&startDate=' + self.parameters['usage_start_date']\n",
    "        url += '&endDate=' + self.parameters['usage_end_date']\n",
    "        url += '&cltoken=' + str(clToken);\n",
    "\n",
    "        return url\n",
    "\n",
    "    # formats the data returned by the API\n",
    "    # string data    Result string from API\n",
    "    def formatData(self, data):\n",
    "\n",
    "        if self.outputDataFormat == 'raw':\n",
    "            return data\n",
    "\n",
    "        elif self.outputDataFormat == 'object':\n",
    "            \n",
    "            if self.outputType == 'xml':\n",
    "                obj = ET.ElementTree(ET.fromstring(data))\n",
    "                return obj\n",
    "\n",
    "            else:\n",
    "                obj = json.loads(data) \n",
    "                return obj\n",
    "\n",
    "        else:\n",
    "            return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe para CORE API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "class CoreClient:\n",
    "    \"\"\"\n",
    "    Client to perform API calls to CORE open access search engine (core.ac.uk).\n",
    "\n",
    "    Navigate between millions of scientific articles and ressources\n",
    "    provided by thousand of data provider through open science infrastructures.\n",
    "\n",
    "    You must register to get an API key : https://core.ac.uk/services/api\n",
    "\n",
    "    API documentation : https://api.core.ac.uk/docs/v3\n",
    "    \"\"\"\n",
    "    ENDPOINT = \"https://api.core.ac.uk/v3/\"\n",
    "\n",
    "    def __init__(self, api_key=None):\n",
    "        self.core_api_key = self.ler_apikeys('core')\n",
    "        self.headers = {\"Authorization\": f\"Bearer {api_key}\", \"User-Agent\": \"Mozilla/5.0 (Linux; U; Android 4.2.2; he-il; NEO-X5-116A Build/JDQ39) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Safari/534.30\"}\n",
    "\n",
    "    def ler_apikeys(self, api):\n",
    "        # Obter caminho absoluto para a pasta home do usuário\n",
    "        home_dir = os.path.expanduser(\"~\")\n",
    "\n",
    "        # Criar caminho completo para o arquivo secrets.json\n",
    "        secrets_file_path = os.path.join(home_dir, \"secrets.json\")\n",
    "\n",
    "        # Verificar se o arquivo existe\n",
    "        if os.path.exists(secrets_file_path):\n",
    "            # Abra o arquivo secrets.json para leitura\n",
    "            with open(secrets_file_path, 'r') as secrets_file:\n",
    "                secrets = json.load(secrets_file)\n",
    "            try:\n",
    "                # Acessar as chaves de API\n",
    "                api_key = secrets[api+\"_api_key\"]\n",
    "                return api_key\n",
    "            except:\n",
    "                print(f\"Chave para API '{api}' não cadastrada em secrets.json\")\n",
    "                return None   \n",
    "        else:\n",
    "            print(\"O arquivo secrets.json não foi encontrado na pasta home.\")\n",
    "\n",
    "    def query_api(self, url_fragment, query,limit=100):\n",
    "        api_endpoint = \"https://api.core.ac.uk/v3/\"\n",
    "        encoded_query=urllib.parse.quote(query)\n",
    "        headers = {\"Authorization\": f\"Bearer [{self.core_api_key}]\", \"User-Agent\": \"Mozilla/5.0 (Linux; U; Android 4.2.2; he-il; NEO-X5-116A Build/JDQ39) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Safari/534.30\"}\n",
    "        \n",
    "        response = requests.get(f\"{api_endpoint}{url_fragment}?api_key={self.core_api_key}&q={encoded_query}&limit={limit}\", headers=headers)\n",
    "        \n",
    "        # Verifica se a requisição foi bem-sucedida (código de status 200)\n",
    "        if response.status_code == 200:\n",
    "            # Extrai os cabeçalhos personalizados da resposta\n",
    "            remaining_limit = response.headers.get(\"X-RateLimitRemaining\")\n",
    "            retry_after = response.headers.get(\"X-RateLimit-Retry-After\")\n",
    "            overall_limit = response.headers.get(\"X-RateLimit-Limit\")\n",
    "\n",
    "            # Exibe as informações obtidas\n",
    "            print(f\"Limites Restantes: {remaining_limit}\")\n",
    "            print(f\"Tempo até a Próxima Tentativa: {retry_after} segundos\")\n",
    "            print(f\"Limite Total: {overall_limit}\")\n",
    "\n",
    "        else:\n",
    "            # Exibe uma mensagem de erro caso a requisição não seja bem-sucedida\n",
    "            print(f\"Erro na requisição. Código de status: {response.status_code}\")\n",
    "            print(f\"Error code {response.status_code}, {response.content}\")\n",
    "            print(f\"Error response {response.keys()}\")\n",
    "            remaining_limit = response.headers.get(\"X-RateLimitRemaining\")\n",
    "            retry_after = response.headers.get(\"X-RateLimit-Retry-After\")\n",
    "            overall_limit = response.headers.get(\"X-RateLimit-Limit\")\n",
    "\n",
    "            # Exibe as informações obtidas\n",
    "            print(f\"Limites Restantes: {remaining_limit}\")\n",
    "            print(f\"Tempo até a Próxima Tentativa: {retry_after} segundos\")\n",
    "            print(f\"Limite Total: {overall_limit}\")            \n",
    "\n",
    "    def core_find(self, query=\"\", entity=\"works\", recent=False, limit=10, offset=0,\n",
    "            title=None, abstract=None, doi=None, oai=None, issn=None, fields=[], types=[],\n",
    "            year_min=None, year_max=None):\n",
    "        \"\"\"\n",
    "        Find open science ressources through CORE API similarly to the\n",
    "        web interface.\n",
    "\n",
    "        Search any CORE entities like Works, Journals or Data Providers\n",
    "        by DOI, ISSN, fields, types... or through a custom query to include\n",
    "        anything.\n",
    "\n",
    "        Custom method to facilitate research based on the API method search.\n",
    "        \"\"\"\n",
    "        #Prepare all parameters to build a single query\n",
    "        query_parameters = []\n",
    "\n",
    "        if query:\n",
    "            query_parameters.append(query)\n",
    "\n",
    "        #Generate parameters for suggested search field of the web interface\n",
    "        for search_field in [\"title\", \"abstract\", \"doi\", \"oai\", \"issn\"]:\n",
    "            field_value = locals()[search_field]\n",
    "            if field_value:\n",
    "                query_parameters.append(f'{search_field}:\"{field_value}\"')\n",
    "\n",
    "        #Format requested fields of study and document types (research, thesis, etc...)\n",
    "        if fields:\n",
    "            query_parameters.append(self._format_field(\"fieldsOfStudy\", fields))\n",
    "        if types:\n",
    "            query_parameters.append(self._format_field(\"documentType\", types))\n",
    "\n",
    "        #Specify date range for the selection\n",
    "        if year_min or year_max:\n",
    "            year_min = year_min if year_min is not None else 1789\n",
    "            year_max = year_max if year_max is not None else datetime.now().year\n",
    "            query_parameters.append(f\"(yearPublished>={year_min} AND \" + \\\n",
    "                    f\"yearPublished<={year_max})\")\n",
    "\n",
    "        #Define sort by publication date to retrieve recent ressources\n",
    "        sort_field = \"publishedDate:desc\" if recent else None\n",
    "\n",
    "        #Format the final query string and perform the API search request\n",
    "        formatted_query = \" AND \".join(query_parameters)\n",
    "        search_output = self.core_search(query=formatted_query, entity=entity, \\\n",
    "                limit=limit, offset=offset, sort=sort_field)\n",
    "\n",
    "        return search_output.get(\"results\")\n",
    "\n",
    "    def core_search(self, query, entity=\"works\", limit=100, offset=0, sort=None):\n",
    "        \"\"\"\n",
    "        Search method of the API to find research, journals, data providers and all CORE API entities.\n",
    "\n",
    "        API search documentation : https://api.core.ac.uk/docs/v3#tag/Search\n",
    "\n",
    "        entity: Type of the entity\n",
    "            \"works\" \"outputs\" \"data-providers\" \"journals\"\n",
    "        \"\"\"\n",
    "        # search_url = self.ENDPOINT+f\"search/{entity}\"+f\"?api_key={self.core_api_key}\"\n",
    "        search_url = os.path.join(self.ENDPOINT,f\"search/{entity}\")\n",
    "        parameters = {\n",
    "                \"limit\" : limit,\n",
    "                \"offset\" : offset,\n",
    "                }\n",
    "        if sort:\n",
    "            parameters[\"sort\"] = sort\n",
    "        \n",
    "        encoded_query = urllib.parse.quote(query)\n",
    "        encoded_url = search_url+f\"?q=\"+encoded_query\n",
    "        # print(f\"DEBUG: search encoded_url: {encoded_url}\")\n",
    "        \n",
    "        # Headers incluindo api_key para acesso\n",
    "        api_call = requests.get(encoded_url, \n",
    "                                 headers=self.headers, \n",
    "                                 data=json.dumps(parameters))\n",
    "\n",
    "        if api_call.status_code != 200:\n",
    "            # Exibe uma mensagem de erro caso a requisição não seja bem-sucedida\n",
    "            print(f\"Erro requisição CORE API com código {api_call.status_code} {api_call.content.decode()}\")  \n",
    "            return {}\n",
    "        # else:\n",
    "        #     # Exibe dados dos limites da API para requisições com sucesso:\n",
    "        #     remaining_limit = api_call.headers.get(\"X-RateLimitRemaining\")\n",
    "        #     retry_after = api_call.headers.get(\"X-RateLimit-Retry-After\")\n",
    "        #     overall_limit = api_call.headers.get(\"X-RateLimit-Limit\")\n",
    "\n",
    "        #     # Imprimir dados de requisições restante para os limites da API\n",
    "        #     if overall_limit:\n",
    "        #         # Exibe as informações obtidas\n",
    "        #         print(f\"\\nLimite Restante: {remaining_limit}\")\n",
    "        #         print(f\" Limites Totais: {overall_limit}\")            \n",
    "        #         print(f\" Tempo restante: {retry_after} segundos\") \n",
    "            \n",
    "        return api_call.json()\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_field(field_name, field_values):\n",
    "        formatted_fields = [f'{field_name}:\"{value}\"' for value in field_values]\n",
    "        return \"(\\'\" + \" OR \".join(formatted_fields) + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def core_search(query, entity=\"outputs\", limit=10, offset=0, sort=None):\n",
    "#     \"\"\"\n",
    "#     Search method of the API (using POST) to find research, journals,\n",
    "#     data providers and all CORE API entities.\n",
    "\n",
    "#     API search documentation : https://api.core.ac.uk/docs/v3#tag/Search\n",
    "\n",
    "#     entity: Type of the entity\n",
    "#         \"works\" \"outputs\" \"data-providers\" \"journals\"\n",
    "        \n",
    "#     \"\"\"\n",
    "#     from datetime import datetime, timedelta\n",
    "#     import pytz, numpy as np\n",
    "\n",
    "#     def minutes_to_reset_limits(retry_after):\n",
    "#         actual_time = datetime.now()\n",
    "        \n",
    "#         # Converte a string de retry_after para um objeto timedelta\n",
    "#         retry_after_timedelta = timedelta(seconds=retry_after)\n",
    "        \n",
    "#         # Calcula a diferença em minutos e arredonda\n",
    "#         minutes_from_now = round(retry_after_timedelta.total_seconds() / 60)\n",
    "#         return minutes_from_now\n",
    "\n",
    "#     # def minutes_to_reset_limits(date_string):\n",
    "#     #     # Para converter para o fuso horário de Brasília (UTC-3)\n",
    "#     #     brasilia_timezone = pytz.timezone('America/Sao_Paulo')\n",
    "#     #     actual_time = datetime.now().astimezone(brasilia_timezone)\n",
    "#     #     print(f\"Horário atual em Brasília: {actual_time}\")\n",
    "\n",
    "#     #     date_format = \"%Y-%m-%dT%H:%M:%S%z\"\n",
    "#     #     converted_datetime = datetime.strptime(date_string, date_format)\n",
    "#     #     print(f\" Limites API CORE renovam: {converted_datetime}\")\n",
    "\n",
    "#     #     minutes_from_now = (converted_datetime - actual_time)//60\n",
    "#     #     print(f\"Falta {minutes_from_now} minutos para renovar limites da API\")\n",
    "#     #     return minutes_from_now\n",
    "\n",
    "#     headers = {\"User-Agent\": \"Mozilla/5.0 (Linux; U; Android 4.2.2; he-il; NEO-X5-116A Build/JDQ39) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Safari/534.30\"}\n",
    "\n",
    "#     coreapi = CoreClient()\n",
    "#     core_api_key = coreapi.ler_apikeys('core')\n",
    "#     ENDPOINT_CORE_SEARCH = \"https://api.core.ac.uk/v3/search/\"\n",
    "    \n",
    "#     # search_url = os.path.join(ENDPOINT_CORE_SEARCH,f\"{entity}?api_key={core_api_key}\")\n",
    "#     search_url = os.path.join(ENDPOINT_CORE_SEARCH,f\"{entity}\")\n",
    "    \n",
    "#     parameters = {\n",
    "#             \"limit\" : limit,\n",
    "#             \"offset\" : offset,\n",
    "#             }\n",
    "#     if sort:\n",
    "#         parameters[\"sort\"] = sort\n",
    "    \n",
    "#     encoded_query = urllib.parse.quote(query)\n",
    "    \n",
    "#     encoded_url = search_url+f\"?q=\"+encoded_query\n",
    "#     print(f\"DEBUG: search encoded_url: {encoded_url}\")\n",
    "    \n",
    "#     api_call = requests.get(encoded_url, \n",
    "#                             headers=headers, \n",
    "#                             data=json.dumps(parameters))\n",
    "\n",
    "#     if api_call.status_code != 200:\n",
    "#         # Exibe uma mensagem de erro caso a requisição não seja bem-sucedida\n",
    "#         print(f\"Erro requisição CORE API com código {api_call.status_code} {api_call.content.decode()}\")  \n",
    "#         return {}\n",
    "#     else:\n",
    "#         # Exibe dados dos limites da API para requisições com sucesso:\n",
    "#         remaining_limit = api_call.headers.get(\"X-RateLimitRemaining\")\n",
    "#         retry_after = api_call.headers.get(\"X-RateLimit-Retry-After\")\n",
    "#         overall_limit = api_call.headers.get(\"X-RateLimit-Limit\")\n",
    "\n",
    "#         # Imprimir dados de requisições restante para os limites da API\n",
    "#         if overall_limit:\n",
    "#             # Exibe as informações obtidas\n",
    "#             print(f\"\\nLimite Restante: {remaining_limit}\")\n",
    "#             print(f\" Limites Totais: {overall_limit}\")            \n",
    "#             date_format = \"%Y-%m-%dT%H:%M:%S%z\"\n",
    "#             agora = datetime.strptime(retry_after, date_format).astimezone(pytz.timezone('America/Sao_Paulo'))\n",
    "#             print(f\" Hora renovação: {agora}\") \n",
    "        \n",
    "#     return api_call.json()\n",
    "\n",
    "# core_search('\"machine learning\" AND innovation AND ontology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes geradas nesta pesquisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import pytz\n",
    "import IPython\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import hashlib\n",
    "import requests\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import ipywidgets as wg\n",
    "import concurrent.futures\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "from itertools import combinations\n",
    "from datetime import datetime, timedelta\n",
    "from json.decoder import JSONDecodeError\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import widgets, Button, Output, HBox, VBox\n",
    "from requests.exceptions import HTTPError, ConnectionError, Timeout\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    from IPython.display import display\n",
    "    import ipywidgets as widgets\n",
    "except ImportError:\n",
    "    pass\n",
    "        \n",
    "class ApiCallTracker:\n",
    "    def __init__(self):\n",
    "        self.last_call_date = datetime.now().date()\n",
    "        self.last_reset = datetime.now()\n",
    "        self.failed_call_counts = {'ieee': 0, 'core':0, 'springer': 0}\n",
    "        self.call_counts = {'ieee': {'calls_per_day': 0}, \n",
    "                            'core': {'calls_per_day': 0}, \n",
    "                            'springer': {'calls_per_day': 0},\n",
    "                            }\n",
    "        self.limits = {'ieee': {'per_day': 200}, \n",
    "                       'core': {'per_day': 5000}, \n",
    "                       'springer': {'per_day': 5000}}\n",
    "\n",
    "    def register_call(self, api_name):\n",
    "        if api_name in self.call_counts:\n",
    "            self.call_counts[api_name]['calls_per_day'] += 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid API name: {api_name}\")\n",
    "\n",
    "    def register_failed_call(self, api_name):\n",
    "        if api_name in self.failed_call_counts:\n",
    "            self.failed_call_counts[api_name] += 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid API name: {api_name}\")\n",
    "\n",
    "    def get_api_call_info(self, api_name):\n",
    "        return self.call_counts.get(api_name, {'calls_per_day': 0})['calls_per_day'], \\\n",
    "               self.limits.get(api_name, {'per_day': 0})['per_day'] - self.call_counts.get(api_name, {'calls_per_day': 0})['calls_per_day']\n",
    "\n",
    "    def _reset_if_new_day(self):\n",
    "        current_date = datetime.now().date()\n",
    "        if current_date != self.last_call_date:\n",
    "            self.last_call_date = current_date\n",
    "            for key in self.call_counts.keys():\n",
    "                self.call_counts[key]['calls_per_day'] = 0\n",
    "\n",
    "    def _reset_counters_if_needed(self):\n",
    "        current_time = datetime.now()\n",
    "        if current_time - self.last_reset >= timedelta(days=1):\n",
    "            self._reset_if_new_day()\n",
    "            self.last_reset = current_time\n",
    "\n",
    "    def can_make_call(self, api_name):\n",
    "        self._reset_counters_if_needed()\n",
    "        calls_made_today = self.call_counts.get(api_name, {'calls_per_day': 0})['calls_per_day']\n",
    "        daily_limit = self.limits.get(api_name, {'per_day': 0})['per_day']\n",
    "        if calls_made_today < daily_limit:\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Limit reached for {api_name}. Calls made today: {calls_made_today}, Remaining calls: {daily_limit - calls_made_today}\")\n",
    "            return False\n",
    "        \n",
    "    def get_daily_api_usage_info(self):\n",
    "        usage_info = {}\n",
    "        for api_name, limits in self.limits.items():\n",
    "            if api_name in self.call_counts:\n",
    "                calls_made_today = self.call_counts[api_name]['calls_per_day']\n",
    "                remaining_calls = limits['per_day'] - calls_made_today\n",
    "                usage_info[api_name] = {\n",
    "                    \"calls_made_today\": calls_made_today,\n",
    "                    \"remaining_calls\": remaining_calls\n",
    "                }\n",
    "        return usage_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchPaperSearcher:\n",
    "    def __init__(self, keyword='',api_key='',base_url='',\n",
    "                 daily_limit=200,search_titles=True, tracker=None):    \n",
    "        self.keyword = keyword\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.daily_limit = daily_limit\n",
    "        self.search_titles = search_titles\n",
    "        self.tracker = tracker if tracker is not None else ApiCallTracker()\n",
    "        self.total_results = 0\n",
    "        self.calls_needed = 0\n",
    "        self.results = []\n",
    "\n",
    "    def renew_limits_time(self):\n",
    "        now_utc = datetime.now(pytz.utc)\n",
    "        next_midnight_utc = now_utc.replace(hour=0, minute=1, second=0, microsecond=0) + timedelta(days=1)\n",
    "        time_to_wait = next_midnight_utc - now_utc\n",
    "        hours, remainder = divmod(time_to_wait.seconds, 3600)\n",
    "        minutes, _ = divmod(remainder, 60)\n",
    "        return hours, minutes\n",
    "\n",
    "    def make_api_call(self, start=1, rows=100):\n",
    "        search_query = f\"title:{self.keyword}\" if self.search_titles else self.keyword\n",
    "        encoded_query = urllib.parse.quote(search_query)\n",
    "        \n",
    "        api_name = 'springer' if 'springernature' in self.base_url else 'ieee' if 'ieee' in self.base_url else 'core'\n",
    "        if not self.tracker.can_make_call(api_name):\n",
    "            successful_calls, remaining_calls = self.tracker.get_api_call_info(api_name)\n",
    "            print(f\"API call limit reached for {api_name}. Successful calls: {successful_calls}, Remaining calls: {remaining_calls}\")\n",
    "            return None\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        params = None\n",
    "        \n",
    "        if 'springer' in self.base_url:\n",
    "            url = self.base_url.format(encoded_query, self.api_key) + f\"&s={start}&p={rows}&contentType=Article\"\n",
    "        \n",
    "        elif 'ieee' in self.base_url:\n",
    "            url = self.base_url.format(self.api_key, encoded_query) + f\"&start_record={start}&max_records={rows}&querytext={encoded_query}&contentType=Article\"\n",
    "        \n",
    "        elif 'core.ac.uk' in self.base_url:\n",
    "            encoded_query = urllib.parse.quote(self.keyword)\n",
    "            url = self.base_url.format(self.api_key, encoded_query)\n",
    "            # headers = {'Authorization': f'Bearer {self.api_key}'}\n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "            params = {'offset': (start - 1) * rows, 'limit': rows, 'stats': False}\n",
    "            # print(url)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, \n",
    "                                    headers=headers, \n",
    "                                    params=params,\n",
    "                                    )\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            if response.status_code == 404:\n",
    "                print(\"        Erro 404: Recurso não encontrado. A URL pode estar incorreta ou o recurso não existe na base de dados da API.\")\n",
    "            elif response.status_code == 401:\n",
    "                print(f\"        Erro 401: Não autorizado. Verifique credenciais de acesso à API {api_name}.\")\n",
    "            elif response.status_code == 403:\n",
    "                print(f\"        Erro 403: Acesso negado, limite de download diário da API {api_name} esgotado por hoje.\")\n",
    "                hours, minutes = self.renew_limits_time()\n",
    "                print(f\"        Aguarde {hours}h:{minutes}min para fazer novos downloads na API {api_name}.\")\n",
    "            elif response.status_code == 500:\n",
    "                print(f\"        Erro 500: Erro interno do servidor. Problema no servidor da API {api_name}, tente novamente mais tarde.\")\n",
    "            else:\n",
    "                print(f\"        Erro HTTP: {http_err}\")\n",
    "\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"        Erro de conexão: Não foi possível se conectar ao servidor da API.\")\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"        Erro de tempo limite: A requisição à API demorou muito para responder.\")\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"        Um erro inesperado ocorreu: {err}\")\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            # print(f\"        Erro na requisição à API {api_name}: Status code {response.status_code}\")\n",
    "            self.tracker.register_failed_call(api_name)\n",
    "            return None\n",
    "        self.tracker.register_call(api_name)\n",
    "        \n",
    "        try:\n",
    "            return response.json()\n",
    "        except JSONDecodeError:\n",
    "            self.tracker.register_failed_call(api_name)\n",
    "            print(f\"        Erro ao decodificar JSON da API {api_name}\")\n",
    "            return None\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"        An error occurred: {err}\")\n",
    "            \n",
    "    def get_total_results(self):\n",
    "        response = self.make_api_call()\n",
    "        if response:\n",
    "            if 'springer' in self.base_url:\n",
    "                total_results = int(response['result'][0]['total'])\n",
    "            elif 'ieee' in self.base_url:\n",
    "                # total_searched = int(response['total_searched'])\n",
    "                total_results = int(response['total_records'])\n",
    "            elif 'core' in self.base_url:\n",
    "                total_results = int(response['totalHits'])\n",
    "            elif 'crossref' in self.base_url:\n",
    "                total_results = int(response['message']['total-results'])\n",
    "            return total_results\n",
    "        else:\n",
    "            print(\"        Erro na função get_total_results...\")\n",
    "            return 0\n",
    "\n",
    "    def is_jupyter_env(self):\n",
    "        try:\n",
    "            shell = get_ipython().__class__.__name__\n",
    "            return shell == 'ZMQInteractiveShell'\n",
    "        except NameError:\n",
    "            return False  # Não está em um ambiente Jupyter\n",
    "        \n",
    "    def confirm_execution(self):\n",
    "        self.display_search_info()\n",
    "        if self.is_jupyter_env():\n",
    "            button_yes = widgets.Button(description=\"Sim\")\n",
    "            button_no = widgets.Button(description=\"Não\")\n",
    "            prompt = widgets.Label(value=\"Deseja continuar com a busca? (Sim/Não)\")\n",
    "\n",
    "            def on_yes_clicked(b):\n",
    "                print(\"Executando a busca...\")\n",
    "                self.execute_search()\n",
    "\n",
    "            def on_no_clicked(b):\n",
    "                print(\"Busca cancelada.\")\n",
    "\n",
    "            button_yes.on_click(on_yes_clicked)\n",
    "            button_no.on_click(on_no_clicked)\n",
    "\n",
    "            display(VBox([prompt, HBox([button_yes, button_no])]))\n",
    "        else:\n",
    "            response = input(\"Deseja continuar com a busca? (Sim/Não): \")\n",
    "            if response.lower() == 'sim':\n",
    "                print(\"Executando a busca...\")\n",
    "                self.execute_search()\n",
    "            else:\n",
    "                print(\"Busca cancelada.\")\n",
    "\n",
    "    # Métodos para busca no CrossRef            \n",
    "    def search_crossref(self, query):\n",
    "        base_url = \"https://api.crossref.org/works\"\n",
    "        params = {'query': query, 'rows': 1000, 'select': 'title,DOI,abstract,created'}\n",
    "\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"        Erro na recuperação CrossRef: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "        data = response.json()\n",
    "        return self.process_crossref_results(data['message']['items'])\n",
    "\n",
    "    def process_crossref_results(self, items):\n",
    "        processed = []\n",
    "        for item in items:\n",
    "            processed.append({\n",
    "                \"title\": item.get(\"title\", [\"\"])[0],\n",
    "                \"doi\": item.get(\"DOI\", \"\"),\n",
    "                \"abstract\": item.get(\"abstract\", \"\"),\n",
    "                \"publicationDate\": item.get(\"created\", {}).get(\"date-time\", \"\")\n",
    "            })\n",
    "        return processed\n",
    "\n",
    "    # Métodos para busca na CORE API\n",
    "    def search_core(self, query, start=1, rows=10):\n",
    "        core_base_url = \"https://api.core.ac.uk/v3/search/works/\"\n",
    "        encoded_query = urllib.parse.quote(query)\n",
    "        headers = {'Authorization': f'Bearer {self.api_key}'}\n",
    "        offset = (start - 1) * rows\n",
    "        params = {'q': encoded_query, 'limit': rows, 'offset': offset, 'stats': False}\n",
    "        response = requests.get(core_base_url, headers=headers, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"        Erro na recuperação de CORE API: Status code {response.status_code}\")\n",
    "            return None\n",
    "        return response.json()\n",
    "\n",
    "    # Método para busca na IEEE Xplore\n",
    "    def search_ieee(self):\n",
    "        # Formatar a consulta\n",
    "        query = f\"title:({urllib.parse.quote(self.keyword)})\" if self.search_titles else urllib.parse.quote(self.keyword)\n",
    "        \n",
    "        # Montar a URL corretamente\n",
    "        base_url = f\"https://ieeexploreapi.ieee.org/api/v1/search/articles?apikey={self.api_key}&querytext={query}&s=1&p=100&contentType=Article\"\n",
    "          \n",
    "        while True:\n",
    "            response = requests.get(base_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"        Erro na recuperação em IEEE: Status code {response.status_code}\")\n",
    "                break\n",
    "            try:\n",
    "                data = response.json()\n",
    "            except JSONDecodeError:\n",
    "                print(\"        Erro ao decodificar JSON\")\n",
    "                break\n",
    "\n",
    "            total_len = len(data.get('articles', []))\n",
    "            for n, record in enumerate(data.get('articles', [])):\n",
    "                print(f\"{n + 1}: {record.get('title')}\")\n",
    "\n",
    "                self.results.append({\n",
    "                    \"title\": record.get(\"title\"),\n",
    "                    \"doi\": record.get(\"doi\"),\n",
    "                    \"publicationDate\": record.get(\"publicationYear\"),\n",
    "                    \"publisher\": record.get(\"publisher\"),\n",
    "                    \"abstract\": record.get(\"abstract\"),\n",
    "                    \"authors\": [author.get(\"preferredName\") for author in record.get(\"authors\", [])],\n",
    "                    \"publicationName\": record.get(\"publicationTitle\"),\n",
    "                    \"publicationType\": record.get(\"contentType\"),\n",
    "                    \"openAccess\": False,  # IEEE Xplore doesn't provide openAccess information\n",
    "                    \"onlineDate\": None,  # IEEE Xplore doesn't provide onlineDate information\n",
    "                    \"subjects\": record.get(\"mdSubjects\", []),\n",
    "                    \"disciplines\": [],  # IEEE Xplore doesn't provide disciplines information\n",
    "                    \"conferenceInfo\": {\n",
    "                        \"confSeriesName\": record.get(\"conferenceSeries\"),\n",
    "                        \"confEventAbbreviation\": record.get(\"conferenceAbbreviation\"),\n",
    "                        \"confEventLocation\": record.get(\"conferenceLocation\"),\n",
    "                        \"confEventDate\": record.get(\"conferenceDate\"),\n",
    "                        \"confEventURL\": record.get(\"conferenceURL\")\n",
    "                    },\n",
    "                    \"isbn\": None,  # IEEE Xplore doesn't provide ISBN information\n",
    "                    \"url\": [record.get(\"pdfUrl\")]\n",
    "                    # Adicione outros campos conforme necessário\n",
    "                })\n",
    "\n",
    "            # Verificar se há mais páginas\n",
    "            if 'nextPage' in data:\n",
    "                next_page_url = data['nextPage']\n",
    "                base_url = f\"https://ieeexploreapi.ieee.org{next_page_url}\"\n",
    "            else:\n",
    "                break\n",
    "        self.results_df = pd.DataFrame(self.results)\n",
    "\n",
    "    # Métodos para busca na SpringerNature API\n",
    "    def search_springer(self):\n",
    "        base_url = \"http://api.springernature.com/openaccess/json\"\n",
    "        params = {\"q\": f'title:{self.keyword}', \"api_key\": self.api_key, \"p\": 100,\"openaccess\": \"true\"}\n",
    "        lido=0\n",
    "        while True:\n",
    "            response = requests.get(base_url, params=params)\n",
    "\n",
    "            # Verifique se a resposta é bem-sucedida antes de decodificar o JSON\n",
    "            if response.status_code != 200:\n",
    "                print(f\"        Erro na recuperação da API Springer: Status code {response.status_code}\")\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                data = response.json()\n",
    "            except JSONDecodeError:\n",
    "                print(\"        Erro ao decodificar JSON da API Springer\")\n",
    "                break\n",
    "            \n",
    "            total_len=len(data.get('records', []))\n",
    "\n",
    "            for n,record in enumerate(data.get('records', [])):\n",
    "                # Tratamento para campos que podem ser dicionários ou strings\n",
    "                lido+=1\n",
    "                abstract = record.get(\"abstract\")\n",
    "                abstract_text = abstract.get(\"p\", \"\") if isinstance(abstract, dict) else abstract\n",
    "\n",
    "                conferenceInfo = record.get(\"conferenceInfo\")\n",
    "                conferenceInfo_processed = self.process_conferenceInfo(conferenceInfo) if isinstance(conferenceInfo, dict) else conferenceInfo\n",
    "\n",
    "                print(f\"{lido:>4}: {record.get('title')}\")\n",
    "\n",
    "                self.results.append({\"title\": record.get(\"title\"),\n",
    "                                     \"doi\": record.get(\"doi\"), \"publicationDate\": record.get(\"publicationDate\"), \"publisher\": record.get(\"publisher\"), \"abstract\": abstract_text, \"authors\": [creator.get(\"creator\") for creator in record.get(\"creators\", [])], \"publicationName\": record.get(\"publicationName\"), \"publicationType\": record.get(\"publicationType\"), \"openAccess\": record.get(\"openAccess\"), \"onlineDate\": record.get(\"onlineDate\"), \"subjects\": record.get(\"subjects\"), \"disciplines\": record.get(\"disciplines\"), \"conferenceInfo\": conferenceInfo_processed,\"isbn\": {\"print\": record.get(\"printIsbn\"), \"electronic\": record.get(\"electronicIsbn\")}, \"url\": [url.get(\"value\") for url in record.get(\"url\", [])]\n",
    "                                     })\n",
    "\n",
    "            # Verificar se há mais páginas\n",
    "            if 'nextPage' in data:\n",
    "                next_page_url = data['nextPage']\n",
    "                params = {k: v for k, v in [param.split(\"=\") for param in next_page_url.split(\"&\")[1:]]}\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Convertendo para DataFrame\n",
    "        self.results_df = pd.DataFrame(self.results)\n",
    "\n",
    "    # # Métodos detalhados para API Springer\n",
    "    # def calculate_calls_and_confirm(self):\n",
    "    #     total_calls_needed = math.ceil(self.total_results / self.results_per_page)\n",
    "    #     remaining_calls = self.tracker.get_remaining_calls('springer', self.daily_limit)\n",
    "\n",
    "    #     if total_calls_needed > remaining_calls:\n",
    "    #         print(\"Não há chamadas suficientes disponíveis para hoje.\")\n",
    "    #         return\n",
    "\n",
    "    #     confirm_message = f\"{self.total_results} resultados encontrados, requer {total_calls_needed} chamadas. Deseja continuar? (s/n): \"\n",
    "    #     if self.is_jupyter_env():\n",
    "    #         button = widgets.Button(description=\"Confirmar\")\n",
    "    #         output = widgets.Output()\n",
    "\n",
    "    #         def on_button_clicked(b):\n",
    "    #             with output:\n",
    "    #                 self.search_all_pages()\n",
    "            \n",
    "    #         button.on_click(on_button_clicked)\n",
    "    #         display(button, output)\n",
    "    #     else:\n",
    "    #         confirmation = input(confirm_message)\n",
    "    #         if confirmation.lower() == 's':\n",
    "    #             self.search_all_pages()\n",
    "\n",
    "    # def make_api_call_titles(self, start=1, rows=100):\n",
    "    #     url = self.base_url.format(self.keyword, self.api_key)+f\"&s={start}&p={rows}&q=title:{self.keyword}\"\n",
    "    #     response = requests.get(url)\n",
    "    #     return response.json()\n",
    "        \n",
    "    # def get_total_title_results(self):\n",
    "    #     response_titles = self.make_api_call_titles()\n",
    "    #     total_titles_results = int(response_titles['result'][0]['total'])\n",
    "    #     return total_titles_results\n",
    "\n",
    "    # def display_search_info(self):\n",
    "    #     total_results = self.get_total_results()\n",
    "    #     total_calls = self.calculate_calls_needed(total_results)\n",
    "    #     print(f\"Total de resultados: {total_results}, Chamadas necessárias: {total_calls}\")\n",
    "      \n",
    "    # def extract_all_results(self):\n",
    "    #     total_results = self.get_total_results()\n",
    "    #     total_calls = self.calculate_calls_needed(total_results)\n",
    "\n",
    "    #     if self.tracker.get_remaining_calls('springer', self.daily_limit) < total_calls:\n",
    "    #         print(\"Not enough daily quota to fetch all results.\")\n",
    "    #         return\n",
    "\n",
    "    #     confirmation = input(f\"About to make {total_calls} calls to fetch {total_results} results. Proceed? (y/n): \")\n",
    "    #     if confirmation.lower() != 'y':\n",
    "    #         print(\"Aborting operation.\")\n",
    "    #         return\n",
    "\n",
    "    #     for i in range(total_calls):\n",
    "    #         start_record = i * 100 + 1\n",
    "    #         self.tracker.register_call('springer')\n",
    "    #         response = self.make_api_call(start_record)\n",
    "    #         self.results.extend(response['records'])\n",
    "\n",
    "    #     print(\"All results fetched.\")\n",
    "\n",
    "    # def process_conferenceInfo(self, conferenceInfo):\n",
    "    #     processed_info = {}\n",
    "\n",
    "    #     processed_info['confSeriesName'] = conferenceInfo.get('confSeriesName', '')\n",
    "    #     processed_info['confEventAbbreviation'] = conferenceInfo.get('confEventAbbreviation', '')\n",
    "    #     processed_info['confEventLocation'] = self.process_confEventLocation(conferenceInfo.get('confEventLocation', []))\n",
    "    #     processed_info['confEventDate'] = self.process_confEventDate(conferenceInfo.get('confEventDateStart', []), conferenceInfo.get('confEventDateEnd', []))\n",
    "    #     processed_info['confEventURL'] = conferenceInfo.get('confEventURL', '')\n",
    "\n",
    "    #     return processed_info\n",
    "\n",
    "    # def process_confEventLocation(self, locations):\n",
    "    #     # Processa informações de localização do evento\n",
    "    #     location_str = []\n",
    "    #     for location in locations:\n",
    "    #         city = location.get('city', '')\n",
    "    #         country = \", \".join([country['value'] for country in location.get('country', [])])\n",
    "    #         location_str.append(f\"{city}, {country}\")\n",
    "    #     return \"; \".join(location_str)\n",
    "\n",
    "    # def process_confEventDate(self, startDate, endDate):\n",
    "    #     # Processa datas de início e fim do evento\n",
    "    #     format_date = lambda date: f\"{date.get('year', '')}-{date.get('month', '')}-{date.get('day', '')}\"\n",
    "    #     start_date_str = \"; \".join([format_date(date) for date in startDate])\n",
    "    #     end_date_str = \"; \".join([format_date(date) for date in endDate])\n",
    "    #     return f\"De {start_date_str} até {end_date_str}\"\n",
    "\n",
    "    # sprgcall=\"http://api.springernature.com/openaccess/json?q={}&api_key={}\"\n",
    "    # ieeecall=\"https://ieeexploreapi.ieee.org/api/v1/search/articles?parameter=querytext&apikey={}&title%3Agraph%20OR%20ontology%20AND%20%22innovation%20model%22&s=1&p=100&contentType=Article\"\n",
    "\n",
    "    ## Verificar qual parâmetro poderia cumprir o papel de remaining_calls sem login na API IEEEXplore \n",
    "    # def get_api_status(self, api_name, daily_limit):\n",
    "    #     \"\"\" Retorna o número de chamadas restantes para uma API específica \"\"\"\n",
    "    #     self._reset_if_new_day()\n",
    "    #     calls_made = self.call_counts.get(api_name, 0)\n",
    "    #     remaining_calls = daily_limit - calls_made\n",
    "    #     return {\"calls_made\": calls_made, \"remaining_calls\": remaining_calls}\n",
    "        \n",
    "    ## Buscas em outras APIs        \n",
    "    # def science_direct(self):\n",
    "    #     # Implementar a busca no Science Direct da Elsevier\n",
    "    #     pass\n",
    "\n",
    "    # def search_pubmed(self):\n",
    "    #     # Implementar a busca no PubMed\n",
    "    #     pass\n",
    "\n",
    "    # def search_acm(self):\n",
    "    #     # Implementar a busca na ACM Digital Library\n",
    "    #     pass\n",
    "\n",
    "    # def execute_search(self):\n",
    "    #     # Método para executar todas as buscas\n",
    "    #     self.search_ieee()\n",
    "    #     self.search_springer()\n",
    "    #     self.search_crossref()\n",
    "    #     # self.science_direct\n",
    "    #     # self.search_pubmed()\n",
    "    #     # self.search_acm()\n",
    "    #     # self.remove_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedResearchPaperAggregator:\n",
    "    def __init__(self, query, period, springer_api_key, ieee_api_key, core_api_key, sciencedirect_api_key):\n",
    "        self.query = query\n",
    "        self.period = period\n",
    "        self.springer_api_key = springer_api_key\n",
    "        self.ieee_api_key = ieee_api_key\n",
    "        self.core_api_key = core_api_key\n",
    "        self.sciencedirect_api_key = sciencedirect_api_key\n",
    "        self.springer_results = []\n",
    "        self.ieee_results = []\n",
    "        self.core_results = []\n",
    "        self.crossref_results = []\n",
    "        self.unified_results = []\n",
    "        self.search_history = []        \n",
    "        self.springer_daily_limit = 5000\n",
    "        self.core_client = CoreClient(core_api_key)  # Instância da classe CoreClient\n",
    "        self.core_daily_limit = 5000 # 10mil tokens/dia, 1-5 tokens/chamada, 10t/min\n",
    "        self.ieee_daily_limit = 200 # limite de requisições que baixam dados por dia\n",
    "        self.tracker = ApiCallTracker()\n",
    "\n",
    "    def get_springer_total_results(self):\n",
    "        springer_url = \"http://api.springernature.com/openaccess/json?q={}&api_key={}\"\n",
    "        self.springer_searcher = ResearchPaperSearcher(\n",
    "            self.query, \n",
    "            self.springer_api_key, \n",
    "            springer_url, \n",
    "            self.springer_daily_limit, \n",
    "            self.tracker)\n",
    "        return self.springer_searcher.get_total_results()\n",
    "\n",
    "    def get_ieee_total_results(self):\n",
    "        # ieee_url = \"https://ieeexploreapi.ieee.org/api/v1/search/articles?parameter=querytext&apikey={}&title%3A{}&contentType=Article\"\n",
    "        ieee_url = \"https://ieeexploreapi.ieee.org/api/v1/search/articles?&apikey={}&abstract%3A{}\"\n",
    "        self.ieee_searcher = ResearchPaperSearcher(\n",
    "            self.query, \n",
    "            self.ieee_api_key, \n",
    "            ieee_url, \n",
    "            self.ieee_daily_limit, \n",
    "            self.tracker)\n",
    "        return self.ieee_searcher.get_total_results()\n",
    "\n",
    "    def get_core_total_results(self):\n",
    "        core_url = \"https://api.core.ac.uk/v3/search/works/?api_key={}&q={}\"\n",
    "        self.core_searcher = ResearchPaperSearcher(\n",
    "            self.query, \n",
    "            self.core_api_key, \n",
    "            core_url, \n",
    "            self.core_daily_limit, \n",
    "            self.tracker)\n",
    "        return self.core_searcher.get_total_results()\n",
    "\n",
    "    # API REST do crossref usa score de relevância e não operadores booleanos, aqui filtramos os 5000 registros mais relevantes por score e implementamos os filtros com operadores booleanos\n",
    "    def process_boolean_operators_crossref(self, query):\n",
    "        processed_query = query.replace(' AND ', ' ').replace(' OR ', ' ').replace(' NOT ', ' ')\n",
    "        return processed_query\n",
    "\n",
    "    def get_crossref_total_results(self, query):\n",
    "        # processed_query = self.process_boolean_operators_crossref(query)\n",
    "        processed_query = urllib.parse.quote(query)\n",
    "        # print(f\"Debug - Query para CrossRef: {processed_query}\")\n",
    "        url = \"https://api.crossref.org/works\"\n",
    "        params = {\n",
    "            'query': processed_query,\n",
    "            'rows': 1,  # Somente para obter o total de resultados\n",
    "            'filter': 'type:journal-article',\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"        Error in CrossRef API request: {response.status_code}\")\n",
    "            return 0\n",
    "        data = response.json()\n",
    "        total_results = data['message']['total-results']\n",
    "        # print(f\"{total_results:>7} total de similaridades encontradas.\")\n",
    "        return total_results\n",
    "\n",
    "    # Métodos para quantificar resultados antes de recuperar\n",
    "    def calculate_calls_needed(self, total_results, results_per_call=100):\n",
    "        return -(-total_results // results_per_call)  # Round up division\n",
    "\n",
    "    def contar_termos(self, query):\n",
    "        # Remover operadores booleanos AND, OR, NOT\n",
    "        query_sem_operadores = re.sub(r' AND | OR | NOT ', ' ', query)\n",
    "        \n",
    "        # Tratar termos entre aspas como um único termo\n",
    "        termos = re.findall(r'\"[^\"]+\"|\\S+', query_sem_operadores)\n",
    "        return len(termos)\n",
    "\n",
    "    def encontrar_queries_redundantes(self, queries):\n",
    "        queries_redundantes = []\n",
    "        for query in queries:\n",
    "            termos = re.findall(r'\"[^\"]+\"|\\S+', query)\n",
    "            if self.termo_contido_em_outro(termos):\n",
    "                queries_redundantes.append(query)\n",
    "        return queries_redundantes\n",
    "\n",
    "    def termo_contido_em_outro(self, termos):\n",
    "        for termo in termos:\n",
    "            # Remover aspas para a comparar\n",
    "            termo_limpo = termo.replace('\"', '')\n",
    "            for outro_termo in termos:\n",
    "                if termo != outro_termo and termo_limpo in outro_termo.replace('\"', ''):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def generate_queries(self, keywords):\n",
    "        queries = []\n",
    "        for r in range(1, len(keywords) + 1):\n",
    "            for combo in combinations(keywords, r):\n",
    "                termos = [f'\"{term}\"' if ' ' in term else term for term in combo]\n",
    "\n",
    "                if not self.termo_contido_em_outro(termos):\n",
    "                    query = ' AND '.join(termos)\n",
    "                    queries.append(query)\n",
    "\n",
    "        queries_ordenadas = sorted(queries, key=self.contar_termos)\n",
    "        return queries_ordenadas\n",
    "\n",
    "    def process_multiple_queries(self, queries):\n",
    "        for n, query in enumerate(queries):\n",
    "            self.query = query\n",
    "            print(f\"\\nProcessando Query {n + 1}/{len(queries)}: '{query}'\")\n",
    "\n",
    "            total_springer_results = self.get_springer_total_results()\n",
    "            print(f\"{total_springer_results:>7} Resultados por busca booleana na SpringerNature\")\n",
    "\n",
    "            total_ieee_results = self.get_ieee_total_results()\n",
    "            print(f\"{total_ieee_results:>7} Resultados por busca booleana na IEEEXplore\")\n",
    "\n",
    "            total_core_results = self.get_core_total_results()\n",
    "            print(f\"{total_core_results:>7} Registros relacionados por relevância no agregador CORE\")\n",
    "\n",
    "            total_crossref_results = self.get_crossref_total_results(self.query)\n",
    "            # df_cref = self.fetch_crossref_results(query)\n",
    "            # total_crossref_results = len(df_cref)\n",
    "            print(f\"{total_crossref_results:>7} Registros relacionados por relevância no agregador CrossRef\")\n",
    "\n",
    "            self.query_history_update(total_springer_results, total_ieee_results, total_core_results, total_crossref_results)\n",
    "            total_results = total_springer_results + total_ieee_results + total_core_results + total_crossref_results\n",
    "            print(f\"Resultados relacionados antes da busca booleana nos agregadores: {total_results}\")\n",
    "\n",
    "            if total_results == 0:\n",
    "                print(f\"Nenhum resultado encontrado para a query: '{query}'\")\n",
    "                continue\n",
    "        \n",
    "        return pd.DataFrame(self.search_history)\n",
    "\n",
    "    def query_history_update(self, springer_results, ieee_results, core_results, crossref_results):\n",
    "        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "        self.search_history.append({\n",
    "            \"data\": current_time, \n",
    "            \"query\": self.query, \n",
    "            \"ieee_results\": ieee_results,\n",
    "            \"springer_results\": springer_results,\n",
    "            \"core_results\": core_results,\n",
    "            \"crossref_results\": crossref_results\n",
    "        })\n",
    "\n",
    "    def dataframe_search_history(self):\n",
    "        # Transformar o histórico de buscas em um DataFrame\n",
    "        return pd.DataFrame(self.search_history)\n",
    "    \n",
    "    def print_search_history(self):\n",
    "        print(\"\\nHistórico de buscas:\")\n",
    "        for entry in self.search_history:\n",
    "            print(f\"Data: {entry['data']}, Query: {entry['query']}, \"\n",
    "                  f\"Springer: {entry['springer_results']}, IEEE: {entry['ieee_results']}, CORE: {entry['core_results']}, CrossRef: {entry['crossref_results']}\")\n",
    "\n",
    "    # Montar dataframe para exibir o quantitativo de buscas em cada API\n",
    "    def query_total_results(self, queries):\n",
    "        if not self.tracker.can_make_call('springer'):\n",
    "            print(\"Limite de chamadas atingido para Springer. Não é possível processar as queries.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if not self.tracker.can_make_call('ieee'):\n",
    "            print(\"Limite de chamadas atingido para IEEE. Não é possível processar as queries.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        results = []\n",
    "        for query in queries:\n",
    "            self.query = query\n",
    "            springer_results = self.get_springer_total_results()\n",
    "            ieee_results = self.get_ieee_total_results()\n",
    "            core_results = self.get_core_total_results()\n",
    "            crossref_items = self.get_crossref_paginated_results(query, limit=5000)\n",
    "            # df_cref = pd.DataFrame(crossref_items)\n",
    "            # print(f\"Debug - crossref_items: {crossref_items}\")\n",
    "            crossref_results = len(crossref_items)  # para contar todos os itens\n",
    "            # crossref_results = len(self.filter_results_by_dynamic_threshold(crossref_items))\n",
    "            # print(f\"Debug - crossref_results: {crossref_results}\")\n",
    "\n",
    "            self.query_history_update(springer_results, ieee_results, core_results, crossref_results)\n",
    "            results.append({\"query\": query, \"springer_results\": springer_results, \"ieee_results\": ieee_results, \"core_results\": core_results, \"crossref_results\": crossref_results})\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def safe_request(url, max_attempts=3, initial_delay=2):\n",
    "        attempts = 0\n",
    "        delay = initial_delay\n",
    "\n",
    "        while attempts < max_attempts:\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    return response.json()\n",
    "                elif response.status_code == 429:\n",
    "                    print(\"Atingido o limite de requisições. Aguardando para tentar novamente...\")\n",
    "                    time.sleep(delay)  # Espera antes de tentar novamente\n",
    "                    delay *= 2  # Aumenta o delay para a próxima tentativa\n",
    "                else:\n",
    "                    print(f\"        Erro na requisição: Status code {response.status_code}\")\n",
    "                    break  # Para outras respostas de erro, interrompe as tentativas\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"       Erro na requisição: {e}\")\n",
    "\n",
    "            attempts += 1\n",
    "            time.sleep(delay)  # Espera antes da próxima tentativa\n",
    "            delay *= 2  # Aumenta o delay para a próxima tentativa\n",
    "\n",
    "        return None\n",
    "\n",
    "    ## Efetivamente baixar os resultados de busca encontrados\n",
    "    # Extrair e uniformizar dados vindos da SpringerNature\n",
    "    '''Campos disponibilizados na SpringerNature:\n",
    "    ['contentType', 'identifier', 'language', 'url', 'title', 'creators', 'publicationName', 'doi', \n",
    "    'publisher', 'publisherName', 'publicationDate', 'publicationType', 'printIsbn', 'electronicIsbn', \n",
    "    'genre', 'openAccess', 'onlineDate', 'seriesId', 'copyright', 'abstract', 'conferenceInfo', \n",
    "    'subjects', 'disciplines', 'issn', 'eIssn', 'volume', 'number', 'issueType', 'topicalCollection', \n",
    "    'startingPage', 'endingPage', 'journalId', 'coverDate', 'printDate']\n",
    "    '''\n",
    "    def extract_spring_abstract_content(self, abstract):\n",
    "        # Verifica se a chave 'p' existe no dicionário\n",
    "        if 'p' in abstract:\n",
    "            content = abstract['p']\n",
    "            # Se o conteúdo for uma lista, junta os elementos em uma string única\n",
    "            if isinstance(content, list):\n",
    "                return ' '.join(content)\n",
    "            # Se for uma string, retorna a string\n",
    "            elif isinstance(content, str):\n",
    "                return content\n",
    "        # Se a chave 'p' não existir, retorna uma string vazia\n",
    "        return ''\n",
    "\n",
    "    # Função para extrair o valor endereço do campo URL que está em 'value'\n",
    "    def extract_url_value(self, url_list):\n",
    "        if url_list and isinstance(url_list, list) and 'value' in url_list[0]:\n",
    "            return url_list[0]['value']\n",
    "        return None\n",
    "\n",
    "    # Efetivamente buscar e baixar resultados da query\n",
    "    def fetch_springer_results(self):\n",
    "        springer_url = \"http://api.springernature.com/openaccess/json?q={}&api_key={}\"\n",
    "        springer_searcher = ResearchPaperSearcher(self.query, self.springer_api_key, springer_url, daily_limit=5000)\n",
    "        total_results = springer_searcher.get_total_results()\n",
    "        if total_results == 0:\n",
    "            return pd.DataFrame()\n",
    "        with tqdm(total=total_results, desc=\"Baixando dados SpringerNature\") as pbar:    \n",
    "            for start in range(0, total_results, 100):\n",
    "                try:\n",
    "                    results_list = springer_searcher.make_api_call(start=start+1, rows=100)\n",
    "                    # print(f'DEBUG SpringerNature list {type(results_list)} | {len(results_list)}')\n",
    "                    if results_list:\n",
    "                        records = results_list['records']\n",
    "                        self.springer_results.extend(records)\n",
    "                        pbar.update(len(records))\n",
    "                except Exception as e:\n",
    "                    print('Erro ao recuperar dados da SpringerNature')\n",
    "                    print(e)\n",
    "                    return pd.DataFrame()        \n",
    "        \n",
    "        try:\n",
    "            # Criar DataFrame diretamente da lista de resultados\n",
    "            df_sprg = pd.DataFrame(self.springer_results)\n",
    "\n",
    "            # Selecionar apenas as colunas desejadas e renomeando-as\n",
    "            df_sprg = df_sprg[['identifier', \n",
    "                               'contentType', \n",
    "                               'onlineDate', \n",
    "                               'title', \n",
    "                               'abstract', \n",
    "                               'url', \n",
    "                               'openAccess']]\n",
    "            \n",
    "            df_sprg.columns = ['DOI', \n",
    "                               'TIPO', \n",
    "                               'DATE', \n",
    "                               'TITLE',\n",
    "                               'ABSTRACT', \n",
    "                               'URL', \n",
    "                               'OPENACCESS']\n",
    "            \n",
    "            df_sprg['DOI'] = df_sprg['DOI'].str.replace(r'doi:', '', regex=True)\n",
    "            df_sprg['ABSTRACT'] = df_sprg['ABSTRACT'].apply(lambda x: self.extract_spring_abstract_content(x))\n",
    "            df_sprg['URL'] = df_sprg['URL'].apply(self.extract_url_value)\n",
    "            df_sprg['URL_PDF'] = None\n",
    "            df_sprg['FULLTEXT'] = None\n",
    "            df_sprg['API'] = 'SpringerNature'\n",
    "            return df_sprg\n",
    "        except Exception as e:\n",
    "            print('Não foi possível criar o dataframe de resultados Springer')\n",
    "            print(e)\n",
    "    \n",
    "    # def baixar_artigo(self, url):\n",
    "    #     headers = {'api_key': self.springer_api_key}\n",
    "    #     try:\n",
    "    #         response = requests.get(url, headers=headers)\n",
    "    #         if response.status_code == 200:\n",
    "    #             return response.content\n",
    "    #         else:\n",
    "    #             print(f\"Erro ao baixar o artigo: {response.status_code}\")\n",
    "    #             return None\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Erro ao baixar o artigo: {e}\")\n",
    "    #         return None\n",
    "        \n",
    "    # def fetch_springer_results(self):\n",
    "    #     springer_url = \"http://api.springernature.com/openaccess/json\"\n",
    "    #     springer_searcher = ResearchPaperSearcher(self.query, \n",
    "    #                                               self.springer_api_key, \n",
    "    #                                               springer_url, \n",
    "    #                                               daily_limit=5000)\n",
    "    #     total_results = springer_searcher.get_total_results()\n",
    "    #     if total_results == 0:\n",
    "    #         return pd.DataFrame()\n",
    "    #     with tqdm(total=total_results, desc=\"Baixando dados SpringerNature\") as pbar:    \n",
    "    #         for start in range(0, total_results, 100):\n",
    "    #             params = {\n",
    "    #                 'q': self.query,\n",
    "    #                 'api_key': self.springer_api_key,\n",
    "    #                 's': start,\n",
    "    #                 'p': 100\n",
    "    #             }\n",
    "    #             try:\n",
    "    #                 response = requests.get(springer_url, params=params)\n",
    "    #                 if response.status_code == 200:\n",
    "    #                     results_list = response.json()\n",
    "    #                     records = results_list['records']\n",
    "    #                     self.springer_results.extend(records)\n",
    "    #                     pbar.update(len(records))\n",
    "    #                 else:\n",
    "    #                     print(f'Erro na API: {response.status_code}')\n",
    "    #                     break\n",
    "    #             except Exception as e:\n",
    "    #                 print('Erro ao recuperar dados da SpringerNature')\n",
    "    #                 print(e)\n",
    "    #                 break\n",
    "    #     try:\n",
    "    #         df_sprg = pd.DataFrame(self.springer_results)\n",
    "    #         df_sprg = df_sprg[['identifier', 'contentType', 'onlineDate', 'title', 'abstract', 'url', 'openAccess']]\n",
    "            \n",
    "    #         df_sprg.columns = ['DOI', 'TIPO', 'DATE', 'TITLE', 'ABSTRACT', 'URL', 'OPENACCESS']\n",
    "    #         df_sprg['DOI'] = df_sprg['DOI'].str.replace(r'doi:', '', regex=True)\n",
    "    #         df_sprg['ABSTRACT'] = df_sprg['ABSTRACT'].apply(lambda x: self.extract_spring_abstract_content(x))\n",
    "    #         df_sprg['URL'] = df_sprg['URL'].apply(self.extract_url_value)\n",
    "    #         df_sprg['FULLTEXT'] = df_sprg.apply(lambda x: self.baixar_artigo(x['URL']) if x['OPENACCESS'] else None, axis=1)\n",
    "    #         df_sprg['API'] = 'SpringerNature'\n",
    "    #         return df_sprg\n",
    "    #     except Exception as e:\n",
    "    #         print('Não foi possível criar o dataframe de resultados Springer')\n",
    "    #         print(e) \n",
    "\n",
    "    # Extrair e uniformizar dados vindos da IEEEXplore\n",
    "    '''\n",
    "    Campos da IEEEXplore: \n",
    "    ['doi', 'title', 'publisher', 'isbn', 'rank', 'authors', 'access_type', 'content_type', 'abstract', 'article_number', 'pdf_url', 'html_url', 'abstract_url', 'publication_title', 'conference_location', 'conference_dates', 'publication_number', 'is_number', 'publication_year', 'publication_date', 'start_page', 'end_page', 'citing_paper_count','citing_patent_count', 'download_count', 'insert_date', 'index_terms', 'isbn_formats', 'issue', 'issn', 'volume', 'partnum']\n",
    "    '''\n",
    "    def fetch_ieee_results(self):\n",
    "        ieee_url = \"https://ieeexploreapi.ieee.org/api/v1/search/articles?&apikey={}&abstract%3A{}\"\n",
    "        ieee_searcher = ResearchPaperSearcher(self.query, self.ieee_api_key, ieee_url, daily_limit=200, tracker=self.tracker)\n",
    "        total_results = ieee_searcher.get_total_results()\n",
    "        if total_results == 0:\n",
    "            return pd.DataFrame()\n",
    "        results_per_call = 100\n",
    "\n",
    "        with tqdm(total=total_results, desc=\"Baixando dados da IEEE Xplore\") as pbar:    \n",
    "            for start in range(0, total_results, results_per_call):\n",
    "                try:\n",
    "                    results_list = ieee_searcher.make_api_call(start=start+1,\n",
    "                                                            rows=results_per_call)\n",
    "                    # print(f'DEBUG IEEE dict {type(results_list)} | {len(results_list)}')\n",
    "                    if results_list:\n",
    "                        articles = results_list.get('articles', [])\n",
    "                        self.ieee_results.extend(articles)\n",
    "                        pbar.update(len(articles))\n",
    "                        # print(f'DEBUG IEEE list {type(self.ieee_results)} | {len(self.ieee_results)}')\n",
    "                except Exception as e:\n",
    "                    print('Não foi possível criar o dataframe de resultados IEEE')\n",
    "                    print(e)\n",
    "        try:\n",
    "            # Criar DataFrame diretamente da lista de resultados\n",
    "            df_ieee = pd.DataFrame(self.ieee_results)            \n",
    "            # pprint(f\"{list(all_results.keys())}\")\n",
    "            df_ieee = df_ieee[[\n",
    "                'doi',\n",
    "                'content_type',\n",
    "                'publication_date',\n",
    "                'title',\n",
    "                'abstract',\n",
    "                'html_url',\n",
    "                'pdf_url']]\n",
    "\n",
    "            df_ieee.columns=(\n",
    "                'DOI',\n",
    "                'TIPO',\n",
    "                'DATE',\n",
    "                'TITLE',\n",
    "                'ABSTRACT',\n",
    "                'URL',\n",
    "                'URL_PDF')\n",
    "            \n",
    "            df_ieee['OPENACCESS'] = None\n",
    "            df_ieee['FULLTEXT'] = None\n",
    "            df_ieee['API'] = 'IEEEXplore'\n",
    "            return df_ieee        \n",
    "        except Exception as e:\n",
    "            print(f\"        Erro ao recuperar dados da API IEEE: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # Extrair e uniformizar dados vindos da busca por trabalhos com API CORE\n",
    "    ''' https://api.core.ac.uk/docs/v3#tag/Search/null\n",
    "    Parâmetros para busca em Works: total_hits, limit, offset, scroll_id, string, results, tooks, es_took \n",
    "    Campos em cada resultado CORE/search/works:\n",
    "    [accepted_date, arxiv_id, authors, citation_count, contributors, outputs, created_date, data_providers, deposited_date, abstract, document_type, doi, download_url, field_of_study, full_text, id, identifiers, title, language, mag_id, oai_ids, published_date, publisher, pubmed_id, references, source_fulltext_urls, journals, updated_date, year_published, links]\n",
    "    '''\n",
    "\n",
    "    def fetch_core_results(self, query, limit=100, period=None):\n",
    "        total_results = self.get_core_total_results()\n",
    "        if total_results == 0:\n",
    "            return pd.DataFrame()\n",
    "        try:\n",
    "            # Adicionar o intervalo de anos na busca, se fornecido\n",
    "            year_min, year_max = period if period else (None, None)\n",
    "\n",
    "            # Usar o método find da instância core_client\n",
    "            core_results = self.core_client.core_find(query=query, limit=limit, entity=\"works\", year_min=year_min, year_max=year_max)\n",
    "            \n",
    "            # Transformar os resultados em um DataFrame\n",
    "            df_core = pd.DataFrame(core_results)\n",
    "            # print(f\"DEBUG: fetch_coreresults df_core.keys() {df_core.keys()}\")\n",
    "            df_core = df_core[[\n",
    "                'doi',\n",
    "                'documentType',\n",
    "                'publishedDate',\n",
    "                'title',\n",
    "                'abstract',\n",
    "                'downloadUrl',\n",
    "                'links',\n",
    "                'fullText',]]\n",
    "\n",
    "            df_core.columns = (\n",
    "                'DOI',\n",
    "                'TIPO',\n",
    "                'DATE',\n",
    "                'TITLE',\n",
    "                'ABSTRACT',\n",
    "                'URL',\n",
    "                'URL_PDF',\n",
    "                'FULLTEXT')\n",
    "            \n",
    "            df_core['OPENACCESS'] = 1\n",
    "            df_core['API'] = 'CORE'\n",
    "        \n",
    "            return df_core\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('Não foi possível criar o dataframe de resultados CORE')\n",
    "            print(e)\n",
    "            return pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Extrair e uniformizar dados vindos do CrossRef implementando filtragem booleana\n",
    "    '''\n",
    "    https://www.crossref.org/documentation/retrieve-metadata/rest-api/\n",
    "    Campos da CrossRef: \n",
    "    ['indexed', 'reference-count', 'publisher', 'license', 'content-domain', 'short-container-title', 'published-print', 'DOI', 'type', 'created', 'page', 'update-policy', 'source', 'is-referenced-by-count', 'title', 'prefix', 'volume', 'author', 'member', 'reference', 'container-title', 'language', 'link', 'deposited', 'score', 'resource', 'issued', 'references-count', 'alternative-id', 'URL', 'ISSN', 'issn-type', 'subject', 'published', 'assertion', 'article-number', 'issue', 'published-online', 'journal-issue', 'abstract', 'original-title', 'published-other', 'subtitle', 'funder', 'accepted', 'update-to', 'relation', 'editor', 'archive']\n",
    "    '''\n",
    "\n",
    "    def get_full_text(doi):\n",
    "        opener = urllib.request.build_opener()\n",
    "        opener.addheaders = [('Accept', 'application/vnd.crossref.unixsd+xml')]\n",
    "        r = opener.open(doi)\n",
    "        print (r.info()['Link'])\n",
    "\n",
    "    def filter_dataframe_by_query_combined(self, df, query):\n",
    "        # Determina o operador lógico (AND ou OR) e as palavras-chave\n",
    "        if ' AND ' in query:\n",
    "            keywords = query.split(' AND ')\n",
    "            logical_operator = 'and'\n",
    "        elif ' OR ' in query:\n",
    "            keywords = query.split(' OR ')\n",
    "            logical_operator = 'or'\n",
    "        else:\n",
    "            keywords = [query]\n",
    "            logical_operator = 'or'\n",
    "\n",
    "        # Combina título e resumo em uma única coluna\n",
    "        df['combined'] = df['TITLE'].astype(str) + \" \" + df['ABSTRACT'].astype(str)\n",
    "\n",
    "        # Cria uma série booleana para cada palavra-chave na coluna combinada\n",
    "        conditions = [df['combined'].str.contains(keyword, case=False, na=False) for keyword in keywords]\n",
    "\n",
    "        # Combina todas as condições\n",
    "        if logical_operator == 'and':\n",
    "            combined_condition = pd.concat(conditions, axis=1).all(axis=1)\n",
    "        else: # 'or'\n",
    "            combined_condition = pd.concat(conditions, axis=1).any(axis=1)\n",
    "\n",
    "        # Filtra o dataframe\n",
    "        filtered_df = df[combined_condition]\n",
    "\n",
    "        # Remove a coluna combinada se não for mais necessária\n",
    "        filtered_df = filtered_df.drop(columns=['combined'])\n",
    "\n",
    "        return filtered_df\n",
    "\n",
    "    def format_date(self, date_parts):\n",
    "        try:\n",
    "            year, month, day = date_parts['date-parts'][0]\n",
    "            date = datetime(year, month, day)\n",
    "            return date.strftime('%Y-%m-%d')\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def parse_query(self, query):\n",
    "        # Remove espaços extras e divide a query em tokens\n",
    "        query = re.sub(r'\\s+', ' ', query.strip())\n",
    "        tokens = re.split(r'(\\s+AND\\s+|\\s+OR\\s+|\\(|\\))', query)\n",
    "        print(f\"DEBUG: parse_query Tokens: {tokens}\")\n",
    "        return [token.strip() for token in tokens if token.strip()]\n",
    "\n",
    "    def filter_dataframe_by_complex_query(self, df, query, original_query=None):\n",
    "        if original_query is None:\n",
    "            original_query = query\n",
    "        if 'combined' not in df.columns:\n",
    "            combined = df['TITLE'].astype(str) + \" \" + df['ABSTRACT'].astype(str)\n",
    "            new_df = df.assign(combined=combined)\n",
    "            print(\"DEBUG: Coluna 'combined' adicionada ao DataFrame.\")\n",
    "        tokens = self.parse_query(query)\n",
    "        combined_indices_set = set(new_df.index)\n",
    "        print(f\"DEBUG: {len(combined_indices_set)} Resultados parciais. Conjunto de índices combinados: {combined_indices_set}\")\n",
    "        current_operator = 'AND'\n",
    "        for token in tokens:\n",
    "            if token in ['AND', 'OR']:\n",
    "                current_operator = token\n",
    "                print(f\"DEBUG: Operador atual: {current_operator}\")\n",
    "            else:\n",
    "                token_indices = set(self.process_token(new_df, token, original_query))\n",
    "                print(f\"DEBUG: Índices antes do operador '{current_operator}': {combined_indices_set}\")\n",
    "                if current_operator == 'AND':\n",
    "                    combined_indices_set &= token_indices\n",
    "                elif current_operator == 'OR':\n",
    "                    combined_indices_set |= token_indices\n",
    "                print(f\"DEBUG: Índices após aplicação do token '{token}': {combined_indices_set}\")\n",
    "\n",
    "        indices_list = list(combined_indices_set)\n",
    "        print(f\"DEBUG: Lista de índices combinados finais: {indices_list}\")\n",
    "        filtered_df = new_df.loc[indices_list]\n",
    "\n",
    "        return filtered_df\n",
    "\n",
    "    def process_token(self, df, token, original_query):\n",
    "        if token.startswith('(') and token.endswith(')'):\n",
    "            subquery = token[1:-1]  # Remove os parênteses\n",
    "            if len(subquery) < len(original_query):\n",
    "                subquery_df = self.filter_dataframe_by_complex_query(df, subquery, original_query)\n",
    "                return set(subquery_df.index)\n",
    "            else:\n",
    "                # Tratar erro ou subquery inválida\n",
    "                print(\"Erro: A subquery é maior ou igual à consulta original.\")\n",
    "                return set()\n",
    "        elif token.startswith('\"') and token.endswith('\"'):\n",
    "            # Processamento de token com aspas\n",
    "            escaped_token = re.escape(token[1:-1])\n",
    "        else:\n",
    "            # Processamento de token simples\n",
    "            escaped_token = re.escape(token)\n",
    "\n",
    "        # Processa uma condição simples\n",
    "        condition = df['combined'].str.contains(escaped_token, case=False, na=False, regex=True)\n",
    "        matching_indices = df.index[condition].tolist()\n",
    "        print(f\"DEBUG: process_token Token: '{token}', Quantidade: {len(matching_indices)}\")\n",
    "        return set(matching_indices)\n",
    "\n",
    "    def fetch_crossref_filtered_results(self, query):\n",
    "        print(\"\\nIniciando busca no CrossRef...\")\n",
    "        total_results = self.get_crossref_total_results(query)\n",
    "        sample_limit = min(total_results, 10000)\n",
    "        print(f\"{total_results} resultados, baixando {sample_limit} mais relevantes\")\n",
    "        list_items = self.get_crossref_paginated_results(query, total_results, limit=sample_limit)\n",
    "        df_cref = pd.DataFrame(list_items)\n",
    "        # print(\"DEBUG - Colunas originais:\",list(df_cref.keys()))\n",
    "        if not list_items:\n",
    "            print(\"Nenhum resultado encontrado para a consulta.\")\n",
    "            return []\n",
    "\n",
    "        df_cref = df_cref[[\n",
    "            'DOI',\n",
    "            'type',\n",
    "            'created',\n",
    "            'title',\n",
    "            'abstract',\n",
    "            'URL',\n",
    "            'link',\n",
    "            ]]\n",
    "        \n",
    "        df_cref.columns=('DOI',\n",
    "                        'TIPO',\n",
    "                        'DATE',\n",
    "                        'TITLE',\n",
    "                        'ABSTRACT',\n",
    "                        'URL',\n",
    "                        'URL_PDF')\n",
    "        \n",
    "        df_cref['DATE'] = df_cref['DATE'].apply(self.format_date)\n",
    "        df_cref['TITLE'] = df_cref['TITLE'].apply(lambda x: x[0] if isinstance(x, list) and x else x)\n",
    "        df_cref['ABSTRACT'] = df_cref['ABSTRACT'].str.replace(r'<jats:p>|</jats:p>', '', regex=True)\n",
    "        df_cref['FULLTEXT'] = None\n",
    "        df_cref['API'] = 'CrossRef'\n",
    "        # print(\"DEBUG - Colunas uniformes:\",list(df_cref.keys()))\n",
    "\n",
    "        ## Filtrar com booleanos de acordo com a query realizada\n",
    "        df_cref = self.filter_dataframe_by_complex_query(df_cref, query)\n",
    "        if 'combined' in df_cref.columns:\n",
    "            df_cref = df_cref.drop(columns=['combined'])\n",
    "        print(f\"{len(df_cref):>5} registros atendem operadores booleanos da estratégia de busca '{query}'\")\n",
    "\n",
    "        return df_cref\n",
    "\n",
    "    # Recuperar os dados dos principais resultados achados por scores de relevância\n",
    "    # def get_crossref_paginated_results(self, query, total_results, limit=5000):\n",
    "    #     items = []\n",
    "    #     rows_per_query = 1000\n",
    "\n",
    "    #     # Criação do widget de texto para exibir o progresso da paginação\n",
    "    #     progress_widget = wg.Label()\n",
    "    #     display(progress_widget)  # Exibe o widget\n",
    "\n",
    "    #     for offset in range(0, min(limit, total_results), rows_per_query):\n",
    "    #         # Atualiza o widget de texto com o progresso\n",
    "    #         progress_widget.value = f\"Paginando resultados {offset}/{total_results}\"\n",
    "\n",
    "    #         url = \"https://api.crossref.org/works?query.bibliographic\"\n",
    "    #         params = {'query': query, 'rows': rows_per_query, 'offset': offset, 'filter': 'type:journal-article'}\n",
    "    #         response = requests.get(url, params=params)\n",
    "    #         if response.status_code != 200:\n",
    "    #             print(f\"Erro na paginação na API CrossRef: {response.status_code}\")\n",
    "    #             print(\"Detalhes do Erro:\", response.text)\n",
    "    #             break\n",
    "    #         else:\n",
    "    #             data = response.json()\n",
    "    #             items.extend(data['message']['items'])\n",
    "\n",
    "    #     # Mensagem final no widget\n",
    "    #     progress_widget.value = \"Paginação completa!\"\n",
    "    #     return items\n",
    "\n",
    "    def get_crossref_paginated_results(self, query, total_results, limit=9999):\n",
    "        items = []\n",
    "        rows_per_query = 1000  # Ajuste conforme necessário para equilibrar carga e desempenho\n",
    "\n",
    "        # Criação do widget de texto para exibir o progresso da paginação\n",
    "        progress_widget = wg.Label()\n",
    "        display(progress_widget)\n",
    "\n",
    "        user_agent = \"gml_search/v0 (marcos@bahlis.com.br)\"\n",
    "\n",
    "        for offset in range(0, min(limit, total_results), rows_per_query):\n",
    "            progress_widget.value = f\"Paginando resultados {offset}/{total_results}\"\n",
    "\n",
    "            url = \"https://api.crossref.org/works\"\n",
    "            params = {\n",
    "                'query.bibliographic': query,\n",
    "                'rows': rows_per_query,\n",
    "                'offset': offset,\n",
    "                'filter': f'type:journal-article,from-pub-date:{self.period[0]}-01-01,until-pub-date:{self.period[1]}-12-31'\n",
    "            }\n",
    "            headers = {'User-Agent': user_agent}\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Erro na paginação na API CrossRef: {response.status_code}\")\n",
    "                print(\"Detalhes do Erro:\", response.text)\n",
    "                break  # Considerar implementar lógica de retry\n",
    "            else:\n",
    "                data = response.json()\n",
    "                items.extend(data['message']['items'])\n",
    "\n",
    "        progress_widget.value = \"Paginação completa!\"\n",
    "        return items\n",
    "\n",
    "\n",
    "    def filter_results_by_dynamic_threshold(self, items, normalized_scores):\n",
    "        # Calculando o valor do percentil 100\n",
    "        threshold = np.percentile(normalized_scores, 100)\n",
    "\n",
    "        # Filtrando os itens cujos scores estão no top 100%\n",
    "        filtered_items = [item for item, score in zip(items, normalized_scores) if score >= threshold]\n",
    "        return filtered_items\n",
    "\n",
    "    def filter_results_by_score(self, items, score_threshold=0.5):\n",
    "        scores = [item['score'] for item in items]\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_scores = scaler.fit_transform(np.array(scores).reshape(-1, 1)).flatten()\n",
    "\n",
    "        filtered_items = [item for item, score in zip(items, normalized_scores) if score >= score_threshold]\n",
    "        return filtered_items\n",
    "\n",
    "    def save_normalized_scores(self, normalized_scores, processed_query):\n",
    "        # Check if the directory exists, if not, create it\n",
    "        directory = '../data/'\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        # Create a DataFrame from normalized scores\n",
    "        scores_df = pd.DataFrame({'normalized_scores': normalized_scores})\n",
    "\n",
    "        # Replace any special characters in the query string to ensure a valid filename\n",
    "        sanitized_query = ''.join(e for e in processed_query if e.isalnum())\n",
    "\n",
    "        # Save normalized scores to disk\n",
    "        filename = f'{directory}crossref_normalized_{sanitized_query}.csv'\n",
    "        scores_df.to_csv(filename, index=False)\n",
    "\n",
    "    ## Métodos para realizar as buscas e recuperar resultados em todas as APIs\n",
    "    def search_and_confirm(self):\n",
    "        print(\"Buscando nas APIs SpringerNature, IEEEXplore, CORE, aguarde...\")\n",
    "        clear_output(wait=True)\n",
    "        while True:\n",
    "            print(f\"\\nQuery: {self.query}\\n\")\n",
    "            total_ieee_results = self.get_ieee_total_results()\n",
    "            print(f\"Total de resultados IEEEXplore: {total_ieee_results}\")\n",
    "            total_springer_results = self.get_springer_total_results()\n",
    "            print(f\"Total de resultados   Springer: {total_springer_results}\")\n",
    "            total_core_results = self.get_core_total_results()\n",
    "            print(f\"Total de resultados       CORE: {total_core_results}\")\n",
    "            # df_cref = self.fetch_crossref_results(self.query)\n",
    "            # total_crossref_results = len(df_cref)\n",
    "            total_crossref_results = self.get_crossref_total_results(self.query)\n",
    "            print(f\"Total de resultados   CrossRef: {total_crossref_results}\")\n",
    "            self.query_history_update(total_springer_results, total_ieee_results, total_core_results, total_crossref_results)\n",
    "            total_results = total_springer_results + total_ieee_results + total_crossref_results\n",
    "            # print(f\"Quantidade antes de deduplicar: {total_results}\")\n",
    "\n",
    "            df_sprg = pd.DataFrame()\n",
    "            df_ieee = pd.DataFrame()\n",
    "            df_core = pd.DataFrame()\n",
    "            df_cref = pd.DataFrame()\n",
    "\n",
    "            # Forçando a renderização da saída\n",
    "            sys.stdout.flush()\n",
    "            data_results = pd.DataFrame()\n",
    "\n",
    "            # Solicita confirmação para continuar\n",
    "            confirmation = input(\"\\nDeseja recuperar os dados dos resultados? (Sim/Não/Alterar): \").lower()\n",
    "            if confirmation in ['','s','sim']:\n",
    "                try:\n",
    "                    # print(\"Iniciando download IEEE...\")\n",
    "                    df_ieee = self.fetch_ieee_results()\n",
    "                    # print(f'DEBUG IEEE outdf {type(df_ieee)} | {len(df_ieee)}')\n",
    "                    if not df_ieee.empty:\n",
    "                        print(f\"{len(df_ieee.index)} registros recuperados na IEEEXplore\")\n",
    "                        print(f\"DEBUG Colunas: {list(df_ieee.keys())}\")\n",
    "                    else:\n",
    "                        print(\"        Não foram recuperados registros na IEEEXplore\")\n",
    "                except Exception as e:\n",
    "                    print(\"        Erro ao rodar fetch_ieee_results()\")\n",
    "                    print(f\"        Mensagem de Erro: {e}\")\n",
    "\n",
    "                try:\n",
    "                    # print(\"Iniciando download SpringerNature...\")\n",
    "                    df_sprg = self.fetch_springer_results()\n",
    "                    # print(f'DEBUG Spring outdf {type(df_sprg)} | {len(df_sprg)}')\n",
    "                    # Verifica se df_sprg NÃO está vazio\n",
    "                    if not df_sprg.empty:\n",
    "                        print(f\"{len(df_sprg)} registros recuperados de SpringerNature\")\n",
    "                        print(f\"DEBUG Colunas: {list(df_sprg.keys())}\")\n",
    "                    else:\n",
    "                        print(\"        Não foram recuperados registros na SpringerNature\")\n",
    "                except Exception as e:\n",
    "                    print(\"        Erro ao rodar fetch_springer_results()\")\n",
    "                    print(f\"        Mensagem de Erro: {e}\")\n",
    "\n",
    "                print(\"\\nBuscas booleanas nos registros relacionados nos agregadores...\")\n",
    "                try:\n",
    "                    # print(\"Iniciando download CORE...\")\n",
    "                    df_core = self.fetch_core_results(self.query, self.period)\n",
    "                    # print(f'DEBUG CORE out DataFrame {type(df_core)}')\n",
    "                    if not df_core.empty:\n",
    "                        print(f\"{len(df_core)} registros recuperados na API CORE\")\n",
    "                        print(f\"DEBUG Colunas: {list(df_core.keys())}\")\n",
    "                    else:\n",
    "                        print(\"        Não foi possível ler dataframe da API CORE\")\n",
    "                except Exception as e:\n",
    "                    print(\"        Erro ao baixar resultados da API CORE\")\n",
    "                    print(f\"        Mensagem de Erro: {e}\")\n",
    "\n",
    "                try:\n",
    "                    df_cref = self.fetch_crossref_filtered_results(self.query)\n",
    "                    # print(f'DEBUG CrossRef out DataFrame {type(df_cref)}')\n",
    "                    if not df_cref.empty:\n",
    "                        print(f\"{len(df_cref)} resultados baixados da CrossRef\")\n",
    "                        print(f\"DEBUG Colunas: {list(df_cref.keys())}\")\n",
    "                    else:\n",
    "                        print(\"        Não foi possível obter resulados da CrossRef\")\n",
    "                except Exception as e:\n",
    "                    print(\"        Erro ao baixar resultados da API CrossRef\")\n",
    "                    print(f\"        Mensagem de Erro: {e}\")\n",
    "                \n",
    "                # Concatenação dos dataframes\n",
    "                print(\"\\nConcatenando resultados...\")\n",
    "                print(f\"{len(df_sprg):>4} resultados na busca booleana na SpringerNature\")\n",
    "                print(f\"{len(df_ieee):>4} resultados na busca booleana na IEEEXplore\")\n",
    "                print(f\"{len(df_core):>4} resultados na busca booleana dentre {total_core_results} relacionados no agregador CORE\")\n",
    "                print(f\"{len(df_cref):>4} resultados na busca booleana dentre {total_crossref_results} relacionados no agregador CrossRef\")\n",
    "                \n",
    "                data_results = pd.concat([df_sprg, df_ieee, df_core, df_cref], ignore_index=True)\n",
    "                print(f\"{len(data_results)} registros recuperados\")\n",
    "                \n",
    "                return total_results, data_results, df_sprg, df_ieee, df_core, df_cref\n",
    "\n",
    "            elif confirmation in ['a','alterar']:\n",
    "                self.query = input(\"Digite a nova query de busca: \")\n",
    "                self.print_search_history()\n",
    "            else:\n",
    "                print(\"Busca cancelada.\")\n",
    "                return 0\n",
    "\n",
    "    # Métodos para deduplicar resultados de busca entre todas APIs\n",
    "    def generate_hash(self, article):\n",
    "        title = article.get(\"title\", \"\").lower()\n",
    "        doi = article.get(\"doi\", \"\").lower()\n",
    "        return hashlib.md5((title + doi).encode()).hexdigest()\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        unique_articles = {}\n",
    "        for article in self.springer_results + self.ieee_results:\n",
    "            hash_key = self.generate_hash(article)\n",
    "            if hash_key not in unique_articles:\n",
    "                unique_articles[hash_key] = article\n",
    "        self.unified_results = list(unique_articles.values())\n",
    "\n",
    "    def fetch_and_store_results(self):\n",
    "        try:\n",
    "            self.springer_results = self.fetch_springer_results()\n",
    "            self.ieee_results = self.fetch_ieee_results()\n",
    "            self.core_results = self.fetch_core_results()\n",
    "        except (HTTPError, ConnectionError, Timeout) as e:\n",
    "            print(f\"        Error occurred on method fetch_and_store_results: {e}\")\n",
    "\n",
    "    def perform_search_and_aggregation(self):\n",
    "        self.fetch_and_store_results()\n",
    "        self.remove_duplicates()\n",
    "        print(f\"Total de artigos únicos após deduplicação: {len(self.unified_results)}\")\n",
    "\n",
    "        # Deduplicar os resultados\n",
    "        self.deduplicate_results()\n",
    "\n",
    "        # Retornar ou processar os resultados unificados\n",
    "        return self.unified_results\n",
    "    \n",
    "    ## Métodos de benchmarking entre singlethread e multithread\n",
    "    def fetch_with_multiprocessing(self):\n",
    "        with Pool(processes=10) as pool:\n",
    "            ranges = [(start,) for start in range(0, 1000, 100)]  # Exemplo: Busca de 0 a 1000 em passos de 100\n",
    "            results_springer = pool.starmap(self.fetch_springer_results, ranges)\n",
    "            results_ieee = pool.starmap(self.fetch_ieee_results, ranges)\n",
    "\n",
    "            # Agora, processar os resultados\n",
    "            # Resultados em listas separadas pode-se combiná-los ou processá-los separadamente\n",
    "            combined_results = []\n",
    "            for result_list in results_springer:\n",
    "                combined_results.extend(result_list)\n",
    "\n",
    "            for result_list in results_ieee:\n",
    "                combined_results.extend(result_list)\n",
    "\n",
    "            # Armazenar os resultados combinados\n",
    "            self.unified_results = combined_results\n",
    "\n",
    "    def fetch_with_multithreading(self):\n",
    "        def worker(method, start):\n",
    "            method(start)\n",
    "\n",
    "        threads = []\n",
    "        for start in range(0, 1000, 100):  # Exemplo: busca em blocos de 100\n",
    "            t_springer = threading.Thread(target=worker, args=(self.fetch_springer_results, start))\n",
    "            t_ieee = threading.Thread(target=worker, args=(self.fetch_ieee_results, start))\n",
    "            threads.extend([t_springer, t_ieee])\n",
    "            t_springer.start()\n",
    "            t_ieee.start()\n",
    "\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "    async def async_fetch(url, session):\n",
    "        async with session.get(url) as response:\n",
    "            if response.status == 200:\n",
    "                return await response.json()\n",
    "            else:\n",
    "                raise Exception(f\"        Erro na recuperação assíncrona de dados: Status code {response.status}\")\n",
    "\n",
    "    async def fetch_with_asyncio(self):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = []\n",
    "            for start in range(0, 1000, 100):\n",
    "                url_springer = f\"http://api.springernature.com/openaccess/json?q={self.query}&api_key={self.springer_api_key}&start={start}\"\n",
    "                url_ieee = f\"https://ieeexploreapi.ieee.org/api/v1/search/articles?apikey={self.ieee_api_key}&querytext={self.query}&start_record={start}&max_records=100\"\n",
    "                tasks.append(asyncio.ensure_future(self.async_fetch(url_springer, session)))\n",
    "                tasks.append(asyncio.ensure_future(self.async_fetch(url_ieee, session)))\n",
    "\n",
    "            results = await asyncio.gather(*tasks)\n",
    "\n",
    "            # Resultados em lista para processar e/ou armazenar\n",
    "            self.unified_results = self.process_async_results(results)\n",
    "\n",
    "    def process_async_results(self, results):\n",
    "        # Processar os resultados e retornar uma lista consolidada\n",
    "        combined_results = []\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            if i % 2 == 0:  # Springer results are at even indices\n",
    "                combined_results.extend(self.process_springer_result(result))\n",
    "            else:  # IEEE results are at odd indices\n",
    "                combined_results.extend(self.process_ieee_result(result))\n",
    "\n",
    "        return combined_results\n",
    "\n",
    "    def process_springer_result(self, result):\n",
    "        # Processar resultado individual da Springer\n",
    "        # Limites em https://dev.springernature.com/admin/applications/\n",
    "        springer_processed = []\n",
    "        for record in result.get('records', []):\n",
    "            # Extrai a URL do artigo, se disponível\n",
    "            article_url = next((url_item['value'] for url_item in record.get('url', []) if url_item['value']), None)\n",
    "\n",
    "            # Extrai o resumo, se disponível\n",
    "            abstract = record.get('abstract', {}).get('p', None)\n",
    "\n",
    "            # Extrai os nomes dos criadores, se disponíveis\n",
    "            creators = [creator['creator'] for creator in record.get('creators', [])]\n",
    "\n",
    "            processed_record = {\"title\": record.get(\"title\"), \"doi\": record.get(\"doi\"), \"publicationDate\": record.get(\"publicationDate\"), \"publicationName\": record.get(\"publicationName\"), \"url\": article_url, \"abstract\": abstract, \"creators\": creators}\n",
    "\n",
    "            springer_processed.append(processed_record)\n",
    "        return springer_processed\n",
    "\n",
    "    def process_ieee_result(self, result):\n",
    "        # Processa um resultado IEEE \n",
    "        # Limites em https://developer.ieee.org/apps/mykeys\n",
    "        ieee_processed = []\n",
    "        for article in result.get('articles', []):\n",
    "            authors = [author['full_name'] for author in article.get('authors', {}).get('authors', [])]\n",
    "            ieee_processed.append({\n",
    "                \"title\": article.get(\"title\"),\n",
    "                \"doi\": article.get(\"doi\"),\n",
    "                \"publicationDate\": article.get(\"publication_year\"),\n",
    "                \"publisher\": article.get(\"publisher\"),\n",
    "                \"abstract\": article.get(\"abstract\"),\n",
    "                \"authors\": authors,\n",
    "                \"issn\": article.get(\"issn\"),\n",
    "                \"isbn\": [isbn['value'] for isbn in article.get(\"isbn_formats\", {}).get(\"isbns\", [])],\n",
    "                \"pdf_url\": article.get(\"pdf_url\"),\n",
    "                \"html_url\": article.get(\"html_url\"),\n",
    "                \"conferenceLocation\": article.get(\"conference_location\"),\n",
    "                \"conferenceDates\": article.get(\"conference_dates\"),\n",
    "                \"citing_paper_count\": article.get(\"citing_paper_count\"),\n",
    "                \"download_count\": article.get(\"download_count\"),\n",
    "                \"ieee_terms\": article.get(\"index_terms\", {}).get(\"ieee_terms\", {}).get(\"terms\", [])\n",
    "            })\n",
    "        return ieee_processed\n",
    "    \n",
    "    def measure_execution_time(self, method, *args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        method(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        return end_time - start_time\n",
    "\n",
    "    def benchmark_methods(self):\n",
    "        benchmarks = []\n",
    "\n",
    "        # Medindo o tempo de execução para o método assíncrono\n",
    "        async_time = self.measure_execution_time(self.fetch_with_asyncio)\n",
    "\n",
    "        # Medindo o tempo de execução para o método multithreading\n",
    "        multithreads_time = self.measure_execution_time(self.fetch_with_multithreading)\n",
    "\n",
    "        # Medindo o tempo de execução para o método multiprocessing\n",
    "        multiprocessing_time = self.measure_execution_time(self.fetch_with_multiprocessing)\n",
    "\n",
    "        benchmarks.append({\n",
    "            'execution_times': {\n",
    "                'async_python': async_time,\n",
    "                'multithreads_python': multithreads_time,\n",
    "                'multiprocess_python': multiprocessing_time\n",
    "            },\n",
    "            'request_count': len(self.unified_results),\n",
    "            'time_spent': {\n",
    "                'scraping': async_time + multithreads_time + multiprocessing_time,  # Total time spent\n",
    "                'data_processing': self.measure_execution_time(self.remove_duplicates)\n",
    "            }\n",
    "        })\n",
    "\n",
    "        return benchmarks    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução das buscas nas APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar chaves das APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessar as API keys no sistema local\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Obter caminho absoluto para a pasta home do usuário\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "\n",
    "# Criar caminho completo para o arquivo secrets.json\n",
    "secrets_file_path = os.path.join(home_dir, \"secrets.json\")\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if os.path.exists(secrets_file_path):\n",
    "    # Abra o arquivo secrets.json para leitura\n",
    "    with open(secrets_file_path, 'r') as secrets_file:\n",
    "        secrets = json.load(secrets_file)\n",
    "\n",
    "    # Acessar as chaves de API\n",
    "    springer_api_key = secrets['springer_api_key']\n",
    "    ieee_api_key = secrets['ieee_api_key']\n",
    "    core_api_key = secrets['core_api_key']\n",
    "    sciencedirect_api_key = secrets['sciencedirect_api_key']\n",
    "else:\n",
    "    print(\"ATENÇÃO!!!\")\n",
    "    print(\"O arquivo secrets.json não foi encontrado na pasta home.\")\n",
    "    print(\"Não será possível acessar as páginas de busca de cada API.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificar quantidade de chamadas realizadas e limites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = ApiCallTracker()\n",
    "daily_usage_info = tracker.get_daily_api_usage_info()\n",
    "pprint(daily_usage_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO:\n",
    "# tracker = ApiCallTracker()\n",
    "# daily_limit_springer = 1000  # limite diário API da Springer\n",
    "# daily_limit_ieee = 200       # limite diário API da IEEEXplore\n",
    "\n",
    "# springer_status = tracker.get_api_status(\"springer\", daily_limit_springer)\n",
    "# ieee_status = tracker.get_api_status(\"ieee\", daily_limit_ieee)\n",
    "\n",
    "# print(f\"Springer API - Chamadas feitas: {springer_status['calls_made']}, Chamadas restantes: {springer_status['remaining_calls']}\")\n",
    "# print(f\"IEEE API - Chamadas feitas: {ieee_status['calls_made']}, Chamadas restantes: {ieee_status['remaining_calls']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montar lista de buscas com base nas palavras-chave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GML Innovation Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a instância para buscar uma lista de queries\n",
    "period=[2019,2024]\n",
    "aggregator = UnifiedResearchPaperAggregator(\"\",\n",
    "                                            period, \n",
    "                                            springer_api_key, \n",
    "                                            ieee_api_key, \n",
    "                                            core_api_key,\n",
    "                                            sciencedirect_api_key)\n",
    "\n",
    "# Exemplo sem controle de redundância nos termos daria 31 queries\n",
    "queries = ['\"graph machine learning\"',\n",
    " '\"machine learning\"',\n",
    " 'graph',\n",
    " 'innovation',\n",
    " 'ontology',\n",
    " '\"graph machine learning\" AND \"machine learning\"',\n",
    " '\"graph machine learning\" AND graph',\n",
    " '\"graph machine learning\" AND innovation',\n",
    " '\"graph machine learning\" AND ontology',\n",
    " '\"machine learning\" AND graph',\n",
    " '\"machine learning\" AND innovation',\n",
    " '\"machine learning\" AND ontology',\n",
    " 'graph AND innovation',\n",
    " 'graph AND ontology',\n",
    " 'innovation AND ontology',\n",
    " '\"graph machine learning\" AND \"machine learning\" AND graph',\n",
    " '\"graph machine learning\" AND \"machine learning\" AND innovation',\n",
    " '\"graph machine learning\" AND \"machine learning\" AND ontology',\n",
    " '\"graph machine learning\" AND graph AND innovation',\n",
    " '\"graph machine learning\" AND graph AND ontology',\n",
    " '\"graph machine learning\" AND innovation AND ontology',\n",
    " '\"machine learning\" AND graph AND innovation',\n",
    " '\"machine learning\" AND graph AND ontology',\n",
    " '\"machine learning\" AND innovation AND ontology',\n",
    " 'graph AND innovation AND ontology',\n",
    " '\"graph machine learning\" AND \"machine learning\" AND graph AND innovation',\n",
    " '\"graph machine learning\" AND \"machine learning\" AND graph AND ontology',\n",
    " '\"graph machine learning\" AND \"machine learning\" AND innovation AND ontology',\n",
    " '\"graph machine learning\" AND graph AND innovation AND ontology',\n",
    " '\"machine learning\" AND graph AND innovation AND ontology',\n",
    " '\"graph machine learning\" AND \"machine learning\" AND graph AND innovation AND ontology']\n",
    "\n",
    "queries_redundantes = aggregator.encontrar_queries_redundantes(queries)\n",
    "queries_redundantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = [\"graph machine learning\", \"machine learning\", 'graph', 'innovation', 'ontology']\n",
    "total_possivel = 2**len(concepts)-1\n",
    "\n",
    "# Ordenando as queries com base no número de termos\n",
    "query_list = aggregator.generate_queries(concepts)\n",
    "print(f\"{len(query_list)} queries não redundantes, de {total_possivel} possíveis\")\n",
    "query_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AHP immunotherapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a instância para buscar uma lista de queries\n",
    "period=[2019,2024]\n",
    "aggregator = UnifiedResearchPaperAggregator(\"\",\n",
    "                                            period, \n",
    "                                            springer_api_key, \n",
    "                                            ieee_api_key, \n",
    "                                            core_api_key,\n",
    "                                            sciencedirect_api_key)\n",
    "\n",
    "# Exemplo sem controle de redundância nos termos daria 31 queries\n",
    "queries = ['AHP',\n",
    "           '\"Analytic Hierarchy Process\"',\n",
    "           '\"immunotherapy\"',\n",
    "           '\"immunobiologicals\"',\n",
    "           '\"biopharmaceutical development\"',\n",
    "           '\"success factors\"',\n",
    "        ]\n",
    "\n",
    "queries_redundantes = aggregator.encontrar_queries_redundantes(queries)\n",
    "queries_redundantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = [\"analytic hierarchy process\", \n",
    "            \"immunotherapy\", \n",
    "            'immunobiological',\n",
    "            'immunobiologicals',\n",
    "            'Immunobiologicals',\n",
    "            '\"success factors\"']\n",
    "total_possivel = 2**len(concepts)-1\n",
    "\n",
    "# Ordenando as queries com base no número de termos\n",
    "query_list = aggregator.generate_queries(concepts)\n",
    "print(f\"{len(query_list)} queries não redundantes, de {total_possivel} possíveis\")\n",
    "query_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantificar resultados de cada estratégia de busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processar múltiplas consultas com ESC a cada uma pra não baixar dados ainda\n",
    "df = aggregator.process_multiple_queries(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir o DataFrame com o histórico de buscas\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ordem_alfabetica = df.sort_values(by='query', ascending=True)\n",
    "# df_ordem_alfabetica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetando a ordenação do índice do dataframe para ordem decrescente de resultados\n",
    "df_ordem_quantitativa = df.sort_values(by='ieee_results', ascending=False)\n",
    "df_ordem_quantitativa.reset_index(drop=True, inplace=True)\n",
    "df_ordem_quantitativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_index = pd.DataFrame(df_ordem_quantitativa.values, columns=df_ordem_quantitativa.columns)\n",
    "df_no_index.set_index('data',inplace=True)\n",
    "df_no_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo o DataFrame para formato LaTeX com style do pandas não fica bom\n",
    "# tabela_latex = df_no_index.style.to_latex()\n",
    "# print(tabela_latex)\n",
    "\n",
    "def generate_latex_table(df):\n",
    "    # Adicione mais formatações se necessário\n",
    "    latex_table = \"\"\"\n",
    "\\\\begin{table}[htbp]\n",
    "    \\\\centering\n",
    "    \\\\caption{Result of searches in the IEEE and Springer-Nature databases and in the CORE and CrossRef aggregators on 01/29/2024}\n",
    "    \\\\adjustbox{max width=\\\\textwidth}{\n",
    "    \\\\begin{tabular}{l*{4}{S[table-format=7]}}\n",
    "        \\\\textbf{Query} & \\\\textbf{IEEE} & \\\\textbf{Springer} & \\\\textbf{CORE} & \\\\textbf{Crossref} \\\\\\\\\n",
    "\"\"\"\n",
    "    \n",
    "    # Itera sobre as linhas do DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Adiciona uma linha à tabela LaTeX\n",
    "        latex_table += f\"        {row['query']} & {row['ieee_results']} & {row['springer_results']} & {row['core_results']} & {row['crossref_results']} \\\\\\\\\\n\"\n",
    "\n",
    "    # Adiciona o restante do código LaTeX\n",
    "    latex_table += \"\"\"    \\\\end{tabular}\n",
    "    }\n",
    "\\\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "    # Retorna a tabela LaTeX gerada\n",
    "    return latex_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 'df' com colunas 'Query', 'IEEE', 'Springer', 'CORE' e 'Crossref'\n",
    "tabela_latex = generate_latex_table(df_ordem_quantitativa[['query','ieee_results','springer_results','core_results','crossref_results']])\n",
    "print(tabela_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de APIs com suas informações\n",
    "apis_info = [\n",
    "    {\"name\": \"Springer\", \"api_key\": springer_api_key, \n",
    "     \"base_url\": \"http://api.springernature.com/openaccess/json\", \"daily_limit\": 5000},\n",
    "    {\"name\": \"IEEE\", \"api_key\": ieee_api_key, \n",
    "     \"base_url\": \"https://ieeexploreapi.ieee.org/api/v1/search/articles\", \"daily_limit\": 200},\n",
    "     ]\n",
    "\n",
    "searcher = ResearchPaperSearcher()\n",
    "hours, minutes = searcher.renew_limits_time()\n",
    "print(f\"{hours}h:{minutes}min para renovar os limites de downloads das APIs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estratégia de busca escolhida para baixar conteúdos completos\n",
    "\n",
    "Conhecidas quantidades de resultados dos termos e combinações, optou-se pela estratégia de busca a seguir para realizar o processamento de obtenção do conteúdo completo dos achados de busca: \n",
    "\n",
    "<b><center>\"machine learning\" AND innovation AND ontology</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a instância para buscar uma lista de queries\n",
    "query = '\"machine learning\" AND innovation AND ontology'\n",
    "period = [2019,2024]\n",
    "perion = None\n",
    "aggregator = UnifiedResearchPaperAggregator(query,\n",
    "                                            period, \n",
    "                                            springer_api_key, \n",
    "                                            ieee_api_key, \n",
    "                                            core_api_key,\n",
    "                                            sciencedirect_api_key)\n",
    "\n",
    "# Buscar e confirmar a quantidade de resultados\n",
    "total_results, data_results, df_sprg, df_ieee, df_core, df_cref = aggregator.search_and_confirm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(data_results)} resultados agregados das diversas APIs\")\n",
    "print(data_results['API'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar resultados em disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df):\n",
    "    pathdata=os.path.join(os.getcwd(),'../../../../','data/csv')\n",
    "    filepath=os.path.join(pathdata,'df_resultados_busca.csv')\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(\"DataFrame com resultados de busca salvo com sucesso.\")\n",
    "\n",
    "save_dataframe(data_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reiniciar de dados salvos previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathfilename=os.path.join(os.getcwd(),'../../../../','data/csv','df_resultados_busca.csv')\n",
    "data_results = pd.read_csv(pathfilename)\n",
    "data_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotagem do quantitativos de achados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def filter_last_five_years(df, date_column):\n",
    "    # Garantir que a coluna 'DATE' seja do tipo datetime\n",
    "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "\n",
    "    # Extrair o ano da coluna 'DATE'\n",
    "    df['YEAR'] = df[date_column].dt.year\n",
    "\n",
    "    # Determinar o ano atual\n",
    "    current_year = datetime.now().year\n",
    "\n",
    "    # Filtrar para os últimos 6 anos\n",
    "    filtered_df = df[(df['YEAR'] >= (current_year - 5)) & (df['YEAR'] <= current_year)]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def process_date_column(df, date_column):\n",
    "    # Converter 'DATE' para datetime e extrair o ano\n",
    "    df['YEAR'] = pd.to_datetime(df[date_column], errors='coerce').dt.year\n",
    "    return df.dropna(subset=['YEAR'])\n",
    "\n",
    "def generate_yearly_frequency_graph(df, year_column):\n",
    "    # Contar a frequência de cada ano\n",
    "    year_counts = df[year_column].value_counts().sort_index()\n",
    "\n",
    "    # Verificar o ano mínimo e máximo e converter para inteiros\n",
    "    min_year = int(df[year_column].min())\n",
    "    max_year = int(df[year_column].max())\n",
    "\n",
    "    # Garantir que todos os anos no intervalo estejam representados\n",
    "    all_years = pd.Series(index=pd.Index(range(min_year, max_year + 1), name='YEAR'), dtype='int').fillna(0)\n",
    "    year_counts = all_years.add(year_counts, fill_value=0)\n",
    "\n",
    "    # Converter a Series em um DataFrame para facilitar o plot\n",
    "    year_counts_df = year_counts.reset_index()\n",
    "    year_counts_df.columns = ['YEAR', 'COUNT']\n",
    "\n",
    "    # Determinar o valor máximo de contagem para ajustar o eixo y\n",
    "    max_count = year_counts_df['COUNT'].max()\n",
    "    y_axis_limit = max_count + 0.1 * max_count  # Aumentar em 10% para espaço extra\n",
    "\n",
    "    # Criar o gráfico de barras usando Plotly\n",
    "    fig = px.bar(year_counts_df, x='YEAR', y='COUNT', \n",
    "                 title='Frequency of articles by recent years',\n",
    "                 labels={'COUNT': 'Number of Articles', 'YEAR': 'Ano'},\n",
    "                 text='COUNT')  # Adiciona rótulos de texto nas barras\n",
    "\n",
    "    # Atualizar os rótulos de texto das barras\n",
    "    fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "\n",
    "    # Personalizar a fonte dos rótulos de texto e ajustar o eixo y\n",
    "    fig.update_layout(\n",
    "        uniformtext_minsize=18, uniformtext_mode='hide', \n",
    "        xaxis=dict(title='Year', tickmode='linear', dtick=1),\n",
    "        yaxis=dict(title='Number of Articles', range=[0, y_axis_limit])\n",
    "    )\n",
    "\n",
    "    fig.show(renderer=\"notebook\")\n",
    "\n",
    "    return year_counts_df\n",
    "\n",
    "def generate_frequency_date(df, date_column):\n",
    "    # Criar uma cópia independente para evitar SettingWithCopyWarning\n",
    "    df_filtered = df[df[date_column].apply(lambda x: bool(re.match(r'^\\d{4}-\\d{2}-\\d{2}$', str(x))))].copy()\n",
    "\n",
    "    # Extrair o ano de cada data\n",
    "    df_filtered['YEAR'] = df_filtered[date_column].str.split('-').str[0]\n",
    "\n",
    "    # Contar a frequência de cada ano\n",
    "    year_counts = df_filtered['YEAR'].value_counts().sort_index()\n",
    "\n",
    "    # Converter a Series em um DataFrame para facilitar o plot\n",
    "    year_counts_df = year_counts.reset_index()\n",
    "    year_counts_df.columns = ['YEAR', 'COUNT']\n",
    "\n",
    "    # Criar o gráfico de barras usando Plotly\n",
    "    fig = px.bar(year_counts_df, x='YEAR', y='COUNT', \n",
    "                 title='Frequência de Artigos por Ano',\n",
    "                 labels={'COUNT': 'Número de Artigos', 'YEAR': 'Ano'},\n",
    "                 text='COUNT')  # Adiciona rótulos de texto nas barras\n",
    "\n",
    "    # Atualizar os rótulos de texto das barras\n",
    "    fig.update_traces(texttemplate='%{text}', textposition='outside',\n",
    "                      textfont={'size': 14, 'color': 'black', 'family': 'Arial, sans-serif'})\n",
    "\n",
    "    # Atualiza layout para acomodar os rótulos de texto\n",
    "    fig.update_layout(uniformtext_minsize=14, uniformtext_mode='hide')\n",
    "\n",
    "    # fig.show()\n",
    "    fig.show(renderer=\"notebook\")\n",
    "\n",
    "    return year_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results = filter_last_five_years(data_results, 'DATE')\n",
    "l6y_df = generate_yearly_frequency_graph(filtered_results, 'YEAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = l6y_df['COUNT'].sum()\n",
    "print(f\"{int(total)} resultados\")\n",
    "l6y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização dos conteúdos de texto nos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = [2019,2024]\n",
    "filtered_df = data_results.query(f'YEAR >= {period[0]} and YEAR <= {period[1]}')\n",
    "print(len(filtered_df))\n",
    "print(list(filtered_df.keys()))\n",
    "filtered_df[['API','YEAR','TITLE','OPENACCESS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização em Títulos de cada palavra-chave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innovation_recent_results = filtered_df[filtered_df[['TITLE', 'ABSTRACT']].apply(lambda x: x.str.contains('innovation', \n",
    "                                                                                                                    case=False, \n",
    "                                                                                                                    na=False)).any(axis=1)]\n",
    "print(f\"{len(innovation_recent_results)} resultados com 'innovation' no título ou resumo, dentro dos cinco últimos anos completos e do atual ano corrente\")\n",
    "\n",
    "innovation_recent_results[['YEAR', 'TITLE','TIPO','API']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'ontology'\n",
    "ontology_recent_results = filtered_df[filtered_df[['TITLE', 'ABSTRACT']].apply(lambda x: x.str.contains(term, \n",
    "                                                                                                                    case=False, \n",
    "                                                                                                                    na=False)).any(axis=1)]\n",
    "print(f\"{len(ontology_recent_results)} resultados com '{term}' no título ou resumo, dentro dos cinco últimos anos completos e do atual ano corrente\")\n",
    "\n",
    "ontology_recent_results[['YEAR', 'TITLE','TIPO','API']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['ontology', 'innovation']\n",
    "\n",
    "def all_terms_present(text, terms):\n",
    "    if pd.isna(text):\n",
    "        return False  # Retorna False se o texto for NaN\n",
    "    return all(term.lower() in text.lower() for term in terms)\n",
    "\n",
    "# Filtrar linhas que contêm todos os termos em 'TITLE' ou 'ABSTRACT'\n",
    "innontol_recent_results = filtered_df[\n",
    "    filtered_df[['TITLE', 'ABSTRACT']]\n",
    "    .apply(lambda row: all_terms_present(row['TITLE'], terms) or \n",
    "                      all_terms_present(row['ABSTRACT'], terms), axis=1)\n",
    "]\n",
    "\n",
    "print(f\"{len(innontol_recent_results)} resultados com '{terms}' no título ou resumo, dentro dos cinco últimos anos completos e do atual ano corrente\")\n",
    "\n",
    "innontol_recent_results[['YEAR', 'TITLE', 'ABSTRACT', 'TIPO', 'API']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busca e extração de textos completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "# import PyPDF2\n",
    "# from io import BytesIO\n",
    "# import os\n",
    "\n",
    "# class ArticleExtractor:\n",
    "#     def __init__(self):\n",
    "#         self.content_identifiers = [\"main\", \n",
    "#                                     \"content\", \n",
    "#                                     \"article\", \n",
    "#                                     \"post\", \n",
    "#                                     \"text\", \n",
    "#                                     \"body\", \n",
    "#                                     \"maincontent\", \n",
    "#                                     \"box-abstract-main\"]\n",
    "        \n",
    "#         self.headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "#     def find_main_content(self, soup):\n",
    "#         for identifier in self.content_identifiers:\n",
    "#             content = soup.find(id=identifier)\n",
    "#             if content:\n",
    "#                 return content\n",
    "\n",
    "#             contents = soup.find_all(class_=identifier)\n",
    "#             if contents:\n",
    "#                 return max(contents, key=lambda x: len(x.text))\n",
    "\n",
    "#         return soup.body\n",
    "\n",
    "#     def extract_text_recursively(self, element):\n",
    "#         text_elements = []\n",
    "\n",
    "#         if isinstance(element, NavigableString):\n",
    "#             text = element.strip()\n",
    "#             if text:\n",
    "#                 text_elements.append(text)\n",
    "#         elif isinstance(element, Tag):\n",
    "#             if element.name in ['h1', 'h2', 'strong']:\n",
    "#                 subtitle = element.get_text(strip=True)\n",
    "#                 if subtitle:\n",
    "#                     text_elements.append(subtitle + \":\\n\")\n",
    "#             elif element.name in ['div', 'p']:\n",
    "#                 for child in element.children:\n",
    "#                     child_text = self.extract_text_recursively(child)\n",
    "#                     if child_text:\n",
    "#                         text_elements.append(child_text)\n",
    "\n",
    "#         return ' '.join(text_elements)\n",
    "\n",
    "#     def fetch_full_text(self, url):\n",
    "#         try:\n",
    "#             response = requests.get(url, headers=self.headers)\n",
    "#             response.raise_for_status()\n",
    "\n",
    "#             if 'application/pdf' in response.headers.get('Content-Type', ''):\n",
    "#                 return self.extract_text_from_pdf(response.content)\n",
    "#             else:\n",
    "#                 soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#                 main_content = self.find_main_content(soup)\n",
    "#                 full_text = self.extract_text_recursively(main_content)\n",
    "#                 return os.linesep.join([line for line in full_text.splitlines() if line.strip()])\n",
    "\n",
    "#         except requests.RequestException as e:\n",
    "#             return f\"Erro ao acessar {url}: {e}\"\n",
    "\n",
    "#     def extract_text_from_pdf(self, pdf_content):\n",
    "#         try:\n",
    "#             with BytesIO(pdf_content) as pdf_file:\n",
    "#                 reader = PyPDF2.PdfFileReader(pdf_file)\n",
    "#                 return ' '.join([reader.getPage(i).extractText() for i in range(reader.numPages)])\n",
    "#         except Exception as e:\n",
    "#             return f\"Erro ao extrair texto do PDF: {e}\"\n",
    "\n",
    "# # Exemplo de uso\n",
    "# extractor = ArticleExtractor()\n",
    "# for index, row in innontol_recent_results.iterrows():\n",
    "#     if row['URL'] and isinstance(row['URL'], str):\n",
    "#         full_text = extractor.fetch_full_text(row['URL'])\n",
    "#         print(f\"=\"*120)\n",
    "#         print(f\"Texto completo para:\\n{row['TITLE']}:\")\n",
    "#         pprint(full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Função para construir URL a partir de um DOI\n",
    "def doi_para_url(doi):\n",
    "    return f\"https://doi.org/{doi}\"\n",
    "\n",
    "# Função para extrair o conteúdo de um artigo com base no seu DOI\n",
    "def extrair_conteudo_por_doi(doi):\n",
    "    try:\n",
    "        # Construir URL a partir do DOI fornecido\n",
    "        url = doi_para_url(doi)\n",
    "        \n",
    "        # Fazer requisição GET para a URL\n",
    "        resposta = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        secoes = {}\n",
    "        # Verificar se a requisição foi bem-sucedida\n",
    "        if resposta.status_code == 200:\n",
    "            # Fazer parsing do conteúdo HTML\n",
    "            soup = BeautifulSoup(resposta.text, 'html.parser')\n",
    "            # Verificar domínio na URL final após redirecionamentos\n",
    "            dominio = resposta.url.split('/')[2]\n",
    "            print(f\"DOI {doi} extraindo de: {dominio}\")\n",
    "            # Aplicar lógica condicional baseada no domínio\n",
    "            if \"springer.com\" in dominio:\n",
    "                titulo_artigo = soup.find('h1', class_='c-article-title').text.strip()\n",
    "                # print(f\"Título: {titulo_artigo}\")\n",
    "                secoes['Titulo'] = titulo_artigo\n",
    "                # Inicializar dicionário para armazenar o título e conteúdo das seções\n",
    "                discard = ['Notes','References','Author information','Inline Recommendations', 'Funding','Rights and permissions', 'About this article','Additional information']\n",
    "                # Encontrar todas as seções com um atributo data-title\n",
    "                for secao in soup.find_all('section', attrs={\"data-title\": True}):\n",
    "                    titulo_secao = secao['data-title']\n",
    "                    if titulo_secao not in discard:\n",
    "                        conteudo_secao = secao.get_text(strip=True).lstrip(titulo_secao)\n",
    "                        conteudo_secao = re.sub(r'Footnote\\d+', ' ', conteudo_secao)\n",
    "                        conteudo_secao = re.sub(r'  ', ' ', conteudo_secao)\n",
    "                        # Armazenar conteúdo no dicionário usando o título da seção como chave\n",
    "                        secoes[titulo_secao] = conteudo_secao\n",
    "            elif \"plos.org\" in dominio:\n",
    "                # Procurar título do artigo\n",
    "                titulo_artigo = soup.find('h1', id='artTitle').text.strip()\n",
    "                if titulo_artigo:\n",
    "                    # print(f\"Título: {titulo_artigo}\")\n",
    "                    secoes['Titulo'] = titulo_artigo\n",
    "                else:\n",
    "                    print(\"Erro ao extrair título da PLOS\")\n",
    "                # Encontrar todas as divs que contêm a classe \"toc-section\"\n",
    "                secoes_div = soup.find_all('div', class_=lambda x: x and 'toc-section' in x)\n",
    "                if secoes_div:\n",
    "                    discard = ['References','Supporting information']\n",
    "                    for secao_div in secoes_div:\n",
    "                        # Extrair o título da seção procurando por um elemento <h2> dentro da div\n",
    "                        titulo_secao = secao_div.find('h2')\n",
    "                        if titulo_secao:\n",
    "                            titulo_secao_texto = titulo_secao.text.strip()\n",
    "                            if titulo_secao_texto and titulo_secao_texto not in discard:\n",
    "                                # print(f\"Título de Seção: {titulo_secao_texto}\")\n",
    "                                # Procurar por subseções dentro da seção atual\n",
    "                                subsecoes = secao_div.find_all('h3')\n",
    "                                if not subsecoes:  # Se não houver subseções, imprimir o conteúdo da seção\n",
    "                                    conteudo_secao = ' '.join([paragrafo.text.strip() for paragrafo in secao_div.find_all('p')])\n",
    "                                    # pprint(conteudo_secao, width=110)\n",
    "                                    secoes[titulo_secao_texto] = conteudo_secao\n",
    "                                else:  # Processar cada subseção individualmente\n",
    "                                    for subsecao in subsecoes:\n",
    "                                        titulo_subsecao = subsecao.text.strip()\n",
    "                                        # print(f\"{titulo_subsecao}\")\n",
    "                                        conteudo_subsecao = \"\"\n",
    "                                        elemento_atual = subsecao.find_next_sibling()\n",
    "                                        while elemento_atual and elemento_atual.name not in ['h2', 'h3']:\n",
    "                                            if elemento_atual.name == 'p':\n",
    "                                                conteudo_subsecao += elemento_atual.text.strip() + \" \"\n",
    "                                            elemento_atual = elemento_atual.find_next_sibling()\n",
    "                                        if conteudo_subsecao:  # Se houver conteúdo na subseção, imprime-o\n",
    "                                            # pprint(conteudo_subsecao, width=110)\n",
    "                                            secoes[titulo_subsecao] = conteudo_subsecao  \n",
    "                else:\n",
    "                    print(\"Erro ao extrair seções da PLOS\")\n",
    "            else:\n",
    "                return \"Domínio não suportado ou não identificado.\"\n",
    "            return secoes\n",
    "        else:\n",
    "            return \"Falha ao acessar o artigo através do DOI.\"\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao processar o DOI {doi}: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteudos = {}\n",
    "for doi in innontol_recent_results['DOI']:\n",
    "    print(\"=\"*110)\n",
    "    conteudo = extrair_conteudo_por_doi(doi)\n",
    "    conteudos[doi]=conteudo\n",
    "    if conteudo:\n",
    "        try:\n",
    "            for secion, contend in conteudo.items():\n",
    "                print(\"-\"*110)\n",
    "                print(f\"{secion}:\")\n",
    "                pprint(contend.replace('\\xa0', ' '), width=110)\n",
    "        except:\n",
    "            print(\"Erro em:\")\n",
    "            print(conteudo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, spacy\n",
    "from collections import Counter\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Carregar o modelo de linguagem do spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extrair_principais_pontos(conteudos):\n",
    "    pontos_chave = {}\n",
    "    for id_artigo, secoes in conteudos.items():\n",
    "        pontos_chave[id_artigo] = {}\n",
    "        for titulo_secao, conteudo in secoes.items():\n",
    "            if titulo_secao not in ['Titulo', 'Abstract']:  # Ignora título e resumo\n",
    "                # Resumo da seção\n",
    "                resumo = resumir_secao(conteudo)\n",
    "                # Conceitos-chave\n",
    "                conceitos_chave = extrair_conceitos_chave(conteudo)\n",
    "                # Síntese de métodos e resultados (se aplicável)\n",
    "                metodos_resultados = sintetizar_metodos_resultados(conteudo) if titulo_secao in ['Methods', 'Results'] else ''\n",
    "                # Síntese de conclusões (se aplicável)\n",
    "                conclusoes = sintetizar_conclusoes(conteudo) if titulo_secao == 'Conclusions' else ''\n",
    "                pontos_chave[id_artigo][titulo_secao] = {\n",
    "                    \"Resumo\": resumo,\n",
    "                    \"Conceitos-Chave\": conceitos_chave,\n",
    "                    \"Métodos e Resultados\": metodos_resultados,\n",
    "                    \"Conclusões\": conclusoes\n",
    "                }\n",
    "    return pontos_chave\n",
    "\n",
    "def extrair_principais_sentencas(conteudo, n_sentencas=3):\n",
    "    \"\"\"\n",
    "    Este método analisa o conteúdo de uma seção, calcula a pontuação de cada sentença com base na frequência das palavras-chave (excluindo stopwords e pontuações), e retorna as sentenças mais representativas como um resumo da seção. Você pode ajustar o número de sentenças a serem incluídas no resumo alterando o parâmetro n_sentencas.\n",
    "    \"\"\"\n",
    "    # Processar o conteúdo da seção\n",
    "    doc = nlp(conteudo)\n",
    "    \n",
    "    # Contar a frequência das palavras-chave (excluindo stopwords e pontuações)\n",
    "    palavras = [token.text.lower() for token in doc if token.text.lower() not in STOP_WORDS and token.pos_ != 'PUNCT']\n",
    "    frequencia_palavras = Counter(palavras)\n",
    "    max_frequencia = frequencia_palavras.most_common(1)[0][1]\n",
    "\n",
    "    # Normalizar as frequências das palavras\n",
    "    for palavra in frequencia_palavras:\n",
    "        frequencia_palavras[palavra] = (frequencia_palavras[palavra] / max_frequencia)\n",
    "    \n",
    "    # Calcular a pontuação das sentenças baseada na frequência das palavras\n",
    "    pontuacao_sentencas = {}\n",
    "    for sent in doc.sents:\n",
    "        for palavra in sent:\n",
    "            if palavra.text.lower() in frequencia_palavras.keys():\n",
    "                if sent in pontuacao_sentencas.keys():\n",
    "                    pontuacao_sentencas[sent] += frequencia_palavras[palavra.text.lower()]\n",
    "                else:\n",
    "                    pontuacao_sentencas[sent] = frequencia_palavras[palavra.text.lower()]\n",
    "    \n",
    "    # Ordenar as sentenças pela pontuação e selecionar as n_sentencas mais representativas\n",
    "    resumido = sorted(pontuacao_sentencas.keys(), key=lambda x: pontuacao_sentencas[x], reverse=True)[:n_sentencas]\n",
    "    resumo = ' '.join([sent.text for sent in resumido])\n",
    "\n",
    "    return resumo\n",
    "\n",
    "def resumir_secao(conteudo):\n",
    "    # Pré-processamento: remover caracteres especiais e números\n",
    "    conteudo = re.sub('[^a-zA-Z]', ' ', conteudo)\n",
    "    conteudo = re.sub('\\s+', ' ', conteudo)\n",
    "    # Dividir o conteúdo em sentenças\n",
    "    sentencas = sent_tokenize(conteudo)\n",
    "    # Processar o conteúdo usando spaCy para extrair entidades nomeadas e palavras-chave\n",
    "    doc = nlp(conteudo.lower())  # Processar o conteúdo em minúsculas para normalização\n",
    "    # Contar a frequência das palavras, excluindo stopwords e considerando entidades nomeadas\n",
    "    palavras = [token.text for token in doc if token.text not in STOP_WORDS and not token.is_punct and not token.is_space]\n",
    "    frequencia_palavras = Counter(palavras)\n",
    "    # Identificar e dar peso extra a entidades nomeadas\n",
    "    entidades_nomeadas = list(doc.ents)\n",
    "    for entidade in entidades_nomeadas:\n",
    "        if entidade.text in frequencia_palavras:\n",
    "            frequencia_palavras[entidade.text] *= 2  # Aumentar a importância das entidades nomeadas\n",
    "    max_frequencia = max(frequencia_palavras.values())\n",
    "    # Calcular a pontuação de cada sentença com base na frequência das palavras\n",
    "    pontuacao_sentencas = {}\n",
    "    for sentenca in sentencas:\n",
    "        doc_sentenca = nlp(sentenca.lower())\n",
    "        for token in doc_sentenca:\n",
    "            if token.text in frequencia_palavras.keys():\n",
    "                if sentenca in pontuacao_sentencas:\n",
    "                    pontuacao_sentencas[sentenca] += frequencia_palavras[token.text]\n",
    "                else:\n",
    "                    pontuacao_sentencas[sentenca] = frequencia_palavras[token.text]\n",
    "    # Ordenar as sentenças por pontuação e selecionar as top N\n",
    "    resumo_sentencas = sorted(pontuacao_sentencas, key=pontuacao_sentencas.get, reverse=True)[:3]\n",
    "    # Combinar as sentenças selecionadas no resumo final\n",
    "    resumo = '. '.join(resumo_sentencas)\n",
    "    return resumo\n",
    "\n",
    "def extrair_conceitos_chave(conteudo):\n",
    "    # Pré-processamento: remover caracteres especiais e números, exceto espaços\n",
    "    conteudo = re.sub('[^a-zA-Z\\s]', '', conteudo)\n",
    "    # Processar o conteúdo com spaCy\n",
    "    doc = nlp(conteudo)\n",
    "    # Identificar entidades nomeadas, substantivos e adjetivos\n",
    "    entidades = [entidade.text.lower() for entidade in doc.ents]\n",
    "    substantivos_adj = [token.text.lower() for token in doc if token.pos_ in ['NOUN', 'ADJ'] and token.text.lower() not in STOP_WORDS]\n",
    "    # Combinar entidades e substantivos/adjetivos, contabilizar e identificar os mais frequentes\n",
    "    termos_chave = entidades + substantivos_adj\n",
    "    frequencia_termos = Counter(termos_chave)\n",
    "    # Selecionar os 10 termos mais frequentes como conceitos-chave\n",
    "    conceitos_chave = frequencia_termos.most_common(10)\n",
    "    # Retornar apenas os termos (ignorar as contagens)\n",
    "    return [termo for termo, _ in conceitos_chave]\n",
    "\n",
    "def sintetizar_metodos_resultados(conteudo):\n",
    "    # Processar o texto com spaCy\n",
    "    doc = nlp(conteudo)\n",
    "    # Focar em verbos para métodos e substantivos para resultados\n",
    "    metodos = []\n",
    "    resultados = []\n",
    "    secao_atual = ''\n",
    "    for token in doc:\n",
    "        # Identificar métodos por sentenças que contenham palavras-chave relacionadas à metodologia\n",
    "        if token.text in ['methods', 'methodology', 'materials']:\n",
    "            secao_atual = 'metodos'\n",
    "            continue\n",
    "        elif token.text in ['Results', 'Findings']:\n",
    "            secao_atual = 'resultados'\n",
    "            continue\n",
    "        # Extrair informações baseadas na seção atual e partes do discurso\n",
    "        if secao_atual == 'metodos' and token.pos_ == 'VERB':\n",
    "            metodos.append(token.lemma_)  # Usar a forma base (lemma) para evitar repetições\n",
    "        elif secao_atual == 'resultados' and token.pos_ in ['NOUN', 'NUM']:\n",
    "            resultados.append(token.text.lower())\n",
    "    # Remover stopwords dos resultados e duplicatas\n",
    "    resultados = [resultado for resultado in resultados if resultado not in STOP_WORDS]\n",
    "    metodos_unicos = list(set(metodos))\n",
    "    resultados_unicos = list(set(resultados))\n",
    "    # Sintetizar informações extraídas\n",
    "    if metodos_unicos != []:\n",
    "        sintese_metodos = \"Termos utilizados em métodos: \" + \", \".join(metodos_unicos) + \".\"\n",
    "    else:\n",
    "        sintese_metodos = []\n",
    "    if resultados_unicos != []:\n",
    "        sintese_resultados = \"Termos utilizados em resultados: \" + \", \".join(resultados_unicos) + \".\"\n",
    "    else:\n",
    "        sintese_resultados = []\n",
    "    return sintese_metodos, sintese_resultados\n",
    "\n",
    "def sintetizar_conclusoes(conteudo):\n",
    "    # Processar o texto com spaCy\n",
    "    doc = nlp(conteudo)\n",
    "    # Identificar conclusões: procurar por sentenças chave com base em palavras e frases comuns de conclusão\n",
    "    palavras_chave = ['conclude', 'conclusion', 'summarize', 'summary', 'finally', 'implications', 'future work']\n",
    "    conclusoes = []\n",
    "    for sent in doc.sents:\n",
    "        # Verificar se a sentença contém alguma das palavras-chave (lemmatização para capturar variações)\n",
    "        if any(word.lemma_ in palavras_chave for word in sent):\n",
    "            conclusoes.append(sent.text)\n",
    "    # Sintetizar as sentenças de conclusão encontradas\n",
    "    sintese_conclusoes = \" \".join(conclusoes)\n",
    "    return sintese_conclusoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrar extrações de resumos e conceitos-chave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As sentenças mais representativas em um texto são definidas com base em sua relevância e capacidade de capturar os principais pontos ou conceitos discutidos no texto. No contexto do método de sumarização extrativa que descrevi anteriormente, a relevância de uma sentença é determinada por uma combinação de fatores, incluindo a frequência das palavras-chave e a presença de entidades nomeadas. Aqui está como isso é feito:\n",
    "\n",
    "### <b>Frequência das Palavras-Chave</b>\n",
    "#### Identificação de Palavras-Chave: \n",
    "Inicialmente, as palavras-chave são identificadas no texto processando-o para remover as stopwords (palavras comuns que não contribuem significativamente para o significado da sentença, como \"e\", \"o\", \"em\") e pontuações. As palavras restantes, especialmente substantivos, verbos significativos e adjetivos, são consideradas palavras-chave potenciais.\n",
    "#### Cálculo da Frequência: \n",
    "A frequência de cada palavra-chave é calculada, ou seja, quantas vezes cada palavra-chave aparece no texto. Isso ajuda a identificar os termos que são mais discutidos ou enfatizados no texto.\n",
    "\n",
    "#### Normalização: \n",
    "As frequências das palavras-chave são normalizadas em relação à palavra mais frequente para equilibrar a distribuição e evitar que palavras excessivamente frequentes dominem o resumo.\n",
    "\n",
    "### <b>Pontuação das Sentenças</b>\n",
    "#### Atribuição de Pontuação: \n",
    "Cada sentença no texto é avaliada com base na presença e frequência das palavras-chave que contém. A pontuação de uma sentença é a soma das frequências normalizadas das palavras-chave que aparecem na sentença.\n",
    "\n",
    "#### Relevância de Entidades Nomeadas: \n",
    "Sentenças contendo entidades nomeadas (como nomes de pessoas, organizações, locais) podem receber uma pontuação adicional ou ser consideradas mais relevantes, dependendo do contexto e da implementação, porque tais entidades geralmente representam pontos focais importantes no texto.\n",
    "\n",
    "### <b>Seleção das Sentenças Mais Representativas</b>\n",
    "#### Ordenação e Seleção: \n",
    "As sentenças são ordenadas de acordo com suas pontuações, da mais alta para a mais baixa. As sentenças com as maiores pontuações são consideradas as mais representativas do texto, pois presumivelmente contêm uma concentração maior das ideias principais e tópicos discutidos.\n",
    "#### Extração do Resumo: \n",
    "Um número pré-definido das sentenças mais bem pontuadas é selecionado para compor o resumo. Esse número pode ser ajustado para controlar o comprimento do resumo.\n",
    "\n",
    "Essa abordagem permite que o método de sumarização extrativa identifique e extraia as sentenças que melhor representam o conteúdo e os temas principais do texto, com base na ideia de que a importância de uma sentença é indicada pela concentração e relevância das palavras-chave que ela contém."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in conteudos.items():\n",
    "    for secao, conteudo in j.items():\n",
    "        # pprint(conteudo.replace('\\xa0', ' '), width=110)\n",
    "        resumo_secao = extrair_principais_sentencas(conteudo.replace('\\xa0', ' ').replace('\\n', ''))\n",
    "        print(\"-\"*110)\n",
    "        pprint(resumo_secao,width=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteudos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in conteudos.items():\n",
    "    for secao, conteudo in j.items():\n",
    "        # pprint(conteudo.replace('\\xa0', ' '), width=110)\n",
    "        metodos, resultados = sintetizar_metodos_resultados(conteudo)\n",
    "        if metodos != [] or resultados != []:\n",
    "            print(\"-\"*110)\n",
    "            print(secao)\n",
    "            if metodos != []:\n",
    "                pprint(metodos,width=110)\n",
    "            if resultados != []:\n",
    "                pprint(resultados,width=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração de conceitos-chave\n",
    "conceitos=[]\n",
    "for i,j in conteudos.items():\n",
    "    for x,y in j.items():\n",
    "        conceitos.append((x,extrair_conceitos_chave(y)))\n",
    "    print(\"-\"*110)\n",
    "    for conceito in conceitos:\n",
    "        pprint(conceito,width=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração de resumos\n",
    "resumos=[]\n",
    "for i,j in conteudos.items():\n",
    "    for x,y in j.items():\n",
    "        resumos.append((x,resumir_secao(y)))\n",
    "    print(\"-\"*110)\n",
    "    for resumo in resumos:\n",
    "        pprint(resumo,width=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_link_pdf(doi):\n",
    "    # Endereço da API do CrossRef para consulta de metadados\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    try:\n",
    "        # Fazendo a requisição para a API do CrossRef\n",
    "        resposta = requests.get(url)\n",
    "        resposta.raise_for_status()  # Levanta um erro para respostas de falha\n",
    "        dados = resposta.json()\n",
    "        # Tentando extrair o URL da página do artigo\n",
    "        url_artigo = dados['message'].get('URL', 'URL não encontrado')\n",
    "        # Lógica adicional para tentar encontrar um link direto para o PDF\n",
    "        return url_artigo\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Erro ao acessar a API do CrossRef: {e}\"\n",
    "\n",
    "def extrair_conteudo_por_link(link):\n",
    "    try:\n",
    "        # Faz uma requisição GET para o link do artigo\n",
    "        resposta = requests.get(link)\n",
    "        # Verifica se a requisição foi bem-sucedida\n",
    "        if resposta.status_code == 200:\n",
    "            # Faz o parsing do conteúdo HTML\n",
    "            soup = BeautifulSoup(resposta.text, 'html.parser')\n",
    "            # Extrai o conteúdo relevante\n",
    "            # OBS: A linha abaixo é apenas um exemplo. Você precisará adaptar o seletor\n",
    "            # ao layout específico do site de onde está extraindo o conteúdo.\n",
    "            conteudo = soup.find('section', {'class': 'article-content'}).get_text(strip=True)\n",
    "            return conteudo\n",
    "        else:\n",
    "            return \"Falha ao acessar o artigo.\"\n",
    "    except Exception as e:\n",
    "        return str(e)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Função para extrair o conteúdo de um artigo\n",
    "def extrair_conteudo_por_link(link):\n",
    "    try:\n",
    "        # Faz uma requisição GET para o link do artigo\n",
    "        resposta = requests.get(link)\n",
    "        # Verifica se a requisição foi bem-sucedida\n",
    "        if resposta.status_code == 200:\n",
    "            # Faz o parsing do conteúdo HTML\n",
    "            soup = BeautifulSoup(resposta.text, 'html.parser')\n",
    "            # Verifica o domínio e adapta a extração de conteúdo conforme necessário\n",
    "            if \"springer.com\" in link:\n",
    "                # Extrai o conteúdo relevante para artigos da Springer\n",
    "                conteudo = soup.find('div', class_='main-content')\n",
    "                if conteudo:\n",
    "                    return conteudo.get_text(strip=True)\n",
    "                else:\n",
    "                    return \"Conteúdo relevante não encontrado.\"\n",
    "            else:\n",
    "                # Adapte ou adicione mais condições conforme necessário para outros domínios\n",
    "                return \"Domínio não suportado.\"\n",
    "        else:\n",
    "            return \"Falha ao acessar o artigo.\"\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = innontol_recent_results\n",
    "for n,i in enumerate(df['URL_PDF'].values):\n",
    "    if i:\n",
    "        link = i[0].get('URL')\n",
    "    else:\n",
    "        doi = df['DOI'].values[n]\n",
    "        link = obter_link_pdf(doi)\n",
    "\n",
    "    print(\"Extraindo conteúdo de: \",link)\n",
    "    conteudo = extrair_conteudo_por_link(link)\n",
    "    print(conteudo)\n",
    "    print(\"-\"*110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "\n",
    "class DataFrameMerger:\n",
    "    def __init__(self, dfs, similarity_threshold=90):\n",
    "        self.dfs = dfs\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.duplicates_removed = [0] * len(dfs)\n",
    "        self.unified_df = pd.DataFrame()\n",
    "        self.removed_records = pd.DataFrame()\n",
    "\n",
    "    def is_duplicate(self, title, abstract, existing_records):\n",
    "        # Verifica se o título é curto e usa o resumo para comparação\n",
    "        if len(title) <= 15:\n",
    "            text_to_compare = abstract\n",
    "        else:\n",
    "            text_to_compare = title\n",
    "        for existing_title, existing_abstract in existing_records:\n",
    "            # Escolhe entre título ou resumo para a comparação com base no comprimento\n",
    "            existing_text = existing_abstract if len(existing_title) <= 15 else existing_title\n",
    "            # Calcula a similaridade\n",
    "            similarity = fuzz.token_set_ratio(text_to_compare, existing_text)\n",
    "            if similarity >= self.similarity_threshold:\n",
    "                # Verifica a substring 'Correction to:'\n",
    "                if 'Correction to:' in text_to_compare or 'Correction to:' in existing_text:\n",
    "                    continue\n",
    "                print(f\"\\n{similarity}% similares:\\n'{text_to_compare}'\\n'{existing_text}'\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def remove_duplicates_by_similarity(self, df, existing_records):\n",
    "        non_duplicates = []\n",
    "        duplicates = []\n",
    "        for _, row in df.iterrows():\n",
    "            if not self.is_duplicate(row['TITLE'], row['ABSTRACT'], existing_records):\n",
    "                non_duplicates.append(row)\n",
    "                existing_records.add((row['TITLE'], row['ABSTRACT']))\n",
    "            else:\n",
    "                duplicates.append(row)\n",
    "        # Utilizando concat em vez de append\n",
    "        if duplicates:\n",
    "            self.removed_records = pd.concat([self.removed_records, pd.DataFrame(duplicates)], ignore_index=True)\n",
    "\n",
    "        return pd.DataFrame(non_duplicates)\n",
    "\n",
    "    def merge_dataframes(self):\n",
    "        existing_records = set()\n",
    "        for i, df in enumerate(self.dfs):\n",
    "            if df.empty:\n",
    "                continue\n",
    "            non_duplicates = self.remove_duplicates_by_similarity(df, existing_records)\n",
    "            self.duplicates_removed[i] = len(df) - len(non_duplicates)\n",
    "            self.unified_df = pd.concat([self.unified_df, non_duplicates], ignore_index=True)\n",
    "\n",
    "        return self.unified_df\n",
    "\n",
    "    def report(self):\n",
    "        total_initial = sum(len(df) for df in self.dfs)\n",
    "        total_final = len(self.unified_df)\n",
    "        total_duplicates_removed = sum(self.duplicates_removed)\n",
    "\n",
    "        print(f\"\\n----------------------------------------------------------------------------------------------------------------------------\")\n",
    "        print(f\"Relatório de Remoção de Duplicidades (Similaridade):\")\n",
    "        print(f\"----------------------------------------------------------------------------------------------------------------------------\")\n",
    "        for i, count in enumerate(self.duplicates_removed):\n",
    "            print(f\"Dataframe {i+1}: Entradas Iniciais = {len(self.dfs[i]):>4}, Duplicidades Removidas = {count:>4}\")\n",
    "        print(f\"----------------------------------------------------------------------------------------------------------------------------\")\n",
    "        print(f\"Total Inicial: {total_initial}, Total Final: {total_final}, Total Duplicidades Removidas: {total_duplicates_removed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1, df2, df3, df4 = df_sprg, df_ieee, df_cref, df_core\n",
    "# dfs = [df1, df2, df3, df4]\n",
    "# merger = DataFrameMerger(dfs)\n",
    "# unified_df = merger.merge_dataframes()\n",
    "# merger.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unified_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificar e limpar memória da GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "def check_nvidia_driver_and_cuda():\n",
    "    try:\n",
    "        nvidia_smi_output = subprocess.check_output(['nvidia-smi']).decode('utf-8')\n",
    "        print(\"Driver NVIDIA encontrado:\\n\", nvidia_smi_output)\n",
    "        \n",
    "        cuda_version_match = re.search(r'CUDA Version: (\\d+\\.\\d+)', nvidia_smi_output)\n",
    "        if cuda_version_match:\n",
    "            print(f\"CUDA Version: {cuda_version_match.group(1)} - OK\")\n",
    "        else:\n",
    "            print(\"Não foi possível encontrar a versão do CUDA com nvidia-smi.\")\n",
    "    except Exception as e:\n",
    "        print(\"Falha ao executar nvidia-smi. Verifique a instalação do driver NVIDIA.\")\n",
    "\n",
    "def check_pytorch_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"PyTorch encontrou {torch.cuda.device_count()} GPU(s):\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\" - {torch.cuda.get_device_name(i)} (ID: {i})\")\n",
    "    else:\n",
    "        print(\"PyTorch não encontrou GPUs compatíveis com CUDA.\")\n",
    "\n",
    "def test_basic_cuda_operation():\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            x = torch.rand(5, 5, device='cuda')\n",
    "            print(\"Operação básica de CUDA bem-sucedida.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Falha ao executar uma operação básica de CUDA: {e}\")\n",
    "    else:\n",
    "        print(\"CUDA não disponível. Pulando teste de operação básica.\")\n",
    "\n",
    "check_nvidia_driver_and_cuda()\n",
    "check_pytorch_cuda()\n",
    "test_basic_cuda_operation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Limpa a cache de memória da GPU\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Coleta de lixo do Python (pode ajudar na liberação de memória de objetos não mais referenciados)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from pprint import pprint\n",
    "from IPython.display import display, clear_output\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "class DataframeAnalyzer:\n",
    "    def __init__(self, df, filepath='../data/updated_unified_df.csv'):\n",
    "        self.df = df.copy()\n",
    "        self.filepath = filepath\n",
    "        self.model_name = \"Helsinki-NLP/opus-mt-tc-big-en-pt\"\n",
    "        self.tokenizer = MarianTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = MarianMTModel.from_pretrained(self.model_name)\n",
    "        self.index = self.get_first_unanalyzed_index()\n",
    "        self.create_widgets()\n",
    "        self.update_display()\n",
    "\n",
    "    def create_widgets(self):\n",
    "        self.interest_button = widgets.Button(description=\"Dentro do Escopo\")\n",
    "        self.not_interest_button = widgets.Button(description=\"Fora do Escopo\")\n",
    "        self.interest_button.on_click(self.interest_clicked)\n",
    "        self.not_interest_button.on_click(self.not_interest_clicked)\n",
    "\n",
    "    def interest_clicked(self, b):\n",
    "        if self.index < len(self.df):\n",
    "            self.df.at[self.index, 'INTERESSE'] = 'Dentro do Escopo'\n",
    "            self.save_dataframe()\n",
    "            self.index = self.get_next_unanalyzed_index()\n",
    "            self.update_display()\n",
    "\n",
    "    def not_interest_clicked(self, b):\n",
    "        if self.index < len(self.df):\n",
    "            self.df.at[self.index, 'INTERESSE'] = 'Fora do Escopo'\n",
    "            self.save_dataframe()\n",
    "            self.index = self.get_next_unanalyzed_index()\n",
    "            self.update_display()\n",
    "\n",
    "    def get_next_unanalyzed_index(self):\n",
    "        # Retorna o índice do próximo registro não analisado\n",
    "        return self.df[self.df['INTERESSE'].isna()].index.min()\n",
    "\n",
    "    def translate_text(self, text):\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return \"Texto não disponível para tradução\"\n",
    "\n",
    "        # Dividir o texto em segmentos de até 512 tokens\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        segments = [tokens[i:i + 512] for i in range(0, len(tokens), 512)]\n",
    "\n",
    "        translated_segments = []\n",
    "        for segment in segments:\n",
    "            # Juntar os tokens do segmento e traduzir\n",
    "            segment_text = self.tokenizer.convert_tokens_to_string(segment)\n",
    "            inputs = self.tokenizer(segment_text, return_tensors=\"pt\", padding=True)\n",
    "            with torch.no_grad():\n",
    "                translated = self.model.generate(**inputs)\n",
    "            translated_text = self.tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "            translated_segments.append(translated_text)\n",
    "\n",
    "        # Juntar os segmentos traduzidos\n",
    "        return ' '.join(translated_segments)\n",
    "\n",
    "    def format_and_print(self, text):\n",
    "        if isinstance(text, str):\n",
    "            formatted_text = text.replace('\\n', ' ')\n",
    "            pprint(formatted_text, width=125)\n",
    "        else:\n",
    "            print(\"Texto não disponível\")\n",
    "\n",
    "    def get_first_unanalyzed_index(self):\n",
    "        df_updated = pd.read_csv(self.filepath)\n",
    "        return df_updated[df_updated['INTERESSE'].isna()].index.min()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return \"Texto não disponível para tradução\"\n",
    "        \n",
    "        # Remover tags HTML/XML usando expressão regular\n",
    "        clean_text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "        # Adicionar outras regras de limpeza conforme necessário\n",
    "        clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "\n",
    "        return clean_text\n",
    "\n",
    "    def update_display(self):\n",
    "        clear_output(wait=False)\n",
    "        self.create_widgets()\n",
    "\n",
    "        if pd.notna(self.index) and self.index < len(self.df):\n",
    "            record = self.df.iloc[self.index]\n",
    "            original_title = self.clean_text(record['TITLE'])\n",
    "            original_abstract = self.clean_text(record['ABSTRACT'])\n",
    "            translated_title = self.translate_text(original_title)\n",
    "            translated_abstract = self.translate_text(original_abstract)\n",
    "\n",
    "            self.format_and_print(original_title)\n",
    "            print()\n",
    "            self.format_and_print(translated_title)\n",
    "            print(\"\\nAbstract:\")\n",
    "            self.format_and_print(original_abstract)\n",
    "            print(\"\\nResumo:\")\n",
    "            self.format_and_print(translated_abstract)\n",
    "\n",
    "            display(self.interest_button, self.not_interest_button)\n",
    "        else:\n",
    "            print(\"Todas as análises já foram concluídas.\")\n",
    "\n",
    "        self.update_progress()\n",
    "\n",
    "    def save_dataframe(self):\n",
    "        self.df.to_csv(self.filepath, index=False)\n",
    "        print(\"DataFrame salvo com sucesso.\")\n",
    "        self.update_progress()\n",
    "\n",
    "    def update_progress(self):\n",
    "        df_updated = pd.read_csv(self.filepath)\n",
    "        analyzed = df_updated['INTERESSE'].notna().sum()\n",
    "        total = len(df_updated)\n",
    "        remaining = total - analyzed\n",
    "\n",
    "        print(f\"Analisados {analyzed}/{total}, restam {remaining}\")\n",
    "        print(df_updated['INTERESSE'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reiniciar análises\n",
    "file_path = '../data/updated_unified_df.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "analyzer = DataframeAnalyzer(df, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../data/updated_unified_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase de leitura completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from graphviz import Digraph\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "class PrismaAnalyzer:\n",
    "    def __init__(self, df, prisma_dict='../data/prisma_analysis.json'):\n",
    "        self.df = df\n",
    "        self.prisma_dict = prisma_dict\n",
    "        self.index = 0\n",
    "        self.create_widgets()\n",
    "        self.update_display()\n",
    "\n",
    "    @staticmethod\n",
    "    def find_pdf_link(doi):\n",
    "        \"\"\"\n",
    "        Busca a URL do PDF associado ao DOI fornecido.\n",
    "        Retorna a URL do PDF se encontrado, caso contrário, retorna None.\n",
    "        \"\"\"\n",
    "        base_url = \"https://doi.org/\"\n",
    "        full_url = base_url + doi\n",
    "\n",
    "        try:\n",
    "            response = requests.get(full_url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                # Esta parte depende da estrutura do site específico e pode precisar ser ajustada\n",
    "                pdf_link = soup.find('a', href=True, string='PDF')\n",
    "                return pdf_link['href'] if pdf_link else None\n",
    "        except requests.RequestException:\n",
    "            return None\n",
    "\n",
    "    def search_pdf_link(self, doi):\n",
    "        # Endereço da API do CrossRef para consulta de metadados\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        \n",
    "        try:\n",
    "            # Fazendo a requisição para a API do CrossRef\n",
    "            resposta = requests.get(url)\n",
    "            resposta.raise_for_status()  # Levanta um erro para respostas de falha\n",
    "            dados = resposta.json()\n",
    "            \n",
    "            # Tentando extrair o URL da página do artigo\n",
    "            url_artigo = dados['message'].get('URL', 'URL não encontrado')\n",
    "            \n",
    "            # Lógica adicional para tentar encontrar um link direto para o PDF\n",
    "            try:\n",
    "                self.find_pdf_link(doi)\n",
    "            except:\n",
    "                print(\"Erro ao tentar obter link para o PDF pelo DOI\")\n",
    "\n",
    "            return url_artigo\n",
    "        except requests.RequestException as e:\n",
    "            return f\"Erro ao acessar a API do CrossRef: {e}\"\n",
    "\n",
    "    # Exemplo de uso\n",
    "    # doi = \"10.1000/example\"\n",
    "    # print(search_pdf_link(doi))\n",
    "\n",
    "    def associate_pdf_links(self):\n",
    "        \"\"\"\n",
    "        Associa links de PDFs ao DataFrame com base nos DOIs.\n",
    "        \"\"\"\n",
    "        self.df['PDF_Link'] = self.df['DOI'].apply(self.search_pdf_link)\n",
    "\n",
    "    def create_widgets(self):\n",
    "        # Criar botões e vincular ações\n",
    "        self.inaccessible_button = widgets.Button(description=\"Texto completo inacessível\")\n",
    "        self.different_scenario_button = widgets.Button(description=\"Cenário diferente\")\n",
    "        self.different_scope_button = widgets.Button(description=\"Escopo diferente\")\n",
    "        \n",
    "        # Vincular os botões a funções de clique\n",
    "        self.inaccessible_button.on_click(self.inaccessible_clicked)\n",
    "        self.different_scenario_button.on_click(self.different_scenario_clicked)\n",
    "        self.different_scope_button.on_click(self.different_scope_clicked)\n",
    "        \n",
    "        display(self.inaccessible_button, self.different_scenario_button, self.different_scope_button)\n",
    "\n",
    "    def update_display(self):\n",
    "        clear_output(wait=True)\n",
    "        self.create_widgets()\n",
    "        if self.index < len(self.df):\n",
    "            record = self.df.iloc[self.index]\n",
    "            # Aqui você pode exibir o conteúdo completo do artigo para leitura\n",
    "            print(\"Conteúdo do Artigo:\", record['CONTENT'])\n",
    "        else:\n",
    "            print(\"Todas as leituras já foram concluídas.\")\n",
    "\n",
    "    # Funções para manipular cliques nos botões\n",
    "    def inaccessible_clicked(self, b):\n",
    "        self.update_prisma_dict('Texto completo inacessível')\n",
    "        self.index += 1\n",
    "        self.update_display()\n",
    "\n",
    "    def different_scenario_clicked(self, b):\n",
    "        self.update_prisma_dict('Cenário diferente')\n",
    "        self.index += 1\n",
    "        self.update_display()\n",
    "\n",
    "    def different_scope_clicked(self, b):\n",
    "        self.update_prisma_dict('Escopo diferente')\n",
    "        self.index += 1\n",
    "        self.update_display()\n",
    "\n",
    "    def update_prisma_dict(self, category):\n",
    "        self.prisma_dict['leitura_completa'][category] += 1\n",
    "        self.df.at[self.index, 'LEITURA'] = category\n",
    "\n",
    "    # Função para salvar o DataFrame atualizado e o dicionário Prisma\n",
    "    def save_data(self):\n",
    "        self.df.to_csv('../data/full_text_analysis.csv', index=False)\n",
    "        # Aqui você pode adicionar a lógica para salvar o dicionário Prisma\n",
    "        # em um arquivo ou de outra forma que desejar\n",
    "\n",
    "class PrismaFlowchartSubgraphs:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.flowchart = Digraph()\n",
    "        self.flowchart.attr(rankdir='LR', size='120,120')\n",
    "        self.flowchart.attr('node', shape='rectangle')\n",
    "\n",
    "    def add_box(self, name, label, width='4', height='1.5', style='filled', fillcolor='white', fontcolor='black', fontsize='12', bold=False):\n",
    "        fontname = \"Helvetica\"\n",
    "        if bold:\n",
    "            fontname += \"-Bold\"\n",
    "        self.flowchart.node(name, label, style=style, fillcolor=fillcolor, fontcolor=fontcolor, width=width, height=height, fontsize=fontsize, fontname=fontname)\n",
    "\n",
    "    def add_invisible_node(self, name, width='0', height='0', rank=None):\n",
    "        if rank:\n",
    "            with self.flowchart.subgraph() as s:\n",
    "                s.attr(rank=rank)\n",
    "                s.node(name, style='invis', width=width, height=height)\n",
    "        else:\n",
    "            self.flowchart.node(name, style='invis', width=width, height=height)\n",
    "\n",
    "    def add_edge(self, source, destination, invisible=False, constraint=True):\n",
    "        if invisible:\n",
    "            self.flowchart.edge(source, destination, style='invis')\n",
    "        else:\n",
    "            self.flowchart.edge(source, destination, constraint=str(constraint).lower())\n",
    "\n",
    "    def set_rank_same(self, *nodes):\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            for n in nodes:\n",
    "                s.node(n)\n",
    "\n",
    "    def generate_flowchart(self):\n",
    "        # Inicialização dos dados\n",
    "        n_total = sum(self.data['base'].values())\n",
    "        n_novosachados = sum(self.data['novas_bases'].values())\n",
    "        n_dupl = self.data.get('n_dupl', 0)\n",
    "        n_foraescopo = self.data.get('n_foraescopo', 0)\n",
    "        n_antigos = self.data.get('n_antigos', 0)\n",
    "        n_removidos = sum(self.data['rem_citerios'].values())\n",
    "        n_triagem = n_total+n_novosachados-n_removidos\n",
    "        n_rem_titulo = self.data.get('n_rem_titulo', 0)\n",
    "        n_eleg_res = n_triagem - n_rem_titulo\n",
    "        n_rem_resumo = sum(self.data['leitura_resumos'].values())\n",
    "        n_rem_compl = sum(self.data['leitura_completa'].values())\n",
    "        n_eleg_compl = n_eleg_res - n_rem_resumo\n",
    "        n_incluidos_revisao = n_eleg_compl - n_rem_compl\n",
    "        n_extrabases = sum(self.data['add_extra_index'].values())\n",
    "        n_retiradas = self.data.get('n_retiradas', 0)\n",
    "        n_mantidas = n_extrabases- n_retiradas\n",
    "        n_aposleituraelegiveis = self.data.get('n_extraelegiveis', 0)\n",
    "        n_aposleituranaoelegiveis = self.data.get('n_extranaoelegiveis', 0)\n",
    "        n_incluidas_extra = n_aposleituraelegiveis - n_aposleituranaoelegiveis\n",
    "\n",
    "        # Quinta linha de subgrafos\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            self.add_box('r5c0', 'Incluídos para Revisão', fillcolor='lightblue', fontsize='18', bold=True)\n",
    "            self.add_invisible_node('r5c1')\n",
    "            self.add_edge('r5c0', 'r5c1', invisible=True)\n",
    "            self.add_box('r5c2', f'Total de estudos incluídos na revisão\\n(n={n_incluidos_revisao})\\nTotal de publicações incluídas extra-bases\\n+(n={n_incluidas_extra})')\n",
    "            self.add_edge('r5c1', 'r5c2', invisible=True)\n",
    "\n",
    "        # Quarta linha de subgrafos\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            self.add_box('r4c0', 'Triagem por\\nConteúdo Completo', fillcolor='lightblue', fontsize='18', bold=True)\n",
    "            self.add_invisible_node('r4c1')\n",
    "            self.add_edge('r4c0', 'r4c1', invisible=True)\n",
    "            self.add_box('r4c2', f'Registros avaliadas como elegíveis\\npara leitura completa\\n(n={n_eleg_compl})')\n",
    "            self.add_box('r4c3', f'Registros excluídos\\ncom leitura completa\\n-(n={n_rem_compl})\\n'+\n",
    "                                \"\\n\".join([f\"{res} n= {n}\" for res, n in self.data['leitura_completa'].items()]),\n",
    "                         fillcolor='white', fontcolor='black')\n",
    "            self.add_edge('r4c1', 'r4c2', invisible=True)\n",
    "            self.add_box('r4c4', f'Publicações extra-busca avaliadas\\ncomo elegíveis pós leitura completa\\n+(n={n_incluidas_extra})')\n",
    "            self.add_box('r4c5', f'Publicações extra-busca avaliadas\\ncomo não-elegíveis pós leitura completa\\n-(n={self.data.get(\"n_extranaoelegiveis\", 0)})')\n",
    "            self.add_edge('r4c3', 'r4c4', invisible=True)\n",
    "\n",
    "        # Terceira linha de subgrafos\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            self.add_box('r3c0', 'Triagem por Resumos', fillcolor='lightblue', fontsize='18', bold=True)\n",
    "            self.add_invisible_node('r3c1')\n",
    "            self.add_edge('r3c0', 'r3c1', invisible=True)\n",
    "            self.add_box('r3c2', f'Registros avaliados como\\nelegíveis para leitura resumos\\n(n={n_eleg_res})')\n",
    "            self.add_box('r3c3', f'Registros excluídos\\ncom leitura do resumo\\n-(n={n_rem_resumo})\\n'+\n",
    "                                \"\\n\".join([f\"{res} n= {n}\" for res, n in self.data['leitura_resumos'].items()]),\n",
    "                         fillcolor='white', fontcolor='black')\n",
    "            self.add_edge('r3c1', 'r3c2', invisible=True)\n",
    "            self.add_box('r3c4', f'Publicações extra-busca pesquisadas\\nmantidas para leitura completa\\n(n={n_mantidas})')\n",
    "            self.add_box('r3c5', f'Publicações retiradas pós leitura resumos\\n-(n={n_retiradas})')\n",
    "            self.add_edge('r3c3', 'r3c4', invisible=True)\n",
    "            self.add_edge('r3c4', 'r3c5')\n",
    "\n",
    "        # Segunda linha de subgrafos\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            self.add_box('r2c0', 'Triagem por Títulos', fillcolor='lightblue', fontsize='18', bold=True)\n",
    "            self.add_invisible_node('r2c1')\n",
    "            self.add_edge('r2c0', 'r2c1', invisible=True)\n",
    "            self.add_box('r2c2', f'Total de Registros para triagem\\n(n={n_triagem})')\n",
    "            self.add_box('r2c3', f'Registros excluídos\\ncom leitura do título\\n-(n={self.data.get(\"n_rem_titulo\", 0)})')\n",
    "            self.add_edge('r2c1', 'r2c2', invisible=True)\n",
    "            self.add_invisible_node('r2c4')\n",
    "            self.add_edge('r2c3', 'r2c4', invisible=True)\n",
    "\n",
    "        # Primeira linha de subgrafos\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            self.add_box('r1c0', 'Identificação', fillcolor='lightblue', fontsize='18', bold=True)\n",
    "            self.add_box('r1c1', f\"Estudos incluídos na busca\\nde revisão (n = {n_total})\\n\"+\n",
    "                                \"\\n\".join([f\"{base} n= {n}\" for base, n in self.data['base'].items()]),\n",
    "                         fillcolor='white', fontcolor='black')\n",
    "\n",
    "            self.add_box('r1c2', f'Total de registros achados\\nnas bases de dados\\n(n= {n_total})\\nTotal de novos achados\\n+(n={n_novosachados})\\n'+ \"\\n\".join([f\"{nova_base} n= {n}\" for nova_base, n in self.data['novas_bases'].items()]),\n",
    "                         fillcolor='white', fontcolor='black')\n",
    "            self.add_box('r1c3', f\"Removidos antes da triagem\\n-(n={n_removidos})\\n\"+\"\\n\".join([f\"{rem_citerios} n= {n}\" for rem_citerios, n in self.data['rem_citerios'].items()]),\n",
    "                         fillcolor='white', fontcolor='black')\n",
    "            self.add_box('r1c4', f'Registros identificados em outras bases:\\n+(n={n_extrabases})\\n'+ \"\\n\".join([f\"{extra_base} n= {n}\" for extra_base, n in self.data['add_extra_index'].items()]),\n",
    "                         fillcolor='white', fontcolor='black')\n",
    "            # Arestas da primeira linha\n",
    "            self.add_edge('r1c0', 'r1c1', invisible=True)\n",
    "\n",
    "        # Adicionando os rótulos de linha amarelos no topo\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            self.add_box('r0c1', 'Achados através de\\nbuscas por\\npalavras-chave', fillcolor='yellow', fontsize='16', bold=True)\n",
    "            self.add_box('r0c2', 'Achados através\\nde buscas em\\nbases de dados', width='4', fillcolor='yellow', fontsize='16', bold=True)\n",
    "            self.add_box('r0c3', 'Achados através\\nde buscas em\\nbases de dados e\\nremovidos após análise', width='4', fillcolor='yellow', fontsize='16', bold=True)\n",
    "            self.add_box('r0c4', 'Identificação de estudos\\ndentro dos critérios\\nextra-busca', fillcolor='yellow', fontsize='16', bold=True)\n",
    "            self.add_box('r0c5', 'Identificação de estudos\\ndentro dos critérios\\nextra-busca\\nremovidos após análise', fillcolor='yellow', fontsize='16', bold=True)\n",
    "\n",
    "        # Arestas entre os rótulos amarelos\n",
    "        self.add_invisible_node('r0c0', rank='same')\n",
    "        self.add_edge('r0c0', 'r0c1', invisible=True)\n",
    "        self.add_edge('r0c1', 'r0c2', invisible=True)\n",
    "        self.add_edge('r0c2', 'r0c3', invisible=True)\n",
    "        self.add_edge('r0c3', 'r0c4', invisible=True)\n",
    "        self.add_edge('r0c4', 'r0c5', invisible=True)\n",
    "\n",
    "        # Conectar colunas adjacentes horizontalmente conforme necessário\n",
    "        # Conexões horizontais da primeira linha\n",
    "        self.add_edge('r1c1', 'r1c2')\n",
    "        self.add_edge('r1c2', 'r1c3')\n",
    "        self.add_edge('r1c3', 'r1c4', invisible=True)\n",
    "\n",
    "        # Conexões horizontais da segunda linha\n",
    "        self.add_edge('r2c2', 'r2c3')\n",
    "        self.add_edge('r2c3', 'r2c4', invisible=True)\n",
    "\n",
    "        # Conexões horizontais da terceira linha\n",
    "        self.add_edge('r3c2', 'r3c3')\n",
    "\n",
    "        # Conexões horizontais da quarta linha\n",
    "        self.add_edge('r4c2', 'r4c3')\n",
    "        self.add_edge('r4c3', 'r4c4', invisible=True)\n",
    "        self.add_edge('r4c4', 'r4c5')\n",
    "\n",
    "        # Conexões horizontais da quinta linha\n",
    "        self.add_edge('r5c0', 'r5c1', invisible=True)\n",
    "        self.add_edge('r5c1', 'r5c2', invisible=True)\n",
    "\n",
    "        # Conectar colunas verticalmente com constraint='false' para manter o layout (está falhando)\n",
    "        # Arestas verticais para manter o layout (causando erro atualmente, por isso comentadas)\n",
    "        # self.add_edge('r4c0', 'r5c0')\n",
    "        # self.add_edge('r3c0', 'r4c0')\n",
    "        # self.add_edge('r2c0', 'r3c0')\n",
    "        # self.add_edge('r1c0', 'r2c0')\n",
    "        # self.add_edge('r0c0', 'r1c0')\n",
    "\n",
    "        # Arestas verticais do fluxo de soma (causando erro atualmente, por isso comentadas)\n",
    "        # self.add_edge('r1c1', 'r2c2', constraint='false')\n",
    "        # self.add_edge('r2c2', 'r3c2', constraint='false')\n",
    "        # self.add_edge('r3c2', 'r4c2', constraint='false')\n",
    "        # self.add_edge('r4c2', 'r5c2', constraint='false')\n",
    "        # self.add_edge('r4c4', 'r5c2', constraint='false')\n",
    "\n",
    "        # Configuração do layout e renderização\n",
    "        self.flowchart.engine = 'dot'\n",
    "        return self.flowchart\n",
    "\n",
    "# Estrutura do dicionário vazio\n",
    "empty_data = {\n",
    "    'base': {},\n",
    "    'rem_citerios': {\n",
    "        'Duplicados': 0,\n",
    "        'Fora do período': 0,\n",
    "    },\n",
    "    'novas_bases': {},\n",
    "    'n_rem_titulo': 0,\n",
    "    'leitura_resumos': {\n",
    "        'Resumo inacessível': 0,\n",
    "        'Período diferente': 0,\n",
    "        'Escopo diferente': 0,\n",
    "    },\n",
    "    'leitura_completa': {\n",
    "        'Texto completo inacessivel': 0,\n",
    "        'Cenário diferente': 0,\n",
    "        'Escopo diferente': 0,\n",
    "    },\n",
    "    'add_extra_index': {},\n",
    "    'n_retiradas': 0,\n",
    "    'n_extraelegiveis': 0,\n",
    "    'n_extranaoelegiveis': 0\n",
    "}\n",
    "\n",
    "# Escrever os dados no arquivo JSON\n",
    "with open('../data/prisma_analysis.json', 'w') as json_file:\n",
    "    json.dump(empty_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenção dos conteúdos completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import requests\n",
    "\n",
    "class ContentFetcher:\n",
    "    def __init__(self, prisma_dict='../data/prisma_analysis.json'):\n",
    "        self.prisma_dict = prisma_dict\n",
    "\n",
    "    def update_df_with_pdf_links(self, df, doi_column='DOI', interest_column='INTERESSE', link_column='LINK_PDF'):\n",
    "        \"\"\"\n",
    "        Atualizar o DataFrame com links de PDFs para cada DOI, se 'Dentro do Escopo'.\n",
    "        \"\"\"\n",
    "        # Contador para links de PDF encontrados\n",
    "        pdf_found_counter = 0\n",
    "\n",
    "        # Cria uma barra de progresso tqdm para o DataFrame\n",
    "        tqdm.pandas(desc=\"Buscando PDFs\")\n",
    "\n",
    "        # Função para aplicar em cada linha\n",
    "        def process_row(row):\n",
    "            nonlocal pdf_found_counter\n",
    "            # Tratar apenas registros 'Dentro do Escopo'\n",
    "            if row[interest_column] == 'Dentro do Escopo':\n",
    "                try:\n",
    "                    link = self.extract_pdf_link_from_doi(row[doi_column])\n",
    "                    if link not in ['DOI not available', 'Texto completo inacessível']:\n",
    "                        pdf_found_counter += 1\n",
    "                    return link\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                # Para registros 'Fora do Escopo'\n",
    "                return 'Fora do Escopo'\n",
    "\n",
    "        # Aplica a função process_row em cada linha com barra de progresso\n",
    "        df[link_column] = df.progress_apply(process_row, axis=1)\n",
    "        \n",
    "        # Limpa a saída anterior e exibe o número de links encontrados\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(f\"Links de PDF encontrados: {pdf_found_counter} de {len(df)}\")\n",
    "        return df\n",
    "\n",
    "    def extract_pdf_link_from_doi(self, doi):\n",
    "        # Converter o DOI para uma URL (supondo um formato comum)\n",
    "        doi_url = \"https://doi.org/\" + doi\n",
    "\n",
    "        # Enviar uma solicitação GET para a URL do DOI\n",
    "        response = requests.get(doi_url)\n",
    "        if response.status_code != 200:\n",
    "            return \"Falha na requisição para o DOI: \" + doi\n",
    "\n",
    "        # Obter a URL final após os redirecionamentos (normalmente é a URL da página do artigo)\n",
    "        final_url = response.url\n",
    "\n",
    "        # Extrair o domínio/base da URL\n",
    "        base_url = \"{0.scheme}://{0.netloc}\".format(urlparse(final_url))\n",
    "\n",
    "        # Analisar o conteúdo HTML da página\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Buscar todos os links e verificar se algum contém 'PDF'\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            try:\n",
    "                if 'pdf' in link.get('href', '').lower():\n",
    "                    # Encontrar o link completo para o PDF\n",
    "                    pdf_link = urljoin(base_url, link['href'])\n",
    "                    print(f\"Link encontrado: {pdf_link}\")\n",
    "                    return pdf_link\n",
    "            except:\n",
    "                return 'PDF não disponível no momento'\n",
    "        return \"Link para PDF não encontrado.\"\n",
    "\n",
    "\n",
    "    def download_pdf(self, pdf_url, destination):\n",
    "        \"\"\"\n",
    "        Baixar o PDF de um URL especificado.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(pdf_url)\n",
    "            if response.status_code == 200:\n",
    "                with open(destination, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                return \"Download successful\"\n",
    "            else:\n",
    "                return \"Failed to download PDF\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leitura = pd.read_csv('../data/updated_unified_df.csv')\n",
    "\n",
    "fetcher = ContentFetcher()\n",
    "df_leitura = fetcher.update_df_with_pdf_links(df_leitura)\n",
    "df_leitura.to_csv('../data/updated_unified_dflinks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher = ContentFetcher()\n",
    "\n",
    "for i in innontol_recent_results['DOI']:\n",
    "    print(fetcher.update_df_with_pdf_links(df_leitura))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../data/updated_unified_dflinks.csv')[['DOI','LINK_PDF']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise por leitura completa dos conteúdos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leitura['CONTENT'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leitura[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_links=[]\n",
    "for i in df_leitura['LINK_PDF']:\n",
    "    if i:        \n",
    "        if 'Fora do Escopo' in i:\n",
    "            pdf_links.append('Fora do escopo')\n",
    "        elif 'http' in i:\n",
    "            pdf_links.append('Dentro do escopo, com link obtido')            \n",
    "        elif 'Link para PDF não encontrado' in i:\n",
    "            pdf_links.append('Dentro do escopo, link não encontrado')\n",
    "        elif 'Falha' in i:\n",
    "            pdf_links.append('Dentro do escopo, falha na requisição do link')\n",
    "        else:\n",
    "            pdf_links.append('Dentro do escopo, mas documento não acessível')\n",
    "    else:\n",
    "        pdf_links.append('Dentro do escopo, mas documento não acessível')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pdf_links).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prisma_analyzer = PrismaAnalyzer(df_leitura)\n",
    "prisma_analyzer.associate_pdf_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar a classe e gerar o fluxograma\n",
    "# prisma_chart = PrismaFlowchartSubgraphs(emptydata)\n",
    "# flowchart = prisma_chart.generate_flowchart()\n",
    "\n",
    "# # Visualizar o fluxograma\n",
    "# flowchart.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from graphviz import Digraph\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "class ArticleDataExtractor:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://api.crossref.org/works\"\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    def search_crossref_api_tdm_for_researchers(self, keyword, rows=10):\n",
    "        params = {\n",
    "            \"query\": keyword,\n",
    "            \"rows\": rows,\n",
    "            \"mailto\": \"your-email@example.com\",\n",
    "            \"select\": \"title,DOI,author,published-print,published-online,link\"\n",
    "        }\n",
    "        response = self.session.get(self.base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = json.loads(response.text)\n",
    "            articles = data['message']['items']\n",
    "            return self._process_articles(articles)\n",
    "        else:\n",
    "            return f\"Failed to retrieve data, HTTP Status Code: {response.status_code}\"\n",
    "\n",
    "    def _search_unpaywall(self, doi):\n",
    "        \"\"\"\n",
    "        Busca informações de um artigo usando a API do Unpaywall.\n",
    "\n",
    "        Parameters:\n",
    "            doi (str): O DOI do artigo.\n",
    "\n",
    "        Returns:\n",
    "            dict: Um dicionário contendo informações do artigo, incluindo um link para a versão de acesso aberto, se disponível.\n",
    "        \"\"\"\n",
    "        url = f\"https://api.unpaywall.org/v2/{doi}?email=your-email@example.com\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                title = data.get('title', 'Title not found')\n",
    "                is_oa = data.get('is_oa', False)\n",
    "                oa_url = data.get('best_oa_location', {}).get('url_for_pdf', 'Open access URL not found')\n",
    "                return {\n",
    "                    \"title\": title,\n",
    "                    \"is_oa\": is_oa,\n",
    "                    \"oa_url\": oa_url\n",
    "                }\n",
    "            else:\n",
    "                return {\"title\": \"No Title\", \"is_oa\": False, \"oa_url\": \"Not available\"}\n",
    "        except Exception as e:\n",
    "            return {\"title\": \"Error\", \"is_oa\": False, \"oa_url\": f\"An error occurred: {e}\"}\n",
    "\n",
    "    # def _generate_prisma_diagram(self, prisma_results, exclusion_criteria):\n",
    "    #     \"\"\"\n",
    "    #     Gera um diagrama PRISMA com base nos resultados filtrados.\n",
    "\n",
    "    #     Parameters:\n",
    "    #         prisma_results (dict): Resultados filtrados do método PRISMA.\n",
    "    #         exclusion_criteria (dict): Critérios de exclusão aplicados.\n",
    "    #     \"\"\"\n",
    "    #     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    #     # Criação do diagrama com base nos dados e formatação conforme solicitado.\n",
    "    #     # Implemente a lógica de desenho aqui, seguindo o exemplo da imagem fornecida.\n",
    "        \n",
    "    #     # Exemplo de como preencher as informações no diagrama.\n",
    "    #     # Modifique de acordo com a lógica específica de sua aplicação.\n",
    "    #     ax.text(0.1, 0.8, f\"Artigos identificados: {sum(prisma_results['identified'].values())}\", ha='center', va='center', fontsize=12)\n",
    "    #     ax.text(0.1, 0.7, f\"Artigos após remoção de duplicatas: {prisma_results['non_duplicates']}\", ha='center', va='center', fontsize=12)\n",
    "    #     ax.text(0.1, 0.6, f\"Artigos elegíveis: {prisma_results['eligible']}\", ha='center', va='center', fontsize=12)\n",
    "    #     ax.text(0.1, 0.5, f\"Artigos incluídos: {prisma_results['included']}\", ha='center', va='center', fontsize=12)\n",
    "        \n",
    "    #     # Ocultando os eixos\n",
    "    #     ax.axis('off')\n",
    "\n",
    "    #     # Exibir o diagrama\n",
    "    #     plt.show()\n",
    "\n",
    "    def _generate_prisma_diagram(self, prisma_results, exclusion_criteria):\n",
    "        \"\"\"\n",
    "        Gera um diagrama PRISMA com base nos resultados filtrados usando Graphviz.\n",
    "\n",
    "        Parameters:\n",
    "            prisma_results (dict): Resultados filtrados do método PRISMA.\n",
    "            exclusion_criteria (dict): Critérios de exclusão aplicados.\n",
    "        \"\"\"\n",
    "        dot = Digraph()\n",
    "\n",
    "        # Define os nós do diagrama\n",
    "        dot.node('A', 'Artigos identificados\\n' + str(sum(prisma_results['identified'].values())))\n",
    "        dot.node('B', 'Artigos não duplicados encontrados\\n' + str(prisma_results['non_duplicates']))\n",
    "        dot.node('C', 'Artigos excluídos\\n' + str(sum(exclusion_criteria.get('excluded', []))))\n",
    "        dot.node('D', 'Artigos elegíveis\\n' + str(prisma_results['eligible']))\n",
    "        dot.node('E', 'Artigos incluídos na síntese qualitativa\\n' + str(prisma_results['included']))\n",
    "        \n",
    "        # Conecta os nós do diagrama\n",
    "        dot.edges(['AB', 'BC', 'CD', 'DE'])\n",
    "\n",
    "        # Personaliza o estilo dos nós e arestas\n",
    "        dot.attr('node', shape='box', style='filled', fillcolor='lightgrey')\n",
    "        dot.attr('edge', arrowhead='vee', arrowsize='1')\n",
    "\n",
    "        # Gera e salva o diagrama\n",
    "        dot.render('prisma_diagram', view=True, format='png')\n",
    "\n",
    "    def apply_prisma_method(self, search_results, exclusion_criteria):\n",
    "        \"\"\"\n",
    "        Aplica o método PRISMA para filtrar os resultados da pesquisa de artigos.\n",
    "\n",
    "        Parameters:\n",
    "            search_results (dict): Resultados da pesquisa contendo listas de DOIs por base de dados.\n",
    "            exclusion_criteria (dict): Critérios de exclusão com os números de artigos excluídos por critério.\n",
    "\n",
    "        Returns:\n",
    "            dict: Resultados filtrados após aplicar o método PRISMA.\n",
    "        \"\"\"\n",
    "        initial_counts = {source: len(dois) for source, dois in search_results.items()}\n",
    "        total_identified = sum(initial_counts.values())\n",
    "        total_duplicates_removed = sum(exclusion_criteria.get('duplicates', []))\n",
    "        total_non_duplicates = total_identified - total_duplicates_removed\n",
    "        total_eligible = total_non_duplicates - sum(exclusion_criteria.get('ineligible', []))\n",
    "        total_included = total_eligible - sum(exclusion_criteria.get('excluded', []))\n",
    "\n",
    "        prisma_results = {\n",
    "            'identified': initial_counts,\n",
    "            'non_duplicates': total_non_duplicates,\n",
    "            'eligible': total_eligible,\n",
    "            'included': total_included\n",
    "        }\n",
    "        \n",
    "        self._generate_prisma_diagram(prisma_results, exclusion_criteria)\n",
    "        return prisma_results\n",
    "    \n",
    "    def _get_crossref_metadata(self, doi):\n",
    "        \"\"\"\n",
    "        Busca metadados de um DOI específico usando a API do CrossRef.\n",
    "\n",
    "        Parameters:\n",
    "            doi (str): O DOI do artigo.\n",
    "\n",
    "        Returns:\n",
    "            dict: Metadados recuperados do artigo.\n",
    "        \"\"\"\n",
    "        crossref_url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        try:\n",
    "            response = requests.get(crossref_url)\n",
    "            if response.status_code == 200:\n",
    "                data = json.loads(response.text)\n",
    "                item = data['message']\n",
    "                title = item.get('title', ['No Title'])[0]\n",
    "                abstract = item.get('abstract', 'Abstract not found')\n",
    "                return {\"title\": title, \"abstract\": abstract}\n",
    "            else:\n",
    "                return {\"title\": \"No Title\", \"abstract\": \"Abstract not found\"}\n",
    "        except Exception as e:\n",
    "            return {\"title\": \"Error\", \"abstract\": f\"An error occurred: {e}\"}\n",
    "\n",
    "    def _search_google_scholar(self, doi):\n",
    "        \"\"\"\n",
    "        Busca um DOI no Google Scholar para obter título e resumo.\n",
    "\n",
    "        Parameters:\n",
    "            doi (str): O DOI do artigo.\n",
    "\n",
    "        Returns:\n",
    "            dict: Um dicionário contendo o título e o resumo do artigo.\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0\"\n",
    "        }\n",
    "        query = urllib.parse.quote(doi)\n",
    "        url = f\"https://scholar.google.com/scholar?q={query}\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                # A lógica de extração específica vai aqui. \n",
    "                # Google Scholar não fornece diretamente resumos, então isso pode ser um desafio.\n",
    "                # Você precisará identificar como os títulos e resumos (se disponíveis) são apresentados no HTML e extrair adequadamente.\n",
    "                title = soup.find(...)  # Localize o elemento contendo o título.\n",
    "                abstract = soup.find(...)  # Localize o elemento contendo o resumo.\n",
    "\n",
    "                return {\n",
    "                    \"title\": title.get_text().strip() if title else \"Title not found\",\n",
    "                    \"abstract\": abstract.get_text().strip() if abstract else \"Abstract not found\"\n",
    "                }\n",
    "            else:\n",
    "                return {\"title\": \"No Title\", \"abstract\": \"Abstract not found\"}\n",
    "        except Exception as e:\n",
    "            return {\"title\": \"Error\", \"abstract\": f\"An error occurred: {e}\"}\n",
    "\n",
    "    def extract_article_data(self, links, dois):\n",
    "        extracted_data = {}\n",
    "        session = requests.Session()\n",
    "        headers = self._get_headers()\n",
    "\n",
    "        for doi in dois:\n",
    "            # Primeiro tenta no Unpaywall\n",
    "            logging.info(f\"Buscando dados de acesso aberto para DOI {doi}\")\n",
    "            oa_data = self._search_unpaywall(doi)\n",
    "            if oa_data and oa_data[\"is_oa\"]:\n",
    "                logging.info(\"Artigo de acesso aberto encontrado.\")\n",
    "                extracted_data[doi] = oa_data\n",
    "                continue\n",
    "\n",
    "            # Se não encontrar, tenta no CrossRef\n",
    "            logging.info(f\"Buscando metadados do CrossRef para DOI {doi}\")\n",
    "            crossref_data = self._get_crossref_metadata(doi)\n",
    "            if crossref_data and crossref_data[\"title\"] != \"No Title\":\n",
    "                logging.info(\"Metadados encontrados no CrossRef.\")\n",
    "                extracted_data[doi] = crossref_data\n",
    "                continue\n",
    "\n",
    "            # Se ainda não encontrar, tenta no Google Scholar\n",
    "            logging.info(f\"Buscando no Google Scholar para DOI {doi}\")\n",
    "            scholar_data = self._search_google_scholar(doi)\n",
    "            if scholar_data and scholar_data[\"title\"] != \"No Title\":\n",
    "                logging.info(\"Dados encontrados no Google Scholar.\")\n",
    "                extracted_data[doi] = scholar_data\n",
    "                continue\n",
    "\n",
    "            logging.warning(f\"Dados não encontrados para DOI {doi}\")\n",
    "\n",
    "        return extracted_data\n",
    "\n",
    "    def _get_headers(self):\n",
    "        return {\n",
    "            \"User-Agent\": \"Mozilla/5.0\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Referer\": \"https://www.google.com\"\n",
    "        }\n",
    "\n",
    "    # Ok! fundcionando perfeitamente\n",
    "    def _extract_acm_data(self, soup):\n",
    "        title = soup.find('h1', class_='citation__title')\n",
    "        abstract = soup.find('div', class_='abstractSection abstractInFull')\n",
    "        references = self._extract_references(soup)\n",
    "        if title and abstract:\n",
    "            return title.get_text().strip(), abstract.get_text().strip(), references\n",
    "        else:\n",
    "            return 'Title not found', 'Abstract not found', references\n",
    "\n",
    "    def extract_wiley_data(self, soup):\n",
    "        title = soup.find('h2', class_='citation__title')\n",
    "        summary = soup.find('div', class_='article-section__content')\n",
    "        \n",
    "        references_section = soup.find('section', id='references-section')\n",
    "        references = []\n",
    "        if references_section:\n",
    "            for ref in references_section.find_all('li', {'data-bib-id': True}):\n",
    "                ref_text = ' '.join(ref.stripped_strings)\n",
    "                references.append(ref_text)\n",
    "\n",
    "        return (\n",
    "            title.get_text().strip() if title else 'Title not found',\n",
    "            summary.get_text().strip() if summary else 'Summary not found',\n",
    "            references\n",
    "        )\n",
    "\n",
    "    # def search_ieee_xplore(self, keyword, rows=10):\n",
    "    #     \"\"\"\n",
    "    #     Busca artigos relacionados a uma palavra-chave no IEEE Xplore.\n",
    "\n",
    "    #     Parameters:\n",
    "    #         keyword (str): A palavra-chave para busca.\n",
    "    #         rows (int): O número de resultados a retornar.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list: Uma lista de dicionários contendo dados dos artigos.\n",
    "    #     \"\"\"\n",
    "    #     api_key = 'YOUR_IEEE_XPLORE_API_KEY'  # Substitua com sua chave de API do IEEE Xplore\n",
    "    #     url = 'http://ieeexploreapi.ieee.org/api/v1/search/articles'\n",
    "    #     params = {\n",
    "    #         'apikey': api_key,\n",
    "    #         'format': 'json',\n",
    "    #         'max_records': rows,\n",
    "    #         'start_record': 1,\n",
    "    #         'keyword': keyword,\n",
    "    #     }\n",
    "\n",
    "    #     response = requests.get(url, params=params)\n",
    "    #     if response.status_code == 200:\n",
    "    #         return response.json()['articles']\n",
    "    #     else:\n",
    "    #         raise Exception(f\"Failed to retrieve data, HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "    def _extract_generic_data(self, soup):\n",
    "        # Tentativa de extrair o título do artigo\n",
    "        title = soup.find(['h1', 'h2', 'h3'], class_=['article-title', 'title', 'citation__title'])\n",
    "        if title:\n",
    "            title = title.get_text().strip()\n",
    "        else:\n",
    "            title = 'Title not found'\n",
    "\n",
    "        # Tentativa de extrair o resumo do artigo\n",
    "        abstract = soup.find(['div', 'section'], class_=['abstract', 'abstractSection', 'article-section__content'])\n",
    "        if abstract:\n",
    "            abstract = abstract.get_text().strip()\n",
    "        else:\n",
    "            abstract = 'Abstract not found'\n",
    "\n",
    "        # Tentativa de extrair as referências do artigo\n",
    "        references = []\n",
    "        references_section = soup.find(['ol', 'ul'], class_=['references', 'references__list', 'rlist'])\n",
    "        if references_section:\n",
    "            for ref in references_section.find_all('li'):\n",
    "                ref_text = ' '.join(ref.stripped_strings)\n",
    "                references.append(ref_text)\n",
    "\n",
    "        return title, abstract, references\n",
    "\n",
    "    def _process_articles(self, articles):\n",
    "        results = []\n",
    "        for article in articles:\n",
    "            result = {\n",
    "                \"title\": article.get(\"title\")[0] if article.get(\"title\") else \"No Title\",\n",
    "                \"DOI\": article.get(\"DOI\", \"No DOI\"),\n",
    "                \"authors\": [author.get(\"name\") for author in article.get(\"author\", []) if author.get(\"name\") is not None],\n",
    "                \"published_date\": article.get(\"published-print\", article.get(\"published-online\", \"No Date\")),\n",
    "                \"links\": [link['URL'] for link in article.get(\"link\", []) if link.get('URL')]\n",
    "            }\n",
    "            # Agora passa os DOIs e os links para extract_article_data\n",
    "            extracted_data = self.extract_article_data(result[\"links\"], [result[\"DOI\"]])\n",
    "            result[\"extracted_data\"] = extracted_data\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "    def _extract_authors(self, article):\n",
    "        authors = article.get(\"author\", [])\n",
    "        extracted_authors = []\n",
    "        for author in authors:\n",
    "            author_name = author.get(\"name\")\n",
    "            if author_name:\n",
    "                extracted_authors.append(author_name)\n",
    "        return extracted_authors\n",
    "\n",
    "    def _extract_published_date(self, article):\n",
    "        published_date = article.get(\"published-print\", article.get(\"published-online\"))\n",
    "        if published_date:\n",
    "            date_parts = published_date.get(\"date-parts\", [[]])\n",
    "            year = date_parts[0][0] if len(date_parts[0]) > 0 else \"Unknown Year\"\n",
    "            month = date_parts[0][1] if len(date_parts[0]) > 1 else \"Unknown Month\"\n",
    "            day = date_parts[0][2] if len(date_parts[0]) > 2 else \"Unknown Day\"\n",
    "            return f\"{year}-{month}-{day}\"\n",
    "        else:\n",
    "            return \"No Published Date\"\n",
    "\n",
    "    def _extract_references(self, soup):\n",
    "        references = []\n",
    "        references_list = soup.find('ol', class_='rlist references__list references__numeric')\n",
    "        if references_list:\n",
    "            for item in references_list.find_all('li', class_='references__item'):\n",
    "                reference = item.find('span', class_='references__note')\n",
    "                if reference:\n",
    "                    references.append(reference.get_text().strip())\n",
    "        return references\n",
    "\n",
    "    def _extract_doi(self, link):\n",
    "        \"\"\"\n",
    "        Extrai o DOI de um link, se disponível.\n",
    "\n",
    "        Parameters:\n",
    "            link (str): O link do artigo.\n",
    "\n",
    "        Returns:\n",
    "            str: O DOI extraído ou None se não encontrado.\n",
    "        \"\"\"\n",
    "        # Implemente a lógica para extrair o DOI do link\n",
    "        # Exemplo: Usar expressões regulares para identificar o padrão de um DOI\n",
    "        pass\n",
    "\n",
    "    def _process_articles(self, articles):\n",
    "        results = []\n",
    "        for article in articles:\n",
    "            result = {\n",
    "                \"title\": article.get(\"title\")[0] if article.get(\"title\") else \"No Title\",\n",
    "                \"DOI\": article.get(\"DOI\", \"No DOI\"),\n",
    "                \"authors\": [author.get(\"name\") for author in article.get(\"author\", []) if author.get(\"name\") is not None],\n",
    "                \"published_date\": article.get(\"published-print\", article.get(\"published-online\", \"No Date\")),\n",
    "                \"links\": [link['URL'] for link in article.get(\"link\", []) if link.get('URL')]\n",
    "            }\n",
    "            # Agora passa tanto os links quanto os DOIs para a função extract_article_data\n",
    "            extracted_data = self.extract_article_data(result[\"links\"], [result[\"DOI\"]])\n",
    "            result[\"extracted_data\"] = extracted_data\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "from graphviz import Digraph\n",
    "from IPython.display import display, Image\n",
    "\n",
    "class PrismaFlowchart:\n",
    "    def __init__(self):\n",
    "        self.flowchart = Digraph('PRISMA', filename='prisma_flowchart')\n",
    "        self.flowchart.attr(rankdir='TB')  # TB para cima para baixo, LR para esquerda para direita\n",
    "\n",
    "    def add_phase(self, phase_name, nodes, edges):\n",
    "        with self.flowchart.subgraph(name=f'cluster_{phase_name}') as phase:\n",
    "            phase.attr(label=phase_name)\n",
    "            for node in nodes:\n",
    "                phase.node(node['id'], label=node['label'], shape='box', style='rounded, filled', fillcolor='lightgrey')\n",
    "            for edge in edges:\n",
    "                phase.edge(edge['tail'], edge['head'])\n",
    "\n",
    "    def generate_flowchart(self, path='./diagrams/prisma_flowchart'):\n",
    "        self.flowchart.render(path, format='png', cleanup=True)\n",
    "        return Image(path + '.png')\n",
    "\n",
    "# Dados para o fluxograma PRISMA\n",
    "nodes_identification = [\n",
    "    {'id': 'a1', 'label': 'Artigos identificados\\n(n=75)'},\n",
    "    {'id': 'a2', 'label': 'Artigos identificados\\n(n=109)'},\n",
    "    {'id': 'a3', 'label': 'Artigos identificados\\n(n=66)'},\n",
    "]\n",
    "\n",
    "edges_identification = [\n",
    "    {'tail': 'a1', 'head': 'a2'},\n",
    "    {'tail': 'a2', 'head': 'a3'},\n",
    "]\n",
    "\n",
    "nodes_screening = [\n",
    "    {'id': 'b1', 'label': 'Artigos duplicados removidos\\n(n=24)'},\n",
    "]\n",
    "\n",
    "edges_screening = [\n",
    "    {'tail': 'a3', 'head': 'b1'},\n",
    "]\n",
    "\n",
    "nodes_eligibility = [\n",
    "    {'id': 'c1', 'label': 'Artigos excluídos\\n(n=18)'},\n",
    "    {'id': 'c2', 'label': 'Artigos de revisão\\n(n=15)'},\n",
    "    {'id': 'c3', 'label': 'Outros\\n(n=99)'},\n",
    "]\n",
    "\n",
    "edges_eligibility = [\n",
    "    {'tail': 'b1', 'head': 'c1'},\n",
    "    {'tail': 'c1', 'head': 'c2'},\n",
    "    {'tail': 'c2', 'head': 'c3'},\n",
    "]\n",
    "\n",
    "nodes_included = [\n",
    "    {'id': 'd1', 'label': 'Artigos incluídos na síntese qualitativa\\n(n=48)'},\n",
    "]\n",
    "\n",
    "edges_included = [\n",
    "    {'tail': 'c3', 'head': 'd1'},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados fictícios para o teste da classe PrismaFlowchart\n",
    "# Criação e renderização do fluxograma PRISMA\n",
    "prisma_flowchart = PrismaFlowchart()\n",
    "prisma_flowchart.add_phase('Identificação', nodes_identification, edges_identification)\n",
    "prisma_flowchart.add_phase('Triagem', nodes_screening, edges_screening)\n",
    "prisma_flowchart.add_phase('Elegibilidade', nodes_eligibility, edges_eligibility)\n",
    "prisma_flowchart.add_phase('Inclusão', nodes_included, edges_included)\n",
    "prisma_diagram = prisma_flowchart.generate_flowchart()\n",
    "\n",
    "# Exibição do fluxograma PRISMA\n",
    "display(prisma_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montar Fluxogama PRISMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygraphviz as pgv\n",
    "from IPython.display import Image\n",
    "\n",
    "def generate_prisma_flowchart(data):\n",
    "    # Calcula os valores de cada etapa do PRISMA\n",
    "    n_total = sum(data['base'].values())\n",
    "    n_novosachados = sum(data['novas_bases'].values())\n",
    "    n_removidos = sum(data['rem_citerios'].values())\n",
    "    n_triagem = n_total + n_novosachados - n_removidos\n",
    "    n_rem_titulo = data.get('n_rem_titulo', 0)\n",
    "    n_eleg_res = n_triagem - n_rem_titulo\n",
    "    n_rem_resumo = sum(data['leitura_resumos'].values())\n",
    "    n_rem_compl = sum(data['leitura_completa'].values())\n",
    "    n_eleg_compl = n_eleg_res - n_rem_resumo\n",
    "    n_incluidos_revisao = n_eleg_compl - n_rem_compl\n",
    "    n_extrabases = sum(data['add_extra_index'].values())\n",
    "    n_retiradas = data.get('n_retiradas', 0)\n",
    "    n_mantidas = n_extrabases - n_retiradas\n",
    "    n_aposleituraelegiveis = data.get('n_extraelegiveis', 0)\n",
    "    n_aposleituranaoelegiveis = data.get('n_extranaoelegiveis', 0)\n",
    "    n_incluidas_extra = n_aposleituraelegiveis - n_aposleituranaoelegiveis\n",
    "\n",
    "    # Cria um novo gráfico direcionado\n",
    "    A = pgv.AGraph(strict=False, directed=True)\n",
    "    \n",
    "    # Define atributos globais para os nós e arestas\n",
    "    A.node_attr['shape'] = 'box'\n",
    "    A.node_attr['style'] = 'filled'\n",
    "    A.node_attr['fillcolor'] = 'white'\n",
    "    \n",
    "    # Adiciona nós com os valores calculados\n",
    "    A.add_node('total', label=f'Estudos prévios\\nn={n_total}')\n",
    "    A.add_node('screening', label=f'Registros em triagem\\nn={n_triagem}')\n",
    "    A.add_node('eligible', label=f'Publicações avaliadas para elegibilidade\\nn={n_eleg_res}')\n",
    "    A.add_node('included', label=f'Total de estudos incluídos na revisão\\nn={n_incluidos_revisao}')\n",
    "    A.add_node('additional', label=f'Estudos adicionais das outras bases\\nn={n_mantidas}')\n",
    "    A.add_node('final_included', label=f'Estudos incluídos após leitura completa\\nn={n_incluidas_extra}')\n",
    "    # ... (adicionar todos os outros nós conforme necessário)\n",
    "\n",
    "    # Conecta os nós com arestas\n",
    "    A.add_edge('total', 'screening')\n",
    "    A.add_edge('screening', 'eligible')\n",
    "    A.add_edge('eligible', 'included')\n",
    "    A.add_edge('included', 'additional')\n",
    "    A.add_edge('additional', 'final_included')\n",
    "    # ... (adicionar todas as outras arestas conforme necessário)\n",
    "\n",
    "    # Define um layout e desenha o gráfico\n",
    "    A.layout(prog='dot')\n",
    "    \n",
    "    # Desenha o gráfico num arquivo PNG e exibe a imagem\n",
    "    output_path = './diagrams/simple_prisma_flowchart.png'\n",
    "    A.draw(output_path, format='png')\n",
    "    return Image(output_path)\n",
    "\n",
    "# Dicionário de dados fictícios\n",
    "data_ficticio2 = {\n",
    "    'base': {\n",
    "        'SpringerNature': 42,\n",
    "        'IEEE Xplore': 23,\n",
    "        'CrossRef': 78,\n",
    "        'CORE': 5,\n",
    "    },\n",
    "    'rem_citerios': {\n",
    "        'Duplicados': 15,\n",
    "        'Fora do período': 5,\n",
    "    },\n",
    "    'novas_bases': {\n",
    "        'IEEE': 22,\n",
    "        'ACM': 20,\n",
    "    },\n",
    "    'n_rem_titulo': 20,\n",
    "    'leitura_resumos': {\n",
    "        'Resumo inacessível': 5,\n",
    "        'Período diferente': 10,\n",
    "        'Escopo diferente': 15,\n",
    "    },\n",
    "    'leitura_completa': {\n",
    "        'Texto completo inacessivel': 20,\n",
    "        'Cenário diferente': 15,\n",
    "        'Escopo diferente': 5,\n",
    "    },\n",
    "    'add_extra_index': {\n",
    "        'Bancos de teses e dissertacões': 10,\n",
    "        'Publicação não-indexada': 5,\n",
    "        'Literatura cinza': 5,\n",
    "    },\n",
    "    'n_retiradas': 15,\n",
    "    'n_extraelegiveis': 5,\n",
    "    'n_extranaoelegiveis': 3\n",
    "}\n",
    "\n",
    "# Gera o fluxograma PRISMA com os dados do dicionário\n",
    "prisma_image = generate_prisma_flowchart(data_ficticio2)\n",
    "prisma_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ficticio2 = {\n",
    "    'base': {\n",
    "        'SpringerNature': 42,\n",
    "        'IEEE Xplore': 23,\n",
    "        'CrossRef': 78,\n",
    "        'CORE': 5,\n",
    "    },\n",
    "    'rem_citerios': {\n",
    "        'Duplicados':15,\n",
    "        'Fora do período': 5,\n",
    "    },\n",
    "    'novas_bases': {\n",
    "        'ScienceDirect': 22,\n",
    "        'ACM': 20,\n",
    "    },\n",
    "    'n_rem_titulo': 20,\n",
    "\n",
    "    'leitura_resumos':{\n",
    "        'Resumo inacessível': 5,\n",
    "        'Período diferente': 10,\n",
    "        'Escopo diferente': 15,\n",
    "    },\n",
    "\n",
    "    'leitura_completa':{\n",
    "        'Texto completo inacessivel': 20,\n",
    "        'Cenário diferente': 15,\n",
    "        'Escopo diferente': 5,\n",
    "    },\n",
    "\n",
    "    'add_extra_index': {\n",
    "        'Bancos de teses e dissertações': 10,\n",
    "        'Publicação não-indexada': 5,\n",
    "        'Literatura cinza': 5,\n",
    "    },\n",
    "\n",
    "    'n_retiradas': 15,\n",
    "    'n_extraelegiveis': 5,\n",
    "    'n_extranaoelegiveis': 3\n",
    "}\n",
    "\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "class PrismaFlowchartSubgraphs:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.flowchart = Digraph()\n",
    "        self.flowchart.attr(rankdir='LR', size='120,120')\n",
    "        self.flowchart.attr('node', shape='rectangle')\n",
    "\n",
    "    def add_box(self, name, label, width='4', height='1.5', style='filled', fillcolor='white', fontcolor='black', fontsize='12', bold=False):\n",
    "        fontname = \"Helvetica\"\n",
    "        if bold:\n",
    "            fontname += \"-Bold\"\n",
    "        self.flowchart.node(name, label, style=style, fillcolor=fillcolor, fontcolor=fontcolor, width=width, height=height, fontsize=fontsize, fontname=fontname)\n",
    "\n",
    "    def add_invisible_node(self, name, width='0', height='0', rank=None):\n",
    "        if rank:\n",
    "            with self.flowchart.subgraph() as s:\n",
    "                s.attr(rank=rank)\n",
    "                s.node(name, style='invis', width=width, height=height)\n",
    "        else:\n",
    "            self.flowchart.node(name, style='invis', width=width, height=height)\n",
    "\n",
    "    def add_edge(self, source, destination, invisible=False, constraint=True):\n",
    "        if invisible:\n",
    "            self.flowchart.edge(source, destination, style='invis')\n",
    "        else:\n",
    "            self.flowchart.edge(source, destination, constraint=str(constraint).lower())\n",
    "\n",
    "    def set_rank_same(self, *nodes):\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            for n in nodes:\n",
    "                s.node(n)\n",
    "\n",
    "    def generate_flowchart(self):\n",
    "        # Inicialização dos dados\n",
    "        n_total = sum(self.data['base'].values())\n",
    "        n_novosachados = sum(self.data['novas_bases'].values())\n",
    "        n_dupl = self.data.get('n_dupl', 0)\n",
    "        n_foraescopo = self.data.get('n_foraescopo', 0)\n",
    "        n_antigos = self.data.get('n_antigos', 0)\n",
    "        n_removidos = sum(self.data['rem_citerios'].values())\n",
    "        n_triagem = n_total+n_novosachados-n_removidos\n",
    "        n_rem_titulo = self.data.get('n_rem_titulo', 0)\n",
    "        n_eleg_res = n_triagem - n_rem_titulo\n",
    "        n_rem_resumo = sum(self.data['leitura_resumos'].values())\n",
    "        n_rem_compl = sum(self.data['leitura_completa'].values())\n",
    "        n_eleg_compl = n_eleg_res - n_rem_resumo\n",
    "        n_incluidos_revisao = n_eleg_compl - n_rem_compl\n",
    "        n_extrabases = sum(self.data['add_extra_index'].values())\n",
    "        n_retiradas = self.data.get('n_retiradas', 0)\n",
    "        n_mantidas = n_extrabases- n_retiradas\n",
    "        n_aposleituraelegiveis = self.data.get('n_extraelegiveis', 0)\n",
    "        n_aposleituranaoelegiveis = self.data.get('n_extranaoelegiveis', 0)\n",
    "        n_incluidas_extra = n_aposleituraelegiveis - n_aposleituranaoelegiveis\n",
    "\n",
    "        # Quinta linha de subgrafos\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            self.add_box('r5c0', 'Incluídos para Revisão', fillcolor='lightblue', fontsize='18', bold=True)\n",
    "            self.add_invisible_node('r5c1')\n",
    "            self.add_edge('r5c0', 'r5c1', invisible=True)\n",
    "            self.add_box('r5c2', f'Total de estudos incluídos na revisão\\n(n={n_incluidos_revisao})\\nTotal de publicações incluídas extra-bases\\n+(n={n_incluidas_extra})')\n",
    "            self.add_edge('r5c1', 'r5c2', invisible=True)\n",
    "\n",
    "        # Quarta linha de subgrafos\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            self.add_box('r4c0', 'Triagem por\\nConteúdo Completo', fillcolor='lightblue', fontsize='18', bold=True)\n",
    "            self.add_invisible_node('r4c1')\n",
    "            self.add_edge('r4c0', 'r4c1', invisible=True)\n",
    "            self.add_box('r4c2', f'Registros avaliadas como elegíveis\\npara leitura completa\\n(n={n_eleg_compl})')\n",
    "            self.add_box('r4c3', f'Registros excluídos\\ncom leitura completa\\n-(n={n_rem_compl})\\n'+\n",
    "                                \"\\n\".join([f\"{res} n= {n}\" for res, n in self.data['leitura_completa'].items()]),\n",
    "                         fillcolor='white', fontcolor='black')\n",
    "            self.add_edge('r4c1', 'r4c2', invisible=True)\n",
    "            self.add_box('r4c4', f'Publicações extra-busca avaliadas\\ncomo elegíveis pós leitura completa\\n+(n={n_incluidas_extra})')\n",
    "            self.add_box('r4c5', f'Publicações extra-busca avaliadas\\ncomo não-elegíveis pós leitura completa\\n-(n={self.data.get(\"n_extranaoelegiveis\", 0)})')\n",
    "            self.add_edge('r4c3', 'r4c4', invisible=True)\n",
    "\n",
    "        # Terceira linha de subgrafos\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            self.add_box('r3c0', 'Triagem por Resumos', fillcolor='lightblue', fontsize='18', bold=True)\n",
    "            self.add_invisible_node('r3c1')\n",
    "            self.add_edge('r3c0', 'r3c1', invisible=True)\n",
    "            self.add_box('r3c2', f'Registros avaliados como\\nelegíveis para leitura resumos\\n(n={n_eleg_res})')\n",
    "            self.add_box('r3c3', f'Registros excluídos\\ncom leitura do resumo\\n-(n={n_rem_resumo})\\n'+\n",
    "                                \"\\n\".join([f\"{res} n= {n}\" for res, n in self.data['leitura_resumos'].items()]),\n",
    "                         fillcolor='white', fontcolor='black')\n",
    "            self.add_edge('r3c1', 'r3c2', invisible=True)\n",
    "            self.add_box('r3c4', f'Publicações extra-busca pesquisadas\\nmantidas para leitura completa\\n(n={n_mantidas})')\n",
    "            self.add_box('r3c5', f'Publicações retiradas pós leitura resumos\\n-(n={n_retiradas})')\n",
    "            self.add_edge('r3c3', 'r3c4', invisible=True)\n",
    "            self.add_edge('r3c4', 'r3c5')\n",
    "\n",
    "        # Segunda linha de subgrafos\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            self.add_box('r2c0', 'Triagem por Títulos', fillcolor='lightblue', fontsize='18', bold=True)\n",
    "            self.add_invisible_node('r2c1')\n",
    "            self.add_edge('r2c0', 'r2c1', invisible=True)\n",
    "            self.add_box('r2c2', f'Total de Registros para triagem\\n(n={n_triagem})')\n",
    "            self.add_box('r2c3', f'Registros excluídos\\ncom leitura do título\\n-(n={self.data.get(\"n_rem_titulo\", 0)})')\n",
    "            self.add_edge('r2c1', 'r2c2', invisible=True)\n",
    "            self.add_invisible_node('r2c4')\n",
    "            self.add_edge('r2c3', 'r2c4', invisible=True)\n",
    "\n",
    "        # Primeira linha de subgrafos\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            self.add_box('r1c0', 'Identificação', fillcolor='lightblue', fontsize='18', bold=True)\n",
    "            self.add_box('r1c1', f\"Estudos incluídos na busca\\nde revisão (n = {n_total})\\n\"+\n",
    "                                \"\\n\".join([f\"{base} n= {n}\" for base, n in self.data['base'].items()]),\n",
    "                         fillcolor='white', fontcolor='black')\n",
    "\n",
    "            self.add_box('r1c2', f'Total de registros achados\\nnas bases de dados\\n(n= {n_total})\\nTotal de novos achados\\n+(n={n_novosachados})\\n'+ \"\\n\".join([f\"{nova_base} n= {n}\" for nova_base, n in self.data['novas_bases'].items()]),\n",
    "                         fillcolor='white', fontcolor='black')\n",
    "            self.add_box('r1c3', f\"Removidos antes da triagem\\n-(n={n_removidos})\\n\"+\"\\n\".join([f\"{rem_citerios} n= {n}\" for rem_citerios, n in self.data['rem_citerios'].items()]),\n",
    "                         fillcolor='white', fontcolor='black')\n",
    "            self.add_box('r1c4', f'Registros identificados em outras bases:\\n+(n={n_extrabases})\\n'+ \"\\n\".join([f\"{extra_base} n= {n}\" for extra_base, n in self.data['add_extra_index'].items()]),\n",
    "                         fillcolor='white', fontcolor='black')\n",
    "            # Arestas da primeira linha\n",
    "            self.add_edge('r1c0', 'r1c1', invisible=True)\n",
    "\n",
    "        # Adicionando os rótulos de linha amarelos no topo\n",
    "        with self.flowchart.subgraph() as s:\n",
    "            s.attr(rank='same')\n",
    "            self.add_box('r0c1', 'Achados através de\\nbuscas por\\npalavras-chave', fillcolor='yellow', fontsize='16', bold=True)\n",
    "            self.add_box('r0c2', 'Achados através\\nde buscas em\\nbases de dados', width='4', fillcolor='yellow', fontsize='16', bold=True)\n",
    "            self.add_box('r0c3', 'Achados através\\nde buscas em\\nbases de dados e\\nremovidos após análise', width='4', fillcolor='yellow', fontsize='16', bold=True)\n",
    "            self.add_box('r0c4', 'Identificação de estudos\\ndentro dos critérios\\nextra-busca', fillcolor='yellow', fontsize='16', bold=True)\n",
    "            self.add_box('r0c5', 'Identificação de estudos\\ndentro dos critérios\\nextra-busca\\nremovidos após análise', fillcolor='yellow', fontsize='16', bold=True)\n",
    "\n",
    "        # Arestas entre os rótulos amarelos\n",
    "        self.add_invisible_node('r0c0', rank='same')\n",
    "        self.add_edge('r0c0', 'r0c1', invisible=True)\n",
    "        self.add_edge('r0c1', 'r0c2', invisible=True)\n",
    "        self.add_edge('r0c2', 'r0c3', invisible=True)\n",
    "        self.add_edge('r0c3', 'r0c4', invisible=True)\n",
    "        self.add_edge('r0c4', 'r0c5', invisible=True)\n",
    "\n",
    "        # Conectar colunas adjacentes horizontalmente conforme necessário\n",
    "        # Conexões horizontais da primeira linha\n",
    "        self.add_edge('r1c1', 'r1c2')\n",
    "        self.add_edge('r1c2', 'r1c3')\n",
    "        self.add_edge('r1c3', 'r1c4', invisible=True)\n",
    "\n",
    "        # Conexões horizontais da segunda linha\n",
    "        self.add_edge('r2c2', 'r2c3')\n",
    "        self.add_edge('r2c3', 'r2c4', invisible=True)\n",
    "\n",
    "        # Conexões horizontais da terceira linha\n",
    "        self.add_edge('r3c2', 'r3c3')\n",
    "\n",
    "        # Conexões horizontais da quarta linha\n",
    "        self.add_edge('r4c2', 'r4c3')\n",
    "        self.add_edge('r4c3', 'r4c4', invisible=True)\n",
    "        self.add_edge('r4c4', 'r4c5')\n",
    "\n",
    "        # Conexões horizontais da quinta linha\n",
    "        self.add_edge('r5c0', 'r5c1', invisible=True)\n",
    "        self.add_edge('r5c1', 'r5c2', invisible=True)\n",
    "\n",
    "        # Conectar colunas verticalmente com constraint='false' para manter o layout (está falhando)\n",
    "        # Arestas verticais para manter o layout\n",
    "        # self.add_edge('r4c0', 'r5c0')\n",
    "        # self.add_edge('r3c0', 'r4c0')\n",
    "        # self.add_edge('r2c0', 'r3c0')\n",
    "        # self.add_edge('r1c0', 'r2c0')\n",
    "        # self.add_edge('r0c0', 'r1c0')\n",
    "\n",
    "        # Arestas verticais do fluxo de soma\n",
    "        # self.add_edge('r1c1', 'r2c2', constraint='false')\n",
    "        # self.add_edge('r2c2', 'r3c2', constraint='false')\n",
    "        # self.add_edge('r3c2', 'r4c2', constraint='false')\n",
    "        # self.add_edge('r4c2', 'r5c2', constraint='false')\n",
    "        # self.add_edge('r4c4', 'r5c2', constraint='false')\n",
    "\n",
    "        # Configuração do layout e renderização\n",
    "        self.flowchart.engine = 'dot'\n",
    "        return self.flowchart\n",
    "\n",
    "\n",
    "# Instanciar a classe e gerar o fluxograma\n",
    "prisma_chart = PrismaFlowchartSubgraphs(data_ficticio2)\n",
    "flowchart = prisma_chart.generate_flowchart()\n",
    "\n",
    "# Visualizar o fluxograma\n",
    "flowchart.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygraphviz as pgv\n",
    "from IPython.display import Image\n",
    "\n",
    "# Define the structure of the graph (5x5 nodes, fixed positions)\n",
    "A = pgv.AGraph(strict=False, directed=True)\n",
    "\n",
    "# Set nodes in 5x5 fixed positions, label them according to the given structure.\n",
    "nodes = {}\n",
    "counter = 0\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        node_name = f'node{counter}'\n",
    "        nodes[(i, j)] = node_name\n",
    "        A.add_node(node_name, label='', pos=f\"{i},{4-j}!\")\n",
    "        counter += 1\n",
    "\n",
    "# Manually connect the nodes as per the provided structure.\n",
    "# Connections are made based on the visual layout of the PRISMA flow diagram.\n",
    "A.add_edge(nodes[(0, 1)], nodes[(1, 2)])\n",
    "A.add_edge(nodes[(1, 2)], nodes[(1, 3)])\n",
    "A.add_edge(nodes[(1, 3)], nodes[(2, 4)])\n",
    "A.add_edge(nodes[(2, 4)], nodes[(3, 3)])\n",
    "A.add_edge(nodes[(3, 3)], nodes[(3, 2)])\n",
    "A.add_edge(nodes[(3, 2)], nodes[(4, 1)])\n",
    "\n",
    "A.add_edge(nodes[(1, 0)], nodes[(2, 0)])\n",
    "A.add_edge(nodes[(2, 0)], nodes[(3, 1)])\n",
    "A.add_edge(nodes[(3, 1)], nodes[(3, 2)])\n",
    "\n",
    "A.add_edge(nodes[(2, 2)], nodes[(3, 2)])\n",
    "\n",
    "# Apply neato layout with fixed positions\n",
    "A.layout(prog='neato')\n",
    "\n",
    "# Draw the graph to a file and then display it\n",
    "output_path = './diagrams/diagram5x5.png'\n",
    "A.draw(output_path)\n",
    "Image(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygraphviz as pgv\n",
    "from IPython.display import Image\n",
    "\n",
    "# Define the structure of the graph\n",
    "A = pgv.AGraph(strict=False, directed=True)\n",
    "\n",
    "# Define node attributes to style the graph\n",
    "A.node_attr['shape'] = 'box'\n",
    "A.node_attr['style'] = 'filled'\n",
    "A.node_attr['fillcolor'] = 'white'\n",
    "A.node_attr['fontname'] = 'Arial'\n",
    "\n",
    "# Add nodes with labels as per the PRISMA diagram\n",
    "A.add_node('previous_studies', label='Estudos prévios\\nn=148', shape='box', fillcolor='lightblue')\n",
    "A.add_node('records_identified', label='Registros identificados\\nn=42', shape='box')\n",
    "A.add_node('records_screened', label='Registros em triagem\\nn=55', shape='box')\n",
    "A.add_node('records_excluded', label='Registros excluídos*', shape='plaintext')\n",
    "A.add_node('full_text', label='Publicações pesquisadas para\\nleitura na íntegra\\nn=55', shape='box')\n",
    "A.add_node('full_text_assessed', label='Publicações avaliadas para\\nelegibilidade\\nn=33', shape='box')\n",
    "A.add_node('studies_included', label='Total de estudos incluídos na\\nrevisão\\nn=15', shape='box')\n",
    "# ... (add all other nodes as per the PRISMA diagram)\n",
    "\n",
    "# Connect nodes with edges as per the PRISMA flow\n",
    "A.add_edge('previous_studies', 'records_identified')\n",
    "A.add_edge('records_identified', 'records_screened')\n",
    "A.add_edge('records_screened', 'records_excluded')\n",
    "A.add_edge('records_screened', 'full_text')\n",
    "A.add_edge('full_text', 'full_text_assessed')\n",
    "A.add_edge('full_text_assessed', 'studies_included')\n",
    "# ... (add all other edges as per the PRISMA diagram)\n",
    "\n",
    "# Define layout and draw the graph\n",
    "A.layout(prog='dot')  # The 'dot' layout engine is well-suited for hierarchical graphs like PRISMA\n",
    "\n",
    "# Draw the graph to a file and then display it\n",
    "output_path = './diagrams/simple_prisma_diagram.png'\n",
    "A.draw(output_path, format='png')\n",
    "Image(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the user's request, we'll modify the colors, shapes, and arrows to match the PRISMA flow diagram.\n",
    "\n",
    "# Define colors and shapes for each node as per the PRISMA chart\n",
    "# In the original PRISMA chart, different nodes have different colors and shapes based on their function.\n",
    "# For simplicity, I'll use a couple of colors and shapes to differentiate them.\n",
    "\n",
    "# Update node attributes for colors and shapes\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        node_name = nodes[(i, j)]\n",
    "        # Set the default attributes\n",
    "        attrs = {\n",
    "            'shape': 'box',\n",
    "            'style': 'filled',\n",
    "            'color': 'lightgrey',\n",
    "            'fontcolor': 'black'\n",
    "        }\n",
    "        # Customize attributes for specific nodes based on their position\n",
    "        # This is an arbitrary choice to demonstrate the functionality since the original labels are not provided\n",
    "        if (i + j) % 2 == 0:\n",
    "            attrs.update({\n",
    "                'color': 'lightblue',\n",
    "                'shape': 'ellipse',\n",
    "            })\n",
    "        A.get_node(node_name).attr.update(attrs)\n",
    "\n",
    "# Update edge attributes for colors and arrows\n",
    "for edge in A.edges():\n",
    "    edge.attr.update({\n",
    "        'color': 'red',\n",
    "        'arrowhead': 'normal',\n",
    "        'arrowsize': 1\n",
    "    })\n",
    "\n",
    "# Redraw the graph with the updated attributes\n",
    "A.draw(output_path)\n",
    "Image(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def search_science_direct_api(keyword, api_key):\n",
    "    \"\"\"\n",
    "    Searches the ScienceDirect API for articles related to a specific keyword while adhering to API limits.\n",
    "    \n",
    "    Parameters:\n",
    "        keyword (str): The search keyword.\n",
    "        api_key (str): Your ScienceDirect API key.\n",
    "        \n",
    "    Returns:\n",
    "        int: The number of articles that match the query or an error message.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = \"https://api.elsevier.com/content/search/scidir\"\n",
    "    headers = {\n",
    "        \"X-ELS-APIKey\": api_key,\n",
    "        \"Accept\": \"application/json\",  # Ensure the response is in JSON format\n",
    "        \"User-Agent\": \"My-ScienceDirect-Scraper\"  # User agent header to mimic a real user\n",
    "    }\n",
    "    params = {\n",
    "        \"query\": keyword,\n",
    "        \"count\": 200  # Max number of results allowed per request as per API specs\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Respect the API rate limit (2 requests per second for ScienceDirect Search v2)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Make the API request\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        \n",
    "        # Validate the response\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON response\n",
    "            data = json.loads(response.text)\n",
    "            \n",
    "            # Retrieve the total number of search results\n",
    "            total_results = int(data['search-results']['opensearch:totalResults'])\n",
    "            \n",
    "            return total_results\n",
    "        \n",
    "        else:\n",
    "            return f\"Failed to retrieve data, HTTP Status Code: {response.status_code}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example usage\n",
    "api_key = \"57f20882ed177ae828ae2c1c6d1db58f\"\n",
    "keyword = \"DoWhy\"\n",
    "results = search_science_direct_api(keyword, api_key)\n",
    "print(f\"Number of articles related to '{keyword}' in ScienceDirect: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install elsapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elssearch import ElsSearch\n",
    "import json\n",
    "\n",
    "def search_science_direct_elsapy(api_key, keyword, article_type=['review', 'research-article']):\n",
    "    \"\"\"Search the number of articles of a specific type containing the keyword in ScienceDirect using elsapy.\n",
    "    \n",
    "    Parameters:\n",
    "    - api_key (str): The API key for accessing Elsevier APIs.\n",
    "    - keyword (str): The search keyword.\n",
    "    - article_type (list): List of types of articles to be considered, default is ['review', 'research-article'].\n",
    "    \n",
    "    Returns:\n",
    "    int: Number of articles found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the client\n",
    "    client = ElsClient(api_key)\n",
    "    \n",
    "    # Initialize search object and execute search, specifying the index to search the query in ('scidir' for ScienceDirect)\n",
    "    search_obj = ElsSearch(f\"{keyword} AND ({' OR '.join(['DOCTYPE('+x+')' for x in article_type])})\",'scidir')\n",
    "    search_obj.execute(client, get_all = False)\n",
    "    \n",
    "    # Extract and return the number of results\n",
    "    return search_obj.resultsFound\n",
    "\n",
    "# Example usage\n",
    "api_key = '57f20882ed177ae828ae2c1c6d1db58f'\n",
    "keyword = \"DoWhy\"\n",
    "result_count = search_science_direct_elsapy(api_key, keyword)\n",
    "print(f\"Number of articles related to '{keyword}' in ScienceDirect: {result_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myAuth = HTTPBasicAuth('usuario', 'senha')\n",
    "\n",
    "myCl = ElsClient('[57f20882ed177ae828ae2c1c6d1db58f]')\n",
    "myAuth.read(myCl)\n",
    "myAuth.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "class ArticleDataExtractor:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://api.crossref.org/works\"\n",
    "\n",
    "    def search_crossref_api_tdm_for_researchers(self, keyword, rows=10):\n",
    "        params = {\n",
    "            \"query\": keyword,\n",
    "            \"rows\": rows,\n",
    "            \"mailto\": \"your-email@example.com\",\n",
    "            \"select\": \"title,DOI,author,published-print,published-online,link\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(self.base_url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data = json.loads(response.text)\n",
    "                articles = data['message']['items']\n",
    "                return self._process_articles(articles)\n",
    "            else:\n",
    "                return f\"Failed to retrieve data, HTTP Status Code: {response.status_code}\"\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {e}\"\n",
    "\n",
    "    def _extract_references(self, soup):\n",
    "        references = []\n",
    "        references_list = soup.find('ol', class_='rlist references__list references__numeric')\n",
    "        if references_list:\n",
    "            for item in references_list.find_all('li', class_='references__item'):\n",
    "                reference = item.find('span', class_='references__note')\n",
    "                if reference:\n",
    "                    references.append(reference.get_text().strip())\n",
    "        return references\n",
    "\n",
    "    def _extract_acm_data(self, soup):\n",
    "        title = soup.find('h1', class_='citation__title')\n",
    "        abstract = soup.find('div', class_='abstractSection abstractInFull')\n",
    "        references = self._extract_references(soup)\n",
    "        if title and abstract:\n",
    "            return title.get_text().strip(), abstract.get_text().strip(), references\n",
    "        else:\n",
    "            return 'Title not found', 'Abstract not found', references\n",
    "\n",
    "    def extract_wiley_data(self, soup):\n",
    "        \"\"\"\n",
    "        Extracts title, summary, and references from a Wiley article page.\n",
    "\n",
    "        Parameters:\n",
    "            soup (BeautifulSoup): Parsed HTML content of the Wiley article page.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: The extracted title, summary, and references.\n",
    "        \"\"\"\n",
    "        title = soup.find('h2', class_='citation__title')\n",
    "        summary = soup.find('div', class_='article-section__content')\n",
    "        \n",
    "        references_section = soup.find('section', id='references-section')\n",
    "        references = []\n",
    "        if references_section:\n",
    "            for ref in references_section.find_all('li', {'data-bib-id': True}):\n",
    "                ref_text = ' '.join(ref.stripped_strings)\n",
    "                references.append(ref_text)\n",
    "\n",
    "        return (\n",
    "            title.get_text().strip() if title else 'Title not found',\n",
    "            summary.get_text().strip() if summary else 'Summary not found',\n",
    "            references\n",
    "        )\n",
    "\n",
    "    def extract_article_data(self, links):\n",
    "        extracted_data = {}\n",
    "        session = requests.Session()\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Referer\": \"https://www.google.com\"\n",
    "        }\n",
    "\n",
    "        for link in links:\n",
    "            try:\n",
    "                time.sleep(2)  # Aguarda 2 segundos entre as solicitações\n",
    "                response = session.get(link, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                    if 'dl.acm.org' in link:\n",
    "                        title, abstract, references = self._extract_acm_data(soup)\n",
    "                    elif 'onlinelibrary.wiley.com' in link:\n",
    "                        title, summary, references = self.extract_wiley_data(soup)\n",
    "                        abstract = summary  # Renomeia summary para abstract para manter consistência\n",
    "                    else:\n",
    "                        # Generic extraction logic for other sites\n",
    "                        title = 'Title extraction logic here'\n",
    "                        abstract = 'Abstract extraction logic here'\n",
    "                        references = 'References extraction logic here'\n",
    "\n",
    "                    extracted_data[link] = {\"title\": title, \"abstract\": abstract, \"references\": references}\n",
    "                else:\n",
    "                    extracted_data[link] = f\"Failed to retrieve data, HTTP Status Code: {response.status_code}\"\n",
    "            except Exception as e:\n",
    "                extracted_data[link] = f\"An error occurred: {e}\"\n",
    "        return extracted_data\n",
    "\n",
    "    def _process_articles(self, articles):\n",
    "        results = []\n",
    "        for article in articles:\n",
    "            result = {\n",
    "                \"title\": article.get(\"title\")[0] if article.get(\"title\") else \"No Title\",\n",
    "                \"DOI\": article.get(\"DOI\", \"No DOI\"),\n",
    "                \"authors\": [author.get(\"name\") for author in article.get(\"author\", []) if author.get(\"name\") is not None],\n",
    "                \"published_date\": article.get(\"published-print\", article.get(\"published-online\", \"No Date\")),\n",
    "                \"links\": [link['URL'] for link in article.get(\"link\", []) if link.get('URL')]\n",
    "            }\n",
    "            extracted_data = self.extract_article_data(result[\"links\"])\n",
    "            result[\"extracted_data\"] = extracted_data\n",
    "            results.append(result)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar a classe\n",
    "extractor = ArticleDataExtractor()\n",
    "\n",
    "# Definir palavras-chave e número de resultados\n",
    "keyword = \"graph machine learning\"\n",
    "rows = 1000  # Número de artigos a serem buscados\n",
    "\n",
    "# Obter resultados da pesquisa\n",
    "search_results = extractor.search_crossref_api_tdm_for_researchers(keyword, rows)\n",
    "\n",
    "# Preparar dados para o DataFrame\n",
    "data_for_df = []\n",
    "for result in search_results:\n",
    "    for link, data in result['extracted_data'].items():\n",
    "        data_for_df.append({\n",
    "            \"DOI\": result['DOI'],\n",
    "            \"Title\": data.get(\"title\", \"\"),\n",
    "            \"Abstract\": data.get(\"abstract\", \"\"),\n",
    "            \"References\": data.get(\"references\", [])\n",
    "        })\n",
    "\n",
    "# Converter para DataFrame\n",
    "df = pd.DataFrame(data_for_df)\n",
    "\n",
    "# Exibir o DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resposta exemplo na CORE API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Linux; U; Android 4.2.2; he-il; NEO-X5-116A Build/JDQ39) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Safari/534.30\"}\n",
    "\n",
    "query='graph AND innovation AND ontology'\n",
    "encoded_query=urllib.parse.quote(query)\n",
    "search_url = \"https://api.core.ac.uk/v3/search/works/\"+f\"?api_key={core_api_key}&q={encoded_query}\"\n",
    "print(search_url)\n",
    "response = requests.get(search_url, headers=headers)\n",
    "print(response.status_code)\n",
    "data=response.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_endpoint = \"https://api.core.ac.uk/v3/\"\n",
    "\n",
    "def pretty_json(json_object): print(json.dumps(json_object, indent=2))\n",
    "\n",
    "def get_entity(url_fragment):\n",
    "    # Não está funcionando com api_key no cabeçalho da requisição\n",
    "    # headers={\"Authorization\":\"Bearer \"+core_api_key}\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Linux; U; Android 4.2.2; he-il; NEO-X5-116A Build/JDQ39) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Safari/534.30\"}\n",
    "    try:\n",
    "        response = requests.get(api_endpoint + url_fragment, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json(), response.elapsed.total_seconds()\n",
    "        else:\n",
    "            print(f\"Error code {response.status_code}\") \n",
    "            print(f\"Conteúdo: {response.content}\")\n",
    "            return None, None\n",
    "    except:\n",
    "        print('Não foi possível')\n",
    "\n",
    "works, elapsed = get_entity(\"search/works\")\n",
    "pretty_json(works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api(url_fragment, query,limit=100):\n",
    "    # headers={\"Authorization\":\"Bearer \"+core_api_key}\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Linux; U; Android 4.2.2; he-il; NEO-X5-116A Build/JDQ39) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Safari/534.30\"}\n",
    "    query = {\"q\":query, \"limit\":limit}\n",
    "    response = requests.get(f\"{api_endpoint}{url_fragment}\",data = json.dumps(query), headers=headers)\n",
    "    if response.status_code ==200:\n",
    "        return response.json(), response.elapsed.total_seconds()\n",
    "    else:\n",
    "        print(f\"Error code {response.status_code}\")\n",
    "        return None, None\n",
    "\n",
    "results , elapsed = query_api(\"search/works\",query)\n",
    "\n",
    "pretty_json(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '\"machine learning\" AND graph AND innovation AND ontology'\n",
    "urllib.parse.quote(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url_base = 'https://api.core.ac.uk/v3/search/works/'\n",
    "query = '\"machine learning\" AND graph AND innovation AND ontology'\n",
    "encoded_query = urllib.parse.quote(query)\n",
    "# headers = f\"Authorization': Bearer {core_api_key}\"\n",
    "# print(headers)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Linux; U; Android 4.2.2; he-il; NEO-X5-116A Build/JDQ39) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Safari/534.30\"}\n",
    "params = {\n",
    "    # 'q': query,\n",
    "    'offset': 0,\n",
    "    'limit': 10,\n",
    "    'stats': False,\n",
    "}\n",
    "url = f\"{url_base}?api_key={core_api_key}&q={encoded_query}\"\n",
    "print(url)\n",
    "\n",
    "response = requests.post(url, params=params)\n",
    "response\n",
    "# data = response.json()\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resposta exemplo da IEEEXplrore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_query_ieee = 'https://ieeexploreapi.ieee.org/api/v1/search/articles?apikey=n7z9zut2vzf68nmkack8p3yu&format=json&max_records=100&start_record=1&sort_order=asc&sort_field=article_number&abstract=ontology+AND+innovation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma instância da classe XPLORE\n",
    "xplore = XPLORE(ieee_api_key)\n",
    "\n",
    "# Configura os parâmetros de busca\n",
    "xplore.articleTitle('(graph OR ontology) AND \"Innovation Model\"')\n",
    "xplore.maximumResults(500)\n",
    "\n",
    "# Realiza a busca\n",
    "ieee_results = xplore.callAPI()\n",
    "\n",
    "# Trata os resultados\n",
    "# (Dependendo do formato escolhido, JSON ou XML, o tratamento dos dados pode variar)\n",
    "print(ieee_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_response_ieee = ieee_results\n",
    "ex_response_ieee['total_searched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_response_ieee['total_records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ieee_response = pd.DataFrame(ex_response_ieee['articles'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ieee_response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ieee_response['content_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_df = df_ieee_response.loc[df_ieee_response['content_type'] == 'Journals']\n",
    "journals_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_df[['publication_year', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Partindo de 'journals_df' como DataFrame original\n",
    "# Criar DataFrame com as colunas desejadas\n",
    "filtered_df = journals_df[['publication_year','doi', 'title', 'abstract']]\n",
    "\n",
    "# Listar substrings a serem buscados nos resultados\n",
    "substrings = ['innovation', 'ontology']\n",
    "\n",
    "# Verificar se alguma das substrings está presente em uma string, independente do case\n",
    "def contains_substring(s):\n",
    "    s = str(s)\n",
    "    return any(pd.notna(s) and re.search(substring, s, re.IGNORECASE) for substring in substrings)\n",
    "\n",
    "# Filtrar o DataFrame\n",
    "final_df = filtered_df[filtered_df.apply(lambda row: row.apply(contains_substring).any(), axis=1)]\n",
    "print(f\"{len(final_df.index)} resultados contendo nos títulos ou resumos pelo menos uma das palavras-chave {substrings}\")\n",
    "final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de Chamada à CORE API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://api.core.ac.uk/v3/search/works/?api_key=bqpJg1oMvXCIVDBKjWLr0nEiR7OGucT5&q=graph%20AND%20innovation%20AND%20ontology&"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Linux; U; Android 4.2.2; he-il; NEO-X5-116A Build/JDQ39) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Safari/534.30\"}\n",
    "\n",
    "query='graph AND innovation AND ontology'\n",
    "encoded_query=urllib.parse.quote(query)\n",
    "search_url = \"https://api.core.ac.uk/v3/search/works/\"+f\"?api_key={core_api_key}&q={encoded_query}\"\n",
    "print(search_url)\n",
    "response = requests.get(search_url, headers=headers)\n",
    "print(response.status_code)\n",
    "data=response.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from matplotlib import rcParams\n",
    "import requests\n",
    "import json \n",
    "import seaborn as sns \n",
    "import itertools \n",
    "from datetime import datetime \n",
    "import matplotlib.pyplot as plt \n",
    "import networkx as nx \n",
    "import hashlib \n",
    "import os \n",
    "from operator import itemgetter\n",
    "\n",
    "rcParams['figure.figsize'] = 11.7,8.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'graph AND innovation AND ontology'\n",
    "\n",
    "core_client = CoreClient(core_api_key)\n",
    "total_results = core_client.core_find(query=query, limit=1, offset=0)\n",
    "if total_results:\n",
    "    print(f\"Total results: {len(total_results)}\")\n",
    "else:\n",
    "    print(f\"Nunhum resultado retornado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outras fontes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Se a busca for confirmada e houver resultados\n",
    "# if total_results > 0:\n",
    "#     benchmark_results = aggregator.benchmark_methods()\n",
    "#     print(\"\\nResultados do Benchmark:\")\n",
    "#     print(benchmark_results)\n",
    "#     aggregator.print_search_history()\n",
    "# else:\n",
    "#     print(\"\\nBusca cancelada ou sem resultados para continuar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = ApiCallTracker()\n",
    "\n",
    "# Exemplo para IEEE Xplore\n",
    "if tracker.can_make_call('ieee_xplore', 200): # Limite real da API IEEE Xplore\n",
    "    # Faça a chamada à API IEEE Xplore\n",
    "    searcher = ResearchPaperSearcher(\"Artificial Intelligence\")\n",
    "    searcher.execute_search()\n",
    "    # searcher.generate_prisma_diagram()\n",
    "\n",
    "    tracker.register_call('ieee_xplore')\n",
    "    remaining_calls_ieee = tracker.get_remaining_calls('ieee_xplore', 200)\n",
    "    print(f\"Chamada à API IEEE Xplore realizada. Chamadas restantes: {remaining_calls_ieee}\")\n",
    "else:\n",
    "    remaining_calls_ieee = tracker.get_remaining_calls('ieee_xplore', 200)\n",
    "    print(f\"Limite de chamadas diárias para IEEE Xplore atingido. Chamadas restantes: {remaining_calls_ieee}\")\n",
    "\n",
    "# Exemplo para Google Scholar\n",
    "if tracker.can_make_call('google_scholar', 1000):  # Supondo um limite fictício\n",
    "    # Faça a busca no Google Scholar\n",
    "    tracker.register_call('google_scholar')\n",
    "    remaining_calls_gs = tracker.get_remaining_calls('google_scholar', 1000)\n",
    "    print(f\"Buscas no Google Scholar realizadas. Chamadas restantes: {remaining_calls_gs}\")\n",
    "else:\n",
    "    remaining_calls_gs = tracker.get_remaining_calls('google_scholar', 1000)\n",
    "    print(f\"Limite de chamadas diárias para Google Scholar atingido. Chamadas restantes: {remaining_calls_gs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://scholar.google.com/scholar?hl=pt-BR&as_sdt=0%2C5&as_vis=1&q=%22CausalML%22+%22Inference%22&btnG=\"\n",
    "\"https://scholar.google.com/scholar?hl=pt-BR&as_sdt=0%2C5&as_vis=1&q=%22CausalML%22+AND+%22Inference%22&btnG=\"\n",
    "\"https://scholar.google.com/scholar?hl=pt-BR&as_sdt=0%2C5&as_vis=1&q=%22CausalML%22+AND+%22Inference%22&btnG=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas necessárias\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "def format_gs_url(keywords):\n",
    "    # Base URL\n",
    "    base_url = \"https://scholar.google.com/scholar?as_sdt=0,5&q=\"\n",
    "    \n",
    "    # Parâmetros adicionais que serão constantes\n",
    "    additional_params = \"&hl=pt-BR&as_vis=1\"\n",
    "\n",
    "    # Verificando se keywords é uma única palavra-chave ou uma lista\n",
    "    if isinstance(keywords, list):\n",
    "        # Processamento para uma lista de palavras-chave\n",
    "        formatted_keywords = ['%22' + keyword.replace(\" \", \"%20\") + '%22' for keyword in keywords]\n",
    "        query = 'qs=' + '%20'.join(formatted_keywords)\n",
    "    elif \" \" in keywords:\n",
    "        # Processamento para string com único termo formado por palavra-chave composta\n",
    "        query = 'qs=%22'+ keywords.replace(\" \", \"%20\")+'%22'\n",
    "    else:\n",
    "        query = 'qs=%22'+ keywords.replace(\" \", \"%20\")+'%22'\n",
    "\n",
    "    # Retornando a URL completa\n",
    "    return base_url + query + additional_params\n",
    "\n",
    "def format_sd_url(keywords):\n",
    "    # Base URL\n",
    "    base_url = \"https://www.sciencedirect.com/search?\"\n",
    "    \n",
    "    # Parâmetros adicionais que serão constantes\n",
    "    additional_params = \"&show=100&lastSelectedFacet=subjectAreas&articleTypes=REV%2CFLA&subjectAreas=1800%2C1700\"\n",
    "\n",
    "    # Verificando se keywords é uma única palavra-chave ou uma lista\n",
    "    if isinstance(keywords, list):\n",
    "        # Processamento para uma lista de palavras-chave\n",
    "        formatted_keywords = ['%22' + keyword.replace(\" \", \"%20\") + '%22' for keyword in keywords]\n",
    "        query = 'qs=' + '%20'.join(formatted_keywords)\n",
    "    elif \" \" in keywords:\n",
    "        # Processamento para string com único termo formado por palavra-chave composta\n",
    "        query = 'qs=%22'+ keywords.replace(\" \", \"%20\")+'%22'\n",
    "    else:\n",
    "        query = 'qs=%22'+ keywords.replace(\" \", \"%20\")+'%22'\n",
    "\n",
    "    # Retornando a URL completa\n",
    "    return base_url + query + additional_params\n",
    "\n",
    "def search_google_scholar(keyword):\n",
    "    url = f\"https://scholar.google.com/scholar?q={keyword}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    results = soup.find('div', {'id': 'gs_ab_md'})\n",
    "    if results:\n",
    "        return results.text.split()[1]\n",
    "    else:\n",
    "        return \"Data not available\"\n",
    "\n",
    "def search_pubmed(keyword):\n",
    "    url = f\"https://pubmed.ncbi.nlm.nih.gov/?term={keyword}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    results = soup.find('div', {'class': 'results-amount'})\n",
    "    if results:\n",
    "        return results.text.strip().split()[0]\n",
    "    else:\n",
    "        return \"Data not available\"\n",
    "\n",
    "def search_science_direct_selenium(keywords):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    url = format_sd_url(keywords)\n",
    "    driver.get(url)\n",
    "\n",
    "    article_titles = []\n",
    "    try:\n",
    "        while True:\n",
    "            # Espera pelos elementos dos títulos dos artigos serem carregados\n",
    "            WebDriverWait(driver, 30).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"h2 span.anchor-text span\"))\n",
    "            )\n",
    "            # Extração dos títulos\n",
    "            title_elements = driver.find_elements(By.CSS_SELECTOR, \"h2 span.anchor-text span\")\n",
    "            for element in title_elements:\n",
    "                article_titles.append(element.text)\n",
    "\n",
    "            # Verifica se existe uma próxima página e navega para ela\n",
    "            next_button = driver.find_elements(By.CSS_SELECTOR, \"#srp-pagination-options .next-link a\")\n",
    "            if next_button:\n",
    "                next_button[0].click()\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    except TimeoutException:\n",
    "        return [], \"Timeout: Element not found\"\n",
    "    except Exception as e:\n",
    "        return [], f\"Exception encountered: {e}\"\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return article_titles, len(article_titles)\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     keywords = [\"DoWhy\", \"CausalML\"]\n",
    "\n",
    "#     for keyword in keywords:\n",
    "#         print(f\"Fetching data for {keyword}...\")\n",
    "        \n",
    "#         # Google Scholar\n",
    "#         gs_count = search_google_scholar(keyword)\n",
    "#         print(f\"Number of articles mentioning {keyword} in Google Scholar: {gs_count:5}\")\n",
    "\n",
    "#         # PubMed\n",
    "#         pm_count = search_pubmed(keyword)\n",
    "#         print(f\"Number of articles mentioning {keyword} in         PubMed: {pm_count:5}\")\n",
    "\n",
    "#         # Science Direct\n",
    "#         sd_count = search_science_direct_selenium(keyword)\n",
    "#         print(f\"Number of articles mentioning {keyword} in Science Direct: {sd_count:5}\")\n",
    "#         print(\"-\" * 75)\n",
    "        \n",
    "#         # Adding sleep to avoid rate-limiting\n",
    "#         time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste de montagem das URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos de uso\n",
    "single_keyword = \"Machine\"\n",
    "complex_keyword = \"Machine Learning\"\n",
    "multiple_keywords = [\"innovation model\", \"Graph\", \"Machine Learning\"]\n",
    "\n",
    "print(format_sd_url(single_keyword))\n",
    "print(format_sd_url(complex_keyword))\n",
    "print(format_sd_url(multiple_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_url = f\"https://www.sciencedirect.com/search?qs={keyword}&articleTypes=REV%2CFLA&lastSelectedFacet=articleTypes&subjectAreas=1700%2C1800&show=100\"\n",
    "\n",
    "simple_keyword = \"CausalML\"\n",
    "\n",
    "url1 = format_sd_url(simple_keyword)\n",
    "test1_url = f\"https://www.sciencedirect.com/search?qs=%22CausalML%22&show=100&lastSelectedFacet=subjectAreas&articleTypes=REV%2CFLA&subjectAreas=1800%2C1700\"\n",
    "\n",
    "if url1 == test1_url:\n",
    "    print(\"\\nURL montada com sucesso a partir de palavra-chave simples\")\n",
    "else:\n",
    "    print(\"\\nERRO ao montar a url com termo simples, verifique a função format_url(keywords)\")\n",
    "    print(\"Esperado:\")\n",
    "    print(test1_url)\n",
    "    print(\"Obtido:\")\n",
    "    print(url1)\n",
    "\n",
    "complex_keyword = \"Machine Learning\"\n",
    "\n",
    "url2 = format_sd_url(complex_keyword)\n",
    "test2_url = f\"https://www.sciencedirect.com/search?qs=%22Machine%20Learning%22&show=100&lastSelectedFacet=subjectAreas&articleTypes=REV%2CFLA&subjectAreas=1800%2C1700\"\n",
    "\n",
    "if url2 == test2_url:\n",
    "    print(\"\\nURL montada com sucesso a partir de palavra-chave composta\")\n",
    "else:\n",
    "    print(\"\\nERRO ao montar a url com termo composto, verifique a função format_url(keywords)\")\n",
    "    print(\"Esperado:\")\n",
    "    print(test2_url)\n",
    "    print(\"Obtido:\")\n",
    "    print(url2)\n",
    "\n",
    "keyword_list = [\"innovation model\", \"Graph\", \"Machine Learning\"]\n",
    "\n",
    "url3 = format_sd_url(keyword_list)\n",
    "test3_url = f\"https://www.sciencedirect.com/search?qs=%22innovation%20model%22%20%22Graph%22%20%22Machine%20Learning%22&show=100&lastSelectedFacet=subjectAreas&articleTypes=REV%2CFLA&subjectAreas=1800%2C1700\"\n",
    "\n",
    "if url3 == test3_url:\n",
    "    print(\"\\nURL montada com sucesso a partir de lista de palavras-chave\")\n",
    "else:\n",
    "    print(\"\\nERRO ao montar a url com lista de termos, verifique a função format_url(keywords)\")\n",
    "    print(\"Esperado:\")\n",
    "    print(test3_url)\n",
    "    print(\"Obtido:\")\n",
    "    print(url3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazer requisições de buscas nas bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"DoWhy\", \"CausalML\"]\n",
    "\n",
    "for keyword in keywords:\n",
    "    print(f\"Fetching data for {keyword}...\")\n",
    "    \n",
    "    # Google Scholar\n",
    "    gs_count = search_google_scholar(keyword)\n",
    "    print(f\"Number of articles mentioning {keyword:^20} in Google Scholar: {gs_count:>5}\")\n",
    "\n",
    "    # PubMed\n",
    "    pm_count = search_pubmed(keyword)\n",
    "    print(f\"Number of articles mentioning {keyword:^20} in         PubMed: {pm_count:>5}\")\n",
    "\n",
    "    # Science Direct\n",
    "    sd_titles, sd_count = search_science_direct_selenium(keyword)\n",
    "    print(f\"Number of articles mentioning {keyword:^20} in Science Direct: {sd_count:>5}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    # Adding sleep to avoid rate-limiting\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def search_and_list_crossref_api(keyword, rows=10):\n",
    "    \"\"\"\n",
    "    Searches the CrossRef API for articles related to a specific keyword and lists the results.\n",
    "    \n",
    "    Parameters:\n",
    "        keyword (str): The search keyword.\n",
    "        rows (int): Number of results to return.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of dictionaries with article details or an error message.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = \"https://api.crossref.org/works\"\n",
    "    params = {\n",
    "        \"query\": keyword,\n",
    "        \"rows\": rows,\n",
    "        \"mailto\": \"your-email@example.com\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(base_url, params=params)\n",
    "        \n",
    "        # Validate the response\n",
    "        if response.status_code == 200:\n",
    "            data = json.loads(response.text)\n",
    "            \n",
    "            # Extracting article details\n",
    "            articles = data['message']['items']\n",
    "            results = []\n",
    "            for article in articles:\n",
    "                # Filtrar autores para garantir que todos sejam strings\n",
    "                authors = [author.get(\"name\") for author in article.get(\"author\", []) if author.get(\"name\") is not None]\n",
    "\n",
    "                result = {\n",
    "                    \"title\": article.get(\"title\")[0] if article.get(\"title\") else \"No Title\",\n",
    "                    \"DOI\": article.get(\"DOI\", \"No DOI\"),\n",
    "                    \"authors\": authors,\n",
    "                    \"published_date\": article.get(\"published-print\", article.get(\"published-online\", \"No Date\"))\n",
    "                }\n",
    "                results.append(result)\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        else:\n",
    "            return f\"Failed to retrieve data, HTTP Status Code: {response.status_code}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example usage\n",
    "keyword = \"CausalML\"\n",
    "results = search_and_list_crossref_api(keyword, rows=500)\n",
    "for article in results:\n",
    "    print(f\"Title: {article['title']}, DOI: {article['DOI']}, Authors: {', '.join(article['authors'])}, Published Date: {article['published_date']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "src_text = [\n",
    "    \">>por<< Tom tried to stab me.\",\n",
    "    \">>por<< He has been to Hawaii several times.\"\n",
    "]\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-tc-big-en-pt\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n",
    "\n",
    "for t in translated:\n",
    "    print(tokenizer.decode(t, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"translation\", model=model_name)\n",
    "print(pipe(\">>por<< Tom tried to stab me.\"))\n",
    "\n",
    "# expected output: O Tom tentou esfaquear-me.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Defina o modelo e o tokenizador\n",
    "model_name = 'Helsinki-NLP/opus-mt-tc-big-en-pt' \n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "def translate(src_text, model, tokenizer):\n",
    "    # Realize a tradução\n",
    "    translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n",
    "    # Decode a saída\n",
    "    for t in translated:\n",
    "        print(tokenizer.decode(t, skip_special_tokens=True))\n",
    "    return translated[0]\n",
    "\n",
    "# Exemplo de texto para traduzir\n",
    "text_to_translate = \"This is a sample text to be translated.\"\n",
    "\n",
    "# Traduzir o texto\n",
    "translated_text = translate(text_to_translate, model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
