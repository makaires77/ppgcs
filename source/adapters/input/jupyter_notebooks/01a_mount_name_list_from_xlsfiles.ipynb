{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar Lista de Nomes para extrair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Caminho base do repositório: /home/marcos/gml_classifier\n",
      "JSON entrada de dados: ['dict_list_fioce.json', 'normalized_dict_list.json', 'palavras_chave_area_cnpq.json']\n",
      "Planilhas entrada de dados: ['fioce_colaboradores-2023.ods', 'capes_avaliacao_areas.xlsx', 'classificações_publicadas_todas_as_areas_avaliacao1672761192111.xlsx', 'fioce_colaboradores-2023.xls', 'fiocruz_unidade-desconhecida.xlsx', 'fioce_producao_2008.01-2023.06.xlsx']\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "def get_base_repo():\n",
    "    \"\"\"Retorna o caminho absoluto cinco níveis acima do diretório atual.\"\"\"\n",
    "    current_directory = os.getcwd()\n",
    "    # Construir o caminho para subir cinco níveis\n",
    "    path_five_levels_up = os.path.join(current_directory, '../../../../')\n",
    "    # Normalizar o caminho para o formato absoluto\n",
    "    absolute_path = os.path.abspath(path_five_levels_up)\n",
    "    return absolute_path\n",
    "\n",
    "# Definir a pasta de base do repositório local\n",
    "base_repo_dir = get_base_repo()\n",
    "\n",
    "# Construir os caminhos usando os.path.join\n",
    "folder_utils = os.path.join(base_repo_dir, 'utils')\n",
    "folder_domain = os.path.join(base_repo_dir, 'source', 'domain')\n",
    "folder_data_input = os.path.join(base_repo_dir, 'data', 'input')\n",
    "folder_data_output = os.path.join(base_repo_dir, 'data', 'output')\n",
    "folder_data_compac = os.path.join(base_repo_dir, 'data', 'xml_zip')\n",
    "\n",
    "# Adicionar pastas locais ao sys.path para permitir importação de pacotes\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "from experiment_monitor import ExperimentMonitor\n",
    "from articles_counter import ArticlesCounter\n",
    "from json_fle_manager import JSONFileManager as jfm\n",
    "from experiment_profiler import TimeProfiler\n",
    "\n",
    "# Para o caso de folder_data_prod, que parece ser exclusivo para ambientes Unix\n",
    "folder_data_prod = os.path.join(base_repo_dir, 'data') if not 'win' in sys.platform else None\n",
    "\n",
    "print(f\" Caminho base do repositório: {base_repo_dir}\")\n",
    "print(f\"JSON entrada de dados: {jfm.list_json_files(folder_data_input)}\")\n",
    "print(f\"Planilhas entrada de dados: {os.listdir(folder_data_compac)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 nomes de colaboradores no total, todos vínculos e status\n",
      " 10 tipos de vínculos\n",
      "Tipos de vínculos ['SERVIDOR', 'COORENAÇÃO GERAL', 'TERCEIRIZADO', 'BOLSISTA', 'ESTÁGIO PEC', 'UNADIG', 'NORMATEL', 'SERVIDOR-CEDIDA PARA CORREGEDORIA ', 'SERVIDOR-CEDIDA PARA FIOCRUZ PE', 'SERVIDOR-CEDIDO PARA AUDITORIA INTERNA']\n",
      "  Tipos de status ['ATIVO', 'APOSENTADORIA', 'EXONERADO', 'CONTRATO ENCERRADO', 'REMOÇÃO']\n",
      "57 nomes para extrair currículos\n",
      " 1. Alice Paula Di Sabatino Guimaraes\n",
      " 2. Ana Claudia De Araújo Teixeira\n",
      " 3. Ana Camila Oliveira Alves\n",
      " 4. Angela Christina De Moraes Ostritz\n",
      " 5. Adriana Costa Bacelo\n",
      " 6. Anna Carolina Machado Marinho\n",
      " 7. Antonio Marcos Aires Barbosa\n",
      " 8. Anya Pimentel Gomes Fernandes Vieira Meyer\n",
      " 9. Bruno Bezerra Carvalho\n",
      "10. Carla Freire Celedônio Fernandes\n",
      "11. Carlos Jose Araujo Pinheiro\n",
      "12. Claudia Stutz Zubieta\n",
      "13. Charles Cerqueira De Abreu\n",
      "14. Clarice Gomes E Souza Dabés\n",
      "15. Clarissa Romero Teixeira\n",
      "16. Dayane Alves Costa\n",
      "17. Donat Alexander De Chapeaurouge\n",
      "18. Eduardo Ruback Dos Santos\n",
      "19. Ezequiel Valentim De Melo\n",
      "20. Fabio Miyajima\n",
      "21. Fernando Braga Stehling Dias\n",
      "22. Fernando Ferreira Carneiro\n",
      "23. Galba Freire Moita\n",
      "24. Giovanny Augusto Camacho Antevere Mazzarotto\n",
      "25. Gilvan Pessoa Furtado\n",
      "26. Ivana Cristina De Holanda Cunha Barrêto\n",
      "27. Ivanildo Lopes Farias\n",
      "28. Jaime Ribeiro Filho\n",
      "29. João Baptista Estabile Neto\n",
      "30. João Hermínio Martins Da Silva\n",
      "31. José Luis Passos Cordeiro\n",
      "32. Kamila Matos Albuquerque \n",
      "33. Luciana Coelho Serafim\n",
      "34. Luciana Pereira Lindenmeyer\n",
      "35. Luciana Silvério Alleluia Higino Da Silva\n",
      "36. Luciano Pinto Zorzanelli\n",
      "37. Luis Fernando Pessoa De Andrade\n",
      "38. Luiz Odorico Monteiro De Andrade\n",
      "39. Marcela Helena Gambim Fonseca\n",
      "40. Marcelo Jorge Lopes Coutinho\n",
      "41. Marcos Roberto Lourenzoni\n",
      "42. Marcio Flavio Moura De Araujo \n",
      "43. Margareth Borges Coutinho Gallo\n",
      "44. Marlos De Medeiros Chaves\n",
      "45. Maximiliano Loiola Ponte De Souza\n",
      "46. Nilton Luiz Costa Machado\n",
      "47. Patricia Maria Ferreira da Silva\n",
      "48. Raphael Trevizani\n",
      "49. Regis Bernardo Brandim Gomes\n",
      "50. Renato Caldeira De Souza\n",
      "51. Roberto Nicolete\n",
      "52. Roberto Wagner Junior Freire De Freitas\n",
      "53. Rodrigo Carvalho Nogueira \n",
      "54. Sergio Dos Santos Reis\n",
      "55. Sharmenia De Araujo Soares Nuto\n",
      "56. Vanira Matos Pessoa\n",
      "57. Venúcia Bruna Magalhães Pereira\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ler dados do arquivo Excel do Setor de Recursos Humanos\n",
    "# pathdata = './../data/fioce_colaboradores-2023.xls'\n",
    "file_persons = 'fioce_colaboradores-2023.ods'\n",
    "pathdata = os.path.join(folder_data_compac,file_persons)\n",
    "\n",
    "# Ler apenas os cabeçalhos do arquivo Excel\n",
    "headers = pd.read_excel(pathdata, skiprows=3, header=0, nrows=0).columns\n",
    "# headers\n",
    "\n",
    "# Usar função para indicar quais colunas devem ser eliminadas na leitura\n",
    "def cols_to_keep(col_name):\n",
    "    return col_name not in ['QUANT','Unnamed: 3','Unnamed: 6','Unnamed: 9','ADICIONAL OCUPACIONAL',\n",
    "                            'EMPRESA/BOLSA/PROGRAMA','GESTOR','ADI','POSSE NA FIOCRUZ',\n",
    "                            'VIGÊNCIA BOLSA/ENCERRAMENTO DO CONTRATO','Unnamed: 17',\n",
    "                            'EMAIL INSTITUCIONAL','EMAIL PESSOAL','GENERO','DATA NASCIMENTO',\n",
    "                            'Unnamed: 22','FORMAÇÃO','ENDEREÇO RESIDENCIAL']\n",
    "\n",
    "# Filtrar cabeçalhos com base na função\n",
    "selected_columns = [col for col in headers if cols_to_keep(col)]\n",
    "\n",
    "# Ler dados do arquivo Excel do Setor de Recursos Humanos\n",
    "fioce_pessoal = pd.read_excel(pathdata, skiprows=3, header=0, usecols=selected_columns)\n",
    "print(f'{len(fioce_pessoal.index)} nomes de colaboradores no total, todos vínculos e status')\n",
    "print(f'{len(fioce_pessoal[\"VÍNCULO\"].unique()):3} tipos de vínculos')\n",
    "print('Tipos de vínculos',list(fioce_pessoal['VÍNCULO'].unique()))\n",
    "print('  Tipos de status',list(fioce_pessoal['STATUS'].unique()))\n",
    "filtro1 = fioce_pessoal.VÍNCULO == 'SERVIDOR'\n",
    "filtro2 = fioce_pessoal['STATUS'].isin(['ATIVO', 'AFASTADO'])\n",
    "lista_nomes = fioce_pessoal[(filtro1) & (filtro2)]['NOME'].tolist()\n",
    "print(f'{len(lista_nomes)} nomes para extrair currículos')\n",
    "for i,nome in enumerate(lista_nomes):\n",
    "    print(f'{i+1:2}. {nome}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['STATUS', 'MATRÍCULA', 'NOME', 'ÁREA', 'CARGO', 'VÍNCULO',\n",
       "       'INGRESSO_FIOCE', 'NÍVEL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fioce_pessoal.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MATRÍCULA</th>\n",
       "      <th>NOME</th>\n",
       "      <th>ÁREA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2242450</td>\n",
       "      <td>Alice Paula Di Sabatino Guimaraes</td>\n",
       "      <td>Biotecnologia-GR2 (VIGILÂNCIA GENÔMICA)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1165347</td>\n",
       "      <td>Ana Claudia De Araújo Teixeira</td>\n",
       "      <td>Saúde e Ambiente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1014947</td>\n",
       "      <td>Ana Camila Oliveira Alves</td>\n",
       "      <td>Biotecnologia – Coordenação de Pesquisa e Cole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>626325</td>\n",
       "      <td>Angela Christina De Moraes Ostritz</td>\n",
       "      <td>Coordenação Geral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1896774</td>\n",
       "      <td>Adriana Costa Bacelo</td>\n",
       "      <td>Saúde Digital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Thiago Silva de França</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Rose Mary Faria Torres de Oliveira</td>\n",
       "      <td>Coordenação Geral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Geisa Francisco da Silva</td>\n",
       "      <td>Coordenação Geral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Aline do Monte Gurgel</td>\n",
       "      <td>Coordenação Geral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Gilberto Santiago Araujo</td>\n",
       "      <td>Coordenação Geral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MATRÍCULA                                NOME  \\\n",
       "0     2242450   Alice Paula Di Sabatino Guimaraes   \n",
       "1     1165347      Ana Claudia De Araújo Teixeira   \n",
       "2     1014947           Ana Camila Oliveira Alves   \n",
       "3      626325  Angela Christina De Moraes Ostritz   \n",
       "4     1896774                Adriana Costa Bacelo   \n",
       "..        ...                                 ...   \n",
       "279       NaN              Thiago Silva de França   \n",
       "280       NaN  Rose Mary Faria Torres de Oliveira   \n",
       "281       NaN            Geisa Francisco da Silva   \n",
       "282       NaN               Aline do Monte Gurgel   \n",
       "283       NaN            Gilberto Santiago Araujo   \n",
       "\n",
       "                                                  ÁREA  \n",
       "0              Biotecnologia-GR2 (VIGILÂNCIA GENÔMICA)  \n",
       "1                                     Saúde e Ambiente  \n",
       "2    Biotecnologia – Coordenação de Pesquisa e Cole...  \n",
       "3                                    Coordenação Geral  \n",
       "4                                        Saúde Digital  \n",
       "..                                                 ...  \n",
       "279                                                NaN  \n",
       "280                                  Coordenação Geral  \n",
       "281                                  Coordenação Geral  \n",
       "282                                  Coordenação Geral  \n",
       "283                                  Coordenação Geral  \n",
       "\n",
       "[284 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fioce_pessoal[['MATRÍCULA','NOME','ÁREA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordenação de Pesquisa e Coleções Biológicas                                         7\n",
      "Coordenação de Educação, Informação e Comunicação (SECRETARIA ACADÊMICA)              7\n",
      "Biotecnologia-GR2 (VIGILÂNCIA GENÔMICA)                                               6\n",
      "Biotecnologia-GR3 (BIOTECNOLOGIA)                                                     6\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (INFRAESTRUTURA)                6\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (SGP / ST)                      6\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (PLANEJAMENTO)                  6\n",
      "Saúde e Ambiente                                                                      6\n",
      "Coordenação Geral                                                                     5\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (TIC)                           5\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (PATRIMÔNIO)                    4\n",
      "Saúde Digital                                                                         4\n",
      "Saúde da Família                                                                      4\n",
      "Biotecnologia-GR1 (IMUNOPARASITOLOGIA)                                                4\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (COMPRAS)                       3\n",
      "Coordenação de Educação, Informação e Comunicação (COMUNICAÇÃO)                       3\n",
      "Coordenação Geral (QUALIDADE)                                                         3\n",
      "Biotecnologia - Coordenação de Pesquisa e Coleções Biológicas                         2\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (FINANCEIRO)                    2\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (INFRAESTRUTURA / SIMAM)        2\n",
      "Bio-Manguinhos                                                                        2\n",
      "Coordenação Geral (SECRETARIA EXECUTIVA)                                              2\n",
      "Saúde da Família – Coordenação de Educação, Informação e Comunicação (COORDENAÇÃO)    1\n",
      "Coordenação Geral ( EPSJV/FIOCRUZ)                                                    1\n",
      "Coordenação da Gestão e Desenvolvimento Institucional                                 1\n",
      "Coordenação da Gestão e Desenvolvimento Institucional   (ALMOXARIFADO)                1\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (LOGÍSTICA)                     1\n",
      "Coordenação de Educação, Informação e Comunicação (BIBLIOTECA)                        1\n",
      "Biotecnologia – Coordenação de Pesquisa e Coleções Biológicas                         1\n",
      "Biotecnologia - Coordenação Geral (QUALIDADE)                                         1\n",
      "Coordenação Geral                                                                     1\n",
      "Biotecnologia                                                                         1\n",
      "Saúde Digital - Coordenação Geral                                                     1\n",
      "Biotecnologia – Central Analitica - UNADIG                                            1\n",
      "Saúde Digital - Coordenação de Produção e  Inovação em Saúde                          1\n",
      "Coordenação de Produção e  Inovação em Saúde                                          1\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (COORDENAÇÃO)                   1\n",
      "Biotecnologia - Coordenação Geral                                                     1\n",
      "Coordenação da Gestão e Desenvolvimento Institucional(ALMOXARIFADO)                   1\n",
      "Saúde Digital – Central Analitica - UNADIG                                            1\n",
      "Saúde da Família - Coordenação Geral                                                  1\n",
      "Coordenação de Pesquisa                                                               1\n",
      "Name: ÁREA, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(fioce_pessoal['ÁREA'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordenação de Pesquisa e Coleções Biológicas                                         7\n",
      "Coordenação de Educação, Informação e Comunicação (SECRETARIA ACADÊMICA)              7\n",
      "Biotecnologia-GR2 (VIGILÂNCIA GENÔMICA)                                               6\n",
      "Biotecnologia-GR3 (BIOTECNOLOGIA)                                                     6\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (INFRAESTRUTURA)                6\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (SGP / ST)                      6\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (PLANEJAMENTO)                  6\n",
      "Saúde e Ambiente                                                                      6\n",
      "Coordenação Geral                                                                     5\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (TIC)                           5\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (PATRIMÔNIO)                    4\n",
      "Saúde Digital                                                                         4\n",
      "Saúde da Família                                                                      4\n",
      "Biotecnologia-GR1 (IMUNOPARASITOLOGIA)                                                4\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (COMPRAS)                       3\n",
      "Coordenação de Educação, Informação e Comunicação (COMUNICAÇÃO)                       3\n",
      "Coordenação Geral (QUALIDADE)                                                         3\n",
      "Biotecnologia - Coordenação de Pesquisa e Coleções Biológicas                         2\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (FINANCEIRO)                    2\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (INFRAESTRUTURA / SIMAM)        2\n",
      "Bio-Manguinhos                                                                        2\n",
      "Coordenação Geral (SECRETARIA EXECUTIVA)                                              2\n",
      "Saúde da Família – Coordenação de Educação, Informação e Comunicação (COORDENAÇÃO)    1\n",
      "Coordenação Geral ( EPSJV/FIOCRUZ)                                                    1\n",
      "Coordenação da Gestão e Desenvolvimento Institucional                                 1\n",
      "Coordenação da Gestão e Desenvolvimento Institucional   (ALMOXARIFADO)                1\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (LOGÍSTICA)                     1\n",
      "Coordenação de Educação, Informação e Comunicação (BIBLIOTECA)                        1\n",
      "Biotecnologia – Coordenação de Pesquisa e Coleções Biológicas                         1\n",
      "Biotecnologia - Coordenação Geral (QUALIDADE)                                         1\n",
      "Coordenação Geral                                                                     1\n",
      "Biotecnologia                                                                         1\n",
      "Saúde Digital - Coordenação Geral                                                     1\n",
      "Biotecnologia – Central Analitica - UNADIG                                            1\n",
      "Saúde Digital - Coordenação de Produção e  Inovação em Saúde                          1\n",
      "Coordenação de Produção e  Inovação em Saúde                                          1\n",
      "Coordenação da Gestão e Desenvolvimento Institucional (COORDENAÇÃO)                   1\n",
      "Biotecnologia - Coordenação Geral                                                     1\n",
      "Coordenação da Gestão e Desenvolvimento Institucional(ALMOXARIFADO)                   1\n",
      "Saúde Digital – Central Analitica - UNADIG                                            1\n",
      "Saúde da Família - Coordenação Geral                                                  1\n",
      "Coordenação de Pesquisa                                                               1\n",
      "Name: ÁREA, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(fioce_pessoal['ÁREA'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "equipes=[]\n",
    "for i in fioce_pessoal['ÁREA']:\n",
    "    # print(type(i),i)\n",
    "    try:\n",
    "        i=i.lower()\n",
    "        if 'biotecnologia' in i:\n",
    "            equipes.append('Biotecnologia')\n",
    "        elif 'família' in i:\n",
    "            equipes.append('Saúde da Família')\n",
    "        elif 'ambiente' in i:\n",
    "            equipes.append('Saúde e Ambiente')\n",
    "        elif 'digital' in i:\n",
    "            equipes.append('Saúde Digital')\n",
    "        else:\n",
    "            equipes.append('Administrativa')\n",
    "    except:\n",
    "        equipes.append('Terceirizados')\n",
    "\n",
    "fioce_pessoal['EQUIPE'] = equipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Terceirizados       169\n",
       "Administrativa       73\n",
       "Biotecnologia        23\n",
       "Saúde Digital         7\n",
       "Saúde da Família      6\n",
       "Saúde e Ambiente      6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(equipes).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coordenação de Pesquisa e Coleções Biológicas                                         7\n",
       "Coordenação de Educação, Informação e Comunicação (SECRETARIA ACADÊMICA)              7\n",
       "Biotecnologia-GR2 (VIGILÂNCIA GENÔMICA)                                               6\n",
       "Biotecnologia-GR3 (BIOTECNOLOGIA)                                                     6\n",
       "Coordenação da Gestão e Desenvolvimento Institucional (INFRAESTRUTURA)                6\n",
       "Coordenação da Gestão e Desenvolvimento Institucional (SGP / ST)                      6\n",
       "Coordenação da Gestão e Desenvolvimento Institucional (PLANEJAMENTO)                  6\n",
       "Saúde e Ambiente                                                                      6\n",
       "Coordenação Geral                                                                     5\n",
       "Coordenação da Gestão e Desenvolvimento Institucional (TIC)                           5\n",
       "Coordenação da Gestão e Desenvolvimento Institucional (PATRIMÔNIO)                    4\n",
       "Saúde Digital                                                                         4\n",
       "Saúde da Família                                                                      4\n",
       "Biotecnologia-GR1 (IMUNOPARASITOLOGIA)                                                4\n",
       "Coordenação da Gestão e Desenvolvimento Institucional (COMPRAS)                       3\n",
       "Coordenação de Educação, Informação e Comunicação (COMUNICAÇÃO)                       3\n",
       "Coordenação Geral (QUALIDADE)                                                         3\n",
       "Biotecnologia - Coordenação de Pesquisa e Coleções Biológicas                         2\n",
       "Coordenação da Gestão e Desenvolvimento Institucional (FINANCEIRO)                    2\n",
       "Coordenação da Gestão e Desenvolvimento Institucional (INFRAESTRUTURA / SIMAM)        2\n",
       "Bio-Manguinhos                                                                        2\n",
       "Coordenação Geral (SECRETARIA EXECUTIVA)                                              2\n",
       "Saúde da Família – Coordenação de Educação, Informação e Comunicação (COORDENAÇÃO)    1\n",
       "Coordenação Geral ( EPSJV/FIOCRUZ)                                                    1\n",
       "Coordenação da Gestão e Desenvolvimento Institucional                                 1\n",
       "Coordenação da Gestão e Desenvolvimento Institucional   (ALMOXARIFADO)                1\n",
       "Coordenação da Gestão e Desenvolvimento Institucional (LOGÍSTICA)                     1\n",
       "Coordenação de Educação, Informação e Comunicação (BIBLIOTECA)                        1\n",
       "Biotecnologia – Coordenação de Pesquisa e Coleções Biológicas                         1\n",
       "Biotecnologia - Coordenação Geral (QUALIDADE)                                         1\n",
       "Coordenação Geral                                                                     1\n",
       "Biotecnologia                                                                         1\n",
       "Saúde Digital - Coordenação Geral                                                     1\n",
       "Biotecnologia – Central Analitica - UNADIG                                            1\n",
       "Saúde Digital - Coordenação de Produção e  Inovação em Saúde                          1\n",
       "Coordenação de Produção e  Inovação em Saúde                                          1\n",
       "Coordenação da Gestão e Desenvolvimento Institucional (COORDENAÇÃO)                   1\n",
       "Biotecnologia - Coordenação Geral                                                     1\n",
       "Coordenação da Gestão e Desenvolvimento Institucional(ALMOXARIFADO)                   1\n",
       "Saúde Digital – Central Analitica - UNADIG                                            1\n",
       "Saúde da Família - Coordenação Geral                                                  1\n",
       "Coordenação de Pesquisa                                                               1\n",
       "Name: ÁREA, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fioce_pessoal['ÁREA'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ATIVO                 268\n",
       "CONTRATO ENCERRADO     10\n",
       "REMOÇÃO                 3\n",
       "APOSENTADORIA           2\n",
       "EXONERADO               1\n",
       "Name: STATUS, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fioce_pessoal['STATUS'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NORMATEL                                  140\n",
       "SERVIDOR                                   59\n",
       "UNADIG                                     29\n",
       "BOLSISTA                                   26\n",
       "TERCEIRIZADO                               24\n",
       "ESTÁGIO PEC                                 2\n",
       "COORENAÇÃO GERAL                            1\n",
       "SERVIDOR-CEDIDA PARA CORREGEDORIA           1\n",
       "SERVIDOR-CEDIDA PARA FIOCRUZ PE             1\n",
       "SERVIDOR-CEDIDO PARA AUDITORIA INTERNA      1\n",
       "Name: VÍNCULO, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fioce_pessoal['VÍNCULO'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['STATUS', 'MATRÍCULA', 'NOME', 'ÁREA', 'CARGO', 'VÍNCULO',\n",
       "       'INGRESSO_FIOCE', 'NÍVEL', 'EQUIPE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fioce_pessoal.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>MATRÍCULA</th>\n",
       "      <th>NOME</th>\n",
       "      <th>CARGO</th>\n",
       "      <th>VÍNCULO</th>\n",
       "      <th>INGRESSO_FIOCE</th>\n",
       "      <th>EQUIPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATIVO</td>\n",
       "      <td>2242450</td>\n",
       "      <td>Alice Paula Di Sabatino Guimaraes</td>\n",
       "      <td>Tecnologista em Saúde Pública</td>\n",
       "      <td>SERVIDOR</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>Biotecnologia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATIVO</td>\n",
       "      <td>1165347</td>\n",
       "      <td>Ana Claudia De Araújo Teixeira</td>\n",
       "      <td>Pesquisador em Saúde Pública</td>\n",
       "      <td>SERVIDOR</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>Saúde e Ambiente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATIVO</td>\n",
       "      <td>1014947</td>\n",
       "      <td>Ana Camila Oliveira Alves</td>\n",
       "      <td>Técnico em Pesquisa e Investigação Biomédica</td>\n",
       "      <td>SERVIDOR</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>Biotecnologia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATIVO</td>\n",
       "      <td>626325</td>\n",
       "      <td>Angela Christina De Moraes Ostritz</td>\n",
       "      <td>Enfermeira(o)</td>\n",
       "      <td>SERVIDOR</td>\n",
       "      <td>2017-06-05</td>\n",
       "      <td>Administrativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATIVO</td>\n",
       "      <td>1896774</td>\n",
       "      <td>Adriana Costa Bacelo</td>\n",
       "      <td>Tecnologista em Saúde Pública</td>\n",
       "      <td>SERVIDOR</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>Saúde Digital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>ATIVO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thiago Silva de França</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NORMATEL</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Terceirizados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>APOSENTADORIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rose Mary Faria Torres de Oliveira</td>\n",
       "      <td>Assistente Tecnico de Gestão em Saúde</td>\n",
       "      <td>SERVIDOR</td>\n",
       "      <td>2009-06-22</td>\n",
       "      <td>Administrativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>REMOÇÃO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Geisa Francisco da Silva</td>\n",
       "      <td>Assistente Tecnico de Gestão em Saúde</td>\n",
       "      <td>SERVIDOR-CEDIDA PARA CORREGEDORIA</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>Administrativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>REMOÇÃO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aline do Monte Gurgel</td>\n",
       "      <td>Pesquisador em Saúde Pública</td>\n",
       "      <td>SERVIDOR-CEDIDA PARA FIOCRUZ PE</td>\n",
       "      <td>2015-07-27</td>\n",
       "      <td>Administrativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>REMOÇÃO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gilberto Santiago Araujo</td>\n",
       "      <td>Analista de Gestão em Saúde</td>\n",
       "      <td>SERVIDOR-CEDIDO PARA AUDITORIA INTERNA</td>\n",
       "      <td>2012-09-14</td>\n",
       "      <td>Administrativa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            STATUS MATRÍCULA                                NOME  \\\n",
       "0            ATIVO   2242450   Alice Paula Di Sabatino Guimaraes   \n",
       "1            ATIVO   1165347      Ana Claudia De Araújo Teixeira   \n",
       "2            ATIVO   1014947           Ana Camila Oliveira Alves   \n",
       "3            ATIVO    626325  Angela Christina De Moraes Ostritz   \n",
       "4            ATIVO   1896774                Adriana Costa Bacelo   \n",
       "..             ...       ...                                 ...   \n",
       "279          ATIVO       NaN              Thiago Silva de França   \n",
       "280  APOSENTADORIA       NaN  Rose Mary Faria Torres de Oliveira   \n",
       "281        REMOÇÃO       NaN            Geisa Francisco da Silva   \n",
       "282        REMOÇÃO       NaN               Aline do Monte Gurgel   \n",
       "283        REMOÇÃO       NaN            Gilberto Santiago Araujo   \n",
       "\n",
       "                                            CARGO  \\\n",
       "0                   Tecnologista em Saúde Pública   \n",
       "1                    Pesquisador em Saúde Pública   \n",
       "2    Técnico em Pesquisa e Investigação Biomédica   \n",
       "3                                   Enfermeira(o)   \n",
       "4                   Tecnologista em Saúde Pública   \n",
       "..                                            ...   \n",
       "279                                           NaN   \n",
       "280         Assistente Tecnico de Gestão em Saúde   \n",
       "281         Assistente Tecnico de Gestão em Saúde   \n",
       "282                  Pesquisador em Saúde Pública   \n",
       "283                   Analista de Gestão em Saúde   \n",
       "\n",
       "                                    VÍNCULO INGRESSO_FIOCE            EQUIPE  \n",
       "0                                  SERVIDOR     2022-01-03     Biotecnologia  \n",
       "1                                  SERVIDOR     2015-08-17  Saúde e Ambiente  \n",
       "2                                  SERVIDOR     2019-08-26     Biotecnologia  \n",
       "3                                  SERVIDOR     2017-06-05    Administrativa  \n",
       "4                                  SERVIDOR     2021-07-01     Saúde Digital  \n",
       "..                                      ...            ...               ...  \n",
       "279                                NORMATEL            NaT     Terceirizados  \n",
       "280                                SERVIDOR     2009-06-22    Administrativa  \n",
       "281      SERVIDOR-CEDIDA PARA CORREGEDORIA      2011-02-02    Administrativa  \n",
       "282         SERVIDOR-CEDIDA PARA FIOCRUZ PE     2015-07-27    Administrativa  \n",
       "283  SERVIDOR-CEDIDO PARA AUDITORIA INTERNA     2012-09-14    Administrativa  \n",
       "\n",
       "[284 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fioce_pessoal[['STATUS', \n",
    "               'MATRÍCULA', \n",
    "               'NOME', \n",
    "               'CARGO', \n",
    "               'VÍNCULO',\n",
    "               'INGRESSO_FIOCE',\n",
    "               'EQUIPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classes\n",
    "## Ambiente beakerx\n",
    "# !pip install py2neo\n",
    "# !pip install py2neo --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "# !pip install h5py\n",
    "\n",
    "import time\n",
    "import json\n",
    "import h5py\n",
    "import logging\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, sqlite3, asyncio\n",
    "import os, re, time, traceback, json\n",
    "import warnings, csv, sys, pip, string\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from string import Formatter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from typing import List, Optional, Dict, Union\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "from pyjarowinkler.distance import get_jaro_distance\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from neo4j import GraphDatabase\n",
    "from flask import render_template_string\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common import exceptions\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException, \n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException,\n",
    "    TimeoutException,\n",
    "    WebDriverException\n",
    ")\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "logging.basicConfig(filename='lattes_scraper.log', level=logging.INFO)\n",
    "delay = 10\n",
    "\n",
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    from string import Formatter\n",
    "    \n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    return f.format(fmt, **values)\n",
    "\n",
    "def tempo(start, end):\n",
    "    from datetime import timedelta\n",
    "    t=end-start\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "# https://sh-tsang.medium.com/tutorial-cuda-cudnn-anaconda-jupyter-pytorch-installation-in-windows-10-96b2a2f0ac57\n",
    "\n",
    "def definir_sistema(pastaraiz):\n",
    "    import os\n",
    "    import sys\n",
    "    sistema_operacional =sys.platform\n",
    "    try:\n",
    "        if 'linux' in sistema_operacional:\n",
    "            print('Sistema operacional Linux')\n",
    "            try:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'mak/'\n",
    "                os.listdir(drive+usuario)\n",
    "            except:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'marcos/'\n",
    "            caminho = drive+usuario+pastaraiz+'/'\n",
    "        elif 'win32' in sistema_operacional:\n",
    "            print('Sistema operacional Windows')\n",
    "            drive   = 'C'\n",
    "            print(f'Drive em uso {drive.upper()}')\n",
    "            # drive = 'E'\n",
    "            # drive = input('Indique qual a letra da unidade onde deseja armazenar os arquivos (Ex.: C, E...)')\n",
    "            usuario = 'Users/marco/'\n",
    "            if os.path.isdir(drive+':/'+usuario) is False:\n",
    "                usuario = 'Users/marcos.aires/'\n",
    "            caminho = drive+':/'+usuario+pastaraiz+'/'\n",
    "        else:\n",
    "            print('SO não reconhecido')\n",
    "    except Exception as e:\n",
    "        print('  ERRO!! Ao preparar as pastas locais!')\n",
    "        print(e)\n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "    return caminho, drive, usuario\n",
    "\n",
    "def preparar_pastas(caminho):\n",
    "    import os\n",
    "    # caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    # caminho = drive+':/'+usuario+pastaraiz\n",
    "    # caminho = drive+':/'+pastaraiz\n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'xml_zip/'\n",
    "    pathcsv  = caminho+'csv/'\n",
    "    pathjson = caminho+'json/'\n",
    "    pathfig  = caminho+'fig/'\n",
    "    pathaux  = caminho\n",
    "    pathout  = caminho+'output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XML', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    print()\n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout\n",
    "\n",
    "def try_folders(drives, pastas, pastasraiz):\n",
    "    import os\n",
    "    import sys\n",
    "    sistema_operacional =sys.platform    \n",
    "    if 'linux' in sistema_operacional:\n",
    "        print('Sistema operacional Linux')\n",
    "        try:\n",
    "            drive   = '/home/'\n",
    "            usuario = 'mak/'\n",
    "            os.listdir(drive+usuario)\n",
    "        except:\n",
    "            drive   = '/home/'\n",
    "            usuario = 'marcos/'\n",
    "        chromedriverpath = '/chromedriver/chromedriver'\n",
    "    elif 'win32' in sistema_operacional:\n",
    "        print('Sistema operacional Windows')\n",
    "        drive   = 'C'\n",
    "        print(f'Drive em uso {drive.upper()}')\n",
    "        # drive = 'E'\n",
    "        # drive = input('Indique qual a letra da unidade onde deseja armazenar os arquivos (Ex.: C, E...)')\n",
    "        usuario = 'Users/marco/'\n",
    "        if os.path.isdir(drive+':/'+usuario) is False:\n",
    "            usuario = 'Users/marcos.aires/'\n",
    "        chromedriverpath = '/chromedriver/chromedriver.exe'    \n",
    "    for drive in drives:\n",
    "        for i in pastas:\n",
    "            for j in pastasraiz:\n",
    "                try:\n",
    "                    tested_path = drive + i + j\n",
    "                    if os.path.isfile(tested_path + chromedriverpath):\n",
    "                        logging.info(f\"Listing files in: {tested_path}\")\n",
    "                        logging.info(os.listdir(tested_path))\n",
    "                        return tested_path + '/'\n",
    "                except:\n",
    "                    logging.error('Could not locate a working folder.')\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictToHDF5:\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def create_dataset(self, filename, directory=None):\n",
    "        with h5py.File(f\"{directory or ''}{filename}\", \"w\") as f:\n",
    "            null_group = f.create_group(\"0000\")\n",
    "            for person_dict in self.data_list:  # Corrigido de self.data para self.data_list\n",
    "                if 'curriculo' not in person_dict:\n",
    "                    name = person_dict.get('name', 'Unknown')  # Uso de get() para evitar KeyError\n",
    "                    null_group.attrs[name] = \"No curriculum\"  # Adicionando como atributos ao grupo '0000'\n",
    "                    continue\n",
    "                person_group = f.create_group(person_dict['id'])\n",
    "                for key, value in person_dict.items():\n",
    "                    if value is None:\n",
    "                        continue\n",
    "                    if isinstance(value, list):\n",
    "                        if not value:  # Skip empty lists\n",
    "                            continue\n",
    "\n",
    "                        dtype = type(value[0])\n",
    "                        if dtype == str:\n",
    "                            dt = h5py.string_dtype(encoding='utf-8')\n",
    "                            person_group.create_dataset(key, (len(value),), dtype=dt, data=value)\n",
    "                        else:\n",
    "                            value = np.array(value, dtype=dtype)\n",
    "                            person_group.create_dataset(key, data=value)\n",
    "                    elif isinstance(value, str):\n",
    "                        dt = h5py.string_dtype(encoding='utf-8')\n",
    "                        person_group.create_dataset(key, (1,), dtype=dt, data=value)\n",
    "                    elif isinstance(value, dict) or isinstance(value, list):\n",
    "                        json_str = json.dumps(value)\n",
    "                        dt = h5py.string_dtype(encoding='utf-8')\n",
    "                        person_group.create_dataset(key, (1,), dtype=dt, data=json_str)\n",
    "                    else:\n",
    "                        person_group.create_dataset(key, data=value)\n",
    "\n",
    "    def extract_id_lattes(self, data_dict):\n",
    "        inf_pes = data_dict.get('InfPes', [])\n",
    "        for item in inf_pes:\n",
    "            if 'ID Lattes:' in item:\n",
    "                return item.split('ID Lattes: ')[-1]\n",
    "        return \"0000\" + str(data_dict.get(\"name\"))\n",
    "\n",
    "    # Para visualização\n",
    "    def print_hdf5_structure(self, filepath):\n",
    "        def recursive_print(group, indentation=0):\n",
    "            print(\"  \" * indentation + f\"Group: {group.name}\")\n",
    "            for key in group.keys():\n",
    "                item = group[key]\n",
    "                if isinstance(item, h5py.Dataset):\n",
    "                    print(\"  \" * (indentation + 1) + f\"Dataset: {key}\")\n",
    "                elif isinstance(item, h5py.Group):\n",
    "                    recursive_print(item, indentation + 1)\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            recursive_print(f)\n",
    "\n",
    "    # Para persisistir em N4j\n",
    "    def persist_to_neo4j(self, filepath, neo4j_url, username, password):\n",
    "        graph = Graph(neo4j_url, auth=(username, password))\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            for key in f.keys():\n",
    "                group = f[key]\n",
    "                properties = {}\n",
    "                if key == '0000':  # Tratar grupo \"0000\" diferentemente\n",
    "                    for attr_name, attr_value in group.attrs.items():\n",
    "                        properties[attr_name] = attr_value\n",
    "                    node = Node(\"NoCurriculumGroup\", **properties)  # Criação de um nó específico para o grupo\n",
    "                else:\n",
    "                    for ds_key in group.keys():\n",
    "                        dataset = group[ds_key]\n",
    "                        properties[ds_key] = dataset[()]\n",
    "                    node = Node(\"Person\", **properties)  # Assumindo que o nó seja do tipo \"Person\"\n",
    "                graph.create(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "from neo4j import GraphDatabase\n",
    "from typing import Any, List, Dict, Optional\n",
    "\n",
    "class Neo4jPersister:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(value: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Converts a given value to its most primitive form suitable for storage.\n",
    "        Parameters:\n",
    "        - value: Any, the value to be converted.\n",
    "        Returns:\n",
    "        - Any: The converted value in its most primitive form.\n",
    "        \"\"\"\n",
    "        if isinstance(value, str):\n",
    "            return value  # Directly return the string, ensuring it remains a simple string.\n",
    "        elif isinstance(value, list):\n",
    "            return [Neo4jPersister.convert_to_primitives(elem) for elem in value]\n",
    "        elif isinstance(value, dict):\n",
    "            return {key: Neo4jPersister.convert_to_primitives(val) for key, val in value.items()}\n",
    "        elif isinstance(value, set):\n",
    "            return list(value)  # Convert set to list for JSON serializability.\n",
    "        # ... (handle other types if necessary)\n",
    "        else:\n",
    "            return value  # Return the value as-is if it's already in a primitive form.\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return Neo4jPersister.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise\n",
    "\n",
    "    def persist_data(self, data_dict, label):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MERGE (node:{label}) SET node = $props\"\n",
    "            session.run(query, props=data_dict_primitives)\n",
    "\n",
    "    def update_data(self, node_id, data_dict):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MATCH (node) WHERE id(node) = {node_id} SET node += $props\"\n",
    "            session.run(query, props=data_dict_primitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executar extração de todos os nomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ParseSoup:\n",
    "    def __init__(self, driver):\n",
    "        self.base_url = 'http://buscatextual.cnpq.br'\n",
    "        self.session = requests.Session()\n",
    "        self.failed_extractions = []\n",
    "        self.driver = driver\n",
    "        self.delay = 10\n",
    "        self.soup = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self  # the object to bind to the variable in the `as` clause\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.driver.quit()\n",
    "        self.driver = None\n",
    "\n",
    "    def to_json(self, data_dict: Dict, filename: str) -> None:\n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(data_dict, f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to JSON: {e}\")\n",
    "\n",
    "    def to_hdf5(self, processed_data: List[Dict], hdf5_filename: str) -> None:\n",
    "        try:\n",
    "            with h5py.File(hdf5_filename, 'w') as hdf5_file:\n",
    "                for i, data in enumerate(processed_data):\n",
    "                    # Serializa o dicionário como uma string JSON antes de armazená-lo.\n",
    "                    serialized_data = json.dumps(data)\n",
    "                    hdf5_file.create_dataset(str(i), data=serialized_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to HDF5: {e}\")\n",
    "\n",
    "    def dictlist_to_json(self, data_list: List[Dict], filename: str) -> None:\n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(data_list, f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to JSON: {e}\")\n",
    "\n",
    "    def dictlist_to_hdf5(self, data_list: List[Dict], filename: str, directory=None) -> None:\n",
    "        try:\n",
    "            converter = DictToHDF5(data_list)\n",
    "            converter.create_dataset(filename, directory)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to HDF5: {e}\")\n",
    "    \n",
    "    def format_string(self, input_str):\n",
    "        # Verifica se a entrada é uma string de oito dígitos\n",
    "        if input_str and len(input_str) == 9:\n",
    "            return input_str\n",
    "        elif input_str and len(input_str) == 8:\n",
    "            # Divide a string em duas partes\n",
    "            part1 = input_str[:4]\n",
    "            part2 = input_str[4:]\n",
    "            # Concatena as duas partes com um hífen\n",
    "            formatted_str = f\"{part1}-{part2}\"\n",
    "            return formatted_str\n",
    "        else:\n",
    "            return input_str\n",
    "            \n",
    "    def extract_tit1_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        # Títulos contendo subseções\n",
    "        tit1a = ['Identificação','Endereço','Formação acadêmica/titulação','Pós-doutorado','Formação Complementar',\n",
    "                'Linhas de pesquisa','Projetos de pesquisa','Projetos de extensão','Projetos de desenvolvimento', 'Revisor de periódico','Revisor de projeto de fomento','Áreas de atuação','Idiomas','Inovação']\n",
    "        tit1b = ['Atuação Profissional'] # dados com subseções\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            # Verifique se o título está na lista 'tit1'\n",
    "            if titulo in tit1a:\n",
    "                if verbose:\n",
    "                    print(titulo)\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    divs_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "                    divs_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "                    keys = []\n",
    "                    vals = []\n",
    "                    for i, j in zip(divs_layout_cell_3, divs_layout_cell_9):\n",
    "                        if divs_layout_cell_3 and divs_layout_cell_9:\n",
    "                            key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                            key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                            keys.append(key_text)\n",
    "                            val = j.find('div', class_='layout-cell-pad-5')\n",
    "                            val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                            vals.append(val_text)\n",
    "                            if verbose:\n",
    "                                print(f'      {key_text:>3}: {val_text}')\n",
    "                    agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                    data_dict[titulo] = Neo4jPersister.convert_to_primitives(agg_dict)\n",
    "            if titulo in tit1b:\n",
    "                if verbose:\n",
    "                    print(titulo)\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")               \n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = []\n",
    "                        if verbose:\n",
    "                            print(section_name)\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "                            if 'layout-cell-3' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-9' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "                                    if verbose:\n",
    "                                        print(len(current_data.values()), key, val)\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_data:\n",
    "                                        data_dict[titulo][section_name].append(current_data)  # Armazenamos os dados em uma lista\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "        return data_dict\n",
    "\n",
    "    def extract_tit2_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        database = ''\n",
    "        total_trab_text = 0\n",
    "        total_cite_text = 0\n",
    "        num_fator_h = 0\n",
    "        data_wos_text = ''\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        tit2 = ['Produções', 'Bancas', 'Orientações']\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            # Verifique se o título está na lista 'tit2'\n",
    "            if titulo in tit2:\n",
    "                if verbose:\n",
    "                    print(f'Título: {titulo}')\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = {}\n",
    "                        if verbose:\n",
    "                            print(f'Seção: {section_name}')\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_subsection = None\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                        if section_name == 'Produção bibliográfica':\n",
    "                            subsections = section.find_next_siblings('div', class_='cita-artigos')\n",
    "                            if verbose:\n",
    "                                print(len(subsections), 'subseções')                       \n",
    "                            for subsection in subsections:                            \n",
    "                                if subsection:\n",
    "                                    subsection_name = subsection.find('b').get_text().strip()\n",
    "                                    if verbose:\n",
    "                                        print(f'    Subseção: {subsection_name}') # nomes de subseção como ocorrências \n",
    "                                        print(f'    {len(subsection)} divs na subseção {subsection_name}')                                \n",
    "                                    if subsection_name == 'Citações':\n",
    "                                        current_subsection = subsection_name\n",
    "                                        data_dict[titulo][section_name]['Citações'] = {}\n",
    "                                        sub_section_list = []  \n",
    "                                        ## Extrair quantidade de citações e fator H das divs de subseção com classe lyout-cell-12\n",
    "                                        next_siblings = subsection.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "                                        for sibling in next_siblings:\n",
    "                                            citation_counts = sibling.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que tem os Valores de Citações\n",
    "                                            if citation_counts:\n",
    "                                                for i in citation_counts:\n",
    "                                                    database = i.get_text()\n",
    "                                                    total_trab = i.find_next_sibling(\"div\", class_=\"trab\")\n",
    "                                                    if total_trab:\n",
    "                                                        total_trab_text = total_trab.get_text().split(\"Total de trabalhos:\")[1]\n",
    "                                                    total_cite = i.find_next_sibling(\"div\", class_=\"cita\")\n",
    "                                                    if total_cite:\n",
    "                                                        total_cite_text = total_cite.get_text().split(\"Total de citações:\")[1]\n",
    "                                                    fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "                                                    num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "                                                    data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\")\n",
    "                                                    if data_wos:\n",
    "                                                        try:\n",
    "                                                            data_wos_text = data_wos.get_text().split(\"Data:\")[1].strip()\n",
    "                                                        except:\n",
    "                                                            data_wos_text = data_wos.get_text()\n",
    "                                                    # Converta os valores para tipos de dados adequados\n",
    "                                                    total_trab = int(total_trab_text)\n",
    "                                                    total_cite = int(total_cite_text)\n",
    "                                                    citation_numbers = {\n",
    "                                                        \"Database\": database,\n",
    "                                                        \"Total de trabalhos\": total_trab,\n",
    "                                                        \"Total de citações\": total_cite,\n",
    "                                                        \"Índice_H\": num_fator_h,\n",
    "                                                        \"Data\": data_wos_text\n",
    "                                                    }\n",
    "                                                    # Verifique se a subseção atual já existe no dicionário\n",
    "                                                    if 'Citações' not in data_dict[titulo][section_name]:\n",
    "                                                        data_dict[titulo][section_name]['Citações'] = {}  # Inicialize como uma lista vazia\n",
    "                                                    data_dict[titulo][section_name]['Citações'] = citation_numbers\n",
    "                                                    if verbose:\n",
    "                                                        print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')    \n",
    "                            ## Encontrar a div irmã de div subseção com classe layout-cell-12 com artigos\n",
    "                            vals_jcr = []\n",
    "                            div_artigo_geral = data_cell.findChild(\"div\", id=\"artigos-completos\")\n",
    "                            if verbose:\n",
    "                                print(f'Encontrada {len(div_artigo_geral)} div geral de artigos')\n",
    "                            if div_artigo_geral:\n",
    "                                divs_artigos = div_artigo_geral.find_all('div', class_='artigo-completo')\n",
    "                                if verbose:\n",
    "                                    print(len(divs_artigos), 'divs de artigos')\n",
    "                                current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                                if divs_artigos:                              \n",
    "                                    for div_artigo in divs_artigos:\n",
    "                                        data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = {}\n",
    "                                        ## Extrair filhos da classes de artigos completos que estão à frente\n",
    "                                        sibling = div_artigo.findChild()\n",
    "                                        while sibling:\n",
    "                                            classes = sibling.get('class', [])\n",
    "                                            if 'layout-cell-1' in classes:  # Data key\n",
    "                                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                                sibling = sibling.find_next_sibling()\n",
    "                                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                                    info_dict = {\n",
    "                                                        'data-issn': 'NULL',\n",
    "                                                        'impact-factor': 'NULL',  \n",
    "                                                        'jcr-year': 'NULL',\n",
    "                                                    }\n",
    "                                                    # Remova as tags span da div\n",
    "                                                    for span in sibling.find_all('span'):\n",
    "                                                        span.extract()\n",
    "                                                    val_text = sibling.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "                                                    current_data[key] = val_text\n",
    "                                                    if verbose:\n",
    "                                                        print(len(current_data.values()), key, val)\n",
    "                                                    sup_element = sibling.find('sup')\n",
    "                                                    if sup_element:\n",
    "                                                        raw_jcr_data = sup_element.get_text()\n",
    "                                                        # print('sup_element:',sup_element)\n",
    "                                                        img_element = sup_element.find('img')\n",
    "                                                        # print('img_element:',img_element)\n",
    "                                                        if img_element:\n",
    "                                                            original_title = img_element.get('original-title')\n",
    "                                                            if original_title:\n",
    "                                                                info_list = original_title.split('<br />') if original_title.split('<br />') else original_title\n",
    "                                                                if info_list != 'NULL':\n",
    "                                                                    issn = self.format_string(img_element.get('data-issn'))\n",
    "                                                                    if verbose:\n",
    "                                                                        print(f'impact-factor: {info_list[1].split(\": \")[1]}')\n",
    "                                                                    info_dict = {\n",
    "                                                                        'data-issn': issn,\n",
    "                                                                        'impact-factor': info_list[1].split(': ')[1],\n",
    "                                                                        'jcr-year': info_list[1].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')',''),\n",
    "                                                                        'journal': info_list[0],\n",
    "                                                                    }\n",
    "                                                            else:\n",
    "                                                                if verbose:\n",
    "                                                                    print('Entrou no primeiro Else')\n",
    "                                                                issn = self.format_string(img_element.get('data-issn'))\n",
    "                                                                info_dict = {\n",
    "                                                                    'data-issn': issn,\n",
    "                                                                    'impact-factor': 'NULL',\n",
    "                                                                    'jcr-year': 'NULL',\n",
    "                                                                    'journal': 'NULL',\n",
    "                                                                }\n",
    "                                                    else:\n",
    "                                                        if verbose:\n",
    "                                                                    print('Entrou no segundo Else')\n",
    "                                                        info_dict = {\n",
    "                                                            'data-issn': 'NULL',\n",
    "                                                            'impact-factor': 'NULL',\n",
    "                                                            'jcr-year': 'NULL',\n",
    "                                                            'journal': 'NULL',\n",
    "                                                        }\n",
    "                                                    vals_jcr.append(info_dict)\n",
    "                                                    if verbose:\n",
    "                                                        print(f'         {info_dict}')\n",
    "                                                if 'JCR' not in data_dict:\n",
    "                                                    data_dict['JCR'] = []\n",
    "                                                if verbose:\n",
    "                                                    print(len(vals_jcr))\n",
    "                                                data_dict['JCR'] = vals_jcr\n",
    "                                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                                next_sibling = sibling.find_next_sibling()\n",
    "                                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                                    sibling = None\n",
    "                                                else:\n",
    "                                                    if current_data:\n",
    "                                                        converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                                        data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = converted_data\n",
    "                                            if sibling:\n",
    "                                                sibling = sibling.find_next_sibling()\n",
    "                        else:\n",
    "                            while sibling:\n",
    "                                classes = sibling.get('class', [])\n",
    "                                if 'cita-artigos' in classes:  # Subsection start\n",
    "                                    subsection_name = sibling.find('b').get_text().strip()\n",
    "                                    current_subsection = subsection_name\n",
    "                                    if verbose:\n",
    "                                        print(f'    Subseção: {subsection_name}')\n",
    "                                    data_dict[titulo][section_name][current_subsection] = {}\n",
    "                                    current_data = {}  # Inicializamos o dicionário de dados da subseção atual\n",
    "                                elif 'layout-cell-1' in classes:  # Data key\n",
    "                                    key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                    sibling = sibling.find_next_sibling()\n",
    "                                    if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                        val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                        current_data[key] = val\n",
    "                                elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Subsection or section end\n",
    "                                    next_sibling = sibling.find_next_sibling()\n",
    "                                    if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                        sibling = None\n",
    "                                    else:\n",
    "                                        if current_subsection:\n",
    "                                            data_dict[titulo][section_name][current_subsection] = Neo4jPersister.convert_to_primitives(current_data)  # Armazenamos os dados da subseção atual\n",
    "                                if sibling:\n",
    "                                    sibling = sibling.find_next_sibling()\n",
    "        \n",
    "        # Verifique se os dados dos tooltips estão presentes no objeto soup\n",
    "        if 'tooltips' in soup.attrs:\n",
    "            tooltips_data = soup.attrs['tooltips']\n",
    "            agg = []\n",
    "            for tooltip in tooltips_data:\n",
    "                agg_data = {}\n",
    "                # Extração do ano JCR a partir do \"original_title\"\n",
    "                if tooltip.get(\"original_title\"):\n",
    "                    jcr_year = tooltip[\"original_title\"].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')','')\n",
    "                    agg_data[\"jcr-ano\"] = jcr_year\n",
    "                # Adicionar todas as chaves e valores do tooltip ao dicionário agg_data\n",
    "                for key, value in tooltip.items():\n",
    "                    agg_data[key] = value\n",
    "                agg.append(agg_data)\n",
    "            data_dict['JCR2'] = agg\n",
    "        else:\n",
    "            print('Não foram achados os dados de tooltip')\n",
    "            print(soup.attrs)\n",
    "        return data_dict\n",
    "\n",
    "    def extract_tit3_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        # Títulos da seção 'Eventos'\n",
    "        tit3 = ['Eventos']\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            # Verifique se o título está na lista 'tit3'\n",
    "            if titulo in tit3:\n",
    "                if verbose:\n",
    "                    print(f'Título: {titulo}')\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = []\n",
    "                        if verbose:\n",
    "                            print(section_name)\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "                            if 'layout-cell-1' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "                                    if verbose:\n",
    "                                        print(len(current_data.values()), key, val)\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_data:\n",
    "                                        converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                        data_dict[titulo][section_name] = converted_data\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "        return data_dict\n",
    "\n",
    "    def extract_data(self, soup):\n",
    "        \"\"\"\n",
    "        Aggregates data from various dictionary sources into a consolidated nested dictionary, \n",
    "        ensuring that all nested lists within the dictionaries are transformed into nested dictionaries.\n",
    "        Parameters:\n",
    "        - soup: BeautifulSoup object, representing the parsed HTML content.\n",
    "        Returns:\n",
    "        - dict: An aggregated dictionary containing the consolidated data.\n",
    "        \"\"\"\n",
    "        self.soup = soup\n",
    "        \n",
    "        def convert_list_to_dict(lst):\n",
    "            \"\"\"\n",
    "            Converts a list into a dictionary with indices as keys.\n",
    "            \n",
    "            Parameters:\n",
    "            - lst: list, input list to be transformed.\n",
    "            \n",
    "            Returns:\n",
    "            - dict: Transformed dictionary.\n",
    "            \"\"\"\n",
    "            return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "        def merge_dict(d1, d2):\n",
    "            \"\"\"\n",
    "            Recursively merges two dictionaries, transforming nested lists into dictionaries.\n",
    "            Parameters:\n",
    "            - d1: dict, the primary dictionary into which data is merged.\n",
    "            - d2: dict or list, the secondary dictionary or list from which data is sourced.\n",
    "            Returns:\n",
    "            - None\n",
    "            \"\"\"\n",
    "            # If d2 is a list, convert it to a dictionary first\n",
    "            if isinstance(d2, list):\n",
    "                d2 = convert_list_to_dict(d2)\n",
    "            \n",
    "            for key, value in d2.items():\n",
    "                if isinstance(value, list):\n",
    "                    d2[key] = convert_list_to_dict(value)\n",
    "                if key in d1 and isinstance(d1[key], dict) and isinstance(value, dict):\n",
    "                    merge_dict(d1[key], value)\n",
    "                else:\n",
    "                    d1[key] = value\n",
    "\n",
    "        # Extract necessary information from soup\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "        name = info_list[0]\n",
    "\n",
    "        # Initialization of the aggregated_data dictionary\n",
    "        aggregated_data = {\"labels\": \"Person\", \"name\": name, \"InfPes\": info_list, \"Resumo\": [elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()]}\n",
    "\n",
    "        # Data extraction and merging\n",
    "        for data_extraction_func in [self.extract_tit1_soup, self.extract_tit2_soup, self.extract_tit3_soup]:\n",
    "            extracted_sections = data_extraction_func(soup)\n",
    "            for title, data in extracted_sections.items():\n",
    "                if title not in aggregated_data:\n",
    "                    aggregated_data[title] = {}\n",
    "                merge_dict(aggregated_data[title], data)\n",
    "        return aggregated_data\n",
    "\n",
    "    def convert_list_to_dict(self, lst: List[Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Converts a list into a dictionary with indices as keys.\n",
    "        Parameters:\n",
    "        - lst: List[Any], input list to be transformed.\n",
    "        Returns:\n",
    "        - Dict[str, Any]: Transformed dictionary.\n",
    "        \"\"\"\n",
    "        return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "    def preprocess_data(self, extracted_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Preprocesses the extracted data to ensure that it is in the desired format.\n",
    "        Parameters:\n",
    "        - extracted_data: Dict[str, Any], the data to be preprocessed.\n",
    "        Returns:\n",
    "        - Dict[str, Any]: Preprocessed data.\n",
    "        \"\"\"\n",
    "        return self._recursive_preprocessing(extracted_data)\n",
    "    \n",
    "    def _recursive_preprocessing(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Recursively preprocesses a dictionary, applying conversion methods to its elements.\n",
    "        Parameters:\n",
    "        - data: Dict[str, Any], the data dictionary that needs to be preprocessed.\n",
    "        Returns:\n",
    "        - Dict[str, Any]: The preprocessed data dictionary.\n",
    "        \"\"\"\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, str):\n",
    "                data[key] = self._handle_string_value(value)\n",
    "            elif isinstance(value, list):\n",
    "                data[key] = self._handle_list_value(value)\n",
    "            elif isinstance(value, dict):\n",
    "                data[key] = self._recursive_preprocessing(value)\n",
    "            elif isinstance(value, set):\n",
    "                data[key] = self._handle_set_value(value)\n",
    "        return data\n",
    "\n",
    "    def _handle_set_value(self, value: set) -> list:\n",
    "        \"\"\"\n",
    "        Handles set type values during preprocessing by converting them to lists.\n",
    "        Parameters:\n",
    "        - value: set, the set value to be preprocessed.\n",
    "        Returns:\n",
    "        - list: The preprocessed value as a list.\n",
    "        \"\"\"\n",
    "        return list(value)\n",
    "    \n",
    "    def _handle_string_value(self, value: str) -> Any:\n",
    "        \"\"\"\n",
    "        Handles string type values during preprocessing.\n",
    "        Parameters:\n",
    "        - value: str, the string value to be preprocessed.\n",
    "        Returns:\n",
    "        - Any: The preprocessed value.\n",
    "        \"\"\"\n",
    "        return Neo4jPersister.convert_to_primitives(value)\n",
    "    \n",
    "    def _handle_list_value(self, value: list) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Handles list type values during preprocessing.\n",
    "        Parameters:\n",
    "        - value: list, the list value to be preprocessed.\n",
    "        Returns:\n",
    "        - Dict[str, Any]: The preprocessed value as a dictionary.\n",
    "        \"\"\"\n",
    "        return self.convert_list_to_dict(Neo4jPersister.convert_to_primitives(value))\n",
    "    \n",
    "    def process_single_result(self, extracted_data: Dict, json_filename: str, hdf5_filename: str) -> Optional[Dict]:\n",
    "        try:\n",
    "            # Pre-process data to convert lists and strings to dictionaries\n",
    "            preprocessed_data = self.preprocess_data(extracted_data)\n",
    "            \n",
    "            processed_data = {}\n",
    "            processed_data['name'] = preprocessed_data.get('name', 'N/A')\n",
    "            processed_data['Áreas de atuação'] = preprocessed_data.get('Áreas de atuação', 'N/A')\n",
    "            # processed_data['publications'] = int(preprocessed_data.get('publications', 0))\n",
    "            \n",
    "            self.to_json([processed_data], json_filename)\n",
    "            self.to_hdf5([processed_data], hdf5_filename)\n",
    "            print(processed_data)\n",
    "            return preprocessed_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during single result processing:\\n {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_all_results(self, \n",
    "                            all_extracted_data: List[Dict], \n",
    "                            json_filename: str, \n",
    "                            hdf5_filename: str) -> List[Dict]:\n",
    "        successful_processed_data = []\n",
    "        for extracted_data in all_extracted_data:\n",
    "            processed_data = self.process_single_result(extracted_data, json_filename, hdf5_filename)\n",
    "            if processed_data is not None:\n",
    "                successful_processed_data.append(processed_data)\n",
    "            else:\n",
    "                self.failed_extractions.append(extracted_data)\n",
    "        if self.failed_extractions:\n",
    "            logging.info(\"Retrying failed extractions...\")\n",
    "            for failed_data in self.failed_extractions:\n",
    "                processed_data = self.process_single_result(failed_data, json_filename, hdf5_filename)\n",
    "                if processed_data is not None:\n",
    "                    successful_processed_data.append(processed_data)\n",
    "        self.to_json(successful_processed_data, json_filename)\n",
    "        self.to_hdf5(successful_processed_data, hdf5_filename)\n",
    "        return successful_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sistema operacional Linux\n",
      "Pasta armazenagem local /home/marcos/fioce/\n",
      "\n",
      "Pasta para os xml já existe!\n",
      "Pasta para os CSV já existe!\n",
      "Pasta para os JSON já existe!\n",
      "Pasta para figuras já existe!\n",
      "Pasta para saídas já existe!\n",
      "\n",
      "Caminho da pasta raiz /home/marcos/fioce/\n",
      "Caminho arquivos  XML /home/marcos/fioce/xml_zip/\n",
      "Caminho arquivos JSON /home/marcos/fioce/json/\n",
      "Caminho arquivos  CSV /home/marcos/fioce/csv/\n",
      "Caminho para  figuras /home/marcos/fioce/fig/\n",
      "Pasta arquivos saídas /home/marcos/fioce/output/\n",
      "\n",
      "Conectando com o servidor do CNPq...\n",
      "Sistema operacional Linux\n",
      "\n",
      "ERRO!! ao tentar rodar o chromedriver:\n",
      "session not created: This version of ChromeDriver only supports Chrome version 119\n",
      "Current browser version is 121.0.6167.184 with binary path /opt/google/chrome/chrome\n"
     ]
    }
   ],
   "source": [
    "def connect_driver(caminho):\n",
    "    '''\n",
    "    Conecta ao servidor do CNPq para busca de currículo\n",
    "    '''\n",
    "    import os\n",
    "    import sys\n",
    "    sistema_operacional =sys.platform\n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    # options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    # driver   = webdriver.Chrome(options=options)\n",
    "     \n",
    "    if 'linux' in sistema_operacional:\n",
    "        print('Sistema operacional Linux')\n",
    "        try:\n",
    "            drive   = '/home/'\n",
    "            usuario = 'mak/'\n",
    "            os.listdir(drive+usuario)\n",
    "        except:\n",
    "            drive   = '/home/'\n",
    "            usuario = 'marcos/'\n",
    "        driver_path = caminho+'chromedriver/chromedriver'\n",
    "    elif 'win32' in sistema_operacional:\n",
    "        print('Sistema operacional Windows')\n",
    "        drive   = 'C'\n",
    "        print(f'Drive em uso {drive.upper()}')\n",
    "        # drive = 'E'\n",
    "        # drive = input('Indique qual a letra da unidade onde deseja armazenar os arquivos (Ex.: C, E...)')\n",
    "        usuario = 'Users/marco/'\n",
    "        if os.path.isdir(drive+':/'+usuario) is False:\n",
    "            usuario = 'Users/marcos.aires/'\n",
    "        driver_path = caminho+'chromedriver/chromedriver.exe'\n",
    "    else:\n",
    "        print('SO não reconhecido')\n",
    "\n",
    "    # print(driver_path)\n",
    "    service = Service(driver_path)\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "    except Exception as e:\n",
    "        print(\"\\nERRO!! ao tentar rodar o chromedriver:\")\n",
    "        print(e.msg)\n",
    "        return\n",
    "\n",
    "    url_busca = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    driver.get(url_busca) # acessa a url de busca do CNPQ   \n",
    "    driver.set_window_position(-20, -10)\n",
    "    driver.set_window_size(170, 1896)\n",
    "    driver.mouse = webdriver.ActionChains(driver)\n",
    "    return driver\n",
    "\n",
    "class LattesScraper:\n",
    "    def __init__(self, driver, institution, unit, term):\n",
    "        self.base_url = 'http://buscatextual.cnpq.br'\n",
    "        self.session = requests.Session()\n",
    "        self.driver = driver\n",
    "        self.delay = 10\n",
    "\n",
    "    def wait_for_element(self, css_selector: str, ignored_exceptions=None):\n",
    "        \"\"\"\n",
    "        Waits for the element specified by the CSS selector to load.\n",
    "        :param css_selector: CSS selector of the element to wait for\n",
    "        :param ignored_exceptions: List of exceptions to ignore\n",
    "        \"\"\"\n",
    "        WebDriverWait(self.driver, self.delay, ignored_exceptions=ignored_exceptions).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "    def paginar(self, driver):\n",
    "        '''\n",
    "        Helper function to page results on the search page\n",
    "        '''\n",
    "        numpaginas = []\n",
    "        css_paginacao = \"div.paginacao:nth-child(2)\"\n",
    "        try:\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "            paginacao = self.driver.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "            paginas = paginacao.text.split(' ')\n",
    "            remover = ['', 'anterior', '...']\n",
    "            numpaginas = [x for x in paginas if x not in remover]\n",
    "        except Exception as e:\n",
    "            print('  ERRO!! Ao rodar função paginar():', e)\n",
    "        return numpaginas\n",
    "\n",
    "    def retry(self, func, expected_ex_type=Exception, limit=0, wait_ms=200,\n",
    "              wait_increase_ratio=2, on_exhaust=\"throw\"):\n",
    "        attempt = 1\n",
    "        while True:\n",
    "            try:\n",
    "                return func()\n",
    "            except Exception as ex:\n",
    "                if not isinstance(ex, expected_ex_type):\n",
    "                    raise ex\n",
    "                if 0 < limit <= attempt:\n",
    "                    if on_exhaust == \"throw\":\n",
    "                        raise ex\n",
    "                    return on_exhaust\n",
    "                attempt += 1\n",
    "                time.sleep(wait_ms / 1000)\n",
    "                wait_ms *= wait_increase_ratio\n",
    "\n",
    "    def find_terms(self, NOME, instituicao, unidade, termo, delay, limite):\n",
    "        \"\"\"\n",
    "        Função para manipular o HTML até abir a página HTML de cada currículo   \n",
    "        Parâmeteros:\n",
    "            - NOME: É o nome completo de cada pesquisador\n",
    "            - Instituição, unidade e termo: Strings a buscar no currículo para reduzir duplicidades\n",
    "            - driver (webdriver object): The Selenium webdriver object.\n",
    "            - limite (int): Número máximo de tentativas em casos de erro.\n",
    "            - delay (int): tempo em milisegundos a esperar nas operações de espera.\n",
    "        Retorna:\n",
    "            elm_vinculo, np.NaN, np.NaN, np.NaN, driver.\n",
    "        Em caso de erro retorna:\n",
    "            None, NOME, np.NaN, e, driver\n",
    "        \"\"\"\n",
    "        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "        # Inicializando variáveis para evitar UnboundLocalError\n",
    "        elm_vinculo = None\n",
    "        qte_resultados = 0\n",
    "        ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "        duvidas   = []\n",
    "        force_break_loop = False\n",
    "        try:\n",
    "            # Wait and fetch the number of results\n",
    "            css_resultados = \".resultado\"\n",
    "            WebDriverWait(driver, delay, ignored_exceptions=ignored_exceptions).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "            resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "            ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "            try:\n",
    "                css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "                WebDriverWait(driver, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                div_element = soup.find('div', {'class': 'tit_form'})\n",
    "                match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "                if match:\n",
    "                    qte_resultados = int(match.group(1))\n",
    "                    # print(f'{qte_resultados} resultados para {NOME}')\n",
    "                else:\n",
    "                    return None, NOME, np.NaN, 'Currículo não encontrado', driver\n",
    "            except Exception as e1:\n",
    "                print('  ERRO!! Currículo não disponível no Lattes')\n",
    "                return None, NOME, np.NaN, e1, driver\n",
    "            \n",
    "            ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "            ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "            numpaginas = self.paginar(driver)\n",
    "            if numpaginas == [] and qte_resultados==1:\n",
    "                # capturar link para o primeiro nome resultado da busca\n",
    "                try:\n",
    "                    css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                    WebDriverWait(driver, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                    elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                    nome_vinculo = elm_vinculo.text\n",
    "                except Exception as e2:\n",
    "                    print('  ERRO!! Ao encontrar o primeiro resultado da lista de nomes:', e2)\n",
    "                    \n",
    "                    # Call the handle stale file_error function\n",
    "                    if self.handle_stale_file_error(driver):\n",
    "                        # If the function returns True, it means the error was resolved.\n",
    "                        # try to get the nome_vinculo again:\n",
    "                        try:\n",
    "                            elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                            nome_vinculo = elm_vinculo.text\n",
    "                        except Exception as e3:\n",
    "                            print('  ERRO!! Servidor CNPq indisponível no momento, tentar em alguns minutos:', e3)\n",
    "                            return None, NOME, np.NaN, e3, driver\n",
    "                    else:\n",
    "                        # If the function returns False, it means the error was not resolved within the given retries.\n",
    "                        return None, NOME, np.NaN, e2, driver\n",
    "\n",
    "                    print('  Não foi possível extrair por falha no servidor do CNPq:',e)\n",
    "                    return None, NOME, np.NaN, e2, driver\n",
    "                # print('Clicar no nome único:', nome_vinculo)\n",
    "                try:\n",
    "                    self.retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                        wait_ms=20,\n",
    "                        limit=limite,\n",
    "                        on_exhaust=(f'  Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "                except Exception as e4:\n",
    "                    print('  ERRO!! Ao clicar no único nome encontrado anteriormente',e)\n",
    "                    return None, NOME, np.NaN, e4, driver\n",
    "            \n",
    "            ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "            else:\n",
    "                print(f'       {qte_resultados:>3} homônimos de: {NOME}')\n",
    "                numpaginas = self.paginar(driver)\n",
    "                numpaginas.append('próximo')\n",
    "                iteracoes=0\n",
    "                ## iterar em cada página de resultados\n",
    "                pagin = qte_resultados//10+1\n",
    "                count = None\n",
    "                found = None\n",
    "                for i in range(pagin+1):\n",
    "                    # print(i,'/',pagin)\n",
    "                    iteracoes+=1\n",
    "                    try:\n",
    "                        numpaginas = self.paginar(driver)\n",
    "                        # print(f'       Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                        css_resultados = \".resultado\"\n",
    "                        WebDriverWait(driver, delay).until(\n",
    "                            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                        resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                    except Exception as e:\n",
    "                        print('  ERRO!! Ao paginar:',e)\n",
    "                    ## iterar em cada resultado\n",
    "                    for n,i in enumerate(resultados):\n",
    "                        linhas = i.text.split('\\n\\n')\n",
    "                        # print(linhas)\n",
    "                        if 'Stale file handle' in str(linhas):\n",
    "                            return np.NaN, NOME, np.NaN, 'Stale file handle', driver\n",
    "                        for m,linha in enumerate(linhas):\n",
    "                            # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                            # print('Conteúdo da linha:',linha.lower())\n",
    "                            # print(linha)\n",
    "                            try:\n",
    "                                if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                    # print('Vínculo encontrado!')\n",
    "                                    count=m\n",
    "                                    while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                        count-=1\n",
    "                                    # print('       Identificado vínculo no resultado:', m+1)\n",
    "                                    found = m+1\n",
    "                                    # nome_vinculo = linhas[count].replace('\\n','\\n       ').strip()\n",
    "                                    # print(f'       Achado: {nome_vinculo}')\n",
    "                                    try:\n",
    "                                        css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                        # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                        WebDriverWait(driver, delay).until(\n",
    "                                            EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                        elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                        nome_vinculo = elm_vinculo.text\n",
    "                                        # print('Elemento retornado:',nome_vinculo)\n",
    "                                        self.retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                                            wait_ms=100,\n",
    "                                            limit=limite,\n",
    "                                            on_exhaust=(f'  Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                    except Exception as e5:\n",
    "                                        print('  ERRO!! Ao achar o link do nome com múltiplos resultados')\n",
    "                                        return np.NaN, NOME, np.NaN, e5, driver\n",
    "                                    force_break_loop = True\n",
    "                                    break\n",
    "                            except Exception as e6:\n",
    "                                traceback_str = ''.join(traceback.format_tb(e6.__traceback__))\n",
    "                                print('  ERRO!! Ao procurar vínculo com currículos achados')    \n",
    "                                print(e6,traceback_str)\n",
    "                            ## Caso percorra toda lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                            if m==(qte_resultados):\n",
    "                                print(f'Nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                                duvidas.append(NOME)\n",
    "                                # clear_output(wait=True)\n",
    "                                # driver.quit()\n",
    "                                continue\n",
    "                        if force_break_loop:\n",
    "                            break\n",
    "                    try:\n",
    "                        prox = driver.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                        prox.click()\n",
    "                    except:\n",
    "                        continue\n",
    "                if count:\n",
    "                    nome_vinculo = linhas[count].replace('\\n','\\n       ').strip()\n",
    "                    print(f'       Escolhido homônimo {found}: {nome_vinculo}')\n",
    "                else:\n",
    "                    print(f'       Não foi possível identificar o vínculo de: {NOME}')\n",
    "                    duvidas.append(NOME)\n",
    "            try:\n",
    "                elm_vinculo.text\n",
    "                # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "            except:\n",
    "                return None, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "        except exceptions.TimeoutException:\n",
    "            print(\"  ERRO!! O tempo limite de espera foi atingido.\")\n",
    "            return None, NOME, np.NaN, \"TimeoutException\", driver\n",
    "        except exceptions.WebDriverException as e7:\n",
    "            print(\"  ERRO!! Problema ao interagir com o driver.\")\n",
    "            return None, NOME, np.NaN, e7, driver\n",
    "        except Exception as e8:\n",
    "            print(\"  ERRO 8!! Um erro inesperado ocorreu.\")\n",
    "            print(f'  {e8}')\n",
    "            return None, NOME, np.NaN, e8, driver\n",
    "        # Verifica antes de retornar para garantir que elm_vinculo foi definido\n",
    "        if elm_vinculo is None:\n",
    "            print(\"Vínculo não foi definido.\")\n",
    "            return None, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "        # Retorna a saída de sucesso\n",
    "        return elm_vinculo, np.NaN, np.NaN, np.NaN, driver\n",
    "\n",
    "    def handle_stale_file_error(self, max_retries=5, retry_interval=10):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                error_div = self.driver.find_element(By.CSS_SELECTOR, 'resultado')\n",
    "                linha1 = error_div.fidChild('li')\n",
    "                if 'Stale file handle' in linha1.text:\n",
    "                    time.sleep(retry_interval)\n",
    "                else:\n",
    "                    return True\n",
    "            except NoSuchElementException:\n",
    "                return True\n",
    "        return False\n",
    "       \n",
    "    def extract_data_from_cvuri(self, element) -> dict:\n",
    "        \"\"\"\n",
    "        Extracts data from the cvuri attribute of the given element.\n",
    "        :param element: WebElement object\n",
    "        :return: Dictionary of extracted data\n",
    "        \"\"\"\n",
    "        cvuri = element.get_attribute('cvuri')\n",
    "        parsed_url = urlparse(cvuri)\n",
    "        params = parse_qs(parsed_url.query)\n",
    "        data_dict = {k: v[0] for k, v in params.items()}\n",
    "        return data_dict\n",
    "\n",
    "    def fill_name(self, NOME):\n",
    "        '''\n",
    "        Move cursor to the search field and fill in the specified name.\n",
    "        '''\n",
    "        if self.driver is None:\n",
    "            logging.error(\"O driver não foi inicializado corretamente.\")\n",
    "            return\n",
    "        try:\n",
    "            nome = lambda: self.driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "            nome().send_keys(Keys.CONTROL + \"a\")\n",
    "            nome().send_keys(NOME)\n",
    "        except Exception as e:\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(f'  ERRO!! Ao colar nome para buscar.') #, {traceback_str}\n",
    "        try:            \n",
    "            seletorcss = 'div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "            \n",
    "            seletorcss = \"#botaoBuscaFiltros\"\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        except Exception as e:\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(f'  ERRO!! Ao clicar no botão Buscar.\\n{e}, {traceback_str}')\n",
    "\n",
    "    def return_search_page(self):\n",
    "        url_busca = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "        driver.get(url_busca) # acessa a url de busca do CNPQ        \n",
    "\n",
    "    def check_and_click_vinculo(self, elm_vinculo):\n",
    "        if elm_vinculo is None:\n",
    "            self.return_search_page()\n",
    "            logging.error(\"Vínculo não encontrado, passando para o próximo nome...\")\n",
    "        try:\n",
    "            logging.info(f'Vínculo encontrado no currículo de nome: {elm_vinculo.text}')\n",
    "        except AttributeError:\n",
    "            self.return_search_page()\n",
    "            logging.error(\"Vínculo não encontrado, passando para o próximo nome...\")\n",
    "        try:\n",
    "            # Clicar no botão para abrir o currículo\n",
    "            btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "            time.sleep(0.2)\n",
    "            ActionChains(driver).click(btn_abrir_curriculo).perform()            \n",
    "            # logging.info('Successfully clicked on the vínculo.')\n",
    "        except WebDriverException:\n",
    "            self.return_search_page()\n",
    "            logging.error('Falha ao clicar no link do nome.')\n",
    "\n",
    "    def switch_to_new_window(self):\n",
    "        window_before = self.driver.current_window_handle\n",
    "        WebDriverWait(self.driver, self.delay).until(EC.number_of_windows_to_be(2))\n",
    "        window_after = self.driver.window_handles\n",
    "        new_window = [x for x in window_after if x != window_before][0]\n",
    "        self.driver.switch_to.window(new_window)\n",
    "\n",
    "    def switch_back_to_original_window(self):\n",
    "        current_window = self.driver.current_window_handle\n",
    "        original_window = [x for x in self.driver.window_handles if x != current_window][0]\n",
    "        self.driver.close() # Close the current window\n",
    "        self.driver.switch_to.window(original_window) # Switch back to the original window\n",
    "\n",
    "    def extract_tooltip_data(self) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Extracts tooltip data from articles section using Selenium.\n",
    "        :return: List of dictionaries containing the extracted tooltip data\n",
    "        \"\"\"\n",
    "        tooltip_data_list = []\n",
    "        try:\n",
    "            self.wait_for_element(\"#artigos-completos img.ajaxJCR\", [TimeoutException])\n",
    "            layout_cells = self.driver.find_elements(By.CSS_SELECTOR, '#artigos-completos .layout-cell-11 .layout-cell-pad-5')\n",
    "            for cell in layout_cells:\n",
    "                tooltip_data = {}\n",
    "                try:\n",
    "                    elem_citado = cell.find_element(By.CSS_SELECTOR, '.citado')\n",
    "                    tooltip_data.update(self.extract_data_from_cvuri(elem_citado))\n",
    "                except (ElementNotInteractableException, NoSuchElementException):\n",
    "                    pass\n",
    "                try:\n",
    "                    doi_elem = cell.find_element(By.CSS_SELECTOR, \"a.icone-producao.icone-doi\")\n",
    "                    tooltip_data[\"doi\"] = doi_elem.get_attribute(\"href\")\n",
    "                except NoSuchElementException:\n",
    "                    tooltip_data[\"doi\"] = None\n",
    "                try:\n",
    "                    self.wait_for_element(\"img.ajaxJCR\", [TimeoutException])\n",
    "                    tooltip_elem = self.driver.find_element(By.CSS_SELECTOR, \"img.ajaxJCR\")\n",
    "                    ActionChains(self.driver).move_to_element(tooltip_elem).perform()\n",
    "                    original_title = tooltip_elem.get_attribute(\"original-title\")\n",
    "                    match = re.search(r\"Fator de impacto \\(JCR \\d{4}\\): (\\d+\\.\\d+)\", original_title)\n",
    "                    tooltip_data[\"impact-factor\"] = match.group(1) if match else None\n",
    "                    tooltip_data[\"original_title\"] = original_title.split('<br />')[0].strip()\n",
    "                except (NoSuchElementException, TimeoutException):\n",
    "                    pass\n",
    "                tooltip_data_list.append(tooltip_data)\n",
    "            print(f'       {len(tooltip_data_list):>003} artigos extraídos')\n",
    "            logging.info(f'{len(tooltip_data_list):>003} artigos extraídos')\n",
    "\n",
    "        except TimeoutException as e:\n",
    "            logging.error(f\"Sem respota antes do timeout\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro inesperado ao extrair tooltips: {e}\")\n",
    "        return tooltip_data_list\n",
    "            \n",
    "    def search_profile(self, name, instituicao, unidade, termo):\n",
    "        try:\n",
    "            # Find terms to interact with the web page and extract the profile\n",
    "            profile_element, _, _, _, _ = self.find_terms(\n",
    "                name, \n",
    "                instituicao,  \n",
    "                unidade,  \n",
    "                termo,  \n",
    "                10,  \n",
    "                3  \n",
    "            )\n",
    "            # print('Elemento encontrado:', profile_element)\n",
    "            if profile_element:\n",
    "                return profile_element\n",
    "            else:\n",
    "                self.return_search_page()\n",
    "                logging.info(f'Currículo não encontrado: {name}')\n",
    "\n",
    "        except requests.HTTPError as e:\n",
    "            logging.error(f\"HTTPError occurred: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro inesperado ao buscar: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "    def scrape(self, driver, \n",
    "               name_list, \n",
    "               instituicao, \n",
    "               unidade, \n",
    "               termo, \n",
    "               json_filename, \n",
    "               hdf5_filename):\n",
    "        dict_list=[]\n",
    "        for k, name in enumerate(name_list):\n",
    "            try:\n",
    "                print(f'{k+1:>2}/{len(name_list)}: {name}')\n",
    "                self.fill_name(name)\n",
    "                elm_vinculo = self.search_profile(name, instituicao, unidade, termo)\n",
    "                self.check_and_click_vinculo(elm_vinculo)\n",
    "                self.switch_to_new_window()\n",
    "                \n",
    "                if elm_vinculo:\n",
    "                    tooltip_data_list = self.extract_tooltip_data()\n",
    "                    page_source = driver.page_source\n",
    "                    if page_source is not None:\n",
    "                        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                        soup.attrs['tooltips'] = tooltip_data_list                 \n",
    "                        if soup:\n",
    "                            # print('Extraindo dados do objeto Soup...')\n",
    "                            parse_soup_instance = ParseSoup(driver)\n",
    "                            data = parse_soup_instance.extract_data(soup)\n",
    "                            # Chama métodos de conversão de dicionário individual\n",
    "                            # parse_soup_instance.to_json(data, json_filename)\n",
    "                            # parse_soup_instance.to_hdf5(data, hdf5_filename)\n",
    "                            dict_list.append(data)\n",
    "                    else:\n",
    "                        logging.error(f\"Could not get soup for profile: {name}\")\n",
    "                else:\n",
    "                    logging.error(f\"Currículo não encontrado para: {name}\")\n",
    "\n",
    "                # Fechar janela do currículo e voltar para página de busca\n",
    "                self.switch_back_to_original_window()\n",
    "\n",
    "                # Clicar no botão para fechar janela pop-up\n",
    "                btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnfechar\")))\n",
    "                ActionChains(driver).click(btn_abrir_curriculo).perform()    \n",
    "                # logging.info('Successfully closed pop-up.')       \n",
    "                # # Clicar no botão para fazer nova consulta\n",
    "                # btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "                #     EC.element_to_be_clickable((By.CSS_SELECTOR, \"#botaoBuscaFiltros\")))\n",
    "                # ActionChains(driver).click(btn_abrir_curriculo).perform()\n",
    "                self.return_search_page()\n",
    "                # logging.info('Successfully restarded extraction.')\n",
    "            except TimeoutException as e:\n",
    "                logging.error(f\"Sem resposta antes do timeout para: {name}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erro inesperado ao extrair para: {name}: {str(e)}\")\n",
    "        driver.quit()\n",
    "        return dict_list\n",
    "    \n",
    "if __name__ == \"__main__\":   \n",
    "    drives=['C:/Users/','E:/','./home/']\n",
    "    pastas=['marcos.aires/', 'marco/']\n",
    "    pastasraiz=['kgfioce','fioce']\n",
    "    pasta_dados = '/home/mak/gml_classifier-1/data/input/'\n",
    "    pastaraiz = 'fioce'\n",
    "    caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    pathzip, pathcsv, pathjson, pathfig, pathaux, pathout = preparar_pastas(caminho)\n",
    "\n",
    "    instituicao = 'Fundação Oswaldo Cruz'\n",
    "    unidade = 'Fiocruz Ceará'\n",
    "    termo = 'Ministerio da Saude'\n",
    "    driver = connect_driver(caminho)\n",
    "    if driver:\n",
    "        t0 = time.time()\n",
    "        scraper = LattesScraper(driver, instituicao, unidade, termo)\n",
    "        dict_list = scraper.scrape(driver, lista_nomes, instituicao, unidade, termo,\n",
    "                                pasta_dados+\"output.json\", pasta_dados+\"output.hdf5\")\n",
    "        \n",
    "        print(f'{tempo(t0,time.time())} para busca e extração de dados de {len(lista_nomes)} nomes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(dict_list)} dicionários montados')\n",
    "qte_artigos=0\n",
    "for k,i in enumerate(dict_list):\n",
    "    try:\n",
    "        qte_jcr = len(i['JCR'])\n",
    "    except:\n",
    "        qte_jcr = 0\n",
    "    try:\n",
    "       qte_jcr2 = len(i['JCR2'])\n",
    "    except:\n",
    "       qte_jcr2 = 0\n",
    "    qte_artigos+=qte_jcr\n",
    "    print(f\"{k:>2} {qte_jcr:>03} {qte_jcr2:>03} {i['name']} \")\n",
    "\n",
    "print(f'\\nTotal de artigos em todos períodos: {qte_artigos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('/home/mak/gml_classifier-1/data/input/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('/home/mak/gml_classifier-1/data/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(pathout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo(t0,time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos onde os arquivos JSON e HDF5 serão armazenados\n",
    "json_filename = pasta_dados+\"dict_list.json\"\n",
    "hdf5_filename = pasta_dados+\"dict_list.hdf5\"\n",
    "\n",
    "# Processamento da lista completa e armazenamento dos dados processados\n",
    "with ParseSoup(driver) as parse_soup_instance:\n",
    "    processed_data = parse_soup_instance.process_all_results(dict_list, json_filename, hdf5_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[0]['Áreas de atuação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('lattes_scraper.log', 'r') as f:\n",
    "#     content = f.read()\n",
    "#     print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fioce_pessoal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
