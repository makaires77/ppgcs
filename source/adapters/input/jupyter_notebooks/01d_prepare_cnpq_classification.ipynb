{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><center><img src=\"https://raw.githubusercontent.com/makaires77/fioce/master/assets/logo_fioce.png\" \n",
    "style=\"height:150px\" alt=\"Logo_Unifor\"></center></center>\n",
    "\n",
    "## <center>Analisar Produção Acadêmica Institucional da Fiocruz Ceará por inteligência artificial para agrupamento dinâmico com as terminologias adotadas no CNPq</center>\n",
    "\n",
    "    Antonio Marcos Aires Barbosa – Fiocruz Ceará\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>INTRODUÇÃO</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contexto da demanda**\n",
    "\n",
    "A Vice-Presidência de Pesquisa e Coleções Biológicas da Fiocruz (VPPCB/Fiocruz) solicita especificar as áreas do conhecimento de acordo com os critérios do CNPq. Mediante dados coletados sobre a produção científica da Fiocruz a em seis bases de dados (WoS, Scopus, PubMed, Lilacs, Arca via OasisBr e Currfculo Lattes), visamos neste trabalho classiicar a produção acadêmica da Fiocruz segundo os termos de classificação usados pelo CNPq. Esta identificação servirá de base para uma reorganização das áreas e linhas de pesquisa de todas as unidades da Fiocruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo geral:**\n",
    "\n",
    "    Alinhar as áreas da produção científica/acadêmica constante nas publicações da Fiocruz Ceará com as definições/nomenclatura do CNPq para alinhar a pesquisa desenvolvida pela Fiocruz com áreas já definidas pelo CNPq, em até quatro níveis diferentes de detalhamento.\n",
    "\n",
    "**Objetivos Específicos**\n",
    "\n",
    "    1. Fazer análise de similaridade entre títulos, resumos das publicações constantes das planilhas fornecidas pela VPEIC com as definições do CNPq para classificar cada publciação nas áreas do CNPq mais correlatas aos dados de cada publicação, visando contribuir para redefinição das áreas de pesquisa institucional tornando-as mais alinhadas ao CNPq;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas em analisar produção científica institucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Definição de autoria e coautoria</b>\n",
    "\n",
    "Em relação às diretrizes para atribuição de autoria e coautoria, a Fiocruz aderiu às recomendações estabelecidas pelo Comitê Internacional de Editores de Revistas Médicas (International Committee of Medical Journal Editors - ICMJE). O ICMJE apresenta critérios objetivos, de forma a designar como autores apenas aqueles que merecem crédito e têm responsabilidade sobre o trabalho [1,2]. É de comum acordo que todo colaborador para ser considerado autor deve ter tido contribuição substancial para a construção do manuscrito. \n",
    "\n",
    "<b>Todos</b> os critérios listados abaixo devem ser preenchidos para que uma pessoa seja considerada como autora, são eles: \n",
    "\n",
    "a. Participação significativa da concepção ou desenho do estudo, ou na coleta, aquisição, análise e/ou interpretação de dados;\n",
    "\n",
    "b. Envolvimento na redação do artigo ou revisão crítica relevante do conteúdo intelectual;\n",
    "\n",
    "c. Aprovação da versão final do manuscrito para ser publicado;\n",
    "\n",
    "d. Ser responsável por todos os aspectos do trabalho na garantia da exatidão e integridade de qualquer parte do manuscrito\n",
    "\n",
    "A Fiocruz recomenda a consulta às diretrizes do Comitê de Ética de Publicações (Committee on Publication Ethics – COPE) [3, 4] e do Conselho de Editores em Ciência (Council of Science Editors - CSE) [5] quando for necessário complementar os requisitos de autoria em determinados campos do conhecimento. As diretrizes do COPE e do CSE orientam e apoiam editores, editoras, universidades, institutos de pesquisa e todos os envolvidos nas questões éticas de publicação, permitindo a transparência sobre quem e em que nível contribuiu para o trabalho.\n",
    "\n",
    "Considerando os critérios de autoria acima recomendados, não é permitido a adoção das seguintes práticas na instituição:\n",
    "1. <b>Autoria de Presente:</b>\n",
    "A autoria de presente é a atribuição de crédito a um(a) autor(a) que não tenha feito contribuição significativa para o trabalho.\n",
    "\n",
    "2. <b>Autoria honorária:</b>\n",
    "A autoria honorária consiste na atribuição de crédito a um(a) autor(a) com prestígio ou que ocupa posição sênior masque não fez contribuição significativa para o trabalho.\n",
    "\n",
    "3. <b>Autoria fantasma:</b>\n",
    "A autoria fantasma é a não atribuição de crédito de autoria a alguém que tenha contribuído significativamente para o trabalho.\n",
    "\n",
    "Não se justifica a inclusão de nomes de autores cuja contribuição não se enquadre nos critérios acima. Desta forma, recomenda-se que seja respeitada a ordem de autoria acordada na concepção de qualquer estudo entre os integrantes da pesquisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escopo do Levantamento de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Universo para extração de dados:**\n",
    "- Currículos Lattes dos servidores da Fiocruz Ceará, em função de pesquisa independente de cargos/funções.\n",
    "\n",
    "**Período de análise:** \n",
    "- A partir da data de ingresso (início de atuação na Fiocruz Ceará), até mês corrente do término do estudo.\n",
    "    \n",
    "**Extração computacional dos dados:**\n",
    "- ScriptDadosCurriculo: Publicações (artigos completos em peródicos) de cada um dos indivíduos da amostra.\n",
    "\n",
    "**Dados a ser analizados:**\n",
    "- CoordenaçãoPesquisa: Lista de Nomes de Servidores, com as respectivas datas de entrada de cada um dos servidores na Fiocruz Ceará (ou data de ingresso na Fiocruz, pelo menos)\n",
    "- VPEIC/Observatório: Planilha com lista de artigos publicados pela Fiocruz Ceará no período 2008 a 2023;\n",
    "- VPEIC/Observatório: Planilha com lista de artigos publicados pela Fiocruz sem identificação da unidade que produziu o publicação;\n",
    "- Diretório dos Grupos de Pesquisa CNPq/GdP: Tabela de Áreas do Conhecimento, segmentada em: Grandes Áreas, Áreas  \n",
    "\n",
    "### Níveis de classificação das informações a serem preenchidas\n",
    "\n",
    "A patir de análises nos títulos, resumos e palavras-chave dos artigos, serão aplicadas as orientações da VPPCB sucintamente citadas a seguir para alinhar com os conceitos do CNPq e escolher a área e subáreas em ate três níveis.\n",
    "\n",
    "    - N00: Grande Área   (008 classes para grande área)\n",
    "    - N01: Área          (076 classes para área)\n",
    "    - N02: Subárea       (340 classes para subárea)\n",
    "    - N03: Especialidade (861 classes para especialidades)\n",
    "\n",
    "*Observações:* \n",
    "\n",
    "Sobre a área de conhecimento: Caso a produção científica não esteja contemplada em nenhuma das áreas de conhecimento do CNPq, por favor, preencher na coluna disponível com ate 4 níveis, separando os níveis por \";\"\n",
    "\n",
    "As planilhas ao final das colunas tem espaço reservado para algo não editável da planilha precise ser corrigido.\n",
    "\n",
    "### Regras de associação à Fiocruz Ceará\n",
    "- Considerar apenas a produção dos servidores ativos no momento de início do estudo\n",
    "- Considerar contagem de produção partir da data de ingresso na Fiocruz Ceará (Dados do RH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>MONTAR ÁRVORE DO CONHECIMENTO DO CNPQ</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r {os.path.join(get_path_repo(),'requirements.txt')}\n",
    "# !python.exe -m pip install --upgrade pip\n",
    "# !python -m pip install --upgrade pip setuptools\n",
    "# !python -m pip install weasyprint\n",
    "# %pip install neo4j\n",
    "# %pip install numba\n",
    "# %pip install cairo\n",
    "# %pip install pango\n",
    "# %pip install plotly\n",
    "# %pip install gensim\n",
    "# %pip install pdfkit\n",
    "# %pip install weasyprint\n",
    "# %pip install cffi pycairo\n",
    "# %pip install transformers\n",
    "# %pip install scipy --upgrade\n",
    "# %pip uninstall scipy && pip install scipy\n",
    "# !pango-view --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mak/ppgcs/source/adapters/input/jupyter_notebooks\n",
      "/usr/lib/python310.zip\n",
      "/usr/lib/python3.10\n",
      "/usr/lib/python3.10/lib-dynload\n",
      "\n",
      "/home/mak/.local/lib/python3.10/site-packages\n",
      "/usr/local/lib/python3.10/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "\n",
      "Conteúdos da pasta utils\n",
      "  cnpq_classes.py\n",
      "  descriptive_statistics.py\n",
      "  tests_prepare.py\n",
      "  pytorch_checks.py\n",
      "  articles_counter.py\n",
      "  json_fle_manager.py\n",
      "  settings_backup.json\n",
      "  __pycache__\n",
      "  prepare_testing_environment.py\n",
      "  __init__.py\n",
      "\n",
      "Conteúdos da pasta domains\n",
      "  environment_validator.py\n",
      "  gml_cnpq_classifier.py\n",
      "  funding_embeddings.py\n",
      "  dataset_articles_generator_py.py\n",
      "  html_to_json.py\n",
      "  html_to_json_backup.py\n",
      "  scraper_pasteur.py\n",
      "  translate_en_pt.py\n",
      "  lda_extractor.py\n",
      "  go.mod\n",
      "  curriculum_scrapper.py\n",
      "  lattes_scrapper_original.py\n",
      "  go.sum\n",
      "  lattes_scrapper.py\n",
      "  experiment_monitor.py\n",
      "  lattes_html_parser.py\n",
      "  rdf_to_neo4j.py\n",
      "  dataset_articles_generator_optim_windows.exe\n",
      "  hierarquical_semantic_matcher.py\n",
      "  extract_audio.py\n",
      "  dataset_articles_generator_optim_linux\n",
      "  cnpq_tree.py\n",
      "  scraper_sucupira_edge.py\n",
      "  __pycache__\n",
      "  dataset_articles_generator_linux\n",
      "  html_homonimos\n",
      "  funding_finder.py\n",
      "  cnpq_tree4j.py\n",
      "  chromedriver_manager.py\n",
      "  neo4j_persister.py\n",
      "  Passo01_FormularQuestãoPesquisa.ipynb\n",
      "  fill_missing_data_crossref.py\n",
      "  bert_embeedings_to_neo4j.py\n",
      "  research_process_automation.py\n",
      "  graph_analysis.py\n",
      "  neo4j_roadmap.py\n",
      "  dataset_articles_generator_windows.exe\n",
      "  experiment_profiler.py\n",
      "  report_fioce.py\n",
      "  environment_setup.py\n",
      "  dataset_articles_generator.go\n",
      "  funding_finder_cnpq.py\n",
      "  ontomap_sio.py\n",
      "  gml_classification.py\n",
      "  dataset_articles_generator_optim.go\n",
      "  scraper_fiocruz_ceara.py\n",
      "  recomender_system.py\n",
      "  scraper_sucupira.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/paramiko/transport.py:237: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n",
      "[nltk_data] Downloading package stopwords to /home/mak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mak/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Caminho base do repositório: /home/mak/ppgcs\n",
      "\n",
      "Arquivos de entrada de dados: ['dict_list_temp.json', 'docents_dict_list.json', 'dict_list_discents_combined.json', 'temp_dict_list.json', 'combined_dict_list.json', 'discents_dict_list.json']\n",
      "\n",
      "Arquivos de dados processados:\n",
      "                              42 currículos carregados arquivo: 'docents_dict_list.json'\n",
      "\n",
      "GPU disponível para execução de código.\n",
      "\n",
      "Verificação dos pacotes de PLN\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "for i in sys.path:\n",
    "    print(i)\n",
    "\n",
    "## Base do diretório do repositório em caminho absoluto\n",
    "# base_repo_dir = None\n",
    "# if 'win' in sys.platform:\n",
    "#     base_repo_dir = 'C:\\\\Users\\\\marcos.aires\\\\gml_classifier-1'  # Caminho base no Windows\n",
    "# else:\n",
    "#     base_repo_dir = '/home/mak/gml_classifier-1'  # Caminho base em Linux\n",
    "\n",
    "## Base do diretório do repositório em caminho relativo\n",
    "# os.listdir('./../data/')\n",
    "# folder_data_dev = './../data/'\n",
    "# list(os.listdir(folder_data_dev))\n",
    "\n",
    "def get_path_repo():\n",
    "    \"\"\"Retorna o caminho absoluto cinco níveis acima do diretório atual.\"\"\"\n",
    "    current_directory = os.getcwd()\n",
    "    # Construir o caminho para subir cinco níveis\n",
    "    path_five_levels_up = os.path.join(current_directory, '../../../../')\n",
    "    # Normalizar o caminho para o formato absoluto\n",
    "    absolute_path = os.path.abspath(path_five_levels_up)\n",
    "    return absolute_path\n",
    "\n",
    "base_repo_dir = get_path_repo()\n",
    "\n",
    "# Construir caminhos usando os.path.join para evitar problemas entre Linux/Windows\n",
    "os.listdir(get_path_repo())\n",
    "os.listdir(os.path.join(get_path_repo(), 'source', 'domain'))\n",
    "folder_utils = os.path.join(get_path_repo(),'utils')\n",
    "folder_domain = os.path.join(get_path_repo(), 'source','domain/')\n",
    "folder_data_input = os.path.join(base_repo_dir, '_data', 'in_csv')\n",
    "folder_data_output = os.path.join(base_repo_dir, '_data', 'out_json')\n",
    "# Para o caso de folder_data_prod, que parece ser exclusivo para ambientes Unix\n",
    "folder_data_prod = os.path.join(base_repo_dir, '_data') if not 'win' in sys.platform else None\n",
    "\n",
    "print('\\nConteúdos da pasta utils')\n",
    "for i in os.listdir(folder_utils):\n",
    "    print(f\"  {i}\")\n",
    "\n",
    "print('\\nConteúdos da pasta domains')\n",
    "for i in os.listdir(folder_domain):\n",
    "    print(f\"  {i}\")\n",
    "\n",
    "# Adicionar pastas locais ao sys.path para importar pacotes criados\n",
    "sys.path.append(folder_utils)\n",
    "sys.path.append(folder_domain)\n",
    "from bert_embeedings_to_neo4j import BertEmbeddingsToNeo4j\n",
    "from json_fle_manager import JSONFileManager as jfm\n",
    "from experiment_monitor import ExperimentMonitor\n",
    "from articles_counter import ArticlesCounter\n",
    "from experiment_profiler import TimeProfiler\n",
    "from lda_extractor import LDAExtractor\n",
    "from report_fioce import ReportHTML\n",
    "\n",
    "print(f\"\\nCaminho base do repositório: {base_repo_dir}\")\n",
    "print(f\"\\nArquivos de entrada de dados: {jfm.list_json_files(folder_data_input)}\")\n",
    "print(f\"\\nArquivos de dados processados:\")\n",
    "list(os.listdir(os.path.join(folder_data_output)))\n",
    "\n",
    "# Definir arquivo dados brutos a processar e gerar dataset\n",
    "profiler = TimeProfiler()\n",
    "monitor = ExperimentMonitor(base_repo_dir, profiler)\n",
    "dict_json = 'docents_dict_list.json'\n",
    "dict_list = monitor.load_from_json(folder_data_input,dict_json)\n",
    "print(f\"{' '*30}{len(dict_list)} currículos carregados arquivo: '{dict_json}'\")\n",
    "\n",
    "if monitor.is_gpu_available():\n",
    "    print(f\"\\nGPU disponível para execução de código.\")\n",
    "else:\n",
    "    print(f\"\\nNão foi detectada nenhuma GPU configurada corretamente no ambiente.\")\n",
    "\n",
    "print(f\"\\nVerificação dos pacotes de PLN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela de Áreas de Conhecimento CNPQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://lattes.cnpq.br/documents/11871/24930/TabeladeAreasdoConhecimento.pdf/d192ff6b-3e0a-4074-a74d-c280521bd5f7\n",
    "\n",
    "http://lattes.cnpq.br/web/dgp/arvore-do-conhecimento\n",
    "\n",
    "<b> Árvore do conhecimento </b>\n",
    "\n",
    "    Ciências Agrárias.\n",
    "    Ciências Biológicas.\n",
    "    Ciências da Saúde.\n",
    "    Ciências Exatas e da Terra.\n",
    "    Engenharias.\n",
    "    Ciências Humanas.\n",
    "    Ciências Sociais Aplicadas.\n",
    "    Lingüística, Letras e Artes.\n",
    "\n",
    "<b> Setores de Aplicação </b>\n",
    "\n",
    "https://lattes.cnpq.br/web/dgp/setores-de-aplicacao-2002-2010 (Link Quebrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc9f76778514d8bbb983f7c0c5a35c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando páginas do PDF das Áreas de pesquisa do CNPq..:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dos códigos   identificados: 1285\n",
      "Total de descrições identificadas: 1285\n",
      "Nenhum erro de códigos em descrições!\n",
      "144 possíveis erros de descrição detectados.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e203d6f9eea3422bae9a965e9fd733e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Corrigindo descrições...:   0%|          | 0/1285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Codigo</th>\n",
       "      <th>Descricao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00.00.00-3</td>\n",
       "      <td>Ciências Exatas e da Terra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.01.00.00-8</td>\n",
       "      <td>Matemática</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.01.01.00-4</td>\n",
       "      <td>Álgebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.01.01.01-2</td>\n",
       "      <td>Conjuntos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.01.01.02-0</td>\n",
       "      <td>Lógica Matemática</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>8.03.08.02-3</td>\n",
       "      <td>Roteiro e Direção  Cinematográficos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>8.03.08.03-1</td>\n",
       "      <td>Técnicas de Registro e Processamento de Filmes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>8.03.08.04-0</td>\n",
       "      <td>Interpretação Cinematográfica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>8.03.09.00-3</td>\n",
       "      <td>Artes do Vídeo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>8.03.10.00-1</td>\n",
       "      <td>Educação Artística</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1285 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Codigo                                       Descricao\n",
       "0     1.00.00.00-3                      Ciências Exatas e da Terra\n",
       "1     1.01.00.00-8                                      Matemática\n",
       "2     1.01.01.00-4                                         Álgebra\n",
       "3     1.01.01.01-2                                       Conjuntos\n",
       "4     1.01.01.02-0                               Lógica Matemática\n",
       "...            ...                                             ...\n",
       "1280  8.03.08.02-3             Roteiro e Direção  Cinematográficos\n",
       "1281  8.03.08.03-1  Técnicas de Registro e Processamento de Filmes\n",
       "1282  8.03.08.04-0                   Interpretação Cinematográfica\n",
       "1283  8.03.09.00-3                                  Artes do Vídeo\n",
       "1284  8.03.10.00-1                              Educação Artística\n",
       "\n",
       "[1285 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cnpq_tree import CNPQtree\n",
    "cnpq = CNPQtree(get_path_repo())\n",
    "\n",
    "caminho = os.path.join(folder_data_input,'cnpq_tabela-areas-conhecimento.pdf')\n",
    "df_areas = cnpq.extrair_areas(caminho)\n",
    "df_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir por nível de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def contar_marcadores(texto):\n",
    "    padrao = r'\\.00'\n",
    "    ocorrencias = re.findall(padrao, texto)\n",
    "    return len(ocorrencias)\n",
    "\n",
    "cat_grandeareas=[]\n",
    "cat_subareas=[]\n",
    "cat_areas=[]\n",
    "cat_especialidades=[]\n",
    "\n",
    "for cod,des in zip(df_areas['Codigo'],df_areas['Descricao']):\n",
    "    k = contar_marcadores(cod)\n",
    "    if k==3:\n",
    "        cat_grandeareas.append((cod,des))\n",
    "    elif k==2:\n",
    "        cat_areas.append((cod,des))\n",
    "    elif k==1:\n",
    "        cat_subareas.append((cod,des))\n",
    "    elif k==0:\n",
    "        cat_especialidades.append((cod,des))\n",
    "    else:\n",
    "        print('Erro na separação')\n",
    "        print(f'{k} {cod}{des}')\n",
    "\n",
    "print(f'{len(cat_grandeareas):4} Grandes Áreas')\n",
    "print(f'{len(cat_areas):4} Áreas')\n",
    "print(f'{len(cat_subareas):4} Subáreas')\n",
    "print(f'{len(cat_especialidades):4} Especialidades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_grandeareas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_areas[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_subareas[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_especialidades[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_areas.to_csv(folder_data_output+'cnpq_areas-pesquisa.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>COMPATIBILIZAR COM ÁREAS DA CAPES</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibilizar CNPq com Áreas de Avaliação CAPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gov.br/capes/pt-br/acesso-a-informacao/acoes-e-programas/avaliacao/sobre-a-avaliacao/areas-avaliacao/sobre-as-areas-de-avaliacao/sobre-as-areas-de-avaliacao\n",
    "\n",
    "Atualizado em 04/08/2023 12h20 (Acesso em Agosto de 2023)\n",
    "\n",
    "Áreas da Avaliação\n",
    "Com o intuito de facilitar o desenvolvimento das atividades de avaliação, as 49 áreas de avaliação são agregadas, por critério de afinidade, em dois níveis:\n",
    "\n",
    "    • Primeiro nível: Colégios \n",
    "        COLÉGIO DE CIÊNCIAS DA VIDA\n",
    "        COLÉGIO DE HUMANIDADES\n",
    "        COLÉGIO DE CIÊNCIAS EXATAS, TECNOLÓGICAS E MULTIDISCIPLINAR\n",
    "    \n",
    "    • Segundo nível: Grandes Áreas."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.gov.br/capes/pt-br/acesso-a-informacao/acoes-e-programas/avaliacao/sobre-a-avaliacao/areas-avaliacao/sobre-as-areas-de-avaliacao/sobre-as-areas-de-avaliacao#areas\n",
    "\n",
    "ÁREAS DE AVALIAÇÃO\n",
    "\n",
    "COLÉGIO DE CIÊNCIAS DA VIDA\n",
    "    Ciências Agrárias\n",
    "        Ciências Agrárias I\n",
    "        Ciência de Alimentos\n",
    "        Medicina Veterinária\n",
    "        Zootecnia/Recursos Pesqueiros\n",
    "\n",
    "    Ciências Biológicas\n",
    "        Biodiversidade\n",
    "        Ciências Biológicas I\n",
    "        Ciências Biológicas II\n",
    "        Ciências Biológicas III\n",
    "\n",
    "    Ciências da Saúde\n",
    "        Educação Física\n",
    "        Enfermagem\n",
    "        Farmácia\n",
    "        Medicina I\n",
    "        Medicina II\n",
    "        Medicina III\n",
    "        Nutrição\n",
    "        Odontologia\n",
    "        Saúde Coletiva\n",
    "\n",
    "COLÉGIO DE HUMANIDADES\n",
    "    CIÊNCIAS HUMANAS\t \t\n",
    "        Antropologia / Arqueologia\n",
    "        Ciência Política e Relações Internacionais\n",
    "        Ciências da Religião e Teologia\n",
    "        Educação\n",
    "        Filosofia\n",
    "        Geografia\n",
    "        História\n",
    "        Psicologia\n",
    "        Sociologia\n",
    "\n",
    "    CIÊNCIAS SOCIAIS APLICADAS\t \t\n",
    "        Administração Pública e de Empresas, Ciências Contábeis e Turismo\n",
    "        Arquitetura, Urbanismo e Design\n",
    "        Comunicação e Informação\n",
    "        Direito\n",
    "        Economia\n",
    "        Planejamento Urbano e Regional / Demografia\n",
    "        Serviço Social\n",
    "\n",
    "    LINGUÍSTICA, LETRAS E ARTES\n",
    "\t \t Artes\n",
    "\t \t Linguística e Literatura\n",
    "\n",
    "\n",
    "COLÉGIO DE CIÊNCIAS EXATAS, TECNOLÓGICAS E MULTIDISCIPLINAR\n",
    "    CIÊNCIAS EXATAS E DA TERRA\n",
    "        Astronomia / Física\n",
    "        Ciência da Computação\n",
    "        Geociências\n",
    "        Matemática / Probabilidade e Estatística\n",
    "        Química\n",
    "\n",
    "    ENGENHARIAS\n",
    "        Engenharias I\n",
    "        Engenharias II\n",
    "        Engenharias III\n",
    "        Engenharias IV\n",
    "\n",
    "    MULTIDISCIPLINAR\n",
    "        Biotecnologia\n",
    "        Ciências Ambientais\n",
    "        Ensino\n",
    "        Interdisciplinar\n",
    "        Materiais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Áreas do Conhecimento do CNPq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gov.br/capes/pt-br/acesso-a-informacao/acoes-e-programas/avaliacao/instrumentos/documentos-de-apoio-1/tabela-de-areas-de-conhecimento-avaliacao\n",
    "\n",
    "Fonte: www.gov.br, publicado em 19/09/2020 19h38 Atualizado em 24/10/2022 18h03\n",
    "\n",
    "A classificação das Áreas do Conhecimento tem finalidade eminentemente prática, objetivando proporcionar às Instituições de ensino, pesquisa e inovação uma maneira ágil e funcional de sistematizar e prestar informações concernentes a projetos de pesquisa e recursos humanos aos órgãos gestores da área de ciência e tecnologia.\n",
    "\n",
    "A organização das Áreas do Conhecimento na tabela apresenta uma hierarquização em quatro níveis, do mais geral ao mais específico, abrangendo nove grandes áreas nas quais se distribuem as 49 áreas de avaliação da CAPES. Estas áreas de avaliação, por sua vez, agrupam áreas básicas (ou áreas do conhecimento), subdivididas em subáreas e especialidades:\n",
    "\n",
    "1º nível - Grande Área: aglomeração de diversas áreas do conhecimento, em virtude da afinidade de seus objetos, métodos cognitivos e recursos instrumentais refletindo contextos sociopolíticos específicos;\n",
    "\n",
    "2º nível – Área do Conhecimento (Área Básica): conjunto de conhecimentos inter-relacionados, coletivamente construído, reunido segundo a natureza do objeto de investigação com finalidades de ensino, pesquisa e aplicações práticas;\n",
    "\n",
    "3º nível - Subárea: segmentação da área do conhecimento (ou área básica) estabelecida em função do objeto de estudo e de procedimentos metodológicos reconhecidos e amplamente utilizados;\n",
    "\n",
    "4º nível - Especialidade: caracterização temática da atividade de pesquisa e ensino. Uma mesma especialidade pode ser enquadrada em diferentes grandes áreas, áreas básicas e subáreas."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Montar a interação com as Áras de avaliação da CAPES\n",
    "\n",
    "    1.00.00.00-3        Ciências exatas e da Terra\n",
    "        ÁREA DE AVALIAÇÃO: MATEMÁTICA / PROBABILIDADE E ESTATÍSTICA\n",
    "            1.01.00.00-8    Matemática\n",
    "            1.02.00.00-2    Probabilidade e Estatística\n",
    "\n",
    "        ÁREA DE AVALIAÇÃO: CIÊNCIA DA COMPUTAÇÃO\n",
    "            1.03.00.00-7    Ciência da Computação\n",
    "        \n",
    "        ÁREA DE AVALIAÇÃO: ASTRONOMIA / FÍSICA \n",
    "            1.04.00.00-1    Astronomia\n",
    "            1.05.00.00-6\tFísica\n",
    "\n",
    "        ÁREA DE AVALIAÇÃO: QUÍMICA\n",
    "            1.06.00.00-0\tQuímica\n",
    "\n",
    "        ÁREA DE AVALIAÇÃO: GEOCIÊNCIAS\n",
    "            1.07.00.00-5\tGeoCiências\n",
    "\n",
    "    2.00.00.00-6\tCiências Biológicas\n",
    "        ÁREA DE AVALIAÇÃO: CIÊNCIAS BIOLÓGICAS I\n",
    "            1.08.00.00-0\tOceanografia\n",
    "            2.01.00.00-0\tBiologia Geral\n",
    "            2.02.00.00-5\tGenética\n",
    "            2.03.00.00-0\tBotânica    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorias do Periodicos Capes\n",
    "\n",
    "https://buscador-periodicos-capes-gov-br.ez68.periodicos.capes.gov.br/V/QB984FN2Y5SQTN9JQMSD3BEF1LYT9I6UP1Q51GN5L4YC6JV1EX-14773?func=find-db-info&doc_num=000002739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'categorias_capes.txt'\n",
    "pathfilename = os.path.join(folder_data_input,filename)\n",
    "class_capes = pd.read_csv(pathfilename, header=None, sep=';')\n",
    "class_capes.columns = ['CATEGORIAS_SUBCATEGORIAS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_capes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoria=[]\n",
    "subcategoria=[]\n",
    "\n",
    "for i in class_capes['CATEGORIAS_SUBCATEGORIAS'].values:\n",
    "    try:\n",
    "        cat, subcat = i.split('/ ')\n",
    "    except:\n",
    "        print(f'Erro ao dividir: {i}')\n",
    "    categoria.append(cat.strip())\n",
    "    subcategoria.append(subcat.strip())\n",
    "\n",
    "qte_cat = len(pd.Series(categoria).unique())\n",
    "qte_subcat = len(pd.Series(subcategoria).unique())\n",
    "\n",
    "desc_grandeareas = [tupla[-1] for tupla in cat_grandeareas]\n",
    "desc_subareas = [tupla[-1] for tupla in cat_subareas]\n",
    "desc_areas = [tupla[-1] for tupla in cat_areas]\n",
    "desc_especialidades = [tupla[-1] for tupla in cat_especialidades]\n",
    "\n",
    "print('ANÁLISE DA CLASSIFICAÇÃO EXTRAÍDA DO PORTAL DE PERIÓDICOS DA CAPES')\n",
    "\n",
    "print(f'\\n=>CATEGORIAS CAPES: Contém {qte_cat} Categorias no total, podendo ser:')\n",
    "for i in pd.Series(categoria).unique():\n",
    "    if i in desc_grandeareas:\n",
    "        print(f'  É uma das     Áreas do CNPq: \"{i}\"')\n",
    "    elif i in desc_subareas:\n",
    "        print(f'  É uma das  Subáreas do CNPq: \"{i}\"')\n",
    "    elif i in desc_areas:\n",
    "        print(f'  É uma Especialidade do CNPq: \"{i}\"')\n",
    "    elif i in desc_especialidades:\n",
    "        print(f'  É uma Especialidade do CNPq: \"{i}\"')\n",
    "    else:\n",
    "        print(f'  Classe está ausente no CNPq: \"{i}\"')\n",
    "\n",
    "print(f'\\n=>SUBCATEGORIAS CAPES: Contém {qte_subcat} Subcategorias no total, podendo ser:')\n",
    "for i in pd.Series(subcategoria).unique():\n",
    "    if i in desc_grandeareas:\n",
    "        print(f'  É uma das     Áreas do CNPq: \"{i}\"')\n",
    "    elif i in desc_subareas:\n",
    "        print(f'  É uma das  Subáreas do CNPq: \"{i}\"')\n",
    "    elif i in desc_areas:\n",
    "        print(f'  É uma Especialidade do CNPq: \"{i}\"')\n",
    "    elif i in desc_especialidades:\n",
    "        print(f'  É uma Especialidade do CNPq: \"{i}\"')\n",
    "    else:\n",
    "        print(f'  Classe está ausente no CNPq: \"{i}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Valores possíveis de k: {list(range(len(df_areas.index)//600+1))}')\n",
    "k=0\n",
    "n=600\n",
    "df_areas[n*k:(k+1)*n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_areas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>COMPATIBILIZAR COM ATIVIDADES ECONÔMICAS</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atividades econômicas na CNAE 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação nacional de atividades econômicas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fonte:\n",
    "https://concla.ibge.gov.br/classificacoes/por-tema/atividades-economicas/classificacao-nacional-de-atividades-economicas.html\n",
    "\n",
    "Estrutura: \n",
    "\n",
    "    1º nível: 21   Seções\n",
    "    2º nível: 87   Divisões\n",
    "    3° nível: 285  Grupos\n",
    "    4º nível: 673  Classes\n",
    "    5º nível: 1301 Subclasses\n",
    "\n",
    "Descrição: A Classificação Nacional de Atividades Econômicas-CNAE é a classificação oficialmente adotada pelo Sistema Estatístico Nacional e pelos órgãos federais gestores de registros administrativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fonte:\n",
    "https://concla.ibge.gov.br/busca-online-cnae.html?view=estrutura\n",
    "\n",
    "Seções:\n",
    "\n",
    "    A\t01 .. 03\tAGRICULTURA, PECUÁRIA, PRODUÇÃO FLORESTAL, PESCA E AQÜICULTURA\n",
    "    B\t05 .. 09\tINDÚSTRIAS EXTRATIVAS\n",
    "    C\t10 .. 33\tINDÚSTRIAS DE TRANSFORMAÇÃO\n",
    "    D\t35 .. 35\tELETRICIDADE E GÁS\n",
    "    E\t36 .. 39\tÁGUA, ESGOTO, ATIVIDADES DE GESTÃO DE RESÍDUOS E DESCONTAMINAÇÃO\n",
    "    F\t41 .. 43\tCONSTRUÇÃO\n",
    "    G\t45 .. 47\tCOMÉRCIO; REPARAÇÃO DE VEÍCULOS AUTOMOTORES E MOTOCICLETAS\n",
    "    H\t49 .. 53\tTRANSPORTE, ARMAZENAGEM E CORREIO\n",
    "    I\t55 .. 56\tALOJAMENTO E ALIMENTAÇÃO\n",
    "    J\t58 .. 63\tINFORMAÇÃO E COMUNICAÇÃO\n",
    "    K\t64 .. 66\tATIVIDADES FINANCEIRAS, DE SEGUROS E SERVIÇOS RELACIONADOS\n",
    "    L\t68 .. 68\tATIVIDADES IMOBILIÁRIAS\n",
    "    M\t69 .. 75\tATIVIDADES PROFISSIONAIS, CIENTÍFICAS E TÉCNICAS\n",
    "    N\t77 .. 82\tATIVIDADES ADMINISTRATIVAS E SERVIÇOS COMPLEMENTARES\n",
    "    O\t84 .. 84\tADMINISTRAÇÃO PÚBLICA, DEFESA E SEGURIDADE SOCIAL\n",
    "    P\t85 .. 85\tEDUCAÇÃO\n",
    "    Q\t86 .. 88\tSAÚDE HUMANA E SERVIÇOS SOCIAIS\n",
    "    R\t90 .. 93\tARTES, CULTURA, ESPORTE E RECREAÇÃO\n",
    "    S\t94 .. 96\tOUTRAS ATIVIDADES DE SERVIÇOS\n",
    "    T\t97 .. 97\tSERVIÇOS DOMÉSTICOS\n",
    "    U\t99 .. 99\tORGANISMOS INTERNACIONAIS E OUTRAS INSTITUIÇÕES EXTRATERRITORIAIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busca em atividades econômicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sintaxe para busca\n",
    "\n",
    "O mecanismo de busca da CNAE aceita consultas de palavra, palavras completas e incompletas e texto. Os resultados são ordenados inicialmente pelo código podendo o usuário optar pelo tipo de ordenação (código ou descrição).\n",
    "\n",
    "Uma dica importante é usar apenas palavras chaves na sua busca. Ao invés de buscar, por exemplo, Gado de Corte, busque por Gado Corte. \n",
    "\n",
    "O uso de preposições pode levar a resultados indesejados ou de pouca relevância para sua busca. O mecanismo de busca não faz distinção entre maiúscula e minúscula e acentuação.\n",
    "\n",
    "<b> Hierarquia </b>\n",
    "\n",
    "    Seção:\t    M\tATIVIDADES PROFISSIONAIS, CIENTÍFICAS E TÉCNICAS\n",
    "    Divisão:\t72 PESQUISA E DESENVOLVIMENTO CIENTÍFICO\n",
    "    Grupo:              72.1 Pesquisa e desenvolvimento experimental em ciências físicas e naturais\n",
    "                        72.2 Pesquisa e desenvolvimento experimental em ciências sociais e humanas\n",
    "\n",
    "\n",
    "    Seção:\t    M\tATIVIDADES PROFISSIONAIS, CIENTÍFICAS E TÉCNICAS\n",
    "    Divisão: \t72 PESQUISA E DESENVOLVIMENTO CIENTÍFICO\n",
    "    Grupo:\t            72.1 Pesquisa e desenvolvimento experimental em ciências físicas e naturais\n",
    "    Classe:\t                72.10-0 Pesquisa e desenvolvimento experimental em ciências físicas e naturais\n",
    "    Subclasse:                  7210-0/00 Pesquisa e desenvolvimento experimental em ciências físicas e naturais\n",
    "\n",
    "Notas Explicativas:\n",
    "Esta classe compreende:\n",
    "\n",
    "    - as atividades de pesquisa e desenvolvimento realizadas no âmbito das ciências da vida, tais como: medicina, biologia, bioquímica, farmácia, agronomia e conexas\n",
    "\n",
    "    - as atividades de pesquisa e desenvolvimento realizadas no âmbito das ciências físicas e de engenharia, tais como: matemática, física, astronomia, química, geociências e conexas\n",
    "\n",
    "Lista de Descritores\n",
    "Registros encontrados: 16\n",
    "\n",
    "    Código\tDescrição\n",
    "    7210-0\tAGRONOMIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tBIOQUÍMICA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tFARMÁCIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tFAZENDA EXPERIMENTAL; PESQUISA\n",
    "    7210-0\tLABORATÓRIO DE PESQUISA FÍSICA, COMERCIAL E NÃO COMERCIAL\n",
    "    7210-0\tLABORATÓRIO DE PESQUISA QUÍMICA, COMERCIAL E NÃO COMERCIAL\n",
    "    7210-0\tLABORATÓRIO INDUSTRIAL; PESQUISA\n",
    "    7210-0\tMEDICINA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tPESQUISA BIOGENÉTICA\n",
    "    7210-0\tPESQUISA BIOLÓGICA\n",
    "    7210-0\tPESQUISA E DESENVOLVIMENTO COM ACESSO A PATRIMÔNIO GENÉTICO EXISTENTE NO TERRITÓRIO NACIONAL; ATIVIDADES DE\n",
    "    7210-0\tPESQUISA E DESENVOLVIMENTO DAS CIÊNCIAS FÍSICAS E NATURAIS\n",
    "    7210-0\tPESQUISA E DESENVOLVIMENTO EXPERIMENTAL EM CIÊNCIAS FÍSICAS E NATURAIS; ATIVIDADES DE\n",
    "    7210-0\tPESQUISA MATEMÁTICA, FÍSICA, ASTRONOMIA; DESENVOLVIMENTO DE\n",
    "    7210-0\tPESQUISA MÉDICA NÃO COMERCIAL\n",
    "    7210-0\tQUÍMICA; PESQUISA E DESENVOLVIMENTO EM\n",
    "\n",
    "<b> Exemplo de atividades relacionadas ao termo \"Pesquisa\" no CNAE 2.0 </b>\n",
    "\n",
    "Classes encontradas: 45\n",
    "\n",
    "    Código\tDescrição\n",
    "    0159-8\tANIMAIS PARA PESQUISA; CRIAÇÃO DE\n",
    "    0159-8\tBIOTÉRIO; CRIAÇÃO DE ANIMAIS PARA PESQUISA\n",
    "    0170-9\tCAPTURA DE ANIMAIS, MORTOS OU VIVOS, PARA PESQUISA, UTILIZAÇÃO EM ZOOLÓGICOS; SERVIÇOS DE\n",
    "    2651-5\tAPARELHOS E EQUIPAMENTOS PARA LABORATÓRIOS DE PESQUISA CIENTÍFICA; FABRICAÇÃO DE\n",
    "    2651-5\tAPARELHOS E EQUIPAMENTOS PARA LABORATÓRIOS DE PESQUISA E DESENVOLVIMENTO; FABRICAÇÃO DE\n",
    "    2829-1\tCENTRIFUGADOR PARA LABORATÓRIO DE ANÁLISE. ENSAIO OU PESQUISA CIENTÍFICA; FABRICAÇÃO DE\n",
    "    3011-3\tNAVIOS-HOSPITAIS, NAVIOS DE GUERRA, EMBARCAÇÕES PARA PESQUISA CIENTÍFICA E OUTRAS EMBARCAÇÕES SEMELHANTES; FABRICAÇÃO DE\n",
    "    3312-1\tAPARELHOS E EQUIPAMENTOS PARA LABORATÓRIOS DE PESQUISA CIENTÍFICA, MANUTENÇAO E REPARACAO DE\n",
    "    3312-1\tAPARELHOS E EQUIPAMENTOS PARA LABORATÓRIOS DE PESQUISA E DESENVOLVIMENTO, MANUTENÇAO E REPARACAO DE\n",
    "    5030-1\tTRANSPORTE DE MERCADORIAS E PESSOAS PARA SUPRIMENTO E APOIO A PLATAFORMAS DE PESQUISA\n",
    "    6319-4\tBANCO DE INFORMAÇÃO PARA PESQUISA E ANÁLISE; SERVIÇOS DE\n",
    "    7119-7\tPROSPECÇÃO, PESQUISA MINERAL; SERVIÇOS DE\n",
    "    7210-0\tAGRONOMIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tBIOQUÍMICA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tFARMÁCIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tFAZENDA EXPERIMENTAL; PESQUISA\n",
    "    7210-0\tLABORATÓRIO DE PESQUISA FÍSICA, COMERCIAL E NÃO COMERCIAL\n",
    "    7210-0\tLABORATÓRIO DE PESQUISA QUÍMICA, COMERCIAL E NÃO COMERCIAL\n",
    "    7210-0\tLABORATÓRIO INDUSTRIAL; PESQUISA\n",
    "    7210-0\tMEDICINA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tPESQUISA BIOGENÉTICA\n",
    "    7210-0\tPESQUISA BIOLÓGICA\n",
    "    7210-0\tPESQUISA E DESENVOLVIMENTO COM ACESSO A PATRIMÔNIO GENÉTICO EXISTENTE NO TERRITÓRIO NACIONAL; ATIVIDADES DE\n",
    "    7210-0\tPESQUISA E DESENVOLVIMENTO DAS CIÊNCIAS FÍSICAS E NATURAIS\n",
    "    7210-0\tPESQUISA E DESENVOLVIMENTO EXPERIMENTAL EM CIÊNCIAS FÍSICAS E NATURAIS; ATIVIDADES DE\n",
    "    7210-0\tPESQUISA MATEMÁTICA, FÍSICA, ASTRONOMIA; DESENVOLVIMENTO DE\n",
    "    7210-0\tPESQUISA MÉDICA NÃO COMERCIAL\n",
    "    7210-0\tQUÍMICA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7220-7\tARQUEOLOGIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7220-7\tARTES; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7220-7\tDIREITO; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7220-7\tECONOMIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7220-7\tLINGÜÍSTICA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7220-7\tPESQUISA E DESENVOLVIMENTO EM CIÊNCIAS SOCIAIS E HUMANAS\n",
    "    7220-7\tPESQUISA ECONÔMICA, COMERCIAL E NÃO COMERCIAL\n",
    "    7220-7\tPESQUISA EDUCACIONAL\n",
    "    7220-7\tSOCIOLOGIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7320-3\tPESQUISA DE MERCADO E DE OPINIÃO PÚBLICA\n",
    "    7320-3\tPESQUISA DE OPINIÃO PÚBLICA\n",
    "    7320-3\tPESQUISA E COLETA DE DADOS PARA PESQUISAS DE MERCADO E OPINIÃO\n",
    "    7320-3\tPESQUISA MERCADOLÓGICA\n",
    "    7320-3\tPESQUISA POLÍTICA; SERVIÇOS DE\n",
    "    8411-6\tFUNDAÇÃO DE APOIO À PESQUISA E EXTENSÃO\n",
    "    8650-0\tCONSULTORIA EM BIOMEDICINA, EXCETO PARA PESQUISA E DESENVOLVIMENTO; ATIVIDADES DE\n",
    "    9101-5\tDOCUMENTAÇÃO E PESQUISA BIBLIOGRÁFICA; ATIVIDADE DE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>ORGANIZAR DADOS DE PESSOAL</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importação dos módulos de trabalho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyjarowinkler --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "# !python.exe -m pip install --upgrade pip\n",
    "# !pip install selenium --upgrade\n",
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "t00=time.time()\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os, csv, sys, pip, time, string, re\n",
    "import warnings, traceback\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from string import Formatter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Renderização de imagem para exibição dentro da linha do dataframe\n",
    "# def path_to_image_html(path):\n",
    "\n",
    "#     return '<img src=\"'+ path + '\" style=max-height:124px;\"/>'\n",
    "\n",
    "# def show_im():\n",
    "#     CSS = \"\"\"\n",
    "#     .output {\n",
    "#         flex-direction: row;\n",
    "#     }\n",
    "#     \"\"\"\n",
    "\n",
    "#     HTML('<style>{}</style>'.format(CSS))\n",
    "    \n",
    "# def foto():\n",
    "#     '''Aplica filtro no campo com o link para a foto do autor do Dataframe de identificação extraído do currículo Lattes e renderiza a foto em HTML a partir deste link\n",
    "#     Autor: Marcos Aires Fev.2022\n",
    "#     '''\n",
    "#     campo='Link Foto:'\n",
    "#     images_df       = df_identificacao[df_identificacao['ROTULOS'] == campo]  # filter by sport input, must be soccer or basketball for this use case\n",
    "#     image_grid      = df_identificacao['CONTEUDOS']\n",
    "#     image_grid_html = HTML(df_identificacao[:1].to_html(escape=False,formatters=dict(conteudos=path_to_image_html)))\n",
    "#     display(image_grid_html)\n",
    "#     show_im()\n",
    "\n",
    "# def tempo(start, end):\n",
    "#     t=end-start\n",
    "\n",
    "#     tempo = timedelta(\n",
    "#         weeks   = t//(3600*24*7),\n",
    "#         days    = t//(3600*24),\n",
    "#         seconds = t,\n",
    "#         minutes = t//(60),\n",
    "#         hours   = t//(3600),\n",
    "#         microseconds=t//1000000,\n",
    "#         )\n",
    "#     fmt='{H:2}:{M:02}:{S:02}'\n",
    "#     return strfdelta(tempo)\n",
    "\n",
    "\n",
    "# def horas(segundos): \n",
    "#     return time.strftime(\"%H:%M:%S\", time.gmtime(segundos)) \n",
    "\n",
    "\n",
    "# def dias_horas_minutos(td):\n",
    "#     x = (td.days, td.seconds//3600, (td.seconds//60)%60, td.seconds)\n",
    "#     return x #(days, hrs, mins, seconds)\n",
    "\n",
    "\n",
    "# def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "#     \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "#     just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "#     The fmt argument allows custom formatting to be specified.  Fields can \n",
    "#     include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "#     Some examples:\n",
    "#         '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "#         '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "#         '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "#         '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "#     The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "#     default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "#         's', 'seconds', \n",
    "#         'm', 'minutes', \n",
    "#         'h', 'hours', \n",
    "#         'd', 'days', \n",
    "#         'w', 'weeks'\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Convert tdelta to integer seconds.\n",
    "#     if inputtype == 'timedelta':\n",
    "#         remainder = int(tdelta.total_seconds())\n",
    "#     elif inputtype in ['s', 'seconds']:\n",
    "#         remainder = int(tdelta)\n",
    "#     elif inputtype in ['m', 'minutes']:\n",
    "#         remainder = int(tdelta)*60\n",
    "#     elif inputtype in ['h', 'hours']:\n",
    "#         remainder = int(tdelta)*3600\n",
    "#     elif inputtype in ['d', 'days']:\n",
    "#         remainder = int(tdelta)*86400\n",
    "#     elif inputtype in ['w', 'weeks']:\n",
    "#         remainder = int(tdelta)*604800\n",
    "\n",
    "#     f = Formatter()\n",
    "#     desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "#     possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "#     constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "#     values = {}\n",
    "    \n",
    "#     for field in possible_fields:\n",
    "#         if field in desired_fields and field in constants:\n",
    "#             values[field], remainder = divmod(remainder, constants[field])\n",
    "    \n",
    "#     return f.format(fmt, **values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_pastas(caminho):\n",
    "    import sys\n",
    "\n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "    \n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'/xml_zip/'\n",
    "    pathcsv  = caminho+'/csv/'\n",
    "    pathjson = caminho+'/json/'\n",
    "    pathfig  = caminho+'/fig/'\n",
    "    pathaux  = caminho+'/'\n",
    "    pathout  = caminho+'/output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XLS', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    \n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS.: Para efetuar instalações em ambientes com SSL você pode precisar adicionar pelo menos dois parâmetros:\n",
    "\n",
    "    Parâmetro 1 :--trusted-host pypi.org\n",
    "    Parâmetro 2 :--trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para dividir detalhes dos artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padrões de expressão regular\n",
    "pattern_m01a = r' \\. '\n",
    "pattern_m01b = r' et al. '\n",
    "pattern_m01c = r' \\; '\n",
    "pattern_m01d = r'\\. '\n",
    "pattern_vol  = r' v\\. (\\d+)'\n",
    "pattern_pag  = r' p\\. (.*?),'\n",
    "pattern_ano  = r'\\d{4}\\.$'\n",
    "\n",
    "def find_positions(pattern, string):\n",
    "    return [match.start() for match in re.finditer(pattern, string)]\n",
    "\n",
    "def find_vol(input_string):\n",
    "    pattern = r\" v\\. (\\d+)\"\n",
    "    match   = re.search(pattern, input_string)\n",
    "    try:\n",
    "        volume = match.group(1)\n",
    "        return volume, match.start()\n",
    "    except:\n",
    "        return ['','']\n",
    "\n",
    "def find_pag(input_string):\n",
    "    pattern = r\" p\\. (.*?),\"\n",
    "    match   = re.search(pattern, input_string)\n",
    "    try:\n",
    "        pages = match.group(1)\n",
    "        return pages, match.start()\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        return ['','']\n",
    "\n",
    "def find_year(input_string):\n",
    "    pattern = r'\\d{4}\\.$'\n",
    "    match = re.search(pattern, input_string)\n",
    "    year  = match.group().strip('.')\n",
    "    return year, match.start()\n",
    "\n",
    "def find_marker_positions(input_string, markers):\n",
    "    marker_positions = {}\n",
    "    for marker in markers:\n",
    "        position = input_string.find(marker)\n",
    "        if position != -1:\n",
    "            marker_positions[position] = marker\n",
    "    return marker_positions\n",
    "\n",
    "def find_odd(input_string, marker):\n",
    "    odds_positions = []\n",
    "    \n",
    "    for order, position in enumerate([pos for pos, char in enumerate(input_string) if input_string[pos:pos+len(marker)] == marker], start=1):\n",
    "        if order % 2 != 0:\n",
    "            odds_positions.append(position)\n",
    "    \n",
    "    return odds_positions\n",
    "\n",
    "def find_even(input_string, marker):\n",
    "    evens_positions = []\n",
    "    \n",
    "    for order, position in enumerate([pos for pos, char in enumerate(input_string) if input_string[pos:pos+len(marker)] == marker], start=0):\n",
    "        if order % 2 != 0:\n",
    "            evens_positions.append(position)\n",
    "    \n",
    "    return evens_positions\n",
    "\n",
    "\n",
    "def split_string_at_positions(input_string, positions):\n",
    "    substrings = []\n",
    "    start = 0\n",
    "\n",
    "    for position in positions:\n",
    "        substrings.append(input_string[start:position])\n",
    "        start = position\n",
    "\n",
    "    substrings.append(input_string[start:])\n",
    "\n",
    "    return substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_publication_data(input_string):\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # Extract Year\n",
    "    try:\n",
    "        year, year_position = find_year(input_string)\n",
    "        extracted_data['ANO'] = year\n",
    "        input_string = input_string[:year_position]\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred while extracting the year: {e}\")\n",
    "    \n",
    "    # List of patterns in order of priority\n",
    "    priority_patterns = [pattern_m01a, pattern_m01b, pattern_m01c, pattern_m01d]\n",
    "    m1_used = None\n",
    "    p1_end = None\n",
    "\n",
    "    # Identify m1 following the hierarchical preference\n",
    "    for pattern in priority_patterns:\n",
    "        positions = find_positions(pattern, input_string)\n",
    "        if positions:\n",
    "            m1_used = pattern\n",
    "            p1_end = positions[0]\n",
    "            break\n",
    "\n",
    "    # User interaction if no m1 marker is found\n",
    "    if m1_used is None:\n",
    "        user_marker = input(\"No marker found for splitting data for m1. Please specify a marker: \")\n",
    "        positions = find_positions(user_marker, input_string)\n",
    "        if positions:\n",
    "            m1_used = user_marker\n",
    "            p1_end = positions[0]\n",
    "\n",
    "    p1 = input_string[:p1_end].strip()\n",
    "    extracted_data['LISTA_AUTORES'] = p1\n",
    "    # extracted_data['M1_USED'] = m1_used\n",
    "    \n",
    "    # Identify m2 for p2 and p3\n",
    "    p2_end_candidates = [find_vol(input_string)[1], find_pag(input_string)[1]]\n",
    "    m2_labels = ['vol', 'pag']\n",
    "    m2_used = None\n",
    "    p2_end_candidates = [pos for pos in p2_end_candidates if pos is not None and pos != '']\n",
    "    \n",
    "    if len(p2_end_candidates) == 0:\n",
    "        last_dot_space = input_string.rfind('. ')\n",
    "        if last_dot_space != -1:\n",
    "            p2_end_candidates.append(last_dot_space)\n",
    "            m2_used = '. '\n",
    "        else:\n",
    "            user_input = input(\"No suitable marker found for p2 and p3. Specify either 'vol' or 'pag': \")\n",
    "            p2_end_candidates = find_positions(user_input, input_string)\n",
    "            m2_used = user_input\n",
    "    \n",
    "    else:\n",
    "        m2_used = m2_labels[p2_end_candidates.index(min(p2_end_candidates))]\n",
    "    \n",
    "    p2_end = min(p2_end_candidates)\n",
    "    p2 = input_string[p1_end:p2_end].lstrip(m1_used).strip()\n",
    "    extracted_data['ARTIGO_REVISTA'] = p2\n",
    "    # extracted_data['M2_USED'] = m2_used\n",
    "    \n",
    "    p3_start = max(p2_end_candidates)\n",
    "    p3 = input_string[p3_start:].strip()\n",
    "    \n",
    "    volume, _ = find_vol(p3)\n",
    "    pages, _ = find_pag(p3)\n",
    "    \n",
    "    if volume != '':\n",
    "        extracted_data['VOLUME'] = volume\n",
    "    if pages != '':\n",
    "        extracted_data['PAGES'] = pages\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para separar nomes de autores \n",
    "\n",
    "(Funcionando bem somente para separador ';' melhorar para ausência dele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_abreviation(substring):\n",
    "    pattern = r'(?: [a-zA-Z]\\.)|(?: [a-zA-Z] \\.)'\n",
    "    if re.search(pattern, substring):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_surname(substring):\n",
    "    return not is_abreviation(substring) and substring.endswith(', ')\n",
    "\n",
    "def compose_full_name(surname, parts, marker):\n",
    "    full_name = \"\"\n",
    "    for part in parts:\n",
    "        full_name += part + marker\n",
    "    full_name += ' '+surname\n",
    "    return full_name.strip()\n",
    "\n",
    "def split_authors(string, verbose=False):\n",
    "    authors_names = []\n",
    "    \n",
    "    m1a = \"; \"\n",
    "    m1b = \", \"\n",
    "    \n",
    "    if m1a in string and (string.count(m1a) <= string.count(m1b) or m1b not in string):\n",
    "        marker = m1a\n",
    "        authors_names = string.split(marker)\n",
    "        return [x.strip() for x in authors_names]\n",
    "    else:\n",
    "        marker = m1b\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Selected marker: \"{marker}\"')\n",
    "\n",
    "    parts_list = string.split(marker)\n",
    "    name    = \"\"\n",
    "    surname = \"\"\n",
    "    \n",
    "    for part in parts_list:\n",
    "        if is_abreviation(part):\n",
    "            classification = 'Abrev'\n",
    "            name += part + marker\n",
    "        else:\n",
    "            classification = 'Name'\n",
    "            if is_surname(part) or surname == \"\":\n",
    "                classification = 'SOBRENOME'\n",
    "                if surname:\n",
    "                    full_name = compose_full_name(surname.strip(), name.split(marker), ', ')\n",
    "                    if full_name not in authors_names:\n",
    "                        authors_names.append(full_name)\n",
    "                surname = part\n",
    "                name = part + marker\n",
    "            else:\n",
    "                name = part + marker\n",
    "        if verbose:\n",
    "            # print(f'Quantidade de m1a: {string.count(m1a)} \\nQuantidade de m1b: {string.count(m1b)}')\n",
    "            print(f'Autor: {part:40} | Forma: {classification}')\n",
    "\n",
    "        authors_names.append(part.strip(marker).strip())\n",
    "     \n",
    "    return [x.strip() for x in authors_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando a função\n",
    "input_string = 'BARBOSA, ANTONIO MARCOS AIRES ; ANTONIO MARCOS AIRES BARBOSA ; BARBOSA FILHO, A. M. A. ; SUZANA, B. B. A. B. '\n",
    "names = split_authors(input_string, 1)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando a função\n",
    "input_string = 'BARBOSA, ANTONIO MARCOS AIRES, ANTONIO MARCOS AIRES BARBOSA, BARBOSA FILHO, A. M. A., SUZANA, B. B. A. B.'\n",
    "names = split_authors(input_string, 1)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compose_full_name('BARBOSA', 'Antonio Marcos Aires', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'BARBOSA FILHO, A. M. A. ; SUZANA, B. B. A. B. ; ANTONIO MARCOS AIRES BARBOSA '\n",
    "split_authors(string, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de montar dataframes a partir dos dados brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string(citation, verbose=False):\n",
    "    citation = citation.replace('CitaÃ§Ãµes:','Citações:')\n",
    "    # Define potential separators in order of preference\n",
    "    separators = [r' \\. ', r' ; ', r'(?<=[^A-Z\\s])\\. ']\n",
    "    \n",
    "    for sep in separators:\n",
    "            # Split the citation using regex\n",
    "            parts = re.split(sep, citation)\n",
    "            if len(parts) > 1:\n",
    "                authors = parts[0]\n",
    "                if verbose is True:\n",
    "                    print(type(authors))\n",
    "                    print(authors)\n",
    "                remaining_info = citation.split(authors)[1].lstrip(sep)\n",
    "\n",
    "                volume_match = re.search(r'v\\. (\\d+)', remaining_info)\n",
    "                volume = volume_match.group(1) if volume_match and volume_match is not None else ''\n",
    "                if volume_match:\n",
    "                    start_position = volume_match.start()\n",
    "                    title_journal = remaining_info[:start_position-2].strip()\n",
    "                    try:\n",
    "                        journal = title_journal.split('. ')[1]\n",
    "                    except:\n",
    "                        journal = ''\n",
    "                else:\n",
    "                    start_position = len(citation)\n",
    "                    title_journal = remaining_info[:start_position].strip()\n",
    "                    journal = ''\n",
    "                pages_match = re.search(r'p\\. ([\\d\\-]+)', remaining_info)\n",
    "                pages = pages_match.group(1) if pages_match and pages_match is not None else ''\n",
    "\n",
    "                year_match = re.search(r'(\\d{4}\\.)', remaining_info)\n",
    "                year = year_match.group(1).strip('.') if year_match else ''\n",
    "\n",
    "                title   = title_journal.split('. ')[0]\n",
    "                \n",
    "                if type(parts[0]) is str:\n",
    "                    authors = split_authors(parts[0], verbose=False)\n",
    "                else:\n",
    "                    authors = [authors]\n",
    "\n",
    "                return {\n",
    "                    'authors': authors,\n",
    "                    'title': title,\n",
    "                    'journal': journal,\n",
    "                    'volume': volume,\n",
    "                    'pages': pages,\n",
    "                    'year': year\n",
    "                }\n",
    "\n",
    "    return citation, ''\n",
    "    \n",
    "\n",
    "def parse_dataframe(df):\n",
    "    # Initialize an empty list to hold the parsed data\n",
    "    parsed_data = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Extract the 'ARTIGO' value\n",
    "        citation = row['ARTIGO']\n",
    "        \n",
    "        # Parse the citation using the previously defined 'parse_citation' function\n",
    "        parsed_citation = parse_string(citation)\n",
    "        \n",
    "        # Create a new dictionary that combines the parsed citation data with the remaining row data\n",
    "        new_row = parsed_citation.copy() if parsed_citation else {}\n",
    "        for col in ['ANO_PUB', 'STATUS', 'MATRÍCULA', 'NOME', 'ÁREA', 'CARGO', 'VÍNCULO', 'INGRESSO_FIOCE', 'NÍVEL', 'ANO_INGRESSO_FIOCE']:\n",
    "            new_row[col] = row[col]\n",
    "        \n",
    "        # Append this new row dictionary to the list of parsed data\n",
    "        parsed_data.append(new_row)\n",
    "        \n",
    "    # Convert the list of parsed data dictionaries into a new DataFrame\n",
    "    new_df = pd.DataFrame(parsed_data)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def find_authors_and_others(citation):\n",
    "    # Define the separator types and their priority\n",
    "    separators = [' \\. ', '; ', '. ']\n",
    "    \n",
    "    # Check for each separator in the priority order\n",
    "    for sep in separators:\n",
    "        parts = re.split(sep, citation)\n",
    "        \n",
    "        # Special case for '. ' where it might be part of an abbreviated author name\n",
    "        if sep == '. ':\n",
    "            author_parts = []\n",
    "            for i, part in enumerate(parts):\n",
    "                # If the previous part ends with an uppercase letter followed by a space, it is likely an author's abbreviated initial\n",
    "                if i > 0 and (parts[i-1][-1].isupper() and parts[i-1][-2] == ' '):\n",
    "                    author_parts.append(part)\n",
    "                else:\n",
    "                    # Once we encounter a part that doesn't meet the condition, we break and consider the authors' list complete\n",
    "                    break\n",
    "            \n",
    "            if author_parts:\n",
    "                authors = f\"{sep}\".join(parts[:len(author_parts) + 1])\n",
    "                remaining_info = f\"{sep}\".join(parts[len(author_parts) + 1:])\n",
    "                return authors, remaining_info\n",
    "        \n",
    "        else:\n",
    "            if len(parts) > 1:\n",
    "                authors = parts[0]\n",
    "                remaining_info = sep.join(parts[1:])\n",
    "                return authors, remaining_info\n",
    "                \n",
    "    return citation, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GERAR DATAFRAMES COM LISTA DE PUBLICAÇÕES DE CADA AUTOR\n",
    "def montar_publicacoes(df_secoes):\n",
    "    filtro  = 'Artigos completos publicados em periódicos'\n",
    "    artigos = df_secoes[(df_secoes.ROTULOS == filtro)]\n",
    "    print(f'Total de linhas de dados: {len(artigos)}')\n",
    "    \n",
    "    nomes   = df_secoes[df_secoes.ROTULOS=='Nome'].values\n",
    "    print(f'Total de nomes de servidores: {len(nomes)}')  \n",
    "\n",
    "    cont=[]\n",
    "    publicacoes=[]\n",
    "    l_curriculo=[]\n",
    "    l_autores=[]\n",
    "    l_titulo=[]\n",
    "    l_revista=[]\n",
    "    l_ano_pub=[]\n",
    "    l_volume=[]\n",
    "    l_paginas=[]\n",
    "    # l_primautor=[]\n",
    "    # l_ultimautor=[]\n",
    "    # l_coaut=[]\n",
    "    # l_numero=[]\n",
    "    # l_local=[]\n",
    "    # l_doi=[]\n",
    "    \n",
    "    remover =['Ordenar por','Ordem Cronológica','Número de citações Web of science','Número de citações Scopus','Numero de citações Scielo','Primeiro autor','Impacto JCR','Ordem de Importância','Livros publicados/organizados ou edições']\n",
    "    \n",
    "    for n,linha in enumerate(artigos['CONTEUDOS']):\n",
    "        linha = linha.replace('CitaÃ§Ãµes:','Citações:')\n",
    "        c=0\n",
    "        # print(type(linha))\n",
    "        for i in eval(linha):\n",
    "            if i not in remover and len(i)>15 and 'Citações:' not in i:\n",
    "                c+=1\n",
    "                cont.append(c)\n",
    "                publicacoes.append(i)\n",
    "                data = parse_string(i)\n",
    "                l_curriculo.append(nomes[n][0])\n",
    "                l_autores.append(data[\"authors\"])\n",
    "                l_titulo.append(data[\"title\"])\n",
    "                l_revista.append(data[\"journal\"])\n",
    "                l_volume.append(data[\"volume\"])\n",
    "                l_paginas.append(data[\"pages\"])\n",
    "                l_ano_pub.append(data[\"year\"])\n",
    "                #  prim_autor, ult_autor, coaut, titulo, revista, local, volume, numero, paginas, ano_publicacao, doi, problemas  = extrair_detalhes(i)\n",
    "                #  l_primautor.append(prim_autor)\n",
    "                #  l_ultimautor.append(ult_autor)\n",
    "                #  l_coaut.append(coaut)\n",
    "                #  l_numero.append(numero)\n",
    "                #  l_local.append(local)\n",
    "                #  l_doi.append(doi)\n",
    "            \n",
    "    ## Monta novo dataframe para ver primeiro e último autor separados dos demais colaboradores\n",
    "    df_artigos = pd.DataFrame({\n",
    "        'CURRICULO': pd.Series(l_curriculo),\n",
    "        'AUTORES': pd.Series(l_autores),\n",
    "        'TITULO': pd.Series(l_titulo),\n",
    "        'REVISTA': pd.Series(l_revista),\n",
    "        'ANO_PUB': pd.Series(l_ano_pub),\n",
    "        'VOLUME':pd.Series(l_volume),\n",
    "        'PAGINAS': pd.Series(l_paginas),\n",
    "        # 'PRIMEIRO_AUTOR': pd.Series(l_primautor),\n",
    "        # 'ULTIMO_AUTOR': pd.Series(l_ultimautor),\n",
    "        # 'COAUTORES': pd.Series(l_coaut),\n",
    "        # 'LOCAL': pd.Series(l_local),        \n",
    "        # 'DOI': pd.Series(l_doi),\n",
    "    })\n",
    "\n",
    "    return df_artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## GERAR DATAFRAMES COM LISTA DE PUBLICAÇÕES DE CADA AUTOR E DATAFRAME COM LISTA DE COLABORAÇÕES UMA POR LINHA\n",
    "# def dividir(linha_dados):\n",
    "#     '''Identifica os separadores de nomes de autores em uma string\n",
    "#      Recebe: a string e entrega uma lista dividida por possíveis separadores como o ;\n",
    "#     Retorna: linha de dados dividida com o separador escolhido dentro da função\n",
    "#       Autor: Marcos Aires Fev.2022\n",
    "#     '''\n",
    "    \n",
    "#     ## encontrar padrões com regular expressions \n",
    "#     qte_divisor_autores  = re.compile(r';')            #Símbolo \";\" na string\n",
    "#     nome_autor_inicio    = re.compile(r'^[A-ZÀ-ú]+,')  #Uma ou mais ocorrência de letras maiúsculas no início da string\n",
    "#     # nome_abreviatura     = re.compile(r'^[A-Z].')      #Uma ou mais ocorrência de letras\n",
    "     \n",
    "#     ## variáveis para encontrar padrões com expressões regulares\n",
    "#     div_autores     = qte_divisor_autores.findall(linha_dados)\n",
    "#     nomes_autores   = nome_autor_inicio.findall(linha_dados)\n",
    "#     # div_abreviatura = nome_autor_inicio.findall(linha_dados)\n",
    "\n",
    "#     ## parâmetros para classificar cada linha como número de ordem, dados de autor ou citações da publicação\n",
    "#     cond_pntvrg = len(div_autores)>0\n",
    "#     cond_pnt    = len(nomes_autores)>0\n",
    "#     # cond_abrev  = len(div_abreviatura)>0\n",
    "\n",
    "#     if cond_pntvrg is True:\n",
    "#         div_inicial = div_pvirg=linha_dados.split(\";\") \n",
    "#     elif cond_pnt is True:\n",
    "#         div_inicial = div_pvirg=linha_dados.split(\". \")\n",
    "        \n",
    "#     return div_inicial\n",
    "\n",
    "\n",
    "# def montar_dfcolab_linhas(df_artigos):\n",
    "#     '''Aplica filtro em df_dados para montar o dataframe de dados detalhados de artigos publicados\n",
    "#         Recebe: Dataframe com os artigos gerado pela função montar_df_artigos(containers).\n",
    "#        Utiliza: Função extrair_detalhes()\n",
    "#        Retorna: Dataframe df_colabartigos com os dados de PRIMEIRO_AUTOR, ULTIMO_AUTOR, COAUTORES, TITULO, REVISTA, ANO_PUB.\n",
    "#          Autor: Marcos Aires Fev.2022\n",
    "#     '''\n",
    "    \n",
    "#     dados_artigo=df_artigos['dados_artigo']\n",
    "#     l_primautor, l_ultimautor, l_coaut, l_titulo, l_revista, l_local, l_volume, l_numero, l_paginas, l_ano_pub, l_doi = [],[],[],[],[],[],[],[],[],[],[]\n",
    "    \n",
    "#     primeiro_autor=''\n",
    "#     revista=''\n",
    "#     doi=''\n",
    "#     local=''\n",
    "#     volume=''\n",
    "#     numero=''\n",
    "#     paginas=''\n",
    "#     ano_publicacao=''\n",
    "    \n",
    "#     ## Para cada artigo no dataframe extrai os dados detalhados com uso da função de quebra da string\n",
    "#     for i in dados_artigo:\n",
    "#         try:\n",
    "#             primeiro_autor, ultimo_autor, coautores, titulo, revista, local, volume, numero, paginas, ano_publicacao, doi, problemas = extrair_detalhes(i)\n",
    "#             l_primautor.append(primeiro_autor)\n",
    "#             l_ultimautor.append(ultimo_autor)\n",
    "#             l_coaut.append(coautores)\n",
    "#             l_titulo.append(titulo)\n",
    "#             l_revista.append(revista)\n",
    "#             l_local.append(local)\n",
    "#             l_volume.append(volume)\n",
    "#             l_numero.append(numero)\n",
    "#             l_paginas.append(paginas)\n",
    "#             l_ano_pub.append(ano_publicacao)\n",
    "#             clear_output(wait=True)\n",
    "#         except Exception as e:\n",
    "#             print('Erro ao montar colaborações na função montar_dfcolab_linhas()')\n",
    "#             traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "#             print(e,traceback_str)\n",
    "            \n",
    "#     ## Monta novo dataframe de artigos com seus dados individualizados\n",
    "#     df_colabartigos = pd.DataFrame({\n",
    "#         'PRIMEIRO_AUTOR': pd.Series(l_primautor),\n",
    "#         'ULTIMO_AUTOR': pd.Series(l_ultimautor),\n",
    "#         'COAUTORES': pd.Series(l_coaut),\n",
    "#         'TITULO': pd.Series(l_titulo),\n",
    "#         'REVISTA': pd.Series(l_revista),\n",
    "#         'ANO_PUB': pd.Series(l_ano_pub),\n",
    "#         'local': pd.Series(l_local),\n",
    "#         'rev_volume':pd.Series(l_volume),\n",
    "#         'rev_numero':pd.Series(l_numero),\n",
    "#         'paginas': pd.Series(l_paginas),\n",
    "#     })\n",
    "    \n",
    "#     return df_colabartigos, problemas\n",
    "\n",
    "\n",
    "# def padronizar_titulo(titulo_bruto):\n",
    "#     '''Retira acentos, expressão (Org.) e espaços vazios do título da publicação\n",
    "#     Autor: Marcos Aires (Fev.2022)\n",
    "#     '''\n",
    "#     import unicodedata\n",
    "#     import re\n",
    "#     string = ''.join(ch for ch in unicodedata.normalize('NFKD', titulo_bruto) if not unicodedata.combining(ch))\n",
    "#     string = string.replace('(Org)','').replace('(Org.)','').replace('(Org).','').replace('.','')\n",
    "    \n",
    "#     titulo_padronizado = string.strip().strip('\"')\n",
    "    \n",
    "#     return titulo_padronizado\n",
    "\n",
    "\n",
    "# def montar_lista_autores(df_dadosgrupo, df_colunaautores):\n",
    "#     ''' Cria lista com o nome padronizado para cada autor\n",
    "#      Recebe: Dataframe com os dados brutos do grupo de pesquisa; Coluna de um Dataframe com nomes dos membros do grupo de pesquisa, com um nome de autor em cada linha; Lista de tuplas a trocar (origem, destino)\n",
    "#     Utiliza: Funções padronizar_nome(nome), extrair_variantes(df_dadosgrupo)\n",
    "#     Retorna: lista_padronizada com os nomes padronizados\n",
    "#       Autor: Marco Aires (Fev.2022)\n",
    "#     '''\n",
    "    \n",
    "# #     lista_nomes=df_professores['PESQUISADORES']\n",
    "#     lista_padronizada=[]\n",
    "#     origem=[]\n",
    "#     destino=[]\n",
    "#     trocar=extrair_variantes(df_dadosgrupo)\n",
    "#     for i,j in trocar:\n",
    "#         origem.append(i)\n",
    "#         destino.append(j)\n",
    "    \n",
    "#     c=0\n",
    "#     for autor in df_colunaautores:\n",
    "#         # print(' ANTES:',autor)\n",
    "#         c+=1\n",
    "#         try:\n",
    "#             nome_padronizado = padronizar_nome(autor.strip())\n",
    "#         except:\n",
    "#             nome_padronizado = autor.strip()\n",
    "#             pass\n",
    "#         if autor in origem:\n",
    "#             for j,k in zip(origem,destino):\n",
    "#                 if autor.lower().strip()==j.lower().strip():\n",
    "#                     l=k\n",
    "# #             print(f'{c:3d}. {i:<35} ==> {j:<35} ==> {k}')\n",
    "#             lista_padronizada.append(l)\n",
    "#             # print('DEPOIS:',l)\n",
    "#         else:\n",
    "#             # print('DEPOIS:',nome_padronizado)\n",
    "#             lista_padronizada.append(nome_padronizado)\n",
    "    \n",
    "#     return lista_padronizada\n",
    "\n",
    "\n",
    "# def montar_lista_coautores(df_dadosgrupo,df_colunacoautores):\n",
    "#     '''\n",
    "#     Cria lista com o nome padronizado de cada autor\n",
    "#      Recebe: Coluna de um Dataframe com nomes dos membros do grupo de pesquisa, com vários nomes de autor em cada célula\n",
    "#     Utiliza: Funções padronizar_nome(nome), extrair_variantes(df_dadosgrupo)\n",
    "#     Retorna: Lista com os nomes padronizados\n",
    "#       Autor: Marco Aires (Fev.2022)\n",
    "#     '''\n",
    "#     lista_coautores_padronizada=[]\n",
    "#     origem=[]\n",
    "#     destino=[]\n",
    "    \n",
    "#     filtro1   = 'Nome'\n",
    "#     lista_nomes = list(df_dadosgrupo[(df_dadosgrupo.ROTULOS == filtro1)]['CONTEUDOS'].values)\n",
    "#     lista_nomes\n",
    "    \n",
    "#     trocar=extrair_variantes(df_dadosgrupo)\n",
    "#     for i,j in trocar:\n",
    "#         origem.append(i)\n",
    "#         destino.append(j)\n",
    "    \n",
    "#     c=0\n",
    "#     for coautores in df_colunacoautores.values:\n",
    "#         c+=1\n",
    "#         # print(' ANTES:',coautores)\n",
    "#         lista_temp=[]\n",
    "#         for coautor in coautores:\n",
    "#             try:\n",
    "#                 nome_padronizado = padronizar_nome(coautor.strip())\n",
    "#             except:\n",
    "#                 nome_padronizado = coautor.strip()\n",
    "#             if coautor in origem or nome_padronizado in origem:             \n",
    "#                 for j,k in zip(origem,destino):\n",
    "#                     if coautor.lower().strip()==j.lower().strip():\n",
    "#                         l=k\n",
    "#                 lista_temp.append(l)\n",
    "#             else:\n",
    "#                 lista_temp.append(nome_padronizado)      \n",
    "#         # print('DEPOIS:',lista_temp)\n",
    "#         lista_coautores_padronizada.append(lista_temp) \n",
    "    \n",
    "#     return lista_coautores_padronizada\n",
    "\n",
    "\n",
    "\n",
    "# ## MONTAGEM E VISUALIZAÇÃO DOS DATAFRAMES QUE IRÃO GERAR OS GRAFOS\n",
    "# def montar_bipartido(df):\n",
    "#     '''Monta dataframes de colaboração a partir do dataframe de dados brutos df_dados extraídos de cada autor\n",
    "#      Recebe: Dataframe df_dados com dados brutos de cada autor\n",
    "#     Utiliza: Função parse_string(linha_conteudo)\n",
    "#     Retorna: Dois dataframes um com lista de publicações separada por autoria e colaborações outro com uma linha por colaborador\n",
    "#       Autor: Marcos Aires (Fev.2022)\n",
    "#     '''\n",
    "#     filtro='Artigos completos publicados em periódicos'\n",
    "#     # df=df_dados_unico\n",
    "#     dados = df[(df.ROTULOS == filtro)]\n",
    "#     remover=['Ordenar por','Ordem Cronológica','Número de citações Web of science','Número de citações Scopus','Numero de citações Scielo','Primeiro autor','Impacto JCR','Ordem de Importância','Livros publicados/organizados ou edições']\n",
    "#     publicacoes=[]\n",
    "#     cont=[]\n",
    "#     l_curriculo=[]\n",
    "#     l_primautor=[]\n",
    "#     l_ultimautor=[]\n",
    "#     l_coaut=[]\n",
    "#     l_titulo=[]\n",
    "#     l_revista=[]\n",
    "#     l_ano_pub=[]\n",
    "#     l_local=[]\n",
    "#     l_volume=[]\n",
    "#     l_numero=[]\n",
    "#     l_paginas=[]\n",
    "#     l_doi=[]\n",
    "#     l_problemas=[]\n",
    "\n",
    "#     for n,j in enumerate(dados['CONTEUDOS']):\n",
    "#         c=0\n",
    "#         for i in j: \n",
    "#             if i not in remover and len(i)>15 and 'Citações:' not in i:\n",
    "#                 c+=1\n",
    "#                 cont.append(c)\n",
    "#                 publicacoes.append(i)\n",
    "#                 data  = parse_string(i)\n",
    "#                 l_curriculo.append(dados['CURRICULO'].iloc[n])\n",
    "#                 l_autores.append(data[\"authors\"])\n",
    "#                 l_titulo.append(data[\"title\"])\n",
    "#                 l_revista.append(data[\"journal\"])\n",
    "#                 l_volume.append(data[\"volume\"])\n",
    "#                 l_paginas.append(data[\"pages\"])\n",
    "#                 l_ano_pub.append(data[\"year\"])\n",
    "#                 #  l_primautor.append(prim_autor)\n",
    "#                 #  l_ultimautor.append(ult_autor)\n",
    "#                 #  l_coaut.append(coaut)\n",
    "#                 #  l_numero.append(numero)\n",
    "#                 #  l_local.append(local)\n",
    "#                 #  l_doi.append(doi)\n",
    "\n",
    "#     ## Monta novo dataframe para ver primeiro e último autor separados dos demais colaboradores\n",
    "#     df_colabartigos = pd.DataFrame({\n",
    "#         'CURRICULO': pd.Series(l_curriculo),\n",
    "#         'AUTORES': pd.Series(l_autores),\n",
    "#         'PRIMEIRO_AUTOR': pd.Series(l_primautor),\n",
    "#         'ULTIMO_AUTOR': pd.Series(l_ultimautor),\n",
    "#         'COAUTORES': pd.Series(l_coaut),\n",
    "#         'TITULO': pd.Series(l_titulo),\n",
    "#         'REVISTA': pd.Series(l_revista),\n",
    "#         'ANO_PUB': pd.Series(l_ano_pub),\n",
    "#     })\n",
    "\n",
    "\n",
    "#     ## Monta novo dataframe para ver uma linha por cada colaborador na autoria \n",
    "#     l_autores =[]\n",
    "#     l_titulos =[]\n",
    "#     l_anos    =[]\n",
    "#     l_tipos   =[]\n",
    "\n",
    "#     for i in range(len(df_colabartigos['TITULO'])):\n",
    "#         l_anos.append(df_colabartigos['ANO_PUB'][i])\n",
    "#         l_titulos.append(df_colabartigos['TITULO'][i])\n",
    "#         l_autores.append(df_colabartigos['PRIMEIRO_AUTOR'][i])\n",
    "#         l_tipos.append('primeiro_autor')\n",
    "\n",
    "#         for j in df_colabartigos['COAUTORES'][i]:\n",
    "#             l_anos.append(df_colabartigos['ANO_PUB'][i])\n",
    "#             l_titulos.append(df_colabartigos['TITULO'][i])\n",
    "#             try:\n",
    "#                 j=padronizar_nome(j.strip())\n",
    "#             except:\n",
    "#                 pass\n",
    "#             l_autores.append(j)\n",
    "#             l_tipos.append('colaborador')\n",
    "\n",
    "#     for k in range(len(df_colabartigos['TITULO'])):\n",
    "#         if df_colabartigos['ULTIMO_AUTOR'][k] != '':\n",
    "#             ultimo=df_colabartigos['ULTIMO_AUTOR'][k]\n",
    "#             try:\n",
    "#                 ultimo=padronizar_nome(ultimo)\n",
    "#             except:\n",
    "#                 pass\n",
    "#             l_anos.append(df_colabartigos['ANO_PUB'][k])\n",
    "#             l_titulos.append(df_colabartigos['TITULO'][k])\n",
    "#             l_autores.append(ultimo)\n",
    "#             l_tipos.append('ultimo_autor')\n",
    "\n",
    "#     df_bipartido = pd.DataFrame({\n",
    "#         'ANO_PUB': pd.Series(l_anos),\n",
    "#         'TITULO': pd.Series(l_titulos),\n",
    "#         'AUTORES': pd.Series(l_autores),\n",
    "#         'TIPO': pd.Series(l_tipos)\n",
    "#     })\n",
    "\n",
    "#     df_bipartido.sort_values([\"ANO_PUB\", \"TITULO\"],\n",
    "#                                axis = 0, \n",
    "#                                ascending = True,\n",
    "#                                inplace = True,\n",
    "#                                na_position = \"first\")\n",
    "\n",
    "#     df_bipartido.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "#     # filtro='Supervisão de pós-doutorado'\n",
    "#     # filtro='Monografia de conclusão de curso de aperfeiçoamento/especialização'\n",
    "#     # filtro='Trabalho de conclusão de curso de graduação'\n",
    "#     # filtro='Iniciação científica'\n",
    "#     # filtro='Orientações de outra natureza'\n",
    "    \n",
    "    \n",
    "#     filtro='Orientação Dissertação de mestrado'\n",
    "#     dados = df[(df.ROTULOS == filtro)]\n",
    "#     remover=['Ordenar por','Ordem Cronológica','Número de citações Web of science','Número de citações Scopus','Numero de citações Scielo','Primeiro autor','Impacto JCR','Ordem de Importância','Livros publicados/organizados ou edições']   \n",
    "#     orientacoes_mestrado=[]\n",
    "#     # remover=[]\n",
    "#     cont=[]\n",
    "#     for j in dados['CONTEUDOS']:\n",
    "#         c=0\n",
    "#         for i in j: \n",
    "#             if i not in remover and len(i)>15 and 'Citações:' not in i:\n",
    "#                 c+=1\n",
    "#                 cont.append(c)\n",
    "#                 orientacoes_mestrado.append(i)   \n",
    "                \n",
    "#     orientandos=[]\n",
    "#     trabalhos_orientados=[]\n",
    "#     ano_orientacao=[]\n",
    "#     instituicoes_orientacao=[]\n",
    "#     papeis_orientacao=[]\n",
    "\n",
    "#     for j in orientacoes_mestrado:\n",
    "#         i = j.split('. ')\n",
    "#         orientando=i[0]\n",
    "#         trabalho=i[1]\n",
    "#         ano=i[2]\n",
    "#         terceiro_dado=i[3]\n",
    "#         # print(terceiro_dado)\n",
    "#         if len(terceiro_dado) > 5:\n",
    "#             instituicao=i[3]\n",
    "#             papel=i[4].split(': ')[0]\n",
    "#         else:\n",
    "#             instituicao=i[4]\n",
    "#             papel=i[5].split(': ')[0]\n",
    "\n",
    "#         orientandos.append(orientando.title().strip())\n",
    "#         trabalhos_orientados.append(trabalho)\n",
    "#         ano_orientacao.append(ano)\n",
    "#         instituicoes_orientacao.append(instituicao)\n",
    "#         papeis_orientacao.append(papel)  \n",
    "    \n",
    "#     df_orientacoes_mestrado = pd.DataFrame({\n",
    "#         'ANO_CONCLUSÃO': pd.Series(ano_orientacao),\n",
    "#         'TÍTULO': pd.Series(trabalhos_orientados),\n",
    "#         'ORIENTADOS': pd.Series(orientandos),\n",
    "#         'PAPEL_ORIENTAÇÃO': pd.Series(papeis_orientacao),\n",
    "#         'INSTITUIÇÃO': pd.Series(instituicoes_orientacao),\n",
    "#     })\n",
    "\n",
    "\n",
    "#     filtro='Orientação Tese de doutorado'\n",
    "#     dados = df[(df.ROTULOS == filtro)]\n",
    "#     remover=['Ordenar por','Ordem Cronológica','Número de citações Web of science','Número de citações Scopus','Numero de citações Scielo','Primeiro autor','Impacto JCR','Ordem de Importância','Livros publicados/organizados ou edições']   \n",
    "#     orientacoes_doutorado=[]\n",
    "#     # remover=[]\n",
    "#     cont=[]\n",
    "#     for j in dados['CONTEUDOS']:\n",
    "#         c=0\n",
    "#         for i in j: \n",
    "#             if i not in remover and len(i)>15:\n",
    "#                 c+=1\n",
    "#                 cont.append(c)\n",
    "#                 orientacoes_doutorado.append(i)   \n",
    "                \n",
    "#     orientandos=[]\n",
    "#     trabalhos_orientados=[]\n",
    "#     ano_orientacao=[]\n",
    "#     instituicoes_orientacao=[]\n",
    "#     papeis_orientacao=[]\n",
    "\n",
    "#     for j in orientacoes_doutorado:\n",
    "#         i = j.split('. ')\n",
    "#         orientando=i[0]\n",
    "#         trabalho=i[1]\n",
    "#         ano=i[2]\n",
    "#         terceiro_dado=i[3]\n",
    "#         # print(terceiro_dado)\n",
    "#         if len(terceiro_dado) > 5:\n",
    "#             instituicao=i[3]\n",
    "#             papel=i[4].split(': ')[0]\n",
    "#         else:\n",
    "#             instituicao=i[4]\n",
    "#             papel=i[5].split(': ')[0]\n",
    "\n",
    "#         orientandos.append(orientando.title().strip())\n",
    "#         trabalhos_orientados.append(trabalho)\n",
    "#         ano_orientacao.append(ano)\n",
    "#         instituicoes_orientacao.append(instituicao)\n",
    "#         papeis_orientacao.append(papel)  \n",
    "    \n",
    "#     df_orientacoes_doutorado = pd.DataFrame({\n",
    "#         'ANO_CONCLUSÃO': pd.Series(ano_orientacao),\n",
    "#         'TÍTULO': pd.Series(trabalhos_orientados),\n",
    "#         'ORIENTADOS': pd.Series(orientandos),\n",
    "#         'PAPEL_ORIENTAÇÃO': pd.Series(papeis_orientacao),\n",
    "#         'INSTITUIÇÃO': pd.Series(instituicoes_orientacao),\n",
    "#     })\n",
    "    \n",
    "#     return df_colabartigos, df_bipartido, df_orientacoes_mestrado, df_orientacoes_doutorado\n",
    "\n",
    "\n",
    "# def visualizar_bipartido(df_dados):\n",
    "#     '''Renderiza, no notebook ou em uma página HTML, a visualização do grafo de todas as colaborações a partir do dataframe de dados bipartidos\n",
    "#      Recebe: Dataframe com uma linha para relacionamento, ou seja, uma linha para cada autor do artigo repetindo o mesmo artigo para quantos coautores houver.\n",
    "#     Utiliza: Módulo network da biblioteca pyvis\n",
    "#     Retorna: Visualização em HTML do grafo\n",
    "#     Autor: Marcos Aires (Mar.2022)\n",
    "#     '''\n",
    "#     from pyvis.network import Network\n",
    "\n",
    "#     origem =[]\n",
    "#     destino=[]\n",
    "#     pesos=[]\n",
    "#     titulo=[]\n",
    "#     ano=[]\n",
    "#     peso=[]\n",
    "\n",
    "#     for i in range(len(df_dados['AUTORES_PADRONIZADOS'])):\n",
    "#         origem.append(df_dados['AUTORES_PADRONIZADOS'][i].title())\n",
    "#         destino.append(df_dados['TITULO'][i].title())\n",
    "#         titulo.append(df_dados['TITULO'][i])\n",
    "#         ano.append(df_dados['ANO_PUB'][i])\n",
    "#         peso.append(1)\n",
    "\n",
    "#     df_bipartido = pd.DataFrame({\n",
    "#                             'AUTOR': pd.Series(origem),\n",
    "#                             'TITULO': pd.Series(titulo),\n",
    "#                             'ANO_PUB': pd.Series(ano),\n",
    "#                             'PESO': pd.Series(peso),\n",
    "#                            }) \n",
    "\n",
    "#     colab_artigos = Network(height='750px', \n",
    "#                     width='100%', \n",
    "#                     bgcolor='#222222', \n",
    "#                     font_color='white',\n",
    "#                     # fontsize=24,\n",
    "#                     notebook=True,\n",
    "#                            )\n",
    "\n",
    "#     # modelo de física para conformação da rede\n",
    "#     colab_artigos.barnes_hut()\n",
    "\n",
    "#     ## Adicionando subgrafo referente ao componente principal com as colaborações\n",
    "#     sources = df_bipartido['AUTOR']\n",
    "#     targets = df_bipartido['TITULO']\n",
    "#     weights = df_bipartido['PESO']\n",
    "#     ano_pub = df_bipartido['ANO_PUB']\n",
    "\n",
    "#     edge_data = zip(sources, targets, weights)\n",
    "\n",
    "#     for e in edge_data:\n",
    "#         src = e[0]\n",
    "#         dst = e[1]\n",
    "#         w   = e[2]\n",
    "\n",
    "#         colab_artigos.add_node(src, src, title=src)\n",
    "#         colab_artigos.add_node(dst, dst, title=dst)\n",
    "#         colab_artigos.add_edge(src, dst, value=w)\n",
    "\n",
    "\n",
    "#     neighbor_map = colab_artigos.get_adj_list()\n",
    "\n",
    "#     # add neighbor data to node hover data\n",
    "#     for node in colab_artigos.nodes:\n",
    "#         node['title'] += ' COLABOROU_COM:<br>' + '<br>'.join(neighbor_map[node['id']])\n",
    "#         node['value'] = len(neighbor_map[node['id']])\n",
    "\n",
    "#     colab_artigos.show_buttons(filter_=['physics'])\n",
    "#     # colab_artigos.show('coautorias.html')\n",
    "    \n",
    "#     return colab_artigos.show('coautorias.html')\n",
    "\n",
    "\n",
    "# def montar_adj(df_dadoscolab):\n",
    "#     '''Monta uma lista de colaborações, tomando o primeiro autor como origem e colaboradores como destino\n",
    "#     Recebe: DataFrame com dados detalhados dos artigos\n",
    "#     Retorna: DataFrame com lista de aresta de colaborações\n",
    "#     Autor: Marcos Aires Fev.2022\n",
    "#     '''\n",
    "#     origem=[]\n",
    "#     destinos=[]\n",
    "#     lista_destinos=[]\n",
    "#     lista_titulos=[]\n",
    "#     lista_anos=[]\n",
    "\n",
    "#     for i in range(len(df_dadoscolab['PRIMEIRO_AUTOR'])):\n",
    "#         lista_anos.append(df_dadoscolab['ANO_PUB'][i])\n",
    "#         lista_titulos.append(df_dadoscolab['TITULO'][i])\n",
    "#         origem.append(df_dadoscolab['PRIMEIRO_AUTOR'][i])\n",
    "\n",
    "        \n",
    "#     for i in range(len(df_dadoscolab['COAUTORES'])):\n",
    "#         l1=df_dadoscolab['COAUTORES'][i]\n",
    "#         l2=[df_dadoscolab['ULTIMO_AUTOR'][i]]\n",
    "#         destino=l1+l2\n",
    "#         lista_destinos.append(destino)\n",
    "\n",
    "#     df_adj = pd.DataFrame({\n",
    "#                             'ANO_PUB': pd.Series(lista_anos),\n",
    "#                             'TITULO': pd.Series(lista_titulos),\n",
    "#                             'PRIMEIRO_AUTOR': pd.Series(origem),\n",
    "#                             'COAUTORES': pd.Series(lista_destinos),\n",
    "#                            }) \n",
    "\n",
    "#     return df_adj\n",
    "\n",
    "\n",
    "# def montar_adj_primult(df_dados):\n",
    "#     '''Monta uma lista de colaborações, tomando o primeiro autor como origem e ultimo autor como destino\n",
    "#     Recebe: DataFrame com dados detalhados dos artigos\n",
    "#     Retorna: DataFrame com lista de aresta de colaborações\n",
    "#     Autor: Marcos Aires Fev.2022\n",
    "#     '''\n",
    "#     origem=[]\n",
    "#     destinos=[]\n",
    "#     lista_destinos=[]\n",
    "#     lista_titulos=[]\n",
    "#     lista_anos=[]\n",
    "\n",
    "#     for i in range(len(df_dados['PRIMEIRO_AUTOR'])):\n",
    "#         origem.append(df_dados['PRIMEIRO_AUTOR'][i])\n",
    "#         lista_anos.append(df_dados['ANO_PUB'][i])\n",
    "#         lista_titulos.append(df_dados['TITULO'][i])\n",
    "\n",
    "        \n",
    "#     for j in range(len(df_dados['ULTIMO_AUTOR'])):\n",
    "#         destinos.append(df_dados['ULTIMO_AUTOR'][j])\n",
    "\n",
    "#     df_adj_prim_ult = pd.DataFrame({\n",
    "#                             'ANO_PUB': pd.Series(lista_anos),\n",
    "#                             'TITULO': pd.Series(lista_titulos),\n",
    "#                             'PRIMEIRO_AUTOR': pd.Series(origem),\n",
    "#                             'ULTIMO_AUTOR': pd.Series(destinos),\n",
    "#                            }) \n",
    "\n",
    "#     return df_adj_prim_ult\n",
    "\n",
    "\n",
    "# def visualizar_colaboracoes(df_dados):\n",
    "#     '''Renderiza, no notebook ou em uma página HTML, a visualização do grafo de todas as colaborações\n",
    "#     Como parâmetro, recebe um dataframe de dados, onde: df_dados=montar_adj(df_dados)\n",
    "#     Autor: Marcos Aires (Mar.2022)\n",
    "#     '''\n",
    "#     from pyvis.network import Network\n",
    "\n",
    "#     origem =[]\n",
    "#     destino=[]\n",
    "#     pesos=[]\n",
    "#     titulo=[]\n",
    "#     ano=[]\n",
    "#     peso=[]\n",
    "#     lista_autores=[]\n",
    "\n",
    "#     for i in range(len(df_dados['PRIMEIRO_AUTOR'])):\n",
    "#         if i not in lista_autores:\n",
    "#             lista_autores.append(df_dados['PRIMEIRO_AUTOR'][i].title())\n",
    "        \n",
    "#         for j in df_dados['COAUTORES'][i]:\n",
    "#             origem.append(df_dados['PRIMEIRO_AUTOR'][i].title())\n",
    "#             destino.append(j.title())\n",
    "#             if j not in lista_autores:\n",
    "#                 lista_autores.append(j.title())\n",
    "#             titulo.append(df_dados['TITULO'][i])\n",
    "#             ano.append(df_dados['ANO_PUB'][i])\n",
    "#             peso.append(1)\n",
    "\n",
    "#     df_grafo = pd.DataFrame({\n",
    "#                             'PRIMEIRO_AUTOR': pd.Series(origem),\n",
    "#                             'COAUTORES': pd.Series(destino),\n",
    "#                             'TITULO': pd.Series(titulo),\n",
    "#                             'ANO_PUB': pd.Series(ano),\n",
    "#                             'PESO': pd.Series(peso),\n",
    "#                            }) \n",
    "\n",
    "#     colab_artigos = Network(height='750px', \n",
    "#                     width='100%', \n",
    "#                     bgcolor='#222222', \n",
    "#                     font_color='white',\n",
    "#                     # fontsize=24,\n",
    "#                     notebook=True,\n",
    "#                            )\n",
    "\n",
    "#     # modelo de física para conformação da rede\n",
    "#     colab_artigos.barnes_hut()\n",
    "\n",
    "#     ## Adicionando subgrafo referente ao componente principal com as colaborações\n",
    "#     sources = df_grafo['PRIMEIRO_AUTOR']\n",
    "#     targets = df_grafo['COAUTORES']\n",
    "#     weights = df_grafo['PESO']\n",
    "\n",
    "#     titulo  = df_grafo['TITULO']\n",
    "#     ano_pub = df_grafo['ANO_PUB']\n",
    "\n",
    "#     edge_data = zip(sources, targets, weights)\n",
    "\n",
    "#     for e in edge_data:\n",
    "#         src = e[0]\n",
    "#         dst = e[1]\n",
    "#         w = e[2]\n",
    "\n",
    "#         colab_artigos.add_node(src, src, title=src)\n",
    "#         colab_artigos.add_node(dst, dst, title=dst)\n",
    "#         colab_artigos.add_edge(src, dst, value=w)\n",
    "\n",
    "\n",
    "#     # ## Adicionando subgrafo referente às variantes de nome em citações\n",
    "#     # origem = NOME\n",
    "#     # filtro = 'Nome em citações bibliográficas'\n",
    "#     # variantes = df_dados[(df_dados.ROTULOS == filtro)]['CONTEUDOS'].values[0]\n",
    "\n",
    "#     # origens = []\n",
    "#     # pesos = []\n",
    "#     # destinos = variantes\n",
    "\n",
    "#     # # Atribui dados do dataset às variáveis que constroem o grafo\n",
    "#     # for i in range(len(destinos)):\n",
    "#     #     origens.append(origem)\n",
    "#     #     pesos.append(100)\n",
    "\n",
    "#     # arestas = zip(origens, destinos, pesos)\n",
    "#     # for a in arestas:\n",
    "#     #     src = a[0]\n",
    "#     #     dst = a[1]\n",
    "#     #     w = a[2]\n",
    "\n",
    "#     #     colab_artigos.add_node(src, src, title=src, color = \"white\")\n",
    "#     #     colab_artigos.add_node(dst, dst, title=dst)\n",
    "#     #     colab_artigos.add_edge(src, dst, value=w)\n",
    "\n",
    "\n",
    "#     neighbor_map = colab_artigos.get_adj_list()\n",
    "\n",
    "#     # add neighbor data to node hover data\n",
    "#     for node in colab_artigos.nodes:\n",
    "#         node['title'] += ' COLABOROU_COM:<br>' + '<br>'.join(neighbor_map[node['id']])\n",
    "#         node['value'] = len(neighbor_map[node['id']])\n",
    "\n",
    "#     colab_artigos.show_buttons(filter_=['physics'])\n",
    "# #     colab_artigos.show('coautorias.html')\n",
    "    \n",
    "#     return colab_artigos.show('coautorias.html')\n",
    "\n",
    "\n",
    "# def visualizar_colabprincipais(df_dados):\n",
    "#     '''Renderiza, no notebook ou em uma página HTML, a visualização do grafo das principais colaborações.\n",
    "#     Considera como principais as colaborações onde o autor é o primeiro ou o último autor.\n",
    "#     Como parâmetro, recebe um dataframe de dados, onde: df_dados=montar_adj_primult(df_dados)\n",
    "#     Autor: Marcos Aires (Mar.2022)\n",
    "#     '''\n",
    "#     from pyvis.network import Network\n",
    "\n",
    "#     origem =[]\n",
    "#     destino=[]\n",
    "#     pesos=[]\n",
    "#     titulo=[]\n",
    "#     ano=[]\n",
    "#     peso=[]\n",
    "#     lista_autores=[]\n",
    "\n",
    "#     for i in range(len(df_dados['PRIMEIRO_AUTOR'])):\n",
    "#         origem.append(df_dados['PRIMEIRO_AUTOR'][i])\n",
    "#         if i not in lista_autores:\n",
    "#             lista_autores.append(df_dados['PRIMEIRO_AUTOR'][i])\n",
    "        \n",
    "#     for j in range(len(df_dados['ULTIMO_AUTOR'])):\n",
    "#         destino.append(df_dados['ULTIMO_AUTOR'][j])\n",
    "#         if df_dados['ULTIMO_AUTOR'][j] not in lista_autores:\n",
    "#             lista_autores.append(df_dados['ULTIMO_AUTOR'][j])\n",
    "    \n",
    "#     titulo.append(df_dados['TITULO'][i])\n",
    "#     ano.append(df_dados['ANO_PUB'][i])\n",
    "#     peso.append(1)\n",
    "\n",
    "#     df_grafo = pd.DataFrame({\n",
    "#                             'PRIMEIRO_AUTOR': pd.Series(origem),\n",
    "#                             'ULTIMO_AUTOR': pd.Series(destino),\n",
    "#                             'TITULO': pd.Series(titulo),\n",
    "#                             'ANO_PUB': pd.Series(ano),\n",
    "#                             'PESO': pd.Series(peso),\n",
    "#                            }) \n",
    "\n",
    "#     colab_artigos = Network(height='750px', \n",
    "#                     width='100%', \n",
    "#                     bgcolor='#000000', \n",
    "#                     font_color='white',\n",
    "#                     # fontsize=24,\n",
    "#                     notebook=True,\n",
    "#                            )\n",
    "\n",
    "#     # modelo de física para conformação da rede\n",
    "#     colab_artigos.barnes_hut()\n",
    "\n",
    "#     ## Adicionando subgrafo referente ao componente principal com as colaborações\n",
    "#     sources = df_grafo['PRIMEIRO_AUTOR']\n",
    "#     targets = df_grafo['ULTIMO_AUTOR']\n",
    "#     weights = df_grafo['PESO']\n",
    "\n",
    "#     titulo  = df_grafo['TITULO']\n",
    "#     ano_pub = df_grafo['ANO_PUB']\n",
    "\n",
    "#     edge_data = zip(sources, targets, weights)\n",
    "\n",
    "#     for e in edge_data:\n",
    "#         src = e[0]\n",
    "#         dst = e[1]\n",
    "#         w   = e[2]\n",
    "\n",
    "#         colab_artigos.add_node(src, src, title=src)\n",
    "#         colab_artigos.add_node(dst, dst, title=dst)\n",
    "#         colab_artigos.add_edge(src, dst, value=w)\n",
    "\n",
    "\n",
    "#     # ## Adicionando subgrafo referente às variantes de nome em citações\n",
    "#     # origem = NOME\n",
    "#     # filtro = 'Nome em citações bibliográficas'\n",
    "#     # variantes = df_dados[(df_dados.ROTULOS == filtro)]['CONTEUDOS'].values[0]\n",
    "\n",
    "#     # origens = []\n",
    "#     # pesos = []\n",
    "#     # destinos = variantes\n",
    "\n",
    "#     # # Atribui dados do dataset às variáveis que constroem o grafo\n",
    "#     # for i in range(len(destinos)):\n",
    "#     #     origens.append(origem)\n",
    "#     #     pesos.append(100)\n",
    "\n",
    "#     # arestas = zip(origens, destinos, pesos)\n",
    "#     # for a in arestas:\n",
    "#     #     src = a[0]\n",
    "#     #     dst = a[1]\n",
    "#     #     w = a[2]\n",
    "\n",
    "#     #     colab_artigos.add_node(src, src, title=src, color = \"white\")\n",
    "#     #     colab_artigos.add_node(dst, dst, title=dst)\n",
    "#     #     colab_artigos.add_edge(src, dst, value=w)\n",
    "\n",
    "\n",
    "#     neighbor_map = colab_artigos.get_adj_list()\n",
    "\n",
    "#     # add neighbor data to node hover data\n",
    "#     for node in colab_artigos.nodes:\n",
    "#         node['title'] += ' COLABOROU_COM:<br>' + '<br>'.join(neighbor_map[node['id']])\n",
    "#         node['value'] = len(neighbor_map[node['id']])\n",
    "\n",
    "#     colab_artigos.show_buttons(filter_=['physics'])\n",
    "    \n",
    "#     return colab_artigos.show('coautorias_principais.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar funções de Plotagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def get_initials(name):\n",
    "    return ''.join([word[0] for word in name.split()])\n",
    "\n",
    "def plotar_artigos_ano(df_artigosperiodo):\n",
    "    # Contar os valores dos anos da coluna ANO_PUB\n",
    "    count_anos = df_artigosperiodo['ANO_PUB'].value_counts().sort_index()\n",
    "\n",
    "    # Criar o gráfico de barras com plotly express\n",
    "    fig = px.bar(x=range(len(count_anos.index)), \n",
    "                 y=count_anos.values,\n",
    "                 labels={'x':'Ano de publicação', 'y':'Quantidade de artigos'},\n",
    "                 title='Quantidade de Artigos por Ano')\n",
    "\n",
    "    # Ajustar os tick labels do eixo x para mostrar os anos\n",
    "    fig.update_xaxes(tickvals=list(range(len(count_anos.index))), ticktext=count_anos.index, showgrid=False)\n",
    "\n",
    "    # Remover linhas de grade horizontal (eixo y)\n",
    "    fig.update_yaxes(showgrid=False)\n",
    "\n",
    "    # Adicionar a anotação dos valores em cada barra\n",
    "    for index, value in enumerate(count_anos.values):\n",
    "        fig.add_annotation(\n",
    "            x=index,\n",
    "            y=value + (0.05 * max(count_anos.values)),  # Ajustar esta proporção conforme necessário\n",
    "            text=str(value),\n",
    "            showarrow=False,\n",
    "            font_size=20\n",
    "        )\n",
    "\n",
    "    # Adicionar a anotação com a quantidade total de artigos no período\n",
    "    total_artigos = sum(count_anos.values)\n",
    "    fig.add_annotation(\n",
    "        x=len(count_anos.index)/2,\n",
    "        y=max(count_anos.values) + (0.15 * max(count_anos.values)),  # Ajustar esta proporção conforme necessário\n",
    "        text=f\"Total de Participação em Artigos, após entrada na Fiocruz Ceará: {total_artigos}\",\n",
    "        showarrow=False,\n",
    "        font_size=18,\n",
    "        font_color=\"blue\"\n",
    "    )\n",
    "\n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1380,   # largura em pixels\n",
    "        height=600   # altura em pixels\n",
    "    )\n",
    "\n",
    "    # Mostrar o gráfico\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "def plotar_barras_agrupadas(df_artigosperiodo):\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    # Contar os artigos por ano e por CURRICULO\n",
    "    grouped_data = df_artigosperiodo.groupby(['ANO_PUB', 'NOME']).size().reset_index(name='count')\n",
    "\n",
    "    # Usar paleta de cores personalizada \"VIVID\"\n",
    "    colors_vivid = ['#FF595E', '#FFCA3A', '#8AC926', '#1982FC', '#6A0572'] # Adicione mais cores se necessário\n",
    "\n",
    "    # Criar o gráfico de barras com plotly express\n",
    "    fig = px.bar(grouped_data, \n",
    "                 x='ANO_PUB',\n",
    "                 y='count',\n",
    "                 color='CURRICULO',\n",
    "                 color_discrete_sequence=colors_vivid,\n",
    "                 labels={'ANO_PUB':'Ano', 'count':'Quantidade'},\n",
    "                 title='Quantidade de Participações em Artigos por Ano e por Currículo, após entrada na Fiocruz Ceará')\n",
    "\n",
    "    # Adicionar rótulos de dados\n",
    "    for trace, color in zip(fig.data, colors_vivid):\n",
    "        for x_val, y_val in zip(trace.x, trace.y):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[x_val],\n",
    "                    y=[y_val + (0.02 * max(grouped_data['count']))], \n",
    "                    text=[str(y_val)],\n",
    "                    mode=\"text\",\n",
    "                    showlegend=False,\n",
    "                    textfont=dict(color=color)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Remover linhas de grade\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    fig.update_yaxes(showgrid=False)\n",
    "\n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1380,   # largura em pixels\n",
    "        height=1200   # altura em pixels\n",
    "    )\n",
    "\n",
    "    # Mostrar o gráfico\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "def plotar_barras_estaqueadas(df_artigosperiodo):\n",
    "    \n",
    "    df_artigosperiodo['INICIAIS'] = df_artigosperiodo['NOME'].apply(get_initials)\n",
    "    grouped_data = df_artigosperiodo.groupby(['ANO_PUB', 'INICIAIS']).size().reset_index(name='count')\n",
    "\n",
    "    # Usar a paleta \"Plotly\"\n",
    "    colors = px.colors.qualitative.Vivid\n",
    "\n",
    "    fig = px.bar(grouped_data, \n",
    "                 x='ANO_PUB',\n",
    "                 y='count',\n",
    "                #  color='CURRICULO',\n",
    "                 color='INICIAIS',\n",
    "                 barmode='group',\n",
    "                 color_discrete_sequence=colors,\n",
    "                #  labels={'ANO_PUB':'Ano', 'count':'Quantidade'},\n",
    "                 labels={'ANO_PUB':'Ano de Publicação', 'count':'Quantidade de artigos', 'INICIAIS':'Currículo'},\n",
    "                 title='Quantidade de Participação em Artigos por Ano e por Currículo, após entrada na Fiocruz Ceará')\n",
    "\n",
    "    # Adicionar rótulos de dados\n",
    "#     for trace, color in zip(fig.data, colors):\n",
    "#         for x_val, y_val in zip(trace.x, trace.y):\n",
    "#             fig.add_trace(\n",
    "#                 go.Scatter(\n",
    "#                     x=[x_val + 0.01],\n",
    "#                     y=[y_val + (0.02 * max(grouped_data['count']))], \n",
    "#                     text=[str(y_val)],\n",
    "#                     mode=\"text\",\n",
    "#                     showlegend=False,\n",
    "#                     textfont=dict(color=color)\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "    # Posicionar a legenda abaixo do gráfico\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.3,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1600,   # largura em pixels\n",
    "        height=1200   # altura em pixels\n",
    "    )\n",
    "    \n",
    "    # Remover linhas de grade\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    fig.update_yaxes(showgrid=False)\n",
    "\n",
    "    # Mostrar o gráfico\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "def comparativo_curriculos(df_artigosperiodo):\n",
    "\n",
    "    df_artigosperiodo['CURRICULO_INITIALS'] = df_artigosperiodo['NOME'].apply(get_initials)\n",
    "    grouped_data = df_artigosperiodo.groupby(['ANO_PUB', 'CURRICULO_INITIALS']).size().reset_index(name='count')\n",
    "    years = sorted(grouped_data['ANO_PUB'].unique())\n",
    "\n",
    "    # Calcular a soma total para cada currículo\n",
    "    total_counts = grouped_data.groupby('CURRICULO_INITIALS')['count'].sum()\n",
    "\n",
    "    # Ordenar as iniciais dos currículos com base na soma total\n",
    "    sorted_initials = total_counts.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "    # Obter a paleta \"Greens\" e normalize-a para ter uma cor para cada ano\n",
    "    palette = px.colors.sequential.Greens\n",
    "    colors = reversed([palette[i * (len(palette) - 1) // (len(years) - 1)] for i in range(len(years))])\n",
    "    colors = list(colors)  # Converta o resultado para uma lista novamente\n",
    "\n",
    "    # Criar o gráfico com as iniciais ordenadas\n",
    "    traces = []\n",
    "    for idx, year in enumerate(reversed(years)):\n",
    "        year_data = grouped_data[grouped_data['ANO_PUB'] == year]\n",
    "\n",
    "        # Preencher valores ausentes com zero\n",
    "        year_data = year_data.set_index('CURRICULO_INITIALS').reindex(sorted_initials, fill_value=0).reset_index()\n",
    "\n",
    "        # Remover entradas com valores zerados\n",
    "        # year_data = year_data[year_data['count'] > 0]\n",
    "\n",
    "        traces.append(go.Bar(\n",
    "            y=year_data['CURRICULO_INITIALS'],\n",
    "            x=year_data['count'],\n",
    "            name=str(year),\n",
    "            orientation='h',\n",
    "            marker_color=colors[idx]  # Aplique a cor aqui\n",
    "        ))\n",
    "\n",
    "    # Adicionar as traces em ordem à figura\n",
    "    fig = go.Figure(data=traces)\n",
    "\n",
    "    # Configurar o layout para ser empilhado\n",
    "    fig.update_layout(\n",
    "        barmode='stack',\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        title='Quantidade de Participações em Artigos por Currículo e Ano, após entrada na Fiocruz Ceará'\n",
    "    )\n",
    "\n",
    "    # Criar um DataFrame para a soma total por currículo\n",
    "    total_per_curriculo = grouped_data.groupby('CURRICULO_INITIALS')['count'].sum().reset_index()\n",
    "    total_per_curriculo = total_per_curriculo.set_index('CURRICULO_INITIALS').reindex(sorted_initials, fill_value=0).reset_index()\n",
    "\n",
    "    # Calcular a soma total de todos os artigos\n",
    "    total_articles = grouped_data['count'].sum()\n",
    "\n",
    "    # Adicionar a anotação no centro superior da área do gráfico com a soma total de todos os artigos\n",
    "    fig.add_annotation(\n",
    "        x=0.5,  # posição horizontal centrada\n",
    "        y=0.99,  # posição vertical logo acima do topo do gráfico\n",
    "        xref=\"paper\",  # refere-se à posição proporcional do gráfico (0 à esquerda, 1 à direita)\n",
    "        yref=\"paper\",  # refere-se à posição proporcional do gráfico (0 na parte inferior, 1 na parte superior)\n",
    "        text=f\"Total de Participação em Artigos, após entrada na Fiocruz Ceará: {total_articles}\",\n",
    "        showarrow=False,\n",
    "        font=dict(color='blue', size=20),\n",
    "        align=\"center\",\n",
    "        valign=\"top\"\n",
    "    )\n",
    "\n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1380,   # largura em pixels\n",
    "        height=1200   # altura em pixels\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def evolucao_anual(df_artigosperiodo, df_pessoal):\n",
    "    # Removendo duplicidades\n",
    "    # df_artigosperiodo = df_artigosperiodo.drop_duplicates(subset='TITULO')\n",
    "\n",
    "    # Para todos os anos dos dados\n",
    "    min_year = int(df_artigosperiodo['ANO_PUB'].min())\n",
    "    max_year = int(df_artigosperiodo['ANO_PUB'].max())\n",
    "    all_years = list(range(min_year, max_year + 1))\n",
    "\n",
    "    # Quantidade total de artigos por ano\n",
    "    artigos_por_ano = df_artigosperiodo.groupby('ANO_PUB').size().reindex(all_years, fill_value=0).reset_index(name='count')\n",
    "\n",
    "    # Quantidade de pesquisadores únicos que entraram a cada ano\n",
    "    pesquisadores_por_ano = df_pessoal.groupby('ANO_INGRESSO_FIOCE')['NOME'].nunique().reindex(all_years, fill_value=0).reset_index(name='unique_researchers')\n",
    "\n",
    "    # Soma cumulativa de pesquisadores ao longo dos anos\n",
    "    pesquisadores_por_ano['cumulative_researchers'] = pesquisadores_por_ano['unique_researchers'].cumsum()\n",
    "\n",
    "    # Média de artigos por pesquisador\n",
    "    pesquisadores_por_ano['avg_articles_per_researcher'] = artigos_por_ano['count'] / pesquisadores_por_ano['cumulative_researchers']\n",
    "    pesquisadores_por_ano['avg_articles_per_researcher'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "    # Calcular a proporção entre os dois eixos y\n",
    "    max_y1 = artigos_por_ano['count'].max()+10\n",
    "    max_y2 = pesquisadores_por_ano['cumulative_researchers'].max()\n",
    "    ratio = max_y1 / max_y2\n",
    "\n",
    "    # Defina um buffer de 10%\n",
    "    buffer = 0.1\n",
    "    \n",
    "    # Criando a figura com as devidas traces\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Adicionando a trace de barras para a quantidade de artigos\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=artigos_por_ano['ANO_PUB'],\n",
    "        y=artigos_por_ano['count'],\n",
    "        name='Total de Artigos',\n",
    "        text=artigos_por_ano['count'],\n",
    "        textposition='outside'\n",
    "    ))\n",
    "\n",
    "    # Adicionando a trace de linha para a soma cumulativa de pesquisadores\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pesquisadores_por_ano['ANO_INGRESSO_FIOCE'],\n",
    "        y=pesquisadores_por_ano['cumulative_researchers'],\n",
    "        yaxis='y2',\n",
    "        name='Soma Cumulativa de Pesquisadores',\n",
    "        mode='lines+markers+text',\n",
    "        text=pesquisadores_por_ano['cumulative_researchers'],\n",
    "        textposition='top center'\n",
    "    ))\n",
    "\n",
    "    # Adicionando a trace de linha tracejada para a média de artigos por pesquisador\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pesquisadores_por_ano['ANO_INGRESSO_FIOCE'],\n",
    "        y=pesquisadores_por_ano['avg_articles_per_researcher'] * ratio,\n",
    "        yaxis='y2',\n",
    "        name='Média de Artigos por Pesquisador',\n",
    "        mode='lines+text',\n",
    "        line=dict(dash='dash'),\n",
    "        text=pesquisadores_por_ano['avg_articles_per_researcher'].round(2),\n",
    "        textposition='top center'\n",
    "    ))\n",
    "\n",
    "    # Atualizando o layout do gráfico\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(\n",
    "            title='Total de Artigos',\n",
    "            range=[0, max_y1 * (1 + buffer)]\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            title='Soma Cumulativa de Pesquisadores e Média de Artigos por Pesquisador',\n",
    "            overlaying='y',\n",
    "            side='right',\n",
    "            range=[0, max_y2 * (1 + buffer)],\n",
    "            tickvals=list(range(0, int(max_y2 * (1 + buffer)), int(max_y2/10))),\n",
    "            ticktext=list(range(0, int(max_y2 * (1 + buffer)), int(max_y2/10)))\n",
    "        ),\n",
    "        xaxis=dict(tickvals=all_years),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            x=0.2,\n",
    "            y=0.99\n",
    "        ),\n",
    "        title='Quantidade de Participações em Artigos, Soma Cumulativa de Pesquisadores e Média de Artigos por Pesquisador por Ano',\n",
    "    )\n",
    "\n",
    "    # Remover linhas de grade\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    fig.update_yaxes(showgrid=False)\n",
    "    \n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1380,   # largura em pixels\n",
    "        height=800   # altura em pixels\n",
    "    )\n",
    "        \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def evolucao_sem_duplicatas(df_artigosperiodo, df_pessoal, sim_limite=0.95):\n",
    "    import Levenshtein\n",
    "\n",
    "    # Função para verificar similaridade entre strings\n",
    "    def is_similar(str1, str2, threshold):\n",
    "        similarity = (len(str1) - Levenshtein.distance(str1, str2)) / float(len(str1))\n",
    "        return similarity >= threshold\n",
    "\n",
    "    # Removendo duplicidades\n",
    "    titles = df_artigosperiodo['ARTIGO'].tolist()\n",
    "    unique_titles = []\n",
    "    for title in titles:\n",
    "        if not any(is_similar(title, utitle, sim_limite) for utitle in unique_titles):\n",
    "            unique_titles.append(title)\n",
    "    df_artigosperiodo = df_artigosperiodo[df_artigosperiodo['ARTIGO'].isin(unique_titles)]\n",
    "\n",
    "    # Remover duplicatas apenas com base no título exato\n",
    "    df_artigosperiodo = df_artigosperiodo.drop_duplicates(subset='ARTIGO')\n",
    "\n",
    "    # Para todos os anos dos dados\n",
    "    min_year = int(df_artigosperiodo['ANO_PUB'].min())\n",
    "    max_year = int(df_artigosperiodo['ANO_PUB'].max())\n",
    "    all_years = list(range(min_year, max_year + 1))\n",
    "\n",
    "    # Quantidade total de artigos por ano\n",
    "    artigos_por_ano = df_artigosperiodo.groupby('ANO_PUB').size().reindex(all_years, fill_value=0).reset_index(name='count')\n",
    "\n",
    "    # Quantidade de pesquisadores únicos que entraram a cada ano\n",
    "    pesquisadores_por_ano = df_pessoal.groupby('ANO_INGRESSO_FIOCE')['NOME'].nunique().reindex(all_years, fill_value=0).reset_index(name='unique_researchers')\n",
    "\n",
    "    # Soma cumulativa de pesquisadores ao longo dos anos\n",
    "    pesquisadores_por_ano['cumulative_researchers'] = pesquisadores_por_ano['unique_researchers'].cumsum()\n",
    "\n",
    "    # Média de artigos por pesquisador\n",
    "    pesquisadores_por_ano['avg_articles_per_researcher'] = artigos_por_ano['count'] / pesquisadores_por_ano['cumulative_researchers']\n",
    "    pesquisadores_por_ano['avg_articles_per_researcher'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "    # Calcular a proporção entre os dois eixos y\n",
    "    max_y1 = artigos_por_ano['count'].max()+10\n",
    "    max_y2 = pesquisadores_por_ano['cumulative_researchers'].max()\n",
    "    ratio  = max_y1 / max_y2\n",
    "\n",
    "    # Defina um buffer de 10%\n",
    "    buffer = 0.1\n",
    "    \n",
    "    # Criando a figura com as devidas traces\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Adicionando a trace de barras para a quantidade de artigos\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=artigos_por_ano['ANO_PUB'],\n",
    "        y=artigos_por_ano['count'],\n",
    "        name='Total de Artigos',\n",
    "        text=artigos_por_ano['count'],\n",
    "        textposition='outside'\n",
    "    ))\n",
    "\n",
    "    # Adicionando a trace de linha para a soma cumulativa de pesquisadores\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pesquisadores_por_ano['ANO_INGRESSO_FIOCE'],\n",
    "        y=pesquisadores_por_ano['cumulative_researchers'],\n",
    "        yaxis='y2',\n",
    "        name='Soma Cumulativa de Pesquisadores',\n",
    "        mode='lines+markers+text',\n",
    "        text=pesquisadores_por_ano['cumulative_researchers'],\n",
    "        textposition='top center'\n",
    "    ))\n",
    "\n",
    "    # Adicionando a trace de linha tracejada para a média de artigos por pesquisador\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pesquisadores_por_ano['ANO_INGRESSO_FIOCE'],\n",
    "        y=pesquisadores_por_ano['avg_articles_per_researcher'] * ratio,\n",
    "        yaxis='y2',\n",
    "        name='Média de Artigos por Pesquisador',\n",
    "        mode='lines+text',\n",
    "        line=dict(dash='dash'),\n",
    "        text=pesquisadores_por_ano['avg_articles_per_researcher'].round(2),\n",
    "        textposition='top center'\n",
    "    ))\n",
    "\n",
    "    # Atualizando o layout do gráfico\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(\n",
    "            title='Total de Artigos',\n",
    "            range=[0, max_y1 * (1 + buffer)]\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            title='Soma Cumulativa de Pesquisadores e Média de Artigos por Pesquisador',\n",
    "            overlaying='y',\n",
    "            side='right',\n",
    "            range=[0, max_y2 * (1 + buffer)],\n",
    "            tickvals=list(range(0, int(max_y2 * (1 + buffer)), int(max_y2/10))),\n",
    "            ticktext=list(range(0, int(max_y2 * (1 + buffer)), int(max_y2/10)))\n",
    "        ),\n",
    "        xaxis=dict(tickvals=all_years),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            x=0.2,\n",
    "            y=0.99\n",
    "        ),\n",
    "        title='Quantidade de Artigos (sem duplicatas), Soma Cumulativa de Pesquisadores e Média de Artigos por Pesquisador por Ano',\n",
    "    )\n",
    "\n",
    "    # Identificar o último ano\n",
    "    last_year = max(all_years)\n",
    "    \n",
    "    # Adicionar a barra transparente para o último ano\n",
    "    mask_last_year = artigos_por_ano['ANO_PUB'] == last_year\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=[last_year],\n",
    "        y=[artigos_por_ano[mask_last_year]['count'].iloc[0]],\n",
    "        name='Total de Artigos no Ano Atual',\n",
    "        text=str(artigos_por_ano[mask_last_year]['count'].iloc[0]),  # Convertendo para string\n",
    "        textposition='outside',\n",
    "        marker=dict(color='rgba(0,0,0,0)',  # Transparente\n",
    "                    line=dict(color='#1f77b4', width=2))  # Borda com a cor padrão e espessura de 2\n",
    "    ))\n",
    "    \n",
    "    # Remover linhas de grade\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    fig.update_yaxes(showgrid=False)\n",
    "    \n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1380,   # largura em pixels\n",
    "        height=800   # altura em pixels\n",
    "    )\n",
    "        \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def boxplot_media_artigos(df_artigosperiodo, df_pessoal):\n",
    "    # Removendo duplicidades\n",
    "    df_artigosperiodo = df_artigosperiodo.drop_duplicates(subset='ARTIGO')\n",
    "    df_artigosperiodo['CURRICULO_INITIALS'] = df_artigosperiodo['NOME'].apply(get_initials)\n",
    "\n",
    "    # Calculando o total de artigos por pesquisador\n",
    "    artigos_por_pesquisador = df_artigosperiodo.groupby('CURRICULO_INITIALS').size()\n",
    "\n",
    "    # Calculando a média de artigos por pesquisador\n",
    "    anos_ativos = df_artigosperiodo.groupby('CURRICULO_INITIALS')['ANO_PUB'].nunique()\n",
    "    media_artigos_por_pesquisador = artigos_por_pesquisador / anos_ativos\n",
    "\n",
    "    # Retirar possíveis infinitos\n",
    "    media_artigos_por_pesquisador.replace(np.inf, 0, inplace=True)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Box plot\n",
    "    fig.add_trace(go.Box(\n",
    "        y=media_artigos_por_pesquisador,\n",
    "        boxpoints='all',\n",
    "        jitter=0.3,\n",
    "        pointpos=0,\n",
    "        name='Média de Artigos por Pesquisador'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Distribuição da Média de Artigos por Pesquisador',\n",
    "        yaxis_title='Média de Artigos por Pesquisador'\n",
    "    )\n",
    "\n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1380,   # largura em pixels\n",
    "        height=800   # altura em pixels\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def evolucao_artigos(df_artigosperiodo, df_pessoal, threshold=0.8):\n",
    "    # Para todos os anos dos dados\n",
    "    min_year = int(df_artigosperiodo['ANO_PUB'].min())\n",
    "    max_year = int(df_artigosperiodo['ANO_PUB'].max())\n",
    "    all_years = list(range(min_year, max_year + 1))\n",
    "\n",
    "    # 1. Usando TF-IDF para calcular similaridade entre os títulos\n",
    "    vectorizer = TfidfVectorizer().fit_transform(df_artigosperiodo['ARTIGO'])\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_matrix = cosine_similarity(vectors)\n",
    "\n",
    "    # 2. Filtrando títulos com similaridade acima do threshold e removendo duplicatas\n",
    "    indices_to_drop = []\n",
    "    for i in range(cosine_matrix.shape[0]):\n",
    "        for j in range(i + 1, cosine_matrix.shape[1]):\n",
    "            if cosine_matrix[i][j] > threshold:\n",
    "                indices_to_drop.append(j)\n",
    "\n",
    "    indices_to_drop = list(set(indices_to_drop))\n",
    "    df_artigosperiodo = df_artigosperiodo.drop(df_artigosperiodo.index[indices_to_drop])\n",
    "\n",
    "    # 3. Plotar o gráfico de evolução\n",
    "\n",
    "    # Agrupando por ano\n",
    "    artigos_por_ano = df_artigosperiodo.groupby('ANO_PUB').size().reset_index(name='count')\n",
    "\n",
    "    # Preenchimento padrão para todos os anos\n",
    "    fill_colors = ['#1f77b4'] * artigos_por_ano.shape[0]\n",
    "\n",
    "    # Para o último ano, deixar sem preenchimento (somente borda)\n",
    "    last_year = artigos_por_ano['ANO_PUB'].iloc[-1]\n",
    "    fill_colors[-1] = 'rgba(0,0,0,0)'\n",
    "\n",
    "    # Plotagem\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Adicionar barras para cada ano\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=artigos_por_ano['ANO_PUB'],\n",
    "        y=artigos_por_ano['count'],\n",
    "        name='Total de Artigos por Ano',\n",
    "        marker=dict(color=fill_colors,\n",
    "                    line=dict(color='#1f77b4', width=2))\n",
    "    ))\n",
    "\n",
    "    # Adicionar linha de média\n",
    "    mean_articles = artigos_por_ano['count'].mean()\n",
    "    years = artigos_por_ano['ANO_PUB'].values\n",
    "\n",
    "    # Excluindo o último ano da linha de média\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=years[0],\n",
    "        y0=mean_articles,\n",
    "        x1=years[-2],\n",
    "        y1=mean_articles,\n",
    "        line=dict(color=\"Red\", width=2, dash=\"dashdot\")\n",
    "    )\n",
    "\n",
    "    # Configurações adicionais do gráfico\n",
    "    fig.update_layout(\n",
    "        title=\"Evolução da quantidade de participação em artigos por ano concluídos, média e artigos do ano em curso\",\n",
    "        xaxis_title=\"Ano\",\n",
    "        yaxis_title=\"Número de Artigos\",\n",
    "        legend_title=\"Legenda\",\n",
    "        xaxis=dict(tickvals=all_years),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            x=0.2,\n",
    "            y=0.99\n",
    "    ),\n",
    "\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingerir dados de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos arquivos de entrada de dados\n",
    "\n",
    "(listas de pessoal e listas da VPEIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestão das planilhas de produção da VPEIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "pathzip = os.path.join(get_path_repo(),'data/xml_zip/')\n",
    "os.listdir(pathzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(pathzip,'fioce_producao_2008.01-2023.06.xlsx')\n",
    "fioce_2008_2013 = pd.read_excel(filepath, skiprows=1, header=0)\n",
    "print(f'Publicações  FioCE 2008_2023: {len(fioce_2008_2013):5}')\n",
    "# fioce_2008_2013.head(3)\n",
    "\n",
    "fiocuz_producao = pd.read_excel(io=pathzip+'fiocruz_unidade-desconhecida.xlsx', skiprows=1, header=0)\n",
    "print(f'Publicações variadas Fiocruz: {len(fiocuz_producao):5}')\n",
    "# fiocuz_producao.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fioce_2008_2013.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fioce_2008_2013['Autores '].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fioce_2008_2013[['Título', 'Ano', 'Resumo', 'Palavras-chaves do autor','Veículo de publicação', 'Tipologia documental', 'Instiuições','Unidades/ Siglas','Autores ','Áreas do conhecimento CNPq (lattes do servidor)']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiocuz_producao.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiocuz_producao[['Título', 'Ano', 'Resumo', 'Palavras-chaves do autor', 'Veículo de publicação', 'DOI ', 'Tipologia documental', 'Instiuições', 'Unidades/ Siglas', \n",
    "                 'Pais da Instiuição ', 'Autores ', 'Áreas do conhecimento CNPq (lattes do servidor)', 'Áreas temáticas identificadas pelo Observatório',]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar todos colaboradores na Fiocruz Ceará"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler apenas os cabeçalhos do arquivo Excel (planilhas de Recursos Humanos)\n",
    "headers = pd.read_excel(pathzip+'fioce_colaboradores-2023.xls', skiprows=3, header=0, nrows=0).columns\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar função para indicar quais colunas devem ser eliminadas na leitura\n",
    "def cols_to_keep(col_name):\n",
    "    return col_name not in ['QUANT','Unnamed: 3','Unnamed: 6','Unnamed: 9','ADICIONAL OCUPACIONAL',\n",
    "                            'EMPRESA/BOLSA/PROGRAMA','GESTOR','ADI','POSSE NA FIOCRUZ',\n",
    "                            'VIGÊNCIA BOLSA/ENCERRAMENTO DO CONTRATO','Unnamed: 17',\n",
    "                            'EMAIL INSTITUCIONAL','EMAIL PESSOAL','GENERO','DATA NASCIMENTO',\n",
    "                            'Unnamed: 22','FORMAÇÃO','ENDEREÇO RESIDENCIAL']\n",
    "\n",
    "# Filtrar cabeçalhos com base na função\n",
    "selected_columns = [col for col in headers if cols_to_keep(col)]\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(pathzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler dados do arquivo Excel do Setor de Recursos Humanos\n",
    "fioce_pessoal = pd.read_excel(pathzip+'fioce_colaboradores-2023.xls', skiprows=3, header=0, usecols=selected_columns)\n",
    "print(f'{len(fioce_pessoal.index)} nomes de colaboradores no total, todos vínculos e status')\n",
    "print(f'{len(fioce_pessoal[\"VÍNCULO\"].unique()):3} tipos de vínculos')\n",
    "list(fioce_pessoal['VÍNCULO'].unique())\n",
    "# fioce_pessoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fioce_pessoal[fioce_pessoal['VÍNCULO']=='COORENAÇÃO GERAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(fioce_pessoal['NÍVEL'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fioce_pessoal[fioce_pessoal['NÍVEL']=='PHD ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montar lista de nomes de todos colaboradores\n",
    "fioce_pessoal['NOME'] = fioce_pessoal['NOME'].str.strip()\n",
    "lista_colaboradores   = fioce_pessoal['NOME']\n",
    "print(f'{len(lista_colaboradores)} colaboradores ao todo, todos vínculos e status')\n",
    "\n",
    "# for i in lista_colaboradores.sort_values().values:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fioce_pessoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fioce_pessoal.sort_values(by='ÁREA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_data = df_artigosperiodo.groupby(['ANO_PUB', 'CURRICULO']).size().reset_index(name='count')\n",
    "# df_areas = fioce_pessoal.groupby(by='ÁREA').size().reset_index(name='SERVIDORES')\n",
    "# df_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fioce_pessoal[fioce_pessoal.ÁREA=='Coordenação da Gestão e Desenvolvimento Institucional (COMPRAS)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudar recortes de grupos de pessoas para análise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar/extrair dados servidores doutores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ler o arquivo Excel usando apenas as colunas selecionadas\n",
    "# fioce_pessoal = pd.read_excel(pathzip+'fioce_colaboradores-2023.xls', skiprows=3, header=0, usecols=selected_columns)\n",
    "# fioce_pessoal['NOME'] = fioce_pessoal['NOME'].str.strip()\n",
    "\n",
    "# filtro1=fioce_pessoal.VÍNCULO=='SERVIDOR'\n",
    "# fioce_pessoal = fioce_pessoal[filtro1]\n",
    "\n",
    "# filtro2=fioce_pessoal.STATUS=='ATIVO'\n",
    "# fioce_pessoal = fioce_pessoal[filtro2]\n",
    "\n",
    "# filtro_combinado = (fioce_pessoal['NÍVEL'] == 'DOUTORADO') | (fioce_pessoal['NÍVEL'] == 'PHD ')\n",
    "# fioce_pessoal_doutores = fioce_pessoal[filtro_combinado]\n",
    "\n",
    "# lista_servidores_doutores = fioce_pessoal_doutores['NOME']\n",
    "# lista_servidores_doutores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Retirar os currículos que não tem artigos publicados\n",
    "# retirar_doutores  = ['Dayane Alves Costa']\n",
    "\n",
    "# ## Montar lista que será buscada no Lattes\n",
    "# lista_servidores_doutores = [item for item in lista_servidores_doutores if item not in retirar_doutores]\n",
    "# print(f'{len(lista_servidores_doutores)} servidores com doutorado a extrair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# t_ini=time.time()\n",
    "# df_secoes_doutores, sucesso_doutores, json_data = extrair_dados(lista_servidores_doutores, mestres=False, assunto=False)\n",
    "\n",
    "# print('-'*50)\n",
    "# print(tempo(t_ini,time.time()), 'Tempo extração dados do currículo')\n",
    "# print(f'{len(sucesso_doutores)/len(lista_servidores_doutores)*100:.2f}% de sucesso na extração dos servidores com doutorado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_secoes_doutores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_doutores = listar_artigos(df_secoes_doutores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_doutores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_contagem_artigos = contar_artigos(df_artigos_doutores)\n",
    "# df_contagem_artigos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar/extrair servidores ativos, nível mestrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ler o arquivo Excel usando apenas as colunas selecionadas\n",
    "# fioce_pessoal = pd.read_excel(pathzip+'fioce_colaboradores-2023.xls', skiprows=3, header=0, usecols=selected_columns)\n",
    "# fioce_pessoal['NOME'] = fioce_pessoal['NOME'].str.strip()\n",
    "\n",
    "# filtro1=fioce_pessoal.VÍNCULO=='SERVIDOR'\n",
    "# fioce_pessoal = fioce_pessoal[filtro1]\n",
    "\n",
    "# filtro2=fioce_pessoal.STATUS=='ATIVO'\n",
    "# fioce_pessoal = fioce_pessoal[filtro2]\n",
    "\n",
    "# filtro3=fioce_pessoal.NÍVEL=='MESTRADO'\n",
    "# fioce_pessoal = fioce_pessoal[filtro3]\n",
    "\n",
    "# lista_servidores_mestres = fioce_pessoal['NOME']\n",
    "# lista_servidores_mestres.sort_values()\n",
    "\n",
    "# print(f'{len(lista_servidores_mestres)} servidores com, no máximo, mestrado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Retirar os currículos que não tem artigos publicados\n",
    "# retirar_mestres = ['Bruno Bezerra Carvalho',\n",
    "#                    'Luis Fernando Pessoa De Andrade']\n",
    "\n",
    "# lista_servidores_mestres = [item for item in lista_servidores_mestres if item not in retirar_mestres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'{len(lista_servidores_mestres)} currículos de mestres associados à Fiocruz Ceará em 2023')\n",
    "# for n,i in enumerate(lista_servidores_mestres):\n",
    "#     print(f'{n:2}: {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# t_ini=time.time()\n",
    "# df_secoes_mestres, sucesso_mestres, json_data = extrair_dados(lista_servidores_mestres, mestres=True, assunto=False)\n",
    "\n",
    "# print('-'*50)\n",
    "# print(tempo(t_ini,time.time()), 'Tempo extração dados do currículo')\n",
    "# print(f'{len(sucesso_mestres)/len(lista_servidores_mestres)*100:.2f}% de sucesso na extração dos servidores com mestrado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_mestres = montar_publicacoes(df_secoes_mestres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_mestres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_artigos_mestres.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_mestres['CURRICULO'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unir os dois grupos de formação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'{len(df_secoes_mestres.index):4} linhas de dados de {len(lista_servidores_mestres)} mestres')\n",
    "# print(f'{len(df_secoes_doutores.index)} linhas de dados de {len(lista_servidores_doutores)} doutores')\n",
    "# df_secoes = pd.concat([df_secoes_mestres, df_secoes_doutores], ignore_index=True)\n",
    "# print(f'{len(df_secoes.index)} linhas de dados de {len(lista_servidores_mestres)+len(lista_servidores_doutores)} servidores no total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_secoes[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_secoes.to_csv(pathout+'df_secoes_doutores_mestres.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_secoes[df_secoes['ROTULOS']=='Nome'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # pathout='C:/kgfioce/output/'\n",
    "# df_secoes_doutores_mestres = pd.read_csv(pathout+'df_secoes_doutores_mestres.csv', sep=\";\")\n",
    "# print(len(df_secoes_doutores_mestres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_servidores_doutorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_servidores_mestrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar lista de Publicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retirar os currículos que não tem artigos publicados, adicionar mestres com mais de 02 participações em publicações\n",
    "# lista_publicadores = []\n",
    "# retirar_doutores   = ['Dayane Alves Costa','Luciana Coelho Serafim']\n",
    "# adicionar_mestres  = ['Anna Carolina Machado Marinho', 'Claudia Stutz Zubieta','Luciana Silvério Alleluia Higino Da Silva','Marlos De Medeiros Chaves']\n",
    "\n",
    "# # Montar lista que será buscada no Lattes\n",
    "# for i in lista_servidores_doutorado:\n",
    "#     if i not in retirar_doutores:\n",
    "#         lista_publicadores.append(i)\n",
    "\n",
    "# for i in adicionar_mestres:\n",
    "#     lista_publicadores.append(i)\n",
    "\n",
    "# print(f'{len(lista_publicadores)} currículos de mestres/doutores associados à Fiocruz Ceará em 2023, com mais de 02 participações em artigos')\n",
    "# for n,i in enumerate(lista_publicadores):\n",
    "#     print(f'{n:2}: {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# t_ini=time.time()\n",
    "# df_secoes_publicadores, sucesso_publicadores, json_data = extrair_dados(lista_publicadores, mestres=True, assunto=False)\n",
    "\n",
    "# print('-'*50)\n",
    "# print(tempo(t_ini,time.time()), 'Tempo extração dados do currículo')\n",
    "# print(f'Extraídos dados de {len(sucesso_publicadores)}, {len(sucesso_publicadores)/len(lista_publicadores)*100:.2f}% de sucesso na extração dos servidores, com mestrado/doutorado e 02 ou mais participações em artigos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_secoes_publicadores.to_csv(pathout+'df_secoes_publicadores.csv', sep=\";\", index=False)\n",
    "# len(df_secoes_publicadores[df_secoes_publicadores['ROTULOS']=='Nome'].index)\n",
    "\n",
    "# for n,i in enumerate(df_secoes_publicadores['CURRICULO'].unique()):\n",
    "#     print(f'{n+1:2} {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_publicacoes = montar_dfpub(df_secoes_publicadores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos, curriculo_to_articles = montar_lista_pub(df_secoes_publicadores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_doutores_mestres, curriculo_to_articles_doutores_mestres = montar_lista_pub(df_secoes_doutores_mestres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar dados extraídos do Lattes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho para o arquivo JSON\n",
    "json_file_path = os.path.join(folder_data_input,'dict_list_fioce.json')\n",
    "print(json_file_path)\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame, se existir\n",
    "df = pd.read_json(json_file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar participações artigos publicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def montar_dfpub(df):\n",
    "    filtro  = 'Artigos completos publicados em periódicos'\n",
    "    artigos = df[(df.ROTULOS == filtro)]\n",
    "    print(f'Total de linhas de dados: {len(artigos)}')\n",
    "\n",
    "    nomes   = df[df.ROTULOS=='Nome'].values\n",
    "    print(f'Total de servidores: {len(nomes)}')  \n",
    "\n",
    "    # Initialize lists to populate DataFrame\n",
    "    curriculos = []\n",
    "    dados_publ = []\n",
    "    artigos_list = []\n",
    "    autores_list = []\n",
    "\n",
    "    remover = ['Ordenar por','Ordem Cronológica','Número de citações Web of science',\n",
    "               'Número de citações Scopus','Numero de citações Scielo','Primeiro autor',\n",
    "               'Impacto JCR','Ordem de Importância','Livros publicados/organizados ou edições']\n",
    "\n",
    "    for n, row in artigos.iterrows():\n",
    "        linha = row['CONTEUDOS']\n",
    "        curriculo = row['CURRICULO']\n",
    "\n",
    "        for i in eval(linha):\n",
    "            if i not in remover and len(i) > 15 and 'Citações:' not in i:\n",
    "                curriculos.append(curriculo)\n",
    "                dados_publ.append(i)\n",
    "                try:\n",
    "                    parts = i.split(' . ')\n",
    "                    if len(parts) == 2:\n",
    "                        autores_list = parts[0]\n",
    "                        autores_list = parts[1]\n",
    "                    else:\n",
    "                        autores_list = ''\n",
    "                        autores_list = ''\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    # Create DataFrame with 'CURRICULO' as the index\n",
    "    df_publicacoes = pd.DataFrame({\n",
    "        'CURRICULO': curriculos,\n",
    "        'ARTIGOS': dados_publ,\n",
    "        'AUTORES': autores_list,\n",
    "        'ARTIGO': artigos_list,\n",
    "    }).set_index('CURRICULO')\n",
    "\n",
    "    return df_publicacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def montar_lista_pub(df):\n",
    "    filtro = 'Artigos completos publicados em periódicos'\n",
    "    artigos = df[(df.ROTULOS == filtro)]\n",
    "\n",
    "    # Initialize lists to hold DataFrame data\n",
    "    curriculos = []\n",
    "    artigos_list = []\n",
    "    anos_pub = []\n",
    "\n",
    "    # Initialize dictionary to hold curriculo to row mapping\n",
    "    curriculo_to_articles = {}\n",
    "\n",
    "    remover = ['Ordenar por', 'Ordem Cronológica', 'Número de citações Web of science',\n",
    "               'Número de citações Scopus', 'Numero de citações Scielo', 'Primeiro autor',\n",
    "               'Impacto JCR', 'Ordem de Importância', 'Livros publicados/organizados ou edições']\n",
    "\n",
    "    for n, row in artigos.iterrows():\n",
    "        linha = row['CONTEUDOS']\n",
    "        curriculo = row['CURRICULO']\n",
    "        artigos_temp = []  # Temporary list to hold articles for the current curriculum\n",
    "\n",
    "        for i in eval(linha):\n",
    "            if i not in remover and len(i) > 15 and 'Citações:' not in i:\n",
    "                # Extract year of publication using regex\n",
    "                match = re.search(r'\\d{4}\\.$', i)\n",
    "                if match:\n",
    "                    ano_pub = match.group().strip('.')\n",
    "                    anos_pub.append(int(ano_pub))\n",
    "                else:\n",
    "                    anos_pub.append(None)\n",
    "\n",
    "                curriculos.append(curriculo)\n",
    "                artigos_list.append(i)\n",
    "                artigos_temp.append(i)\n",
    "        \n",
    "        # Update the dictionary with the current row of articles\n",
    "        curriculo_to_articles[curriculo] = artigos_temp\n",
    "\n",
    "    # Create DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        'CURRICULO': curriculos,\n",
    "        'ARTIGO': artigos_list,\n",
    "        'ANO_PUB': anos_pub\n",
    "    })\n",
    "    \n",
    "    # Set CURRICULO as the index\n",
    "    output_df.set_index('CURRICULO', inplace=True)\n",
    "\n",
    "    return output_df, curriculo_to_articles\n",
    "\n",
    "# df = pd.read_csv(\"your_dataframe.csv\")\n",
    "# output_df, curriculo_to_articles = montar_lista_pub(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curriculo_to_articles_doutores_mestres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_doutores_mestres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pathout=\"C:/kgfioce/output/\"\n",
    "df_secoes_servidores = pd.read_csv(pathout+'df_secoes_servidores.csv', sep=\";\")\n",
    "df_artigos_servidores, curriculo_to_articles_servidores = montar_lista_pub(df_secoes_servidores)\n",
    "print(len(df_artigos_servidores.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotbar_tudo(df_artigos):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Contagem do número de artigos para cada 'CURRICULO'\n",
    "    article_counts = df_artigos.index.value_counts()\n",
    "    \n",
    "    # Cálculo da soma total de artigos\n",
    "    total_articles = article_counts.sum()\n",
    "\n",
    "    # Criação do gráfico de barras\n",
    "    plt.figure(figsize=(19, 12))\n",
    "    ax = article_counts.plot(kind='bar', color='skyblue')\n",
    "\n",
    "    # Adição dos rótulos de colunas\n",
    "    for i, value in enumerate(article_counts):\n",
    "        plt.text(i, value + 0.5, str(value), ha='center', va='bottom')\n",
    "\n",
    "    # Adição do rótulo centralizado com a soma total de artigos\n",
    "    plt.annotate(f'Total de Artigos: {total_articles}', xy=(0.5, 0.95), xycoords='axes fraction',\n",
    "                 fontsize=24, ha='center', va='center')\n",
    "    \n",
    "    # Configurações adicionais\n",
    "    plt.xlabel('CURRICULO')\n",
    "    plt.ylabel('Quantidade de Artigos')\n",
    "    plt.title('Quantidade de participações em artigos por currículo de servidores, em qualquer tempo', fontsize=24)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotbar_tudo(df_artigos_servidores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrar por datas de ingresso na Fiocruz Ceará"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fioce_pessoal['INGRESSO_FIOCE'] = pd.to_datetime(fioce_pessoal['INGRESSO_FIOCE'])\n",
    "\n",
    "# Certificando-se de que 'INGRESSO_FIOCE' é do tipo int (se for data no formato yyyy, por exemplo)\n",
    "fioce_pessoal['ANO_INGRESSO_FIOCE'] = fioce_pessoal['INGRESSO_FIOCE'].dt.year\n",
    "\n",
    "# Merge entre df_artigos e fioce_pessoal usando 'AUTORES' e 'NOME' como chaves\n",
    "merged_df = df_artigos_servidores.merge(fioce_pessoal, left_on='CURRICULO', right_on='NOME', how='inner')\n",
    "\n",
    "# Filtrar as linhas de acordo com a condição do ano de publicação e da data de ingresso\n",
    "df_artigos_servidores_ingresso_fioce = merged_df[merged_df['ANO_PUB'] >= merged_df['ANO_INGRESSO_FIOCE']]\n",
    "\n",
    "# Opcional: Dropar colunas redundantes ou não necessárias, por exemplo, 'NOME' que é igual a 'AUTORES'\n",
    "# result_df = result_df.drop(columns=['NOME'])\n",
    "\n",
    "# Agora, result_df é o dataframe final desejado\n",
    "df_artigos_servidores_ingresso_fioce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos_servidores_ingresso_fioce.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_df = parse_dataframe(df_artigos_servidores_ingresso_fioce)\n",
    "teste_df.iloc[:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of columns you are interested in\n",
    "selected_columns = ['NOME','MATRÍCULA','ARTIGO', 'ÁREA', 'ANO_PUB', 'ANO_INGRESSO_FIOCE']\n",
    "\n",
    "# Create a new DataFrame containing only the selected columns\n",
    "df_servidores_ingresso_fioce = df_artigos_servidores_ingresso_fioce[selected_columns]\n",
    "df_servidores_ingresso_fioce.rename(columns={'ÁREA': 'SETOR_FIOCE'}, inplace=True)\n",
    "df_servidores_ingresso_fioce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the DataFrame to Excel\n",
    "df_artigos_servidores_ingresso_fioce.to_excel(pathout+\"df_artigos_servidores_ingresso_fioce.xlsx\", sheet_name='FiocruzCeara', index=False)\n",
    "df_artigos_servidores_ingresso_fioce.to_csv(pathout+\"df_artigos_servidores_ingresso_fioce.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos = montar_publicacoes(df_secoes_servidores)\n",
    "print(len(df_artigos.index))\n",
    "df_artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos = montar_publicacoes(df_secoes_servidores)\n",
    "\n",
    "# Converta strings vazias para NaN\n",
    "df_artigos['ANO_PUB'].replace('', pd.NA, inplace=True)\n",
    "\n",
    "# Preencha NaN com 0\n",
    "df_artigos['ANO_PUB'].fillna(0, inplace=True)\n",
    "\n",
    "# Agora converta a coluna para int\n",
    "df_artigos['ANO_PUB'] = df_artigos['ANO_PUB'].astype(int)\n",
    "\n",
    "# Salve o DataFrame\n",
    "df_artigos.to_csv(pathout+'df_artigos.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes das funções de separação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dicts(teste_dict, verbose=False):\n",
    "    \n",
    "    try:\n",
    "        dict2 = parse_string(teste_dict[\"input\"], verbose)\n",
    "    except Exception as e:\n",
    "        print('Nenhum separador funcionou bem para o caso:')\n",
    "        print(e)\n",
    "        print(teste_dict)\n",
    "        return -1.0  # Retorna -1.0 em caso de erro\n",
    "\n",
    "    dict1 = teste_dict[\"expected_output\"]\n",
    "    indices = len(dict1.keys())\n",
    "    sucesso = 0\n",
    "    desvios = 0\n",
    "    lenght = 75\n",
    "    for key in dict1.keys() & dict2.keys():  # Intersecção de chaves dos dois dicionários\n",
    "        if dict1[key] != dict2[key]:\n",
    "            desvios += 1\n",
    "            if verbose:\n",
    "                print('-' * lenght)\n",
    "                print('FALHOU na divisão de:')\n",
    "                print(f'Campo \"{key}\":')\n",
    "                print(f'   Esperado: {dict1[key]}')\n",
    "                print(f'     Obtido: {dict2[key]}')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('-' * lenght)\n",
    "                print('SUCESSO na divisão de:')\n",
    "                print(f'Campo \"{key}\":')\n",
    "                print(f'   Esperado: {dict1[key]}')\n",
    "                print(f'     Obtido: {dict2[key]}')\n",
    "            sucesso += 1\n",
    "    \n",
    "    percentual_sucesso = f'{sucesso / indices * 100:.2f}'\n",
    "    \n",
    "    return percentual_sucesso\n",
    "\n",
    "\n",
    "def run_testes(test_dict, verbose=False):\n",
    "    resultados = {}\n",
    "    if not isinstance(test_dict, dict):\n",
    "        print(\"O parâmetro deve ser um dicionário\")\n",
    "        return resultados\n",
    "    \n",
    "    for name, value in test_dict.items():\n",
    "        result = compare_dicts(value, verbose)\n",
    "        if result is not None:  # Verifica se o resultado é None\n",
    "            print(f'Resultado do teste {name:3}: {result}% de conformidade entre obtido e esperado')\n",
    "            resultados[name] = float(result)\n",
    "        else:\n",
    "            print(f'Falha ao executar o teste {name}')\n",
    "            \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dictionary = {\n",
    "    \"t1\": {\n",
    "        'input': \"PEREIRA, F. O. ; ARRUA, J. M. M. ; RIBEIRO-FILHO, J. . In vitro and ex vivo antibiofilm activity of riparin 1, and its nor and dinor homologs, against dermatophytes. MYCOLOGIA, p. 1-10, 2023.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['PEREIRA, F. O.', 'ARRUA, J. M. M.', 'RIBEIRO-FILHO, J.'],\n",
    "            'title': 'In vitro and ex vivo antibiofilm activity of riparin 1, and its nor and dinor homologs, against dermatophytes',\n",
    "            'journal': 'MYCOLOGIA',\n",
    "            'local': '',\n",
    "            'volume': '',\n",
    "            'page': '1-10',\n",
    "            'year': '2023'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t2\": {\n",
    "        'input':\"VIEIRA-MEYER, Anya Pimentel Gomes Fernandes. RAIZES E PONTES NO FORTALECIMENTO DO SUS. Revista da ESP, v. 17, p. e1712, 2023.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['VIEIRA-MEYER, Anya Pimentel Gomes Fernandes'],\n",
    "            'title': 'RAIZES E PONTES NO FORTALECIMENTO DO SUS',\n",
    "            'journal': 'Revista da ESP',\n",
    "            'local': '',\n",
    "            'volume': '17',\n",
    "            'page': 'e1712',\n",
    "            'year': '2023'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t3\": {\n",
    "        'input':\"Nicolete, Roberto; Rius, Cristina ; Piqueras, Laura ; Jose, Peter J ; Sorgi, Carlos A ; Soares, Edson G ; Sanz, Maria J ; Faccioli, Lúcia H . Leukotriene B4-loaded microspheres: a new therapeutic strategy to modulate cell activation, v. 9, p. 36, 2008.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['Nicolete, Roberto', 'Rius, Cristina', 'Piqueras, Laura','Jose, Peter J', 'Sorgi, Carlos A', 'Soares, Edson G', 'Sanz, Maria J', 'Faccioli, Lúcia H'],\n",
    "            'title': 'Leukotriene B4-loaded microspheres: a new therapeutic strategy to modulate cell activation',\n",
    "            'journal': '',\n",
    "            'local': '',\n",
    "            'volume': '9',\n",
    "            'page': '36',\n",
    "            'year': '2008'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t4\": {\n",
    "        'input':\"TELES, Y. C. F. ; RIBEIRO-FILHO, J. ; BOZZA, Patrícia T. ; AGRA, M. F. ; SIHERI, W. ; IGOLI, J. O. ; GRAY, A. I. ; SOUZA, M. F. V. . Phenolic constituents from (L.) C. Presl. and anti-inflammatory activity of 7,4--di- -methylisoscutellarein. Natural Product Research (Print), p. 1-5, 2015.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['TELES, Y. C. F.', 'RIBEIRO-FILHO, J.', 'BOZZA, Patrícia T.', 'AGRA, M. F.', 'SIHERI, W.', 'IGOLI, J. O.', 'GRAY, A. I.', 'SOUZA, M. F. V.'],\n",
    "            'title': 'Phenolic constituents from (L.) C Presl and anti-inflammatory activity of 7,4--di- -methylisoscutellarein',\n",
    "            'journal': 'Natural Product Research (Print)',\n",
    "            'local': '',\n",
    "            'volume': '',\n",
    "            'page': '1-5',\n",
    "            'year': '2015'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t5\": {\n",
    "        'input':\"BILD, N. ; CHAPEAUROUGE, D. A. ; GFELLER, S. ; BIENZ, S. . The [M-1]+ quasi-molecular Ion in Chemical Ionization Mass Spectrometry, Fragmentation of Bis (benzyloxy) silanes by Intramolecular Reactions. Org. Mass Spectrom., v. 27, p. 896-900, 1992.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['BILD, N.', 'CHAPEAUROUGE, D. A.', 'GFELLER, S.','BIENZ, S.'],\n",
    "            'title': 'The [M-1]+ quasi-molecular Ion in Chemical Ionization Mass Spectrometry, Fragmentation of Bis (benzyloxy) silanes by Intramolecular Reactions',\n",
    "            'journal': 'Mass Spectrom',\n",
    "            'local': '',\n",
    "            'volume': '27',\n",
    "            'page': '896-900',\n",
    "            'year': '1992'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t6\": {\n",
    "        'input':\"DARUGE, E. ; MIYAJIMA, F. ; PARANHOS, L. R. ; DUZ, S. . Identificação Humana por meio de Superposição de Imagens: Caso Clínico. JBC. Jornal Brasileiro de Clínica & Estética em Odontologia, Curitiba - PR, v. 3, n.Mar/Abr, p. 90-96, 1999.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['DARUGE, E.', 'MIYAJIMA, F.', 'PARANHOS, L. R.','DUZ, S.'],\n",
    "            'title': 'Identificação Humana por meio de Superposição de Imagens: Caso Clínico',\n",
    "            'journal': 'Jornal Brasileiro de Clínica & Estética em Odontologia',\n",
    "            'local': 'Curitiba - PR',\n",
    "            'volume': '3',\n",
    "            'page': '90-96',\n",
    "            'year': '1999'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t7\": {\n",
    "        'input':\"SALOMON, TASSILA BUSS, LEWIS F WHITTAKER, CHARLES PRETE, CARLOS A OIKAWA, MARCIO K PEREIRA, RAFAEL HM MOURA, ISABEL CG DELERINO, LUCAS BARRAL-NETTO, MANOEL TAVARES, NATALIA M FRANCA, RAFAEL FO BOAVENTURA, VIVIANE S MIYAJIMA, FABIO MENDRONE-JUNIOR, ALFREDO DE ALMEIDA-NETO, CESAR SALLES, NANCI A FERREIRA, SUZETE C FLADZINSKI, KARINE A DE SOUZA, LUANA M SCHIER, LUCIANE K INOUE, PATRICIA M XABREGAS, LILYANE A CRISPIM, MYUKI AE FRAIJI, NELSON ARAUJO, FERNANDO LV , et al. ; SARS-CoV-2 antibody dynamics in blood donors and COVID-19 epidemiology in eight Brazilian state capitals: A serial cross-sectional study. eLife, v. 11, p. e78233, 2022.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['SALOMON, TASSILA BUSS, LEWIS F WHITTAKER, CHARLES PRETE, CARLOS A OIKAWA, MARCIO K PEREIRA, RAFAEL HM MOURA, ISABEL CG DELERINO, LUCAS BARRAL-NETTO, MANOEL TAVARES, NATALIA M FRANCA, RAFAEL FO BOAVENTURA, VIVIANE S MIYAJIMA, FABIO MENDRONE-JUNIOR, ALFREDO DE ALMEIDA-NETO, CESAR SALLES, NANCI A FERREIRA, SUZETE C FLADZINSKI, KARINE A DE SOUZA, LUANA M SCHIER, LUCIANE K INOUE, PATRICIA M XABREGAS, LILYANE A CRISPIM, MYUKI AE FRAIJI, NELSON ARAUJO, FERNANDO LV , et al'],\n",
    "            'title': 'SARS-CoV-2 antibody dynamics in blood donors and COVID-19 epidemiology in eight Brazilian state capitals: A serial cross-sectional study',\n",
    "            'journal': 'eLife',\n",
    "            'local': '',\n",
    "            'volume': '11',\n",
    "            'page': 'e78233',\n",
    "            'year': '2022'\n",
    "        }\n",
    "    }, \n",
    "\n",
    "    \"t8\": {\n",
    "        'input':\"PEREIRA, F. O. ; ARRUA, J. M. M. ; RIBEIRO-FILHO, J. . In vitro and ex vivo antibiofilm activity of riparin 1, and its nor and dinor homologs, against dermatophytes. MYCOLOGIA, p. 1-10, 2023.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['PEREIRA, F. O.', 'ARRUA, J. M. M.', 'RIBEIRO-FILHO, J.'],\n",
    "            'title': 'In vitro and ex vivo antibiofilm activity of riparin 1, and its nor and dinor homologs, against dermatophytes',\n",
    "            'journal': 'MYCOLOGIA',\n",
    "            'local': '',\n",
    "            'volume': '',\n",
    "            'page': '1-10',\n",
    "            'year': '2023'\n",
    "        }\n",
    "    },       \n",
    "\n",
    "    \"t9\": {\n",
    "        'input':\"MARTIN, A. L. A. R. ; PEREIRA, R. L. S. ; RIBEIRO-FILHO, J. ; MENEZES, I. R. A. ; COUTINHO, H. D. M. ; FONTELES, M. M. F. . In vitro and in silico evidences about the inhibition of MepA efflux pump by coumarin derivatives. MICROBIAL PATHOGENESIS, p. 106246, 2023.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['MARTIN, A. L. A. R.', 'PEREIRA, R. L. S.', 'RIBEIRO-FILHO, J.', 'MENEZES, I. R. A.', 'COUTINHO, H. D. M.', 'FONTELES, M. M. F.'],\n",
    "            'title': 'In vitro and in silico evidences about the inhibition of MepA efflux pump by coumarin derivatives',\n",
    "            'journal': 'MICROBIAL PATHOGENESIS',\n",
    "            'local': '',\n",
    "            'volume': '',\n",
    "            'page': '106246',\n",
    "            'year': '2023'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t10\": {\n",
    "        'input':\"TALKOWSKI, MICHAEL E. MCCANN, KATHLEEN L. CHEN, MICHAEL MCCLAIN, LORA BAMNE, MIKHIL WOOD, JOEL CHOWDARI, KODAVALI V. WATSON, ANNIE PRASAD, KONASALE M. KIROV, GEORGE GEORGIEVA, LYUDMILLA TONCHEVA, DRAGA MANSOUR, HADER LEWIS, DAVID A. OWEN, MICHAEL O'DONOVAN, MICHAEL PAPASAIKAS, PANAGIOTIS SULLIVAN, PATRICK RUDERFER, DOUGLAS YAO, JEFFREY K LEONARD, SHERRY THOMAS, PRAMOD MIYAJIMA, FABIO QUINN, JOHN LOPEZ, A. JAVIER , et al. ; Fine-mapping reveals novel alternative splicing of the dopamine transporter. American Journal of Medical Genetics. Part B, Neuropsychiatric Genetics, v. 153B, p. 1434-1447, 2010.\",\n",
    "        'expected_output': {\n",
    "            'authors': [\"TALKOWSKI, MICHAEL E. MCCANN, KATHLEEN L. CHEN, MICHAEL MCCLAIN, LORA BAMNE, MIKHIL WOOD, JOEL CHOWDARI, KODAVALI V. WATSON, ANNIE PRASAD, KONASALE M. KIROV, GEORGE GEORGIEVA, LYUDMILLA TONCHEVA, DRAGA MANSOUR, HADER LEWIS, DAVID A. OWEN, MICHAEL O'DONOVAN, MICHAEL PAPASAIKAS, PANAGIOTIS SULLIVAN, PATRICK RUDERFER, DOUGLAS YAO, JEFFREY K LEONARD, SHERRY THOMAS, PRAMOD MIYAJIMA, FABIO QUINN, JOHN LOPEZ, A. JAVIER , et al\"],\n",
    "            'title': 'Fine-mapping reveals novel alternative splicing of the dopamine transporter',\n",
    "            'journal': 'American Journal of Medical Genetics. Part B, Neuropsychiatric Genetics',\n",
    "            'local': '',\n",
    "            'volume': '153B',\n",
    "            'page': '1434-1447',\n",
    "            'year': '2010'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t11\": {\n",
    "        'input':\"DA SILVA, LUCAS YURE SANTOS ; PAULO, CICERA LAURA ROQUE ; MOURA, TALYSSON FELISMINO ; ALVES, DANIEL SAMPAIO ; PESSOA, RENATA TORRES ; ARAÚJO, ISAAC MOURA ; DE MORAIS OLIVEIRA-TINTINO, CÍCERA DATIANE ; TINTINO, SAULO RELISON ; NONATO, CARLA DE FATIMA ALVES ; DA COSTA, JOSÉ GALBERTO MARTINS ; RIBEIRO-FILHO, JAIME ; COUTINHO, HENRIQUE DOUGLAS MELO ; KOWALSKA, GRA'YNA ; MITURA, PRZEMYS'AW ; BAR, MAREK ; KOWALSKI, RADOS'AW ; MENEZES, IRWIN ROSE ALENCAR DE . Antibacterial Activity of the Essential Oil of Piper tuberculatum Jacq. Fruits against Multidrug-Resistant Strains: Inhibition of Efflux Pumps and β-Lactamase. PLANTS, v. 12, p. 2377, 2023.\",\n",
    "        'expected_output': {\n",
    "            'authors': [\"DA SILVA, LUCAS YURE SANTOS ; PAULO, CICERA LAURA ROQUE ; MOURA, TALYSSON FELISMINO ; ALVES, DANIEL SAMPAIO ; PESSOA, RENATA TORRES ; ARAÚJO, ISAAC MOURA ; DE MORAIS OLIVEIRA-TINTINO, CÍCERA DATIANE ; TINTINO, SAULO RELISON ; NONATO, CARLA DE FATIMA ALVES ; DA COSTA, JOSÉ GALBERTO MARTINS ; RIBEIRO-FILHO, JAIME ; COUTINHO, HENRIQUE DOUGLAS MELO ; KOWALSKA, GRA'YNA ; MITURA, PRZEMYS'AW ; BAR, MAREK ; KOWALSKI, RADOS'AW ; MENEZES, IRWIN ROSE ALENCAR DE\"],\n",
    "            'title': 'Antibacterial Activity of the Essential Oil of Piper tuberculatum Jacq. Fruits against Multidrug-Resistant Strains: Inhibition of Efflux Pumps and β-Lactamase',\n",
    "            'journal': 'PLANTS',\n",
    "            'local': '',\n",
    "            'volume': '12',\n",
    "            'page': '2377',\n",
    "            'year': '2023'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t12\": {\n",
    "        'input':\"MIYAJIMA, F.; OLLIER, W. ; MAYES, A. ; JACKSON, A. ; THACKER, N. ; RABBITT, P. ; Pendleton, N. ; HORAN, M. ; PAYTON, A. . Brain-derived neurotrophic factor polymorphism Val66Met influences cognitive abilities in the elderly. GENES, BRAIN AND BEHAVIOR (ONLINE), v. ON, p. 31/10/2007, 2007.\",\n",
    "        'expected_output': {\n",
    "            'authors': [\"MIYAJIMA, F.; OLLIER, W. ; MAYES, A. ; JACKSON, A. ; THACKER, N. ; RABBITT, P. ; Pendleton, N. ; HORAN, M. ; PAYTON, A\"],\n",
    "            'title': 'Brain-derived neurotrophic factor polymorphism Val66Met influences cognitive abilities in the elderly',\n",
    "            'journal': 'GENES, BRAIN AND BEHAVIOR (ONLINE)',\n",
    "            'local': '',\n",
    "            'volume': 'ON',\n",
    "            'page': '31/10/2007',\n",
    "            'year': '2007'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t13\": {\n",
    "        'input':\"MIYAJIMA, F.; LIMA, V. P. . Exames em DNA: a superestimação da inovação. Revista de Direito (Itatiba), Leme - SP, v. 1, n.2002, p. 63-65, 2002.\",\n",
    "        'expected_output': {\n",
    "            'authors': [\"MIYAJIMA, F.; LIMA, V. P.\"],\n",
    "            'title': 'Exames em DNA: a superestimação da inovação',\n",
    "            'journal': 'Revista de Direito (Itatiba)',\n",
    "            'local': 'Leme - SP',\n",
    "            'volume': '1',\n",
    "            'page': '63-65',\n",
    "            'year': '2002'\n",
    "        }\n",
    "    },                  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_testes(test_dictionary, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa todos os testes e armazena os resultados em 'resultados_todos_testes'\n",
    "resultados_todos_testes = run_testes(test_dictionary)\n",
    "\n",
    "# Filtra os testes que não atingiram 100% de conformidade\n",
    "testes_filtrados = {k: v for k, v in resultados_todos_testes.items() if v < 100.0}\n",
    "\n",
    "# Exibe os testes filtrados\n",
    "print(\"Testes com menos de 100% de conformidade:\", testes_filtrados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_testes(testes_filtrados, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_excecao = {\"t7\": \"SALOMON, TASSILA BUSS, LEWIS F WHITTAKER, CHARLES PRETE, CARLOS A OIKAWA, MARCIO K PEREIRA, RAFAEL HM MOURA, ISABEL CG DELERINO, LUCAS BARRAL-NETTO, MANOEL TAVARES, NATALIA M FRANCA, RAFAEL FO BOAVENTURA, VIVIANE S MIYAJIMA, FABIO MENDRONE-JUNIOR, ALFREDO DE ALMEIDA-NETO, CESAR SALLES, NANCI A FERREIRA, SUZETE C FLADZINSKI, KARINE A DE SOUZA, LUANA M SCHIER, LUCIANE K INOUE, PATRICIA M XABREGAS, LILYANE A CRISPIM, MYUKI AE FRAIJI, NELSON ARAUJO, FERNANDO LV , et al. ; SARS-CoV-2 antibody dynamics in blood donors and COVID-19 epidemiology in eight Brazilian state capitals: A serial cross-sectional study. eLife, v. 11, p. e78233, 2022.\"}\n",
    "\n",
    "run_testes(input_excecao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converta strings vazias para NaN\n",
    "df_artigos['ANO_PUB'].replace('', pd.NA, inplace=True)\n",
    "\n",
    "# Preencha NaN com 0\n",
    "df_artigos['ANO_PUB'].fillna(0, inplace=True)\n",
    "\n",
    "# Agora converta a coluna para int\n",
    "df_artigos['ANO_PUB'] = df_artigos['ANO_PUB'].astype(int)\n",
    "\n",
    "# Salve o DataFrame\n",
    "df_artigos.to_csv(pathout+'df_artigos.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_servidores_ingresso_fioce.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m,i in enumerate(df_secoes_publicadores['CURRICULO'].unique()):\n",
    "#     if i not in df_artigos['CURRICULO'].unique():\n",
    "#         print(f'Não encontrado nome de: \"{i}\" na lista e artigos montados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos[df_artigos['TITULO'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos[600:1200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrar por datas para recorte mestres/doutores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fioce_pessoal['INGRESSO_FIOCE'] = pd.to_datetime(fioce_pessoal['INGRESSO_FIOCE'])\n",
    "\n",
    "# # Certificando-se de que 'INGRESSO_FIOCE' é do tipo int (se for data no formato yyyy, por exemplo)\n",
    "# fioce_pessoal['ANO_INGRESSO_FIOCE'] = fioce_pessoal['INGRESSO_FIOCE'].dt.year\n",
    "\n",
    "# # Merge entre df_artigos e fioce_pessoal usando 'AUTORES' e 'NOME' como chaves\n",
    "# merged_df = df_artigos_doutores_mestres.merge(fioce_pessoal, left_on='CURRICULO', right_on='NOME', how='inner')\n",
    "\n",
    "# # Filtrar as linhas de acordo com a condição do ano de publicação e da data de ingresso\n",
    "# df_doutores_mestres_ingresso_fioce = merged_df[merged_df['ANO_PUB'] >= merged_df['ANO_INGRESSO_FIOCE']]\n",
    "\n",
    "# # Opcional: Dropar colunas redundantes ou não necessárias, por exemplo, 'NOME' que é igual a 'AUTORES'\n",
    "# # result_df = result_df.drop(columns=['NOME'])\n",
    "\n",
    "# # Agora, result_df é o dataframe final desejado\n",
    "# df_doutores_mestres_ingresso_fioce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_doutores_mestres_ingresso_fioce.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the list of columns you are interested in\n",
    "# selected_columns = ['NOME','MATRÍCULA','ARTIGO', 'ÁREA', 'ANO_PUB', 'ANO_INGRESSO_FIOCE']\n",
    "\n",
    "# # Create a new DataFrame containing only the selected columns\n",
    "# df_artigos_doutores_mestres = df_artigos_doutores_mestres[selected_columns]\n",
    "# df_artigos_doutores_mestres.rename(columns={'ÁREA': 'SETOR_FIOCE'}, inplace=True)\n",
    "# df_artigos_doutores_mestres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exporting the DataFrame to Excel\n",
    "# df_artigos_doutores_mestres.to_excel(pathout+\"df_artigos_doutores_mestres.xlsx\", sheet_name='FiocruzCeara', index=False)\n",
    "# df_artigos_doutores_mestres.to_csv(pathout+\"df_artigos_doutores_mestres.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar recorte de Profissionais Publicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the list of columns you are interested in\n",
    "# selected_columns = ['NOME','MATRÍCULA','ARTIGO', 'ÁREA', 'ANO_PUB', 'ANO_INGRESSO_FIOCE']\n",
    "\n",
    "# # Create a new DataFrame containing only the selected columns\n",
    "# filtered_df = result_df[selected_columns]\n",
    "# filtered_df.rename(columns={'ÁREA': 'SETOR_FIOCE'}, inplace=True)\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exporting the DataFrame to Excel\n",
    "# filtered_df.to_excel(pathout+\"artigos_desde_ano_ingresso_fiocruz_ceara.xlsx\", sheet_name='FiocruzCeara', index=False)\n",
    "# filtered_df.to_csv(pathout+\"artigos_desde_ano_ingresso_fiocruz_ceara.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(result_df['ARTIGO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publicacoes = result_df['ARTIGO'].to_list()\n",
    "# len(publicacoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar lista específica de nomes para extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_busca=[]\n",
    "# for i in lista_servidores_doutores:\n",
    "#     if i != 'Raphael Trevizani Roque De Oliveira':\n",
    "#         lista_busca.append(i)\n",
    "#     else:\n",
    "#         lista_busca.append('Raphael Trevizani')\n",
    "\n",
    "# lista_busca.sort()\n",
    "# # Salvar lista de servidores em arquivo CSV\n",
    "# with open(pathcsv+'lista_servidores-fioce.csv', 'w', newline='') as arquivo:\n",
    "#     escritor = csv.writer(arquivo)\n",
    "#     for item in lista_busca:\n",
    "#         escritor.writerow([item])\n",
    "\n",
    "# # Criar lista de busca com interesse de pesquisa\n",
    "# retirar = ['Carlos Jose Araujo Pinheiro', 'Charles Cerqueira De Abreu', 'Dayane Alves Costa',\n",
    "#            'Ezequiel Valentim De Melo','João Baptista Estabile Neto','Luciano Pinto Zorzanelli',\n",
    "#            'Luciana Coelho Serafim', 'Nilton Luiz Costa Machado','Renato Caldeira De Souza',\n",
    "#            'Sergio Dos Santos Reis']\n",
    "\n",
    "# lista_busca = [item for item in lista_busca if item not in retirar]\n",
    "# with open(pathcsv+'lista_lattes-fioce.csv', 'w', newline='') as arquivo:\n",
    "#     escritor = csv.writer(arquivo)\n",
    "#     for item in lista_busca:\n",
    "#         escritor.writerow([item])\n",
    "\n",
    "# # Ler do arquivo CSV salvo para dataframe\n",
    "# df_busca = pd.read_csv(pathcsv+'lista_lattes-fioce.csv', header=None)\n",
    "# df_busca.columns = ['SERVIDORES_FIOCE']\n",
    "# print(f'{len(df_busca.index)} currículos a extrair')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair dados visando avaliar edital FUNCAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# ## Arquivos fontes para orientadores FUNCAP\n",
    "# arquivo_qualis     = 'classificações_publicadas_todas_as_areas_avaliacao1672761192111.csv'\n",
    "# lista_orientadores = pd.read_csv(pathcsv+'lista_orientadores.csv')\n",
    "\n",
    "# lista_busca = lista_orientadores['ORIENTADOR'].unique()\n",
    "# lista_busca.sort()\n",
    "# print(f'Total de pesquisadores a extrair: {len(lista_orientadores[1:])}')\n",
    "# for i in lista_busca:\n",
    "#     print('    ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extrair_artigos(lista_nomes, mestres=True, assunto=False):\n",
    "#     '''Extrai todas as informações brutas de publicações (artigos e livros) de cada currículo da Plataforma Lattes do CNPQ\n",
    "#      Recebe: Um nome a ser buscado na base do currículo Lattes\n",
    "#     Utiliza: Funções: definir_filtros(), montar_dfcolab_linhas()\n",
    "#     Retorna: Três dataframes: df_identificacao com dados da identificação; df_dados com dados de todas produções; e df_colabartigos com dados das colaborações em artigos\n",
    "#     Autor: Marcos Aires (Jan 2022)\n",
    "#     '''\n",
    "#     import time\n",
    "#     from datetime import date\n",
    "    \n",
    "#     # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "#     t0=time.time()\n",
    "    \n",
    "#     ## INÍCIO DO SCRIPT DE RASPAGEM DA PÁGINA HTML DO CURRÍCULO LATTES\n",
    "#     ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "#     options   = Options()\n",
    "#     # options.add_argument(\"--headless\")\n",
    "#     browser   = webdriver.Chrome(options=options)\n",
    "#     url_buscaespecialista = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=false&textoBusca='\n",
    "#     browser.get(url_buscaespecialista) # acessa a url de busca do CNPQ   \n",
    "#     # browser.set_window_position(2100, 0)\n",
    "#     # browser.set_window_size(1096, 1896)\n",
    "#     # browser.maximize_window()\n",
    "    \n",
    "#     browser.set_window_position(-20, -10)\n",
    "#     size          = browser.get_window_size()\n",
    "#     width1        = size.get(\"width\")\n",
    "#     height1       = size.get(\"height\")\n",
    "#     browser.set_window_size(170, 1896)\n",
    "#     browser.mouse = webdriver.ActionChains(browser)\n",
    "    \n",
    "#     delay   = 10  # seconds \n",
    "#     buscas        = []\n",
    "#     resultados    = []\n",
    "    \n",
    "#     df_dados          = pd.DataFrame()   \n",
    "#     rotulos           = []\n",
    "#     conteudos         = []\n",
    "#     parcial_rotulos   = []\n",
    "#     parcial_conteudos = []\n",
    "#     sucesso           = []\n",
    "#     falhas            = []\n",
    "#     impactos = []\n",
    "#     linhas_dados = []\n",
    "#     artigos = []\n",
    "\n",
    "#     df_parcial = pd.DataFrame({     \n",
    "#             'NOMES': pd.Series(sucesso),\n",
    "#             'ROTULOS': pd.Series(rotulos),\n",
    "#             'CONTEUDOS': pd.Series(conteudos),                    \n",
    "#         })\n",
    "\n",
    "#     t1=time.time()\n",
    "#     print(tempo(t0,t1), 'Tempo de conexão ao servidor do CNPq')\n",
    "#     time.sleep(0.00001)\n",
    "\n",
    "#     count=0\n",
    "#     for NOME in lista_nomes:\n",
    "#         print('-'*100)\n",
    "#         count+=1\n",
    "#         t2       = time.time()\n",
    "#         tdec     = np.round(t2-t0,2)\n",
    "#         restante = len(lista_nomes)-count\n",
    "#         print(f'Extraindo currículo {count}/{len(lista_nomes)}. Resta {restante}. Decorrido:{horas(tdec)}. Previsão de término em {horas(np.round(tdec/count,0)*(restante+1))}')\n",
    "        \n",
    "#         # Definir filtros para busca de nomes\n",
    "#         definir_filtros(browser, mestres, assunto)\n",
    "#         preencher_busca(browser, delay, NOME)      \n",
    "#         window_before  = browser.current_window_handle\n",
    "#         limite=5\n",
    "#         ## Clicar no botão abrir currículo e mudar de aba\n",
    "#         try:\n",
    "#             ## Aguarda, encontra, clica em buscar nome\n",
    "#             link_nome    = achar_busca(browser, delay)\n",
    "#             nome_buscado = []\n",
    "#             nome_achado  = []\n",
    "#             nome_buscado.append(NOME)\n",
    "            \n",
    "#             if link_nome.text == None:\n",
    "#                 xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "#                 # 'Stale file handle'\n",
    "#                 print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "#                 retry(WebDriverWait(browser, delay).until(\n",
    "#                     EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "#                 #    expected_ex_type=ZeroDivisionError, \n",
    "#                    wait_ms=200,\n",
    "#                    limit=limite, \n",
    "#                 #    logger=logger, \n",
    "#                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "#             try:\n",
    "#                 ActionChains(browser).click(link_nome).perform()\n",
    "#                 nome_achado.append(link_nome.text)\n",
    "#             except:\n",
    "#                 print(f'Currículo não encontrado para: {NOME}.')\n",
    "#                 return\n",
    "            \n",
    "#             retry(WebDriverWait(browser, delay).until(\n",
    "#                 EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "#                 #    expected_ex_type=ZeroDivisionError, \n",
    "#                    wait_ms=200,\n",
    "#                    limit=limite, \n",
    "#                 #    logger=logger, \n",
    "#                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "            \n",
    "#             btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "#                 EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "#             time.sleep(0.2)\n",
    "#             ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "#             ## Gerenciamento das janelas abertas no browser\n",
    "#             WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "#             window_after = browser.window_handles\n",
    "#             new_window   = [x for x in window_after if x != window_before][0]\n",
    "#             browser.switch_to.window(new_window)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print('Erro',e)\n",
    "#             print('Tentando nova requisição ao servidor')\n",
    "#             time.sleep(1)\n",
    "#             btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "#                 EC.presence_of_element_located((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "#             ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "#             WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "#             ## Gerenciamento das janelas abertas no browser\n",
    "#             window_after = browser.window_handles\n",
    "#             new_window   = [x for x in window_after if x != window_before][0]\n",
    "#             browser.switch_to.window(new_window)\n",
    "#             time.sleep(1)\n",
    "\n",
    "#         t3=time.time()\n",
    "\n",
    "#         ## O objeto elementos_id abaixo é uma lista de elementos onde as informações de identificação estão contidas\n",
    "#         # acessado através do marcador xpath='//div[@class=\"infpessoa\"]' no HTML para extrair de cada pesquisador\n",
    "#         time.sleep(1)\n",
    "#         xpath='//div[@class=\"infpessoa\"]'\n",
    "#         WebDriverWait(browser, delay).until(\n",
    "#                 EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "#         elementos_id = browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "#         # Fazer com que a primeira informação para cada pesquisador seja o caminho para sua foto e dados de identificação\n",
    "#         try:\n",
    "#             css_selector='.foto'\n",
    "#             link_foto=WebDriverWait(browser, delay).until(\n",
    "#                 EC.visibility_of_element_located((By.CSS_SELECTOR, \".foto\"))).get_attribute(\"src\")\n",
    "#             rotulos.append('Link Foto:')\n",
    "#             conteudos.append(link_foto)            \n",
    "\n",
    "#         except Exception as e:\n",
    "#             traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "#             print('  !!Erro ao extrair imagem do currículo:',e,'\\n', traceback_str)\n",
    "\n",
    "#         for i in range(len(elementos_id)):\n",
    "#             dados = elementos_id[i].text.split('\\n')\n",
    "#             for i in range(len(dados)):\n",
    "#                 if i==0:\n",
    "#                     rotulos.append('Nome completo:')\n",
    "#                     conteudos.append(dados[i])\n",
    "#                 elif 'Bolsista' in dados[i]:\n",
    "#                     rotulos.append('Bolsista CNPq:')\n",
    "#                     conteudos.append(dados[i])\n",
    "#                 elif 'Endereço para acessar este CV: ' in dados[i]:\n",
    "#                     rotulos.append('Link Currículo:')\n",
    "#                     conteudos.append(dados[i].strip('Endereço para acessar este CV: '))\n",
    "#                 elif 'ID Lattes: ' in dados[i]:\n",
    "#                     rotulos.append('ID Lattes:')\n",
    "#                     conteudos.append(dados[i].strip('ID Lattes: '))\n",
    "#                 elif 'Última atualização do currículo em ' in dados[i]:\n",
    "#                     rotulos.append('Data atualização:')\n",
    "#                     conteudos.append(dados[i].strip('Última atualização do currículo em '))\n",
    "#                     dt_atualizacao = dados[i].strip('Última atualização do currículo em ')\n",
    "#                     dtt = datetime.strptime(dt_atualizacao, '%d/%m/%Y').date()\n",
    "#                     defasagem = (date.today()-dtt).days        \n",
    "\n",
    "#         try: \n",
    "#             df_temp =pd.DataFrame({\n",
    "#                 'ROTULOS': pd.Series(rotulos),\n",
    "#                 'CONTEUDOS': pd.Series(conteudos),\n",
    "#                     })\n",
    "#             filtro    = 'Link Foto:'\n",
    "#             fotos     = df_temp[(df_temp.ROTULOS == filtro)]['CONTEUDOS']\n",
    "#             x         = fotos[-1:].index[0]\n",
    "#             df_temp.drop(columns=['ROTULOS'], inplace=True)\n",
    "\n",
    "#             try:\n",
    "#                 foto = HTML(df_temp[x:x+1].to_html(escape=False, formatters=dict(CONTEUDOS=path_to_image_html)))\n",
    "#                 display(foto)\n",
    "#                 print(f'Atualizado em {dt_atualizacao} há {defasagem:>2} dias | {NOME}')    \n",
    "                \n",
    "\n",
    "#             except TimeoutException as t:\n",
    "#                 print('Demora na conexão com servidor, carregamento da foto cancelado')\n",
    "#                 traceback_str = ''.join(traceback.format_tb(t.__traceback__))\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print('Erro ao extrair a foto do pesquisador')\n",
    "#             traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "#             print(e,traceback_str)\n",
    "        \n",
    "#         t4=time.time() \n",
    "\n",
    "#         ## TRECHO PARA EXTRAIR DADOS DOS ARTIGOS\n",
    "#         try:    \n",
    "#             page_source = browser.page_source\n",
    "#             soup = BeautifulSoup(page_source, 'html.parser')\n",
    "#             print(len(soup.text))\n",
    "#             info_spans = soup.find_all('span', class_='informacao-artigo')\n",
    "#             for span in info_spans:\n",
    "#                 linhas_dados.append(span.text)\n",
    "#             artigos.append(linhas_dados)\n",
    "\n",
    "#             ## Fechar janela do currículo\n",
    "#             browser.close()            \n",
    "            \n",
    "#             ## Gerenciamento das janelas abertas no browser\n",
    "#             todas_janelas = browser.window_handles\n",
    "#             browser.switch_to.window(todas_janelas[0])\n",
    "\n",
    "#             ## Fechar a janela pop-up\n",
    "#             close_popup = WebDriverWait(browser, delay).until(\n",
    "#                 EC.element_to_be_clickable((By.XPATH, \"//*[@id='idbtnfechar']\")))\n",
    "#             close_popup.click()\n",
    "            \n",
    "#             # ## Nova Consulta\n",
    "#             # try:\n",
    "#             #     nova_consulta = WebDriverWait(browser, delay).until(\n",
    "#             #         EC.element_to_be_clickable((By.XPATH, \"//*[@id='botaoBuscaFiltros']\")))\n",
    "#             #     nova_consulta.click()\n",
    "#             #     time.sleep(1)\n",
    "\n",
    "#             # except Exception as e:\n",
    "#             #     print('Erro ao reiniciar consulta')\n",
    "#             #     traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "#             #     print(e,traceback_str) \n",
    "            \n",
    "#             t5=time.time()                       \n",
    "\n",
    "#             # print(f' {tempo(t0,t5)} | Tempo de Acesso |  Identificação |   Dados Brutos | Subtotal Tempo | Acumulado')\n",
    "#             # print(f'  Decorrido  |   {tempo(t2,t3)}   |  {tempo(t3,t4)}   |  {tempo(t4,t5)}   |  {tempo(t2,t5)}   | {len(conteudos)} seções')\n",
    "            \n",
    "#             sucesso.append(NOME)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print('Erro ao montar dataframe dados de artigos')\n",
    "#             traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "#             print(e,traceback_str)    \n",
    "#             browser.quit()\n",
    "            \n",
    "#             return df_dados\n",
    "    \n",
    "#     # df_dados =pd.DataFrame({\n",
    "#     #     'ROTULOS': pd.Series(rotulos),\n",
    "#     #     'CONTEUDOS': pd.Series(conteudos),\n",
    "#     #         })\n",
    "    \n",
    "#     t6=time.time()\n",
    "#     print('='*95)\n",
    "#     # print(f' {len(sucesso)} currículos extraídos com sucesso')\n",
    "#     print(f' Tempo total para extrair {len(artigos)} artigos dos currículos: {tempo(t0,t6)}')\n",
    "#     # print('='*95)\n",
    "#     browser.quit()\n",
    "    \n",
    "#     # return df_dados, sucesso, parcial_rotulos, parcial_conteudos\n",
    "#     return pd.DataFrame(artigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arquivos fontes para orientadores FUNCAP:\n",
    "# arquivo_qualis     = 'classificações_publicadas_todas_as_areas_avaliacao1672761192111.csv'\n",
    "# lista_orientadores = pd.read_csv(pathcsv+'lista_orientadores.csv')\n",
    "\n",
    "# lista_busca = lista_orientadores['ORIENTADOR'].unique()\n",
    "# lista_busca.sort()\n",
    "# print(f'Total de pesquisadores a extrair: {len(lista_orientadores[1:])}')\n",
    "# for i in lista_busca:\n",
    "#     print('    ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dados_artigos = extrair_artigos(lista_busca[0:1])\n",
    "# for i in dados_artigos.values:\n",
    "#     print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in dados_artigos.values:\n",
    "#     c=0\n",
    "#     for n,j in enumerate(i):\n",
    "#         c+=1\n",
    "#         if c==1:\n",
    "#             print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dados_artigos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Funções padronização de strings e remoção de variantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PADRONIZAÇÃO DE NOMES DE AUTOR E ANÁLISE DE SIMILARIDADES\n",
    "def padronizar_nome(linha_texto):\n",
    "    '''Procura sobrenomes e abreviaturas e monta nome completo\n",
    "     Recebe: String com todos os sobrenomes e nomes, abreviados ou não\n",
    "    Retorna: Nome completo no formato padronizado em SOBRENOME AGNOME, Prenomes\n",
    "      Autor: Marcos Aires (Mar.2022)\n",
    "    '''\n",
    "    import unicodedata\n",
    "    import re\n",
    "    # print('               Analisando:',linha_texto)\n",
    "    string = ''.join(ch for ch in unicodedata.normalize('NFKD', linha_texto) if not unicodedata.combining(ch))\n",
    "    string = string.replace('(Org)','').replace('(Org.)','').replace('(Org).','').replace('.','').replace('\\'','')\n",
    "    string = string.replace(',,,',',').replace(',,',',')\n",
    "    string = re.sub(r'[0-9]+', '', string)\n",
    "        \n",
    "    # Expressões regulares para encontrar padrões de divisão de nomes de autores\n",
    "    sobrenome_inicio   = re.compile(r'^[A-ZÀ-ú-a-z]+,')                  # Sequência de letras maiúsculas no início da string\n",
    "    sobrenome_composto = re.compile(r'^[A-ZÀ-ú-a-z]+[ ][A-ZÀ-ú-a-z]+,')  # Duas sequências de letras no início da string, separadas por espaço, seguidas por vírgula\n",
    "    letra_abrevponto   = re.compile(r'^[A-Z][.]')                        # Uma letra maiúscula no início da string, seguida por ponto\n",
    "    letra_abrevespaco  = re.compile(r'^[A-Z][ ]')                        # Uma letra maiúscula no início da string, seguida por espaço\n",
    "    letras_dobradas    = re.compile(r'[A-Z]{2}')                         # Duas letras maiúsculas juntas no início da string, seguida por espaço\n",
    "    letras_dobradasini = re.compile(r'[A-Z]{2}[ ]')                      # Duas letras maiúsculas juntas no início da string, seguida por espaço\n",
    "    letras_dobradasfim = re.compile(r'[ ][A-Z]{2}')                      # Duas letras maiúsculas juntas no final da string, precedida por espaço\n",
    "    letras_duasconsnts = re.compile(r'[B-DF-HJ-NP-TV-XZ]{2}')            # Duas Letras maiúsculas e consoantes juntas\n",
    "    letras_tresconsnts = re.compile(r'[B-DF-HJ-NP-TV-XZ]{3}')            # Três Letras maiúsculas e consoantes juntas\n",
    "    \n",
    "    # Agnomes e preprosições a tratar, agnomes vão maiúsculas para sobrenome e preposições vão para minúsculas nos nomes\n",
    "    nomes=[]\n",
    "    agnomes       = ['NETO','JUNIOR','FILHO','SEGUNDO','TERCEIRO']\n",
    "    preposicoes   = ['da','de','do','das','dos']\n",
    "    nome_completo = ''\n",
    "    \n",
    "    # Ajustar lista de termos, identificar sobrenomes compostos e ajustar sobrenome com ou sem presença de vírgula\n",
    "    div_sobrenome      = sobrenome_inicio.findall(string)\n",
    "    div_sbrcomposto    = sobrenome_composto.findall(string)\n",
    "    \n",
    "    # print('-'*100)\n",
    "    # print('                 Recebido:',string)\n",
    "    \n",
    "    # Caso haja vírgulas na string, tratar sobrenomes e sobrenomes compostos\n",
    "    if div_sobrenome != [] or div_sbrcomposto != []:\n",
    "        # print('CASO_01: Há víruglas na string')\n",
    "        div = string.split(', ')\n",
    "        sobrenome     = div[0].strip().upper()\n",
    "        try:\n",
    "            div_espaco    = div[1].split(' ')\n",
    "        except:\n",
    "            div_espaco    = ['']\n",
    "        primeiro      = div_espaco[0].strip('.')\n",
    "        \n",
    "        # print('     Dividir por vírgulas:',div)\n",
    "        # print('      Primeira DivVirgula:',sobrenome)\n",
    "        # print('Segunda DivVrg/DivEspaços:',div_espaco)\n",
    "        # print('      Primeira DivEspaços:',primeiro)\n",
    "               \n",
    "        # Caso primeiro nome sejam somente duas letras maiúsculas juntas, trata-se de duas iniciais\n",
    "        if len(primeiro)==2 or letras_tresconsnts.findall(primeiro):\n",
    "            # print('CASO_01.a: Há duas letras ou três letras consoantes juntas, são iniciais')\n",
    "            primeiro_nome=primeiro[0].strip()\n",
    "            # print('          C01.a1_PrimNome:',primeiro_nome)\n",
    "            nomes.append(primeiro[1].strip().upper())\n",
    "            try:\n",
    "                nomes.append(primeiro[2].strip().upper())\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            # print('CASO_01.b: Primeiro nome maior que 2 caracteres')\n",
    "            primeiro_nome = div_espaco[0].strip().title()\n",
    "            # print('          C01.a2_PrimNome:',primeiro_nome)\n",
    "        \n",
    "        # Montagem da lista de nomes do meio\n",
    "        for nome in div_espaco:\n",
    "            # print('CASO_01.c: Para cada nome da divisão por espaços após divisão por vírgula')\n",
    "            if nome not in nomes and nome.lower()!=primeiro_nome.lower() and nome.lower() not in primeiro_nome.lower() and nome!=sobrenome:   \n",
    "                # print('CASO_01.c1: Se o nome não está nem como primeiro nome, nem sobrenomes')\n",
    "                # print(nome, len(nome))\n",
    "                \n",
    "                # Avaliar se é abreviatura seguida de ponto e remover o ponto\n",
    "                if len(nome)<=2 and nome.lower() not in preposicoes:\n",
    "                    # print('    C01.c1.1_Nome<=02:',nome)\n",
    "                    for inicial in nome:\n",
    "                        # print(inicial)\n",
    "                        if inicial not in nomes and inicial not in primeiro_nome:\n",
    "                            nomes.append(inicial.replace('.','').strip().title())\n",
    "                elif len(nome)==3 and nome.lower() not in preposicoes:\n",
    "                        # print('    C01.c1.2_Nome==03:',nome)\n",
    "                        for inicial in nome:\n",
    "                            if inicial not in nomes and inicial not in primeiro_nome:\n",
    "                                nomes.append(inicial.replace('.','').strip().title())\n",
    "                else:\n",
    "                    if nome not in nomes and nome!=primeiro_nome and nome!=sobrenome and nome!='':\n",
    "                        if nome.lower() in preposicoes:\n",
    "                            nomes.append(nome.replace('.','').strip().lower())\n",
    "                        else:\n",
    "                            nomes.append(nome.replace('.','').strip().title())\n",
    "                        # print(nome,'|',primeiro_nome)\n",
    "                        \n",
    "        #caso haja sobrenome composto que não esteja nos agnomes considerar somente primeiro como sobrenome\n",
    "        if div_sbrcomposto !=[] and sobrenome.split(' ')[1] not in agnomes and sobrenome.split(' ')[0].lower() not in preposicoes:\n",
    "            # print('CASO_01.d: Sobrenome composto sem agnomes')\n",
    "            # print(div_sbrcomposto)\n",
    "            # print('Sobrenome composto:',sobrenome)\n",
    "            \n",
    "            nomes.append(sobrenome.split(' ')[1].title())\n",
    "            sobrenome = sobrenome.split(' ')[0].upper()\n",
    "            # print('Sobrenome:',sobrenome)\n",
    "            \n",
    "            for i in nomes:\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('    Nomes:',nomes)\n",
    "        \n",
    "        #caso haja preposição como agnome desconsiderar e passar para final dos nomes\n",
    "        if div_sbrcomposto !=[] and sobrenome.split(' ')[0].lower() in preposicoes:\n",
    "            # print('CASO_01.e: Preposição no Sobrenome passar para o final dos nomes')\n",
    "            # print('   div_sbrcomposto:', div_sbrcomposto)\n",
    "            # print('Sobrenome composto:',div_sbrcomposto)\n",
    "            \n",
    "            nomes.append(div_sbrcomposto[0].split(' ')[0].lower())\n",
    "            # print('    Nomes:',nomes)\n",
    "            sobrenome = div_sbrcomposto[0].split(' ')[1].upper().strip(',')\n",
    "            # print('Sobrenome:',sobrenome)\n",
    "            \n",
    "            for i in nomes:\n",
    "                # print('CASO_01.e1: Para cada nome avaliar se o sobrenome está na lista')\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('  Nomes:',nomes)\n",
    "        \n",
    "        # print('Ao final do Caso 01')\n",
    "        # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('           Lista de nomes:',nomes, len(nomes),'nomes')\n",
    "        \n",
    "    # Caso não haja vírgulas na string considera sobrenome o último nome da string dividida com espaço vazio\n",
    "    else:\n",
    "        # print('CASO_02: Não há víruglas na string')\n",
    "        try:\n",
    "            div = string.split(' ')\n",
    "            # print('      Divisões por espaço:',div)\n",
    "            \n",
    "            if div[-1] in agnomes: # nome final é um agnome\n",
    "                sobrenome     = div[-2].upper().strip()+' '+div[-1].upper().strip()\n",
    "                for i in div[1:-2]:\n",
    "                    if i not in sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.title().strip())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.lower().strip())\n",
    "            else:\n",
    "                if len(div[-1]) > 2:\n",
    "                    sobrenome     = div[-1].upper().strip()\n",
    "                    primeiro_nome = div[1].title().strip()\n",
    "                    for i in div[1:-1]:\n",
    "                        if i != sobrenome and i not in preposicoes:\n",
    "                            nomes.append(i.title().strip())\n",
    "                        if i in preposicoes:\n",
    "                            nomes.append(i.lower().strip())\n",
    "                else:\n",
    "                    sobrenome     = div[-2].upper().strip()\n",
    "                    for i in div[-1]:\n",
    "                        nomes.append(i.title())\n",
    "                    primeiro_nome = nomes[0].title().strip()\n",
    "                    for i in div[1:-1]:\n",
    "                        if i != sobrenome and i not in preposicoes:\n",
    "                            nomes.append(i.title().strip())\n",
    "                        if i in preposicoes:\n",
    "                            nomes.append(i.lower().strip())\n",
    "        except:\n",
    "            sobrenome = div[-1].upper().strip()\n",
    "            for i in div[1:-1]:\n",
    "                    if i != sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.title().strip())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.lower().strip())\n",
    "            \n",
    "        if sobrenome.lower() != div[0].lower().strip():\n",
    "            primeiro_nome=div[0].title().strip()\n",
    "        else:\n",
    "            primeiro_nome=''\n",
    "        \n",
    "        # print('Ao final do Caso 02')\n",
    "        # print('    Sobrenome sem vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome sem vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('Nomes do meio sem vírgula:',nomes, len(nomes),'nomes')\n",
    "    \n",
    "    # Encontrar e tratar como abreviaturas termos com apenas uma ou duas letras iniciais juntas, com ou sem ponto\n",
    "    for j in nomes:\n",
    "        # print('CASO_03: Avaliar cada nome armazenado na variável nomes')\n",
    "        # Procura padrões com expressões regulares na string\n",
    "        div_sobrenome      = sobrenome_inicio.findall(j)\n",
    "        div_sbrcomposto    = sobrenome_composto.findall(j)\n",
    "        div_abrevponto     = letra_abrevponto.findall(j)\n",
    "        div_abrevespaco    = letra_abrevespaco.findall(j)\n",
    "        div_ltrdobradasini = letras_dobradasini.findall(j)\n",
    "        div_ltrdobradasfim = letras_dobradasfim.findall(j)\n",
    "        div_ltrdobradas    = letras_dobradas.findall(j)\n",
    "        tamanho=len(j)\n",
    "        # print('\\n', div_ltrdobradasini, div_ltrdobradasfim, tamanho, 'em:',j,len(j))\n",
    "        \n",
    "        #caso houver abreviatura com uma letra em maiúscula nos nomes\n",
    "        if div_abrevponto !=[] or tamanho==1:\n",
    "            # print('CASO_03.1: Há abreviaturas uma letra maiúscula nos nomes')\n",
    "            nome = j.replace('.','').strip()\n",
    "            if nome not in nomes and nome != sobrenome and nome != primeiro_nome:\n",
    "                # print('CASO_03.1a: Há abreviaturas uma letra maiúscula nos nomes')\n",
    "                nomes.append(nome.upper())\n",
    "        \n",
    "        #caso houver duas inicias juntas em maiúsculas\n",
    "        elif div_ltrdobradasini !=[] or div_ltrdobradasfim !=[] or div_ltrdobradas !=[] :\n",
    "            # print('CASO_03.2: Há abreviaturas uma letra maiúscula nos nomes')\n",
    "            for letra in j:\n",
    "                # print('CASO_03.2a: Avaliar cada inicial do nome')\n",
    "                if letra not in nomes and letra != sobrenome and letra != primeiro_nome:\n",
    "                    # print('CASO_03.2a.1: Se não estiver adicionar inicial aos nomes')\n",
    "                    nomes.append(letra.upper())\n",
    "        \n",
    "        #caso haja agnomes ao sobrenome\n",
    "        elif sobrenome in agnomes:\n",
    "            # print('CASO_03.3: Há agnomes nos sobrenomes')\n",
    "            sobrenome = nomes[-1].upper()+' '+sobrenome\n",
    "            # print(sobrenome.split(' '))\n",
    "            # print('Sobrenome composto:',sobrenome)\n",
    "            for i in nomes:\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('Nomes do meio:',nomes)\n",
    "            \n",
    "        else:\n",
    "            # print('CASO_03.4: Não há agnomes nos sobrenomes')\n",
    "            if j not in nomes and j not in sobrenome and j != primeiro_nome:\n",
    "                if len(nomes) == 1:\n",
    "                    nomes.append(j.upper())\n",
    "                elif 1 < len(nomes) <= 3:\n",
    "                    nomes.append(j.lower())\n",
    "                else:\n",
    "                    nomes.append(j.title())\n",
    "         \n",
    "        # print('Ao final do Caso 03')\n",
    "        # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('Nomes do meio com vírgula:',nomes, len(nomes),'nomes')\n",
    "        \n",
    "    nomes_meio=' '.join([str for str in nomes]).strip()\n",
    "    # print('        Qte nomes do meio:',nomes,len(nomes))\n",
    "    \n",
    "    if primeiro_nome.lower() == sobrenome.lower():\n",
    "        # print('CASO_04: Primeiro nome é igual ao sobrenome')\n",
    "        try:\n",
    "            primeiro_nome=nomes_meio.split(' ')[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            nomes_meio.remove(sobrenome)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        # print('Ao final do caso 04')\n",
    "        # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('Nomes do meio com vírgula:',nomes, len(nomes),'nomes')\n",
    "    \n",
    "    # Caso sobrenome seja só de 1 letra passá-lo para nomes e considerar o próximo nome como sobrenome\n",
    "    for i in range(len(div)):\n",
    "        if len(sobrenome)==1 or sobrenome.lower() in preposicoes:\n",
    "            # print('CASO_05: Mudar sobrenomes até o adequado')\n",
    "            div    = string.split(', ')\n",
    "            # print('Divisão por vírgulas:',div)\n",
    "            avaliar0       = div[0].split(' ')[0].strip()\n",
    "            if 1< len(avaliar0) < 3:\n",
    "                # print('CASO_05.1: 1 < Sobrenome < 3 fica em minúsculas')\n",
    "                sbrn0          = avaliar0.lower()\n",
    "            else:\n",
    "                # print('CASO_05.2: Sobrenome de tamanho 1 ou maior que 3 fica em maiúsculas')\n",
    "                sbrn0          = avaliar0.title()\n",
    "            # print('sbrn0:',sbrn0, len(sbrn0))\n",
    "            \n",
    "            try:\n",
    "                avaliar1=div[0].split(' ')[1].strip()\n",
    "                # print('avaliar0',avaliar0)\n",
    "                # print('avaliar1',avaliar1)\n",
    "                if 1 < len(avaliar1) <=3:\n",
    "                    sbrn1     = avaliar1.lower()\n",
    "                else:\n",
    "                    sbrn1     = avaliar1.title()\n",
    "                # print('sbrn1:',sbrn1, len(sbrn1))\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if div != []:\n",
    "                # print('CASO_05.3: Caso haja divisão por vírgulas na string')\n",
    "                try:\n",
    "                    div_espaco     = div[1].split(' ')\n",
    "                except:\n",
    "                    div_espaco     = div[0].split(' ')\n",
    "                sobrenome      = div_espaco[0].strip().upper()\n",
    "                try:\n",
    "                    primeiro_nome  = div_espaco[1].title().strip()\n",
    "                except:\n",
    "                    primeiro_nome  = div_espaco[0].title().strip()\n",
    "                if len(sbrn0) == 1:\n",
    "                    # print('CASO_05.3a: Avalia primeiro sobrenome de tamanho 1')\n",
    "                    # print('Vai pros nomes:',str(sbrn0).title())\n",
    "                    nomes_meio = nomes_meio+str(' '+sbrn0.title())\n",
    "                    # print('   NomesMeio:',nomes_meio)\n",
    "\n",
    "                elif 1 < len(sbrn0) <= 3:\n",
    "                    # print('CASO_05.3b: Avalia primeiro sobrenome 1< tamanho <=3')\n",
    "                    # print('Vão pros nomes sbrn0:',sbrn0, 'e sbrn1:',sbrn1)\n",
    "\n",
    "                    div_tresconsoantes = letras_tresconsnts.findall(sobrenome)\n",
    "                    if div_tresconsoantes != []:\n",
    "                        # print('CASO_05.4: Três consoantes como sobrenome')\n",
    "                        for letra in sobrenome:\n",
    "                            nomes.append(letra)\n",
    "\n",
    "                        if len(sobrenome) >2:\n",
    "                            sobrenome=nomes[0]\n",
    "                        else:\n",
    "                            sobrenome=nomes[1]\n",
    "                        nomes.remove(sobrenome)\n",
    "                        primeiro_nome=nomes[0]\n",
    "                        nomes_meio=' '.join([str for str in nomes[1:]]).strip()\n",
    "                        nome_completo=sobrenome.upper()+', '+nomes_meio                \n",
    "                    \n",
    "                    try:                       \n",
    "                        # print(' 05.3b    Lista de Nomes:',nomes_meio)\n",
    "                        nomes_meio=nomes_meio.replace(sbrn0,'')\n",
    "                        # print(' 05.3b ReplaceSobrenome0:',nomes_meio)\n",
    "                        nomes_meio=nomes_meio.replace(sbrn1,'')\n",
    "                        # print(' 05.3b ReplaceSobrenome1:',nomes_meio)\n",
    "                    except Exception as e:\n",
    "                        # print('   Erro ReplaceSobrenome:',e)\n",
    "                        pass\n",
    "                    try:\n",
    "                        nomes_meio.replace(primeiro_nome.title(),'')\n",
    "                        nomes_meio.replace(primeiro_nome.lower(),'')\n",
    "                        nomes_meio.replace(primeiro_nome,'')\n",
    "                        # print(' 05.3b Replace PrimNome:',nomes_meio)\n",
    "                    except Exception as e:\n",
    "                        print('Erro no try PrimeiroNome:',e)\n",
    "                        pass\n",
    "                    nomes_meio = nomes_meio.replace(sobrenome,'')\n",
    "                    try:\n",
    "                        for n,i in enumerate(avaliar1):\n",
    "                            nomes.append(i.upper())\n",
    "                            sbrn1     = avaliar1[0]\n",
    "                        else:\n",
    "                            sbrn1     = avaliar1.title()\n",
    "                        # print('sbrn1:',sbrn1, len(sbrn1))\n",
    "                        nomes_meio = nomes_meio+str(' '+sbrn0)+str(' '+sbrn1)\n",
    "                    except:\n",
    "                        nomes_meio = nomes_meio+str(' '+sbrn0)\n",
    "                    nomes      = nomes_meio.strip().strip(',').split(' ')\n",
    "                    # print(' 05.3b NomesMeio:',nomes_meio)\n",
    "                    # print(' 05.3b     Nomes:',nome)\n",
    "\n",
    "                else:\n",
    "                    # print('CASO_05.3c: Avalia primeiro sobrenome >3')\n",
    "                    nomes_meio = nomes_meio+str(' '+div[0].strip().title())\n",
    "                    nomes      = nomes_meio.strip().split(' ')\n",
    "                    # print(' 05.3c NomesMeio:',nomes_meio)\n",
    "                    # print(' 05.3c     Nomes:',nomes)\n",
    "\n",
    "                nomes_meio=nomes_meio.replace(sobrenome,'').replace(',','').strip()\n",
    "                nomes_meio=nomes_meio.replace(primeiro_nome,'').strip()\n",
    "\n",
    "            # print('Ao final do caso 05')\n",
    "            # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "            # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "            # print('Nomes do meio com vírgula:',nomes, len(nomes),'nomes')\n",
    "    \n",
    "    if sobrenome != '' and primeiro_nome !='':\n",
    "        nome_completo=sobrenome.upper().replace(',','')+', '+primeiro_nome.replace(',','')+' '+nomes_meio.replace(sobrenome,'').replace(',','')\n",
    "    elif sobrenome != '':\n",
    "        nome_completo=sobrenome.upper().replace(',','')+', '+nomes_meio.replace(sobrenome,'').replace(',','')\n",
    "    else:\n",
    "        nome_completo=sobrenome.upper()\n",
    "    \n",
    "#     print('Após ajustes finais')\n",
    "#     print('     Sobrenome:',sobrenome)\n",
    "#     print(' Primeiro Nome:',primeiro_nome)\n",
    "#     print('         Nomes:',nomes)\n",
    "#     print('     NomesMeio:',nomes_meio)        \n",
    "        \n",
    "#     print('                Resultado:',nome_completo)\n",
    "    \n",
    "    return nome_completo.strip()\n",
    "\n",
    "\n",
    "def iniciais_nome(linha_texto):\n",
    "    '''Função para retornar sobrenome+iniciais dos nomes, na forma: SOBRENOME, X Y Z\n",
    "     Recebe: String com nome\n",
    "    Retorna: Tupla com nome e sua versão padronizada em sobrenome+agnomes em maiúsculas, seguida de vírgula e iniciais dos nomes \n",
    "      Autor: Marcos Aires (Mar.2022)\n",
    "    '''\n",
    "    import unicodedata\n",
    "    import re\n",
    "    # print('               Analisando:',linha_texto)\n",
    "    string = ''.join(ch for ch in unicodedata.normalize('NFKD', linha_texto) if not unicodedata.combining(ch))\n",
    "    string = string.replace('(Org)','').replace('(Org.)','').replace('(Org).','').replace('.','')\n",
    "        \n",
    "    # Expressões regulares para encontrar padrões de divisão de nomes de autores\n",
    "    sobrenome_inicio   = re.compile(r'^[A-ZÀ-ú-a-z]+,')                 # Sequência de letras maiúsculas no início da string\n",
    "    sobrenome_composto = re.compile(r'^[A-ZÀ-ú-a-z]+[ ][A-ZÀ-ú-a-z]+,') # Duas sequências de letras no início da string, separadas por espaço, seguidas por vírgula\n",
    "    letra_abrevponto   = re.compile(r'^[A-Z][.]')                       # Uma letra maiúscula no início da string, seguida por ponto\n",
    "    letra_abrevespaco  = re.compile(r'^[A-Z][ ]')                       # Uma letra maiúscula no início da string, seguida por espaço\n",
    "    letras_dobradas    = re.compile(r'[A-Z]{2}')                        # Duas letras maiúsculas juntas no início da string, seguida por espaço\n",
    "    letras_dobradasini = re.compile(r'[A-Z]{2}[ ]')                     # Duas letras maiúsculas juntas no início da string, seguida por espaço\n",
    "    letras_dobradasfim = re.compile(r'[ ][A-Z]{2}')                     # Duas letras maiúsculas juntas no final da string, precedida por espaço\n",
    "        \n",
    "    nomes=[]\n",
    "    agnomes       = ['NETO','JUNIOR','FILHO','SEGUNDO','TERCEIRO']\n",
    "    preposicoes   = ['da','de','do','das','dos','DA','DE','DOS','DAS','DOS','De']\n",
    "    nome_completo = ''\n",
    "    \n",
    "    # Ajustar lista de termos, identificar sobrenomes compostos e ajustar sobrenome com ou sem presença de vírgula\n",
    "    div_sobrenome      = sobrenome_inicio.findall(string)\n",
    "    div_sbrcomposto    = sobrenome_composto.findall(string)\n",
    "    \n",
    "    # Caso haja vírgulas na string, tratar sobrenomes e sobrenomes compostos\n",
    "    if div_sobrenome != [] or div_sbrcomposto != []:\n",
    "        div   = string.split(', ')\n",
    "        sobrenome     = div[0].strip().upper()\n",
    "        try:\n",
    "            div_espaco    = div[1].split(' ')\n",
    "        except:\n",
    "            div_espaco  = ['']\n",
    "        primeiro      = div_espaco[0].strip('.')\n",
    "        \n",
    "        # Caso primeiro nome sejam somente duas letras maiúsculas juntas, trata-se de duas iniciais\n",
    "        if len(primeiro)==2:\n",
    "            primeiro_nome=primeiro[0].strip()\n",
    "            nomes.append(primeiro[1].strip())\n",
    "        else:\n",
    "            primeiro_nome = div_espaco[0].strip().title()\n",
    "        \n",
    "        # Montagem da lista de nomes do meio\n",
    "        for nome in div_espaco:\n",
    "            if nome not in nomes and nome.lower()!=primeiro_nome.lower() and nome.lower() not in primeiro_nome.lower() and nome!=sobrenome:   \n",
    "                # print(nome, len(nome))\n",
    "                \n",
    "                # Avaliar se é abreviatura seguida de ponto e remover o ponto\n",
    "                if len(nome)<=2 and nome.lower() not in preposicoes:\n",
    "                    for inicial in nome:\n",
    "                        # print(inicial)\n",
    "                        if inicial not in nomes and inicial not in primeiro_nome:\n",
    "                            nomes.append(inicial.replace('.','').strip().title())\n",
    "                else:\n",
    "                    if nome not in nomes and nome!=primeiro_nome and nome!=sobrenome and nome!='':\n",
    "                        if nome.lower() in preposicoes:\n",
    "                            nomes.append(nome.replace('.','').strip().lower())\n",
    "                        else:\n",
    "                            nomes.append(nome.replace('.','').strip().title())\n",
    "                        # print(nome,'|',primeiro_nome)\n",
    "                        \n",
    "        #caso haja sobrenome composto que não esteja nos agnomes considerar somente primeiro como sobrenome\n",
    "        if div_sbrcomposto !=[] and sobrenome.split(' ')[1] not in agnomes:\n",
    "            # print(div_sbrcomposto)\n",
    "            # print('Sobrenome composto:',sobrenome)\n",
    "            nomes.append(sobrenome.split(' ')[1].title())\n",
    "            sobrenome = sobrenome.split(' ')[0].upper()\n",
    "            # print('Sobrenome:',sobrenome.split(' '))\n",
    "            for i in nomes:\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('Nomes do meio:',nomes)\n",
    "        \n",
    "        # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('Nomes do meio com vírgula:',nomes, len(nomes),'nomes')\n",
    "        \n",
    "    # Caso não haja vírgulas na string considera sobrenome o último nome da string dividida com espaço vazio\n",
    "    else:\n",
    "        try:\n",
    "            div       = string.split(' ')\n",
    "            if div[-2] in agnomes:\n",
    "                sobrenome = div[-2].upper()+' '+div[-1].strip().upper()\n",
    "                for i in nomes[1:-2]:\n",
    "                    if i not in sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.strip().title())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.strip().lower())\n",
    "            else:\n",
    "                sobrenome = div[-1].strip().upper()\n",
    "                for i in div[1:-1]:\n",
    "                    if i not in sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.strip().title())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.strip().lower())\n",
    "        except:\n",
    "            sobrenome = div[-1].strip().upper()\n",
    "            for i in div[1:-1]:\n",
    "                    if i not in sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.strip().title())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.strip().lower())\n",
    "            \n",
    "        if sobrenome.lower() != div[0].strip().lower():\n",
    "            primeiro_nome=div[0].strip().title()\n",
    "        else:\n",
    "            primeiro_nome=''\n",
    "        \n",
    "        # print('    Sobrenome sem vírgula:',sobrenome)\n",
    "        # print('Primeiro nome sem vírgula:',primeiro_nome)\n",
    "        # print('Nomes do meio sem vírgula:',nomes)\n",
    "    \n",
    "    # Encontrar e tratar como abreviaturas termos com apenas uma ou duas letras iniciais juntas, com ou sem ponto\n",
    "    for j in nomes:\n",
    "        # Procura padrões com expressões regulares na string\n",
    "        div_sobrenome      = sobrenome_inicio.findall(j)\n",
    "        div_sbrcomposto    = sobrenome_composto.findall(j)\n",
    "        div_abrevponto     = letra_abrevponto.findall(j)\n",
    "        div_abrevespaco    = letra_abrevespaco.findall(j)\n",
    "        div_ltrdobradasini = letras_dobradasini.findall(j)\n",
    "        div_ltrdobradasfim = letras_dobradasfim.findall(j)\n",
    "        div_ltrdobradas    = letras_dobradas.findall(j)\n",
    "        tamanho=len(j)\n",
    "        # print('\\n', div_ltrdobradasini, div_ltrdobradasfim, tamanho, 'em:',j,len(j))\n",
    "        \n",
    "        #caso houver abreviatura com uma letra em maiúscula nos nomes\n",
    "        if div_abrevponto !=[] or tamanho==1:\n",
    "            cada_nome = j.replace('.','').strip()\n",
    "            if cada_nome not in nomes and cada_nome != sobrenome and nome != primeiro_nome:\n",
    "                nomes.append(cada_nome)\n",
    "        \n",
    "        #caso houver duas inicias juntas em maiúsculas\n",
    "        elif div_ltrdobradasini !=[] or div_ltrdobradasfim !=[] or div_ltrdobradas !=[] :\n",
    "            for letra in j:\n",
    "                if letra not in nomes and letra != sobrenome and letra != primeiro_nome:\n",
    "                    nomes.append(letra)\n",
    "        \n",
    "        #caso haja agnomes ao sobrenome\n",
    "        elif sobrenome in agnomes:\n",
    "            sobrenome = nomes[-1].upper()+' '+sobrenome\n",
    "            # print(sobrenome.split(' '))\n",
    "            # print('Sobrenome composto:',sobrenome)\n",
    "            for i in nomes:\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('Nomes do meio:',nomes)\n",
    "            \n",
    "        else:\n",
    "            if j not in nomes and j not in sobrenome and j != primeiro_nome:\n",
    "                nomes.append(j)\n",
    "    \n",
    "    nomes_meio=' '.join([str[0] for str in nomes]).strip()\n",
    "    # print('Qte nomes do meio',len(nomes),nomes)\n",
    "    if sobrenome != '' and primeiro_nome !='':\n",
    "        sobrenome_iniciais = sobrenome+', '+primeiro_nome[0]+' '+nomes_meio\n",
    "    elif sobrenome != '':\n",
    "        sobrenome_iniciais = sobrenome\n",
    "    \n",
    "    return sobrenome_iniciais.strip()\n",
    "\n",
    "\n",
    "def similares(lista_autores, lista_grupo, limite_jarowinkler, distancia_levenshtein):\n",
    "    \"\"\"Função para aplicar padronização no nome de autor da lista de pesquisadores e buscar similaridade na lista de coautores\n",
    "     Recebe: Lista de pesquisadores do grupo em análise gerada pela lista de nomes dos coautores das publicações em análise\n",
    "    Utiliza: get_jaro_distance(), editdistance()\n",
    "    Retorna: Lista de autores com fusão de nomes cuja similaridade esteja dentro dos limites definidos nesta função\n",
    "      Autor: Marcos Aires (Fev.2022)\n",
    "      \n",
    "    Refazer: Inserir crítica de, mantendo sequência ordem alfabética, retornar no final nome mais extenso em caso de similaridade;\n",
    "    \"\"\"\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    from IPython.display import clear_output\n",
    "    import editdistance\n",
    "    import numpy as np\n",
    "    import time\n",
    "    \n",
    "    t0=time.time()\n",
    "    \n",
    "    # limite_jarowinkler=0.85\n",
    "    # distancia_levenshtein=6\n",
    "    similares_jwl=[]\n",
    "    similares_regras=[]\n",
    "    similares=[]\n",
    "    tempos=[]\n",
    "    \n",
    "    count=0\n",
    "    t1=time.time()\n",
    "    for i in lista_autores:\n",
    "        count+=1\n",
    "        if count > 0:\n",
    "            tp=time.time()-t1\n",
    "            tmed=tp/count*2\n",
    "            tempos.append(tp)\n",
    "    #     print(\"Analisar similaridades com: \", nome_padronizado)\n",
    "        \n",
    "        count1=0\n",
    "        for nome in lista_autores:\n",
    "            if count1 > 0:\n",
    "                resta=len(lista_autores)-count\n",
    "                print(f'Analisando {count1:3}/{len(lista_autores)} resta analisar {resta:3} nomes. Previsão de término em {np.round(tmed*resta/60,1)} minutos')\n",
    "            else:\n",
    "                print(f'Analisando {count1:3}/{len(lista_autores)} resta analisar {len(lista_autores)-count1} nomes.')\n",
    "            \n",
    "            t2=time.time()\n",
    "            count1+=1            \n",
    "\n",
    "            try:\n",
    "                similaridade_jarowinkler = get_jaro_distance(i, nome)\n",
    "                print(f'{i:40} | {nome:40} | Jaro-Winkler: {np.round(similaridade_jarowinkler,2):4} Levenshtein: {editdistance.eval(i, nome)}')\n",
    "                similaridade_levenshtein = editdistance.eval(i, nome)\n",
    "\n",
    "                # inferir similaridade para nomes que estejam acima do limite ponderado definido, mas não idênticos e não muito distantes em edição\n",
    "                if  similaridade_jarowinkler > limite_jarowinkler and similaridade_jarowinkler!=1 and similaridade_levenshtein < distancia_levenshtein:\n",
    "                    # Crítica no nome mais extenso como destino no par (origem, destino)\n",
    "                    \n",
    "                    similares_jwl.append((i,nome))\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            clear_output(wait=True)\n",
    "    \n",
    "    # Conjunto de regras de validação de similaridade\n",
    "    # Monta uma lista de nomes a serem retirados antes de montar a lista de troca\n",
    "    trocar=[]\n",
    "    retirar=[]\n",
    "    for i in similares_jwl:\n",
    "        sobrenome_i = i[0].split(',')[0]\n",
    "        sobrenome_j = i[1].split(',')[0]\n",
    "\n",
    "        try:\n",
    "            iniciais_i  = iniciais_nome(i[0]).split(',')[1].strip()\n",
    "        except:\n",
    "            iniciais_i  = ''\n",
    "\n",
    "        try:\n",
    "            iniciais_j  = iniciais_nome(i[1]).split(',')[1].strip()\n",
    "        except:\n",
    "            iniciais_j  = ''\n",
    "\n",
    "        try:\n",
    "            primnome_i = i[0].split(',')[1].strip().split(' ')[0].strip()\n",
    "        except:\n",
    "            primnome_i = ''\n",
    "\n",
    "        try:\n",
    "            primnome_j = i[1].split(',')[1].strip().split(' ')[0].strip()\n",
    "        except:\n",
    "            primnome_j = ''    \n",
    "\n",
    "        try:\n",
    "            inicial_i = i[0].split(',')[1].strip()[0]\n",
    "        except:\n",
    "            inicial_i = ''\n",
    "\n",
    "        try:\n",
    "            resto_i   = i[0].split(',')[1].strip().split(' ')[0][1:]\n",
    "        except:\n",
    "            resto_i   = ''\n",
    "\n",
    "        try:\n",
    "            inicial_j = i[1].split(',')[1].strip()[0]\n",
    "        except:\n",
    "            inicial_j = ''\n",
    "\n",
    "        try:\n",
    "            resto_j   = i[1].split(',')[1].strip().split(' ')[0][1:]\n",
    "        except:\n",
    "            resto_j = ''\n",
    "\n",
    "        # Se a distância de edição entre os sobrenomes\n",
    "        if editdistance.eval(sobrenome_i, sobrenome_j) > 2 or inicial_i!=inicial_j:\n",
    "            retirar.append(i)\n",
    "        else:\n",
    "            if primnome_i!=primnome_j and len(primnome_i)>1:\n",
    "                retirar.append(i)\n",
    "            if primnome_i!=primnome_j and len(primnome_i)>1 and len(primnome_j)>1:\n",
    "                retirar.append(i)\n",
    "            if resto_i!=resto_j and resto_i!='':\n",
    "                retirar.append(i)\n",
    "            if len(i[1]) < len(i[0]):\n",
    "                retirar.append(i)\n",
    "            if len(iniciais_i) != len(iniciais_j):\n",
    "                retirar.append(i)\n",
    "\n",
    "    for i in similares_jwl:\n",
    "        if i not in retirar:\n",
    "            trocar.append(i)\n",
    "\n",
    "        if iniciais_nome(i[0]) in iniciais_nome(i[1]) and len(i[0]) < len(i[1]):\n",
    "            trocar.append(i)\n",
    "\n",
    "        if iniciais_nome(i[0]) == iniciais_nome(i[1]) and len(i[0]) < len(i[1]):\n",
    "             trocar.append(i)\n",
    "\n",
    "    # Exemplo de inserção de conhecimentos extra Lattes para melhor resolução de entidades\n",
    "    lista_extra = [\n",
    "                    # ('ALBUQUERQUE, Adriano B', 'ALBUQUERQUE, Adriano Bessa'),\n",
    "                    # ('ALBUQUERQUE, Adriano', 'ALBUQUERQUE, Adriano Bessa'),\n",
    "                    # ('COELHO, Andre L V', 'COELHO, Andre Luis Vasconcelos'),\n",
    "                    # ('DUARTE, Joao B F', 'DUARTE, Joao Batista Furlan'),\n",
    "                    # ('FILHO, Raimir H','HOLANDA FILHO, Raimir'),\n",
    "                    # ('FILHO, Raimir','HOLANDA FILHO, Raimir'),\n",
    "                    # ('FORMIGO, A','FORMICO, Maria Andreia Rodrigues'),\n",
    "                    # ('FORMICO, A','FORMICO, Maria Andreia Rodrigues'),\n",
    "                    # ('FURLAN, J B D', 'FURLAN, Joao Batista Duarte'),\n",
    "                    # ('FURTADO, Elizabeth', 'FURTADO, Maria Elizabeth Sucupira'),\n",
    "                    # ('FURTADO, Elizabeth S', 'FURTADO, Maria Elizabeth Sucupira'),\n",
    "                    # ('FURTADO, Elizabeth Sucupira','FURTADO, Maria Elizabeth Sucupira'),\n",
    "                    # ('FURTADO, M E S', 'FURTADO, Maria Elizabeth Sucupira'),\n",
    "                    # ('FURTADO, Vasco', 'FURTADO, Joao Jose Vasco Peixoto'),\n",
    "                    # ('FURTADO, J P', 'FURTADO, Joao Jose Vasco Peixoto'),\n",
    "                    # ('FURTADO, J V P', 'FURTADO, Joao Jose Vasco Peixoto'),\n",
    "                    # ('FURTADO, Vasco', 'FURTADO, Joao Jose Vasco Peixoto'),\n",
    "                    # ('FURTADO, Elizabeth','FURTADO, Maria Elizabeth Sucupira'),\n",
    "                    # ('HOLANDA, Raimir', 'HOLANDA FILHO, Raimir'),\n",
    "                    # ('LEITE, G S', 'LEITE, Gleidson Sobreira'),\n",
    "                    # ('PEQUENO, T H C', 'PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                    # ('PEQUENO, Tarcisio','PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                    # ('PEQUENO, Tarcisio Cavalcante', 'PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                    # ('PINHEIRO, Placido R', 'PINHEIRO, Placido Rogerio'),\n",
    "                    # ('PINHEIRO, Vladia', 'PINHEIRO, Vladia Celia Monteiro'),\n",
    "                    # ('RODRIGUES, M A F', 'RODRIGUES, Maria Andreia Formico'),\n",
    "                    # ('RODRIGUES, Andreia', 'RODRIGUES, Maria Andreia Formico'),\n",
    "                    # ('JOAO, Batista F Duarte,', 'FURLAN, Joao Batista Duarte'),\n",
    "                    # ('MACEDO, Antonio Roberto M de', 'MACEDO, Antonio Roberto Menescal de'),\n",
    "                    # ('MACEDO, D V', 'MACEDO, Daniel Valente'),\n",
    "                    # ('MENDONCA, Nabor C', 'MENDONCA, Nabor das Chagas'),\n",
    "                    # ('PEQUENO, Tarcisio', 'PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                    # ('PEQUENO, Tarcisio H', 'PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                    # ('PINHEIRO, Mirian C D', 'PINHEIRO, Miriam Caliope Dantas'),\n",
    "                    # ('PINHEIRO, Mirian Caliope Dantas', 'PINHEIRO, Miriam Caliope Dantas'),\n",
    "                    # ('PINHEIRO, P G C D', 'PINHEIRO, Pedro Gabriel Caliope Dantas'),\n",
    "                    # ('PINHEIRO, Pedro G C', 'PINHEIRO, Pedro Gabriel Caliope Dantas'),\n",
    "                    # ('PINHEIRO, Placido R', 'PINHEIRO, Placido Rogerio'),\n",
    "                    # ('PINHEIRO, Vladia', 'PINHEIRO, Vladia Celia Monteiro'),\n",
    "                    # ('ROGERIO, Placido Pinheiro', 'PINHEIRO, Placido Rogerio'),\n",
    "                    # ('REBOUCRAS FILHO, Pedro', 'REBOUCAS FILHO, Pedro Pedrosa'),\n",
    "                    # ('SAMPAIO, A', 'SAMPAIO, Americo Tadeu Falcone'),\n",
    "                    # ('SAMPAIO, Americo', 'SAMPAIO, Americo Tadeu Falcone'),\n",
    "                    # ('SAMPAIO, Americo Falcone', 'SAMPAIO, Americo Tadeu Falcone'),\n",
    "                    # ('SUCUPIRA, Elizabeth Furtado','FURTADO, Maria Elizabeth Sucupira'),\n",
    "                  ]\n",
    "    \n",
    "    trocar=trocar+lista_extra\n",
    "    trocar.sort()\n",
    "    \n",
    "    return trocar\n",
    "\n",
    "\n",
    "def extrair_variantes(df_dadosgrupo):\n",
    "    ''' Utiliza campo de Nome em Citações do currículo como filtro para obter variantes do nome de cada membro\n",
    "     Recebe: Dataframe com os dados brutos do grupo de pesquisa agrupados; lista de nomes de pesquisadores de interesse\n",
    "    Retorna: Lista de tuplas com pares a serem trocados da variante pelo nome padronizado na forma (origem, destino)\n",
    "    '''\n",
    "    filtro1   = 'Nome'\n",
    "    lista_nomes = df_dadosgrupo[(df_dadosgrupo.ROTULOS == filtro1)]['CONTEUDOS'].values\n",
    "\n",
    "    variantes=[]\n",
    "    filtro='Nome em citações bibliográficas'\n",
    "    variantes=df_dadosgrupo[(df_dadosgrupo.ROTULOS == filtro)]['CONTEUDOS'].to_list()\n",
    "\n",
    "    trocar=[]\n",
    "    for j in range(len(variantes)):\n",
    "        padrao_destino = padronizar_nome(lista_nomes[j])\n",
    "        trocar.append((lista_nomes[j], padrao_destino))\n",
    "        for k in variantes[j]:\n",
    "            padrao_origem = padronizar_nome(k)\n",
    "            trocar.append((k, padrao_destino))\n",
    "            trocar.append((padrao_origem, padrao_destino))\n",
    "    \n",
    "    return trocar\n",
    "\n",
    "\n",
    "def inferir_variantes(nome):\n",
    "    ''' Quebra um nome inicialmente por vírgula para achar sobrenomes, e depois por ' ' para achar nomes\n",
    "     Recebe: Par de nomes a comparar, nome1 é nome padronizado na função padronizar_nome(), nome2 é o que será analisado\n",
    "    Utiliza: Função padronizar_nome(nome)\n",
    "    Retorna: Lista de tuplas, no formato (origem, destino), com variantes de nome a serem trocadas pela forma padronizada\n",
    "      Autor: Marco Aires (Fev.2022)\n",
    "    '''\n",
    "    trocar = []\n",
    "    nomes  = []\n",
    "    try:\n",
    "        div0  = nome.split(',').strip()\n",
    "        sobrenome=div0[0]\n",
    "        try:\n",
    "            div1 = div0[1].split(' ').strip()\n",
    "            for i in div1:\n",
    "                nomes.append(i)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    trocar.append(nome, iniciais_nome(nome))\n",
    "    \n",
    "    return trocar\n",
    "\n",
    "\n",
    "def comparar_nomes(nome1,nome2):\n",
    "    ''' Compara dois nomes por seus sobrenomes e iniciais do primeiro nome\n",
    "     Recebe: Par de nomes a comparar, nome1 é nome padronizado na função padronizar_nome(), nome2 é o que será analisado\n",
    "    Utiliza: Função padronizar_nome(nome)\n",
    "    Retorna: Lista de tuplas, no formato (origem, destino), com variantes de nome a serem trocadas pela forma padronizada\n",
    "      Autor: Marco Aires (Fev.2022)\n",
    "    '''\n",
    "    trocar=[]\n",
    "    qte_nomes1=0\n",
    "    nome_padronizado1 = padronizar_nome(nome1)\n",
    "    sobrenome1        = nome_padronizado1.split(',')[0]\n",
    "    if sobrenome1!='':\n",
    "        qte_nomes1+=1\n",
    "    primeiro_nome1    = nome_padronizado1.split(',')[1].split(' ')[0]\n",
    "    if primeiro_nome1!='':\n",
    "        qte_nomes1+=1\n",
    "    inicial_primnome1 = primeiro_nome1[0]\n",
    "    demais_nomes1     = nome_padronizado1.split(',')[1].split(' ')[1:]\n",
    "    qte_nomes1=qte_nomes1+len(demais_nomes1)\n",
    "    \n",
    "    qte_nomes2=0\n",
    "    nome_padronizado2 = padronizar_nome(nome2)\n",
    "    sobrenome2        = nome_padronizado2.split(',')[0]\n",
    "    if sobrenome2!='':\n",
    "        qte_nomes2+=1    \n",
    "    primeiro_nome2    = nome_padronizado2.split(',')[1].split(' ')[0]\n",
    "    if primeiro_nome2!='':\n",
    "        qte_nomes2+=1\n",
    "    inicial_primnome2 = primeiro_nome2[0]\n",
    "    demais_nomes2     = nome_padronizado2.split(',')[1].split(' ')[1:]\n",
    "    qte_nomes2=qte_nomes2+len(demais_nomes2)\n",
    "    \n",
    "    if sobrenome1==sobrenome2 and primeiro_nome1==primeiro_nome2:\n",
    "        trocar.append((nome1,nome_padronizado2))\n",
    "\n",
    "    if sobrenome1==sobrenome2 and primeiro_nome1==primeiro_nome2:\n",
    "        trocar.append((nome1,nome_padronizado2))\n",
    "        \n",
    "    return trocar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"Vasconcelos, GS ; Fernandes, MCR ; Matsui, TC ; Luciano, MCS ; Costa, CL ; Ferraz, CPM ; Dias, FBS ; MIYAJIMA, FABIO ; Araújo, FMC ; Fonseca, MHG . Persistent SARS-COV-2 infection in vaccinated individual with three doses of COVID-19 vaccine. VACCINE, v. 41, p. 1778-1782, 2023.\"\n",
    "# extrair_detalhes(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtro='Artigos completos publicados em periódicos'\n",
    "# artigos = df_secoes[df_secoes.ROTULOS == filtro]\n",
    "\n",
    "# nome = lista_busca[1]\n",
    "# artigos_individual = artigos[artigos.CURRICULO==nome]\n",
    "# lista_individual = artigos_individual['CONTEUDOS']\n",
    "# lista_individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrar por período de tempo unificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos = pd.read_csv(pathout+'df_artigos_servidores_ingresso_fioce.csv', sep='\\t')\n",
    "print(len(df_artigos.index))\n",
    "# df_artigos[601:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_inico = 2008\n",
    "ano_final = 2023\n",
    "df_artigosperiodo = df_artigos[(df_artigos.ANO_PUB >=ano_inico)&(df_artigos.ANO_PUB <=ano_final)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigosperiodo[df_artigosperiodo.ANO_PUB==np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_artigosperiodo[:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrar por período de tempo individualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos = df_artigos_servidores\n",
    "\n",
    "fioce_pessoal['INGRESSO_FIOCE'] = pd.to_datetime(fioce_pessoal['INGRESSO_FIOCE'])\n",
    "\n",
    "# Certificando-se de que 'INGRESSO_FIOCE' é do tipo int (se for data no formato yyyy, por exemplo)\n",
    "fioce_pessoal['ANO_INGRESSO_FIOCE'] = fioce_pessoal['INGRESSO_FIOCE'].dt.year\n",
    "\n",
    "# Merge entre df_artigos e fioce_pessoal usando 'AUTORES' e 'NOME' como chaves\n",
    "merged_df = df_artigos.merge(fioce_pessoal, left_on='CURRICULO', right_on='NOME', how='inner')\n",
    "\n",
    "# Filtrar as linhas de acordo com a condição do ano de publicação e da data de ingresso\n",
    "result_df = merged_df[merged_df['ANO_PUB'] >= merged_df['ANO_INGRESSO_FIOCE']]\n",
    "\n",
    "# Opcional: Dropar colunas redundantes ou não necessárias, por exemplo, 'NOME' que é igual a 'AUTORES'\n",
    "result_df = result_df.drop(columns=['NOME'])\n",
    "\n",
    "# Agora, result_df é o dataframe final desejado\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df.ANO_PUB==2030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_result_df = merged_df[merged_df['ANO_PUB'] < merged_df['ANO_INGRESSO_FIOCE']]\n",
    "print(len(result_df))\n",
    "print(len(out_result_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exibir informações extratídas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigosperiodo.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolucao_artigos(df_artigosperiodo, fioce_pessoal, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigosperiodo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_artigosperiodo.NOME.value_counts()))\n",
    "df_artigosperiodo.NOME.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fioce_pessoal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_media_artigos(df_artigosperiodo, fioce_pessoal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolucao_anual(df_artigosperiodo, fioce_pessoal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolucao_sem_duplicatas(df_artigosperiodo, fioce_pessoal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotar_artigos_ano(df_artigosperiodo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparativo_curriculos(df_artigosperiodo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotar_barras_estaqueadas(df_artigosperiodo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigosperiodo[df_artigosperiodo.NOME=='Anya Pimentel Gomes Fernandes Vieira Meyer'].sort_values(by='ANO_PUB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df[result_df.NOME=='Raphael Trevizani']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qualitatives = px.colors.qualitative.swatches()\n",
    "# sequentials = px.colors.sequential.swatches()\n",
    "\n",
    "# qualitatives.show()\n",
    "# sequentials.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
