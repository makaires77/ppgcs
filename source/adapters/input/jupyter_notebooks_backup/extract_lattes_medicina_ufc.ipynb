{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><center><img src=\"https://raw.githubusercontent.com/makaires77/fioce/master/assets/logo_fioce.png\" \n",
    "style=\"height:150px\" alt=\"Logo_Unifor\"></center></center>\n",
    "\n",
    "## <center>Analisar a Produção Acadêmica do Programa de Pós-Graduação em Ciências Médicas da UFC<br /> sobre estudos em Câncer</center>\n",
    "\n",
    "    Suzana Benetti B. Aires Barbosa - Doutoranda\n",
    "    Antonio Marcos Aires Barbosa    – Doutorando\n",
    "\n",
    "**Introdução**\n",
    "\n",
    "Quais são as pesquisas empreendidas pelos pesquisadores associados ao Programa?\n",
    "\n",
    "**Objetivo geral:**\n",
    "\n",
    "    Achar um tema de interesse transversal para um projeto de doutorado no programa.\n",
    "\n",
    "**Objetivos Específicos**\n",
    "\n",
    "    1. Definir especialidades;\n",
    "    2. Definir orientador;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>PREPARAR AMBIENTE DE DESENVOLVIMENTO E EXECUÇÃO</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importação dos módulos de trabalho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração e teste do ambiente local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}  # Dictionary to store the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sh-tsang.medium.com/tutorial-cuda-cudnn-anaconda-jupyter-pytorch-installation-in-windows-10-96b2a2f0ac57\n",
    "# DEFINIÇÃO DAS FUNÇÕES:\n",
    "def try_amb():\n",
    "    ## Visualizar versões dos principais componentes\n",
    "    import os\n",
    "    import pip\n",
    "    import sys\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    # !pip3 install shutup\n",
    "    # import shutup; shutup.please()   \n",
    "    pyVer      = sys.version\n",
    "    pipVer     = pip.__version__\n",
    "    print('\\nVERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT')\n",
    "    print('Interpretador em uso:', sys.executable)\n",
    "    print('    Ambiente ativado:',os.environ['CONDA_DEFAULT_ENV'])\n",
    "    print('     Python: '+pyVer, '\\n        Pip:', pipVer,'\\n'\n",
    "         )\n",
    "    !nvcc -V\n",
    "\n",
    "def get_cpu_info_windows():\n",
    "    import subprocess\n",
    "\n",
    "    try:\n",
    "        return subprocess.check_output(\"wmic cpu get Name\", shell=True).decode('utf-8').split('\\n')[1].strip()\n",
    "    except:\n",
    "        return \"Informação não disponível\"\n",
    "\n",
    "def get_cpu_info_unix():\n",
    "    import subprocess\n",
    "    try:\n",
    "        return subprocess.check_output(\"lscpu\", shell=True).decode('utf-8')\n",
    "    except:\n",
    "        try:\n",
    "            return subprocess.check_output(\"sysctl -n machdep.cpu.brand_string\", shell=True).decode('utf-8').strip()\n",
    "        except:\n",
    "            return \"Informação não disponível\"\n",
    "        \n",
    "def try_cpu():\n",
    "    import psutil\n",
    "    import platform\n",
    "\n",
    "    # Métricas da CPU\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    cpu_count_logical = psutil.cpu_count(logical=True)\n",
    "    cpu_count_physical = psutil.cpu_count(logical=False)\n",
    "    cpu_freq = psutil.cpu_freq()\n",
    "    cpu_times_percent = psutil.cpu_times_percent(interval=1)\n",
    "\n",
    "    # Informação específica do modelo do processador\n",
    "    if platform.system() == \"Windows\":\n",
    "        cpu_model = get_cpu_info_windows()\n",
    "    else:\n",
    "        cpu_model = get_cpu_info_unix()\n",
    "\n",
    "    # Informações adicionais sobre o Processador\n",
    "    cpu_brand = platform.processor()\n",
    "    cpu_architecture = platform.architecture()[0]\n",
    "    cpu_machine_type = platform.machine()\n",
    "    \n",
    "    # Métricas da Memória RAM\n",
    "    ram = psutil.virtual_memory()\n",
    "    total_ram = ram.total / (1024 ** 3)  # Em GB\n",
    "    used_ram = ram.used / (1024 ** 3)  # Em GB\n",
    "    \n",
    "    # Métricas do Espaço em Disco\n",
    "    disk = psutil.disk_usage('/')\n",
    "    total_disk = disk.total / (1024 ** 3)  # Em GB\n",
    "    used_disk = disk.used / (1024 ** 3)  # Em GB\n",
    "    free_disk = (total_disk - used_disk)\n",
    "    used_disk_percent = (used_disk / total_disk) * 100\n",
    "    free_disk_percent = (1 - (used_disk / total_disk)) * 100\n",
    "\n",
    "    # Exibição das Métricas\n",
    "    print(f\"\\nMarca do Processador: {cpu_brand}\")\n",
    "    print(f\"Modelo do Processador: {cpu_model}\")\n",
    "    print(f\"Frequência da CPU: {cpu_freq.current} MHz\")\n",
    "    # print(f\"Tipo de Máquina: {cpu_machine_type}\")\n",
    "    print(f\"Arquitetura do Processador: {cpu_architecture}\")\n",
    "    print(f\"Número de CPUs físicas: {cpu_count_physical}\")\n",
    "    print(f\"Número de CPUs lógicas: {cpu_count_logical}\")\n",
    "    print(f\"Uso atual CPU: {cpu_percent}%\")\n",
    "    print(f\"Tempos de CPU: user={cpu_times_percent.user}%, system={cpu_times_percent.system}%, idle={cpu_times_percent.idle}%\")\n",
    "    print(f\"\\nTotal de RAM: {total_ram:>5.2f} GB\")\n",
    "    print(f\"Usado em RAM: {used_ram:>5.2f} GB\")\n",
    "    print(f\"Espaço Total em disco: {total_disk:>7.2f} GB\")\n",
    "    print(f\"Espaço em disco usado: {used_disk:>7.2f} GB {used_disk_percent:>4.1f}%\")\n",
    "    print(f\"Espaço em disco livre: {free_disk:>7.2f} GB {free_disk_percent:>4.1f}%\")\n",
    "\n",
    "def try_gpu():\n",
    "    print('\\nVERSÕES DO PYTORCH E GPU DISPONÍVEIS')\n",
    "    try:\n",
    "        import torch\n",
    "        print('    PyTorch:',torch.__version__)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Dispositivo:',device)\n",
    "        print('Disponível :',device,torch.cuda.is_available(),' | Inicializado:',torch.cuda.is_initialized(),'| Capacidade:',torch.cuda.get_device_capability(device=None))\n",
    "        print('Nome GPU   :',torch.cuda.get_device_name(0),'         | Quantidade:',torch.cuda.device_count(),'\\n')\n",
    "    except Exception as e:\n",
    "        print('Erro ao configurar a GPU:',e,'\\n')\n",
    "\n",
    "def try_browser(chromedriver_path):\n",
    "    print('\\nVERSÕES O BROWSER E DO CHROMEDRIVER INSTALADAS')\n",
    "    import os\n",
    "    from selenium import webdriver\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    try:\n",
    "        driver_path = chromedriver_path+'/chromedriver.exe'\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        str1 = driver.capabilities['browserVersion']\n",
    "        str2 = driver.capabilities['chrome']['chromedriverVersion'].split(' ')[0]\n",
    "        print(f'     Versão do browser: {str1}')\n",
    "        print(f'Versão do chromedriver: {str2}')\n",
    "        driver.quit()\n",
    "\n",
    "        if str1[0:3] != str2[0:3]: \n",
    "            print(\"Versões incompatíveis, atualizar chromedriver!\")\n",
    "            print('  Baixar versão atualizada do Chromedriver em:')\n",
    "            print('  https://googlechromelabs.github.io/chrome-for-testing/#stable')\n",
    "            print('     Ex. Versão 116 PARA WINDOWS:')\n",
    "            print('\t    https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/win64/chromedriver-win64.zip')\n",
    "    except Exception as e:\n",
    "        print('Erro ao testar versões:')\n",
    "        print(f'  {e}')\n",
    "\n",
    "def detect_error(html_text):\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    result_list = soup.find('div', {'class': 'resultado'}).find('ol').find_all('li')\n",
    "    for result in result_list:\n",
    "        if 'Stale file handle' in result.text:\n",
    "            raise Exception('Stale File Handle Detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_directory(prompt_message):\n",
    "    \"\"\"\n",
    "    Opens a file dialog to allow the user to select a directory.\n",
    "    Returns the path of the selected directory.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main Tk window\n",
    "    print(prompt_message)\n",
    "    folder_selected = filedialog.askdirectory()\n",
    "    \n",
    "    if folder_selected:\n",
    "        print(f\"You have selected the folder: {folder_selected}\")\n",
    "        return folder_selected\n",
    "    else:\n",
    "        print(\"No folder was selected.\")\n",
    "        return None\n",
    "\n",
    "def configure_environment():\n",
    "    \"\"\"\n",
    "    Allows the user to specify the directory of Chromedriver and the data storage folder.\n",
    "    Returns a dictionary containing paths for both.\n",
    "    \"\"\"\n",
    "    chromedriver_path = select_directory(\"Please select the Chromedriver folder:\")\n",
    "    data_storage_path = select_directory(\"Please select the data storage folder:\")\n",
    "    \n",
    "    if chromedriver_path and data_storage_path:\n",
    "        config = {\n",
    "            \"Chromedriver_Path\": chromedriver_path,\n",
    "            \"Data_Storage_Path\": data_storage_path\n",
    "        }\n",
    "        return config\n",
    "    else:\n",
    "        print(\"Configuration incomplete. Both paths are required.\")\n",
    "        return None\n",
    "\n",
    "def select_directory_jupyter(prompt_message):\n",
    "    \"\"\"\n",
    "    Creates an interactive directory selector in Jupyter Notebook.\n",
    "    \"\"\"\n",
    "    print(prompt_message)\n",
    "    \n",
    "    folder_selector = widgets.Text(\n",
    "        description='Directory:',\n",
    "        value=os.getcwd(),  # Default value is the current working directory\n",
    "        layout=widgets.Layout(width='90%')\n",
    "    )\n",
    "    \n",
    "    def on_submit(change):\n",
    "        folder_selected = change['new']\n",
    "        \n",
    "        if os.path.isdir(folder_selected):\n",
    "            print(f\"You have selected the folder: {folder_selected}\")\n",
    "        else:\n",
    "            print(\"Invalid folder. Please try again.\")\n",
    "    \n",
    "    folder_selector.on_submit(on_submit)\n",
    "    display(folder_selector)\n",
    "    \n",
    "\n",
    "def on_submit_chromedriver(change):\n",
    "    folder_selected = change.new\n",
    "    if os.path.isdir(folder_selected):\n",
    "        print(f\"You have selected the Chromedriver folder: {folder_selected}\")\n",
    "        config[\"Chromedriver_Path\"] = folder_selected\n",
    "\n",
    "def on_submit_data_storage(change):\n",
    "    folder_selected = change.new\n",
    "    if os.path.isdir(folder_selected):\n",
    "        print(f\"You have selected the data storage folder: {folder_selected}\")\n",
    "        config[\"Data_Storage_Path\"] = folder_selected\n",
    "    else:\n",
    "        print(\"Invalid folder for data storage. Please try again.\")\n",
    "\n",
    "## Ambiente Linux (Laptop)\n",
    "# /home/marcos/fioce/chromedriver\n",
    "# /home/marcos/kgfioce\n",
    "\n",
    "## Ambiente Fiocruz Eusébio (Windows)\n",
    "# c:\\Users\\marcos.aires\\fioce\\chromedriver\n",
    "# c:\\kgfioce\\\n",
    "\n",
    "## Ambiente Home (Windows)\n",
    "# c:\\Users\\marco\\fioce\\chromedriver\n",
    "# c:\\kgfioce\\\n",
    "\n",
    "def find_chromedriver_path():\n",
    "    \"\"\"\n",
    "    Search for the existing Chromedriver directory among the predefined paths.\n",
    "    \"\"\"\n",
    "    possible_paths = [\n",
    "        '/home/marcos/fioce/chromedriver',\n",
    "        'c:\\\\Users\\\\marcos.aires\\\\fioce\\\\chromedriver',\n",
    "        'c:\\\\Users\\\\marco\\\\fioce\\\\chromedriver'\n",
    "    ]\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    return None  # Return None if no path is found\n",
    "\n",
    "def find_datafolder():\n",
    "    \"\"\"\n",
    "    Search for the existing Data folder directory among the predefined paths.\n",
    "    \"\"\"\n",
    "    possible_paths = [\n",
    "        '/home/marcos/kgufc_cienciasmedicas',\n",
    "        'c:\\\\kgufc_cienciasmedicas\\\\',\n",
    "    ]\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    return None  # Return None if no path is found\n",
    "\n",
    "def configure_environment_jupyter():\n",
    "    \"\"\"\n",
    "    Specify directory of Chromedriver and the data storage folder in Jupyter Notebook.\n",
    "    \"\"\"\n",
    "    detected_chromedriver = find_chromedriver_path()  # Automatically find the chromedriver path\n",
    "    initial_chromedriver_value = detected_chromedriver if detected_chromedriver else os.getcwd()\n",
    "    if initial_chromedriver_value:\n",
    "        chromedriver_selector = widgets.Text(\n",
    "            description='Chromedriver:',\n",
    "            value=initial_chromedriver_value,\n",
    "            layout=widgets.Layout(width='90%')\n",
    "        )\n",
    "        print(f\"Detected the Chromedriver folder: {initial_chromedriver_value}\")\n",
    "        config[\"Chromedriver_Path\"] = initial_chromedriver_value        \n",
    "    else:\n",
    "        chromedriver_selector = widgets.Text(\n",
    "            description='Chromedriver:',\n",
    "            value=os.getcwd(),\n",
    "            layout=widgets.Layout(width='90%')\n",
    "        )\n",
    "        chromedriver_selector.continuous_update = False\n",
    "        chromedriver_selector.observe(on_submit_chromedriver, names='value')\n",
    "        print(\"Please select the Chromedriver folder:\")\n",
    "        display(chromedriver_selector)        \n",
    "\n",
    "    detected_datafolder = find_datafolder()  # Automatically find the chromedriver path\n",
    "    initial_datafolder = detected_datafolder if detected_datafolder else os.getcwd()\n",
    "    if initial_datafolder:\n",
    "        data_storage_selector = widgets.Text(\n",
    "            description='Chromedriver:',\n",
    "            value=initial_datafolder,\n",
    "            layout=widgets.Layout(width='90%')\n",
    "        )\n",
    "        print(f\"Detected the data storage folder: {initial_datafolder}\")\n",
    "        config[\"Data_Storage_Path\"] = initial_datafolder \n",
    "    else:\n",
    "        data_storage_selector = widgets.Text(\n",
    "            description='Data Storage:',\n",
    "            value=os.getcwd(),\n",
    "            layout=widgets.Layout(width='90%')\n",
    "        )\n",
    "        data_storage_selector.continuous_update = False\n",
    "        data_storage_selector.observe(on_submit_data_storage, names='value')\n",
    "        print(\"Please select the data storage folder:\")\n",
    "        display(data_storage_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução das funções de teste do ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected the Chromedriver folder: c:\\Users\\marco\\fioce\\chromedriver\n",
      "Detected the data storage folder: c:\\kgufc_cienciasmedicas\\\n"
     ]
    }
   ],
   "source": [
    "configure_environment_jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver_path = config.get(\"Chromedriver_Path\", \"Default_Value\")\n",
    "data_storage_path = config.get(\"Data_Storage_Path\", \"Default_Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VERSÕES O BROWSER E DO CHROMEDRIVER INSTALADAS\n",
      "     Versão do browser: 117.0.5938.150\n",
      "Versão do chromedriver: 117.0.5938.88\n"
     ]
    }
   ],
   "source": [
    "try_browser(chromedriver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT\n",
      "Interpretador em uso: c:\\Users\\marco\\.conda\\envs\\beakerx\\python.exe\n",
      "    Ambiente ativado: beakerx\n",
      "     Python: 3.11.2 | packaged by Anaconda, Inc. | (main, Mar 27 2023, 23:35:04) [MSC v.1916 64 bit (AMD64)] \n",
      "        Pip: 23.2.1 \n",
      "\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Jul_11_03:10:21_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.128\n",
      "Build cuda_12.2.r12.2/compiler.33053471_0\n"
     ]
    }
   ],
   "source": [
    "try_amb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VERSÕES DO PYTORCH E GPU DISPONÍVEIS\n",
      "    PyTorch: 2.0.1+cu118\n",
      "Dispositivo: cuda\n",
      "Disponível : cuda True  | Inicializado: False | Capacidade: (7, 5)\n",
      "Nome GPU   : NVIDIA GeForce RTX 2060          | Quantidade: 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Marca do Processador: AMD64 Family 25 Model 33 Stepping 0, AuthenticAMD\n",
      "Modelo do Processador: AMD Ryzen 7 5800X 8-Core Processor\n",
      "Frequência da CPU: 3801.0 MHz\n",
      "Arquitetura do Processador: 64bit\n",
      "Número de CPUs físicas: 8\n",
      "Número de CPUs lógicas: 16\n",
      "Uso atual CPU: 2.8%\n",
      "Tempos de CPU: user=11.7%, system=2.0%, idle=86.1%\n",
      "\n",
      "Total de RAM: 63.94 GB\n",
      "Usado em RAM: 31.37 GB\n",
      "Espaço Total em disco:  465.15 GB\n",
      "Espaço em disco usado:  448.39 GB 96.4%\n",
      "Espaço em disco livre:   16.76 GB  3.6%\n"
     ]
    }
   ],
   "source": [
    "try_cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importar pacotes necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python.exe -m pip install --upgrade pip\n",
    "# !pip install selenium --upgrade\n",
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "t00=time.time()\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os, csv, sys, pip, time, string, re\n",
    "import warnings, traceback\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from string import Formatter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Renderização de imagem para exibição dentro da linha do dataframe\n",
    "def path_to_image_html(path):\n",
    "\n",
    "    return '<img src=\"'+ path + '\" style=max-height:124px;\"/>'\n",
    "\n",
    "def show_im():\n",
    "    CSS = \"\"\"\n",
    "    .output {\n",
    "        flex-direction: row;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    HTML('<style>{}</style>'.format(CSS))\n",
    "    \n",
    "def foto():\n",
    "    '''Aplica filtro no campo com o link para a foto do autor do Dataframe de identificação extraído do currículo Lattes e renderiza a foto em HTML a partir deste link\n",
    "    Autor: Marcos Aires Fev.2022\n",
    "    '''\n",
    "    campo='Link Foto:'\n",
    "    images_df       = df_identificacao[df_identificacao['ROTULOS'] == campo]  # filter by sport input, must be soccer or basketball for this use case\n",
    "    image_grid      = df_identificacao['CONTEUDOS']\n",
    "    image_grid_html = HTML(df_identificacao[:1].to_html(escape=False,formatters=dict(conteudos=path_to_image_html)))\n",
    "    display(image_grid_html)\n",
    "    show_im()\n",
    "\n",
    "def tempo(start, end):\n",
    "    t=end-start\n",
    "\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "\n",
    "def horas(segundos): \n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(segundos)) \n",
    "\n",
    "\n",
    "def dias_horas_minutos(td):\n",
    "    x = (td.days, td.seconds//3600, (td.seconds//60)%60, td.seconds)\n",
    "    return x #(days, hrs, mins, seconds)\n",
    "\n",
    "\n",
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    \n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    \n",
    "    return f.format(fmt, **values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_pastas(caminho):\n",
    "    import sys\n",
    "\n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "    \n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'/xml_zip/'\n",
    "    pathcsv  = caminho+'/csv/'\n",
    "    pathjson = caminho+'/json/'\n",
    "    pathfig  = caminho+'/fig/'\n",
    "    pathaux  = caminho+'/'\n",
    "    pathout  = caminho+'/output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XML', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    \n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Localização das pastas e dados de origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta armazenagem local c:\\kgufc_cienciasmedicas\\\n",
      "\n",
      "Pasta para os xml já existe!\n",
      "Pasta para os CSV já existe!\n",
      "Pasta para os JSON já existe!\n",
      "Pasta para figuras já existe!\n",
      "Pasta para saídas já existe!\n",
      "\n",
      "Caminho da pasta raiz c:\\kgufc_cienciasmedicas\\/\n",
      "Caminho arquivos  XML c:\\kgufc_cienciasmedicas\\/xml_zip/\n",
      "Caminho arquivos JSON c:\\kgufc_cienciasmedicas\\/json/\n",
      "Caminho arquivos  CSV c:\\kgufc_cienciasmedicas\\/csv/\n",
      "Caminho para  figuras c:\\kgufc_cienciasmedicas\\/fig/\n",
      "Pasta arquivos saídas c:\\kgufc_cienciasmedicas\\/output/\n",
      "\n",
      "T01 Importação e Preparação das pastas de trabalho: 00h 00m 00s\n"
     ]
    }
   ],
   "source": [
    "programa  = 'PPGCM'\n",
    "\n",
    "nome_ppos_graduação = 'CIÊNCIAS MÉDICAS'\n",
    "\n",
    "pathzip, pathcsv, pathjson, pathfig, pathaux, pathout = preparar_pastas(data_storage_path)\n",
    "\n",
    "t01 = time.time()\n",
    "print('\\nT01 Importação e Preparação das pastas de trabalho:', tempo(t00,t01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center><b>IMPLEMENTAR SCRIPTS DE EXTRAÇÃO</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de extração e organização de dados de docentes em lote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- path_to_image_html(path)\n",
    "- show_im()\n",
    "- foto()\n",
    "- retry(func, expected_ex_type=Exception, limit=0, wait_ms=50, wait_increase_ratio=2, logger=None, on_exhaust=\"throw\")\n",
    "- achar_busca(browser, delay)\n",
    "- definir_filtros(browser, delay, assunto=False, todosniveis=False)\n",
    "- acessar_busca(browser, delay)\n",
    "- preencher_busca(browser, delay, NOME)\n",
    "- paginar(browser)\n",
    "- restante(lista_nomes, df_dados)\n",
    "- extrair_emlote(lista_nomes, assunto=False, todosniveis=False)\n",
    "- procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite)\n",
    "- extrair_lista_discentes(lista_nomes, instituicao='Fiocruz', unidade='Fiocruz Ceará', termo='Ministério da Saúde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções assessórias para extrair dados dos currículo, sem download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- retry()\n",
    "- detect_error(html_text)\n",
    "- conectar_busca()\n",
    "- preencher_busca(browser, delay, NOME)\n",
    "- achar_busca(browser, delay)\n",
    "- paginar(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonte: https://github.com/davidohana/python-retry-func/blob/master/retry.py\n",
    "# davidohana/python-retry-func is licensed under the Apache License 2.0\n",
    "\n",
    "delay=10\n",
    "\n",
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys    \n",
    "from selenium.webdriver.common.by import By\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "def detect_error(html_text):\n",
    "    if \"Stale file handle\" in html_text:\n",
    "        raise Exception(\"Stale file handle detected.\")\n",
    "\n",
    "def conectar_busca():    \n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    t0=time.time()\n",
    "    options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    \n",
    "    chromedriver_path = config.get(\"Chromedriver_Path\", \"Default_Value\")\n",
    "    driver_path = chromedriver_path+'/chromedriver.exe'\n",
    "    service = Service(driver_path)\n",
    "    browser = webdriver.Chrome(service=service)    \n",
    "    \n",
    "    url_buscaespecialista = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=false&textoBusca='\n",
    "    browser.get(url_buscaespecialista) # acessa a url de busca do CNPQ   \n",
    "    browser.set_window_position(-20, -10)\n",
    "    browser.set_window_size(170, 1896)\n",
    "    browser.mouse = webdriver.ActionChains(browser)\n",
    "    # url        = browser.command_executor._url #\"http://127.0.0.1:60622/hub\"\n",
    "    # session_id = browser.session_id            #'4e167f26-dc1d-4f51-a207-f761eaf73c31'\n",
    "    # t1 = time.time()\n",
    "    # tcon = tempo(t0,t1)\n",
    "    # print(f'{tcon} para conectar ao servidor do CNPq')\n",
    "    # print('Conectado com sucesso em:', url, session_id)   \n",
    "    time.sleep(0.00001)\n",
    "    # return browser, url, session_id\n",
    "    return browser\n",
    "    \n",
    "def preencher_busca(browser, delay, NOME):\n",
    "    \"\"\"\n",
    "    Função para passar o nome para campo de busca\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nome = lambda: browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        seletorcss = 'div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        WebDriverWait(browser, delay).until(EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "\n",
    "        # seletorcss = \"#botaoBuscaFiltros\"\n",
    "        # WebDriverWait(browser, delay).until(EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "\n",
    "        # Retrieve the HTML content to check for error\n",
    "        html_content = browser.page_source\n",
    "        detect_error(html_content)\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e)\n",
    "        \n",
    "        # Check if the error is 'Stale file handle'\n",
    "        if \"Stale file handle\" in str(e):\n",
    "            print(\"Detectado erro 'Stale file handle'. Voltando...\")\n",
    "            time.sleep(2)  # Pause for 2 seconds before clicking the back button\n",
    "            browser.execute_script(\"window.history.go(-1)\")  # Execute JavaScript to navigate back\n",
    "            \n",
    "        time.sleep(1.5)\n",
    "    \n",
    "def achar_busca(browser, delay):\n",
    "    '''\n",
    "    Função para clicar no botão Buscar Currículo\n",
    "    '''\n",
    "    \n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = browser.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)\n",
    "\n",
    "def paginar(browser):\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  #seletorcss=\"div.paginacao:nth-child(4) > a:nth-child(2)\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar funções para extrair dados do Lattes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS.: Para efetuar instalações em ambientes com SSL você pode precisar adicionar pelo menos dois parâmetros:\n",
    "\n",
    "    Parâmetro 1 :--trusted-host pypi.org\n",
    "    Parâmetro 2 :--trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyjarowinkler --trusted-host pypi.org --trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\fioce\\chromedriver\n",
      "c:\\kgufc_cienciasmedicas\\\n"
     ]
    }
   ],
   "source": [
    "print(config.get(\"Chromedriver_Path\", \"Default_Value\"))\n",
    "print(config.get(\"Data_Storage_Path\", \"Default_Value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite):\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        # try:\n",
    "        #     css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "        #     WebDriverWait(browser, delay).until(\n",
    "        #         EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "        #     soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "        #     div_element = soup.find('div', {'class': 'tit_form'})\n",
    "        #     match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "        #     if match:\n",
    "        #         qte_resultados = int(match.group(1))\n",
    "        #         if qte_resultados >1:\n",
    "        #             print(f'{qte_resultados} resultados para {NOME}')\n",
    "        #     else:\n",
    "        #         raise Exception\n",
    "        # except Exception as e:\n",
    "        #     print('Erro ao ler a quantidade de resultados:')\n",
    "        #     print(e)\n",
    "        #     return np.NaN, NOME, np.NaN, e, browser\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))\n",
    "\n",
    "            soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "            # Check for 'Stale file handle' in HTML content\n",
    "            if 'Stale file handle' in soup.get_text():\n",
    "                raise Exception(\"Erro 'Stale file handle' na resposta HTML do servidor CNPq.\")\n",
    "\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                if qte_resultados > 1:\n",
    "                    print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception(\"Não foi possível achar uma quantidade de resultados com procurar_vinculos.\")\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, browser        \n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(browser)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(browser)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(browser)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(browser, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', browser\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, browser\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # browser.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = browser.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', browser\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função procurar_vinculos()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, browser\n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def garantir_checkbox_marcado(browser, css_selector):\n",
    "    try:\n",
    "        checkbox = browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "        \n",
    "        if not checkbox.is_selected():\n",
    "            checkbox.click()\n",
    "            print(\"Checkbox buscar todos níveis de formação marcado.\")\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        print(f\"Checkbox com o seletor {css_selector} não encontrado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao tentar marcar o checkbox: {e}\")\n",
    "\n",
    "def garantir_checkbox_desmarcado(browser, css_selector):\n",
    "    try:\n",
    "        checkbox = browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "        \n",
    "        if checkbox.is_selected():\n",
    "            checkbox.click()\n",
    "            print(\"Checkbox desmarcado.\")\n",
    "        else:\n",
    "            print(\"Checkbox já estava desmarcado.\")\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        print(f\"Checkbox com o seletor {css_selector} não encontrado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao tentar desmarcar o checkbox: {e}\")\n",
    "\n",
    "def definir_filtros(browser, delay, mestres, assunto):\n",
    "    '''\n",
    "    Clica nos check-boxes para definir os filtros de buscas\n",
    "    Para buscar por Assuntos usar parâmetro True, caso omitido fará busca por Nome por default\n",
    "    '''\n",
    "    from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.common.by import By\n",
    "    import traceback   \n",
    "    \n",
    "    ## Aguardar carregar e clicar em checkbox de Assunto\n",
    "    try:\n",
    "        if mestres:\n",
    "            limite=2\n",
    "            ## Aguardar opção dropdown ser carregada e clicar em sua checkbox\n",
    "            css_buscar_demais = '#buscarDemais'\n",
    "            garantir_checkbox_marcado(browser, css_buscar_demais)\n",
    "        # css_estrangeiros  = '#buscarEstrangeiros'\n",
    "\n",
    "        if assunto == True:\n",
    "            ## Implementar número de retentativas para casos de conexão muito instável\n",
    "            limite=5\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#buscaAssunto\"))).click(),\n",
    "                #    expected_ex_type=ZeroDivisionError, \n",
    "                   wait_ms=200,\n",
    "                   limit=limite, \n",
    "                #    logger=logger, \n",
    "                   on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "        \n",
    "            ## Aguardar opção Atuação Profissional ser carregada e clicar em sua checkbox\n",
    "            xpath_atuacaoprofissional = \".//*[contains(text(), 'Atuação profissional')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_atuacaoprofissional))).click()\n",
    "\n",
    "            ## Aguardar opção Ciências da Saúde ser carregada e clicar em sua checkbox\n",
    "            xpath_cienciassaude = \".//*[contains(text(), 'Ciências da Saúde')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_cienciassaude))).click()\n",
    "            #browser.find_element_by_xpath(xpath_cienciassaude).click()        \n",
    "\n",
    "            ## Aguardar opção Enfermagem ser carregada e clicar em sua checkbox\n",
    "            xpath_enfermagem    = \".//*[contains(text(), 'Enfermagem')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_enfermagem))).click()\n",
    "            #browser.find_element_by_xpath(xpath_enfermagem).click()\n",
    "            aplicar_link  = browser.find_element(By.LINK_TEXT, 'Aplicar')\n",
    "            aplicar_link.click()\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f'Erro na função definir_filtros()')\n",
    "        print(e)\n",
    "        # traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "        # print(e, traceback_str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nova_consulta(browser):\n",
    "    try:\n",
    "        btn_filtros = WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//*[@id='botaoBuscaFiltros']\")))\n",
    "        btn_filtros.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Erro ao reiniciar consulta')\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "        print(e,traceback_str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_dados(lista_nomes, mestres=True, assunto=False):\n",
    "    '''Extrai as informações brutas de publicações (artigos e livros) de cada currículo da Plataforma Lattes do CNPQ\n",
    "     Recebe: Uma lista de nomes a ser buscada na base do currículo Lattes\n",
    "    Utiliza: Funções: definir_filtros(), montar_dfcolab_linhas()\n",
    "    Retorna: Três dataframes: df_identificacao com dados da identificação; \n",
    "                              df_dados com dados de todas produções; e \n",
    "                              df_colabartigos com dados das colaborações em artigos\n",
    "    Autor: Marcos Aires (Jan 2022)\n",
    "    '''\n",
    "    import time\n",
    "    from datetime import date\n",
    "    t0=time.time()\n",
    "    \n",
    "    instituicao = 'Universidade Federal do Ceará'\n",
    "    unidade     = 'Ciências Médicas'\n",
    "    termo       = 'Medicina'\n",
    "\n",
    "    browser=conectar_busca()\n",
    "    browser.set_window_position(-20, -10)\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')   \n",
    "    size    = browser.get_window_size()\n",
    "    width1  = size.get(\"width\")\n",
    "    height1 = size.get(\"height\")\n",
    "    browser.set_window_size(170, 1896)\n",
    "    browser.mouse = webdriver.ActionChains(browser)\n",
    "    delay   = 20  # seconds \n",
    "    limite  = 5\n",
    "    \n",
    "    df_dados          = pd.DataFrame()   \n",
    "    rotulos           = []\n",
    "    conteudos         = []\n",
    "    curriculos        = []\n",
    "    falhas            = []\n",
    "    duvidas           = []\n",
    "    sucesso           = []\n",
    "    tipo_erro         = []\n",
    "\n",
    "    # df_parcial = pd.DataFrame({     \n",
    "    #         'NOMES': pd.Series(sucesso),\n",
    "    #         'ROTULOS': pd.Series(rotulos),\n",
    "    #         'CONTEUDOS': pd.Series(conteudos),                    \n",
    "    #     })\n",
    "\n",
    "    t1=time.time()\n",
    "    print(tempo(t0,t1), 'Tempo de conexão ao servidor do CNPq')\n",
    "    time.sleep(0.00001)\n",
    "            \n",
    "    count=0\n",
    "    for NOME in lista_nomes:\n",
    "        try:\n",
    "            # Definir filtros para busca de nomes\n",
    "            if mestres:\n",
    "                css_buscar_demais = '#buscarDemais'\n",
    "                garantir_checkbox_marcado(browser, css_buscar_demais)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            if assunto:\n",
    "                css_buscar_demais = '#buscaAssunto'\n",
    "                garantir_checkbox_marcado(browser, css_buscar_demais)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            print('-'*90)\n",
    "            count+=1\n",
    "            t2       = time.time()\n",
    "            tdec     = np.round(t2-t0,2)\n",
    "            restante = len(lista_nomes)-count\n",
    "            print(f'Extraindo currículo {count}/{len(lista_nomes)}. Resta {restante}. Decorrido:{horas(tdec)}. Previsão de término em {horas(np.round(tdec/count,0)*(restante+1))}')\n",
    "            preencher_busca(browser, delay, NOME)\n",
    "            window_before  = browser.current_window_handle\n",
    "            \n",
    "            # t2a = time.time()\n",
    "            elemento_achado, nome_falha, duvida, erro, browser = procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite)\n",
    "\n",
    "            if str(elemento_achado) == 'nan':\n",
    "                print('Vínculo não encontrado, passando ao próximo nome...')\n",
    "                falhas.append(nome_falha)\n",
    "                duvidas.append(duvida)\n",
    "                tipo_erro.append(erro)\n",
    "                # print(nome_falha)\n",
    "                # print(erro)\n",
    "                # clear_output(wait=True)\n",
    "                raise Exception\n",
    "\n",
    "            print('Vínculo encontrado no currículo de nome:',elemento_achado.text)\n",
    "\n",
    "            # t2b = time.time()\n",
    "            # print(f'{tempo(t2a,t2b)} para achar e escolher o currículo com vínculo: ')\n",
    "\n",
    "            ## Clicar no botão abrir currículo e mudar de aba\n",
    "            try:\n",
    "                ## Aguarda, encontra, clica em buscar nome\n",
    "                link_nome    = achar_busca(browser, delay)\n",
    "                nome_buscado = []\n",
    "                nome_achado  = []\n",
    "                nome_buscado.append(NOME)\n",
    "                \n",
    "                if link_nome.text == None:\n",
    "                    xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "                    # 'Stale file handle'\n",
    "                    print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "                    retry(WebDriverWait(browser, delay).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "                try:\n",
    "                    ActionChains(browser).click(link_nome).perform()\n",
    "                    nome_achado.append(link_nome.text)\n",
    "                except:\n",
    "                    print(f'Currículo não encontrado para: {NOME}.')\n",
    "                    return\n",
    "                \n",
    "                retry(WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "                \n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                time.sleep(0.2)\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print('Erro',e)\n",
    "                print('Tentando nova requisição ao servidor')\n",
    "                time.sleep(1)\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "                time.sleep(1)\n",
    "\n",
    "            t3=time.time()\n",
    "\n",
    "            ## O objeto elementos_id abaixo é uma lista de elementos onde as informações de identificação estão contidas\n",
    "            # acessado através do marcador xpath='//div[@class=\"infpessoa\"]' no HTML para extrair de cada pesquisador\n",
    "            time.sleep(1)\n",
    "            \n",
    "            soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "            xpath='//div[@class=\"infpessoa\"]'\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "            elementos_id = browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "            # Fazer com que a primeira informação para cada pesquisador seja o caminho para sua foto e dados de identificação\n",
    "            try:\n",
    "                css_selector='.foto'\n",
    "                link_foto=WebDriverWait(browser, delay).until(\n",
    "                    EC.visibility_of_element_located((By.CSS_SELECTOR, \".foto\"))).get_attribute(\"src\")\n",
    "                rotulos.append('Link Foto:')\n",
    "                conteudos.append(link_foto)\n",
    "                curriculos.append(NOME)            \n",
    "\n",
    "            except Exception as e:\n",
    "                traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "                print('  !!Erro ao extrair imagem do currículo:',e,'\\n', traceback_str)\n",
    "\n",
    "            for i in range(len(elementos_id)):\n",
    "                dados = elementos_id[i].text.split('\\n')\n",
    "                for i in range(len(dados)):\n",
    "                    if i==0:\n",
    "                        rotulos.append('Nome completo:')\n",
    "                        conteudos.append(dados[i])\n",
    "                        curriculos.append(NOME)\n",
    "                    elif 'Bolsista' in dados[i]:\n",
    "                        rotulos.append('Bolsista CNPq:')\n",
    "                        conteudos.append(dados[i])\n",
    "                        curriculos.append(NOME)\n",
    "                    elif 'Endereço para acessar este CV: ' in dados[i]:\n",
    "                        rotulos.append('Link Currículo:')\n",
    "                        conteudos.append(dados[i].strip('Endereço para acessar este CV: '))\n",
    "                        curriculos.append(NOME)\n",
    "                    elif 'ID Lattes: ' in dados[i]:\n",
    "                        rotulos.append('ID Lattes:')\n",
    "                        conteudos.append(dados[i].strip('ID Lattes: '))\n",
    "                        curriculos.append(NOME)\n",
    "                    elif 'Última atualização do currículo em ' in dados[i]:\n",
    "                        rotulos.append('Data atualização:')\n",
    "                        conteudos.append(dados[i].strip('Última atualização do currículo em '))\n",
    "                        curriculos.append(NOME)\n",
    "                        dt_atualizacao = dados[i].strip('Última atualização do currículo em ')\n",
    "                        dtt = datetime.strptime(dt_atualizacao, '%d/%m/%Y').date()\n",
    "                        defasagem = (date.today()-dtt).days\n",
    "            try: \n",
    "                df_temp =pd.DataFrame({\n",
    "                    'CURRICULO': pd.Series(curriculos),\n",
    "                    'ROTULOS': pd.Series(rotulos),\n",
    "                    'CONTEUDOS': pd.Series(conteudos),\n",
    "                        })\n",
    "                filtro    = 'Link Foto:'\n",
    "                fotos     = df_temp[(df_temp.ROTULOS == filtro)]['CONTEUDOS']\n",
    "                x         = fotos[-1:].index[0]\n",
    "                df_temp.drop(columns=['ROTULOS'], inplace=True)\n",
    "\n",
    "                try:\n",
    "                    foto = HTML(df_temp[x:x+1].to_html(escape=False, formatters=dict(CONTEUDOS=path_to_image_html)))\n",
    "                    display(foto)\n",
    "                    print(f'Atualizado em {dt_atualizacao} há {defasagem:>2} dias | {NOME}')    \n",
    "\n",
    "                except TimeoutException as t:\n",
    "                    print('Demora na conexão com servidor, carregamento da foto cancelado')\n",
    "                    traceback_str = ''.join(traceback.format_tb(t.__traceback__))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print('Erro ao extrair a foto do pesquisador')\n",
    "                traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "                print(e,traceback_str)\n",
    "\n",
    "            t4=time.time()\n",
    "            \n",
    "            try:\n",
    "                ## O objeto ELEMENTOS abaixo é uma lista de elementos onde as informações estão contidas\n",
    "                time.sleep(2)\n",
    "                # A forma antiga .find_elements_by_xpath( deve ser substituída por .find_elements(By.XPATH,\n",
    "                elementos = browser.find_elements(By.XPATH, '//div[@class=\"title-wrapper\"]')\n",
    "\n",
    "                # print('Len Cotainers 01:')\n",
    "                # pprint(elementos)\n",
    "                # for i in elementos:\n",
    "                #     print(i.text)\n",
    "\n",
    "                ## Extração das demais sessões que se organizam em pares rótulo/conteúdo\n",
    "\n",
    "                for i in range(len(elementos)):\n",
    "                    # print(len(elementos))\n",
    "                    sessao=elementos[i].text.split('\\n')[0]\n",
    "                    # print(sessao)\n",
    "                    sessoes=['Formação acadêmica/titulação',\n",
    "                            'Pós-doutorado',\n",
    "                            'Formação Complementar',\n",
    "                            'Linhas de pesquisa',\n",
    "                            'Projetos de pesquisa',\n",
    "                            'Projetos de desenvolvimento',\n",
    "                            'Membro de corpo editorial',\n",
    "                            'Membro de comitê de assessoramento',\n",
    "                            'Revisor de periódico',\n",
    "                            'Áreas de atuação',\n",
    "                            'Prêmios e títulos',\n",
    "                            'Idiomas',\n",
    "                            'Educação e Popularização de C & T',\n",
    "                            'Outras informações relevantes',\n",
    "                            ]\n",
    "\n",
    "                    ## Seção Orientações\n",
    "                    if sessao == 'Orientações':                \n",
    "                        rotulos_subsecoes=[\n",
    "                            'Orientações e supervisões em andamento',\n",
    "                            'Orientações e supervisões concluídas',\n",
    "                        ]\n",
    "\n",
    "                        rotulos_tipo =[\n",
    "                            'Dissertação de mestrado',\n",
    "                            'Tese de doutorado',\n",
    "                            'Supervisão de pós-doutorado',\n",
    "                            'Dissertação de mestrado',\n",
    "                            'Trabalho de conclusão de curso de graduação',\n",
    "                            'Tese de doutorado',\n",
    "                            'Supervisão de pós-doutorado',\n",
    "                            'Iniciação científica',\n",
    "                            'Orientações de outra natureza',\n",
    "                        ]                 \n",
    "                        \n",
    "                        ## Montagem dos dados a partir da extração da seção específica dos containers:\n",
    "                        for i in range(len(elementos)):\n",
    "                            if i != 0:\n",
    "                                secao=elementos[i].text.split('\\n')[0]\n",
    "\n",
    "                                ## Filtra para receber dados somente da seção produção, monta cada linha de dados com a quebra de linha\n",
    "                                if secao == 'Orientações':\n",
    "                                    dados=elementos[i].text.split('\\n')[1:]\n",
    "\n",
    "                        ## Montar a lista de números de linha com os respectivos dados de interesse e dividir em rótulos e conteúdos\n",
    "                        linhas_subsecoes=[]\n",
    "                        linhas_tipos=[]\n",
    "                        linhas_dados=[]\n",
    "                        for i in range(len(dados)):\n",
    "                            for rotulo in rotulos_subsecoes:\n",
    "                                if rotulo in dados[i] and i not in linhas_subsecoes:\n",
    "                                    linhas_subsecoes.append(i)\n",
    "\n",
    "                            for rotulo in rotulos_tipo:\n",
    "                                if rotulo in dados[i] and 'Citações:' not in dados[i] and 'Citações no' not in dados[i] and i not in linhas_tipos:\n",
    "                                    linhas_tipos.append(i)\n",
    "\n",
    "                            if i not in linhas_subsecoes and i not in linhas_tipos and \"Citações no\" not in dados[i] and dados[i] !='':\n",
    "                                linhas_dados.append(dados[i])\n",
    "\n",
    "                        ultima_linha=max(range(len(dados)))\n",
    "                        # print('Última linha:',ultima_linha)\n",
    "                        # print('DadoÚltLinha:',dados[ultima_linha])\n",
    "                        linhas_tipos.append(ultima_linha)\n",
    "\n",
    "                        # print(f'QteTotalLinhas:{len(dados)}, QteDados:{len(linhas_dados)}, QteRotulosCitações:{len(linhas_citacoes)}, QteCitações:{len(linhas_qtecitacoes)}, QteRotulosSubseção:{len(linhas_subsecoes)}, QteRotulosTipos:{len(linhas_tipos)}, QteRotulosRetirar:{len(linhas_retirar)}')\n",
    "\n",
    "                        ## Da lista de espacos monta os pares ordenados de rótulo/conteúdo\n",
    "                        limites_conteudos=[]\n",
    "                        lst_i=linhas_tipos[::1]\n",
    "                        lst_j=linhas_tipos[1::1]\n",
    "                        # print(len(lst_i),lst_i)\n",
    "                        # print(len(lst_j),lst_j)\n",
    "\n",
    "                        for i, j in zip(lst_i,lst_j):\n",
    "                            limites_conteudos.append((i,j))\n",
    "\n",
    "                        # print('LimitesConteúdo:',len(limites_conteudos),limites_conteudos)\n",
    "\n",
    "                        ## Para cada par ordenado separar em colunas de rótulos e coluna de conteúdos\n",
    "                        for i in range(len(limites_conteudos)):\n",
    "                            par=limites_conteudos[i]\n",
    "                            if i<(len(limites_conteudos)-1):\n",
    "                                try:\n",
    "                                    rotulos.append('Orientação '+dados[par[0]])\n",
    "                                    # print('Linhas:', par[0]+1,par[1])\n",
    "                                    conteudos.append(dados[par[0]+1:par[1]])\n",
    "                                    curriculos.append(NOME)\n",
    "                                except:\n",
    "                                    print('Erro ao montar pares ordenados de rótulo/conteúdo')\n",
    "                                    pass\n",
    "                            else:\n",
    "                                try:\n",
    "                                    rotulos.append('Orientação '+dados[par[0]])\n",
    "                                    # print('Linhas:', par[0]+1,par[1])\n",
    "                                    conteudos.append(dados[par[0]+1:par[1]+1])\n",
    "                                    curriculos.append(NOME)\n",
    "                                except:\n",
    "                                    print('Erro ao montar pares ordenados de rótulo/conteúdo')\n",
    "                                    pass\n",
    "\n",
    "                        # Montar dataframe com colunas de rótulos e conteúdos\n",
    "                        # df_orientacoes=pd.DataFrame(zip(rotulos,conteudos), columns=['rotulos','conteudos'])                    \n",
    "                    \n",
    "                    if sessao == 'Identificação':\n",
    "                        dados = elementos[i].text.split('\\n')[1:]\n",
    "                        for i, j in zip(dados[::2], dados[1::2]):\n",
    "                            curriculos.append(NOME)\n",
    "                            rotulos.append(i)\n",
    "                            if i != 'Nome em citações bibliográficas':\n",
    "                                conteudos.append(j)\n",
    "                            else:\n",
    "                                conteudos.append(j.split(';'))\n",
    "                        # print(f'{len(rotulos):>3} Linhas extraídas seção {sessao}...')\n",
    "                    \n",
    "                    ## Seção Produções\n",
    "                    if sessao == 'Produções':\n",
    "                        rotulos_retirar=[\n",
    "                            'Ordenar por', \n",
    "                            'Ordem Cronológica',\n",
    "                            'Número de citações Web of science',\n",
    "                            'Número de citações Scopus',\n",
    "                            'Numero de citações Scielo',\n",
    "                            'Primeiro autor',\n",
    "                            'Impacto JCR',\n",
    "                            'Ordem de Importância',\n",
    "                        ]\n",
    "\n",
    "                        rotulos_citacoes =  [\n",
    "                            'Web of Science',\n",
    "                            'Total de trabalhos',\n",
    "                            'Total de citações',\n",
    "                            'Fator H',\n",
    "                            'SciELO',\n",
    "                            'Total de trabalhos',\n",
    "                            'Total de citações',\n",
    "                            'SCOPUS',\n",
    "                            'Total de trabalhos',\n",
    "                            'Total de citações',\n",
    "                            'Outras',\n",
    "                            'Total de trabalhos',                        \n",
    "                        ]\n",
    "\n",
    "                        rotulos_subsecoes=[\n",
    "                            'Produção bibliográfica',\n",
    "                            'Produção técnica',\n",
    "                            'Demais tipos de produção técnica',\n",
    "                        ]\n",
    "\n",
    "                        rotulos_tipo =[\n",
    "                            'Citações',\n",
    "                            'Artigos completos publicados em periódicos',\n",
    "                            'Livros publicados/organizados ou edições',\n",
    "                            'Capítulos de livros publicados',\n",
    "                            'Textos em jornais de notícias/revistas',\n",
    "                            'Trabalhos completos publicados em anais de congressos',\n",
    "                            'Resumos expandidos publicados em anais de congressos',\n",
    "                            'Resumos publicados em anais de congressos',\n",
    "                            'Resumos publicados em anais de congressos (artigos)',\n",
    "                            'Artigos aceitos para publicação',\n",
    "                            'Apresentações de Trabalho',\n",
    "                            'Outras produções bibliográficas',\n",
    "                            'Assessoria e consultoria',\n",
    "                            'Programas de computador sem registro',\n",
    "                            'Trabalhos técnicos',\n",
    "                            'Entrevistas, mesas redondas, programas e comentários na mídia',\n",
    "                            'Demais tipos de produção técnica',\n",
    "                        ]\n",
    "\n",
    "                        rotulos_qte_citacoes =[\n",
    "                            'Citações:',\n",
    "                        ]\n",
    "\n",
    "                        ## Montagem dos dados a partir da extração da seção específica dos containers:\n",
    "                        for i in range(len(elementos)):\n",
    "                            if i != 0:\n",
    "                                sessao=elementos[i].text.split('\\n')[0]\n",
    "                                ## Filtra para receber dados somente da seção produção, monta cada linha de dados com a quebra de linha\n",
    "                                if sessao == 'Produções':\n",
    "                                    producao_bibliografica_div = sessao.find('div', {'id': 'artigos-completos'})\n",
    "                                    if not producao_bibliografica_div:\n",
    "                                        print('Erro ao achar a div de produções')\n",
    "                                        print(soup)\n",
    "                                        return\n",
    "                                    else:\n",
    "                                        print(f'{len(producao_bibliografica_div)} elementos em producao_bibliografica_div')\n",
    "                                    dados=elementos[i].text.split('\\n')[1:]\n",
    "                                    node_name = NOME\n",
    "                                    producoes = []\n",
    "                                    json_data = {\"Node Name\": node_name, \"Properties\": {}}\n",
    "                                    ## Gerar um arquivo JSON com dados da produção bibliográfica para popular o Neo4j\n",
    "                                    try:\n",
    "                                        for artigo_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "                                            artigo_dict = {}\n",
    "\n",
    "                                            # Extrair os números de ordem dos artigos\n",
    "                                            ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                                            for index, ordem in enumerate(ordens):\n",
    "                                                b_tag = ordem.find('b')\n",
    "                                                # if b_tag:\n",
    "                                                #     print(b_tag.text.strip())\n",
    "                                                \n",
    "                                            try:\n",
    "                                                ano = artigo_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "                                            except:\n",
    "                                                ano = None\n",
    "                                            try:\n",
    "                                                prim_autor = artigo_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "                                            except:\n",
    "                                                prim_autor = None\n",
    "                                            try:\n",
    "                                                jcr = artigo_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "                                            except:\n",
    "                                                jcr = None\n",
    "                                            try:\n",
    "                                                doi = artigo_div.find('a', {'class': 'icone-doi'})['href']\n",
    "                                            except:\n",
    "                                                doi = None\n",
    "                                            # try:\n",
    "                                            #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "                                            # except:\n",
    "                                            #     titulo = None\n",
    "                                            dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "                                            list    = str(dados).split(\" . \")\n",
    "                                            autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "                                            revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "                                            titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "                                            artigo_dict['ano']     = ano\n",
    "                                            artigo_dict['autores'] = autores\n",
    "                                            artigo_dict['revista'] = revista\n",
    "                                            artigo_dict['titulo']  = titulo\n",
    "                                            artigo_dict['jcr']     = jcr\n",
    "                                            artigo_dict['doi']     = doi\n",
    "                                            producoes.append(artigo_dict)\n",
    "                                            json_data[\"Properties\"]['Produções'] = producoes                                        \n",
    "                                    except Exception as e:\n",
    "                                        print('Erro ao acessar seção de Produções:')\n",
    "                                        print(e)\n",
    "                        \n",
    "                        ## Montar a lista de números de linha com os respectivos dados de interesse e dividir em rótulos e conteúdos\n",
    "                        linhas_citacoes=[]\n",
    "                        linhas_qtecitacoes=[]\n",
    "                        linhas_retirar=[]\n",
    "                        linhas_subsecoes=[]\n",
    "                        linhas_tipos=[]\n",
    "                        linhas_dados=[]\n",
    "                        for i in range(len(dados)):\n",
    "                            for rotulo in rotulos_citacoes:\n",
    "                                if rotulo in dados[i] and i not in linhas_citacoes:\n",
    "                                    linhas_citacoes.append(i)\n",
    "\n",
    "                            for rotulo in rotulos_qte_citacoes:\n",
    "                                if rotulo in dados[i] and i not in linhas_qtecitacoes:\n",
    "                                    linhas_qtecitacoes.append(i)\n",
    "\n",
    "                            for rotulo in rotulos_retirar:\n",
    "                                if rotulo in dados[i] and i not in linhas_retirar:\n",
    "                                    linhas_retirar.append(i)\n",
    "\n",
    "                            for rotulo in rotulos_subsecoes:\n",
    "                                if rotulo in dados[i] and i not in linhas_subsecoes:\n",
    "                                    linhas_subsecoes.append(i)\n",
    "\n",
    "                            for rotulo in rotulos_tipo:\n",
    "                                if rotulo in dados[i] and 'Citações:' not in dados[i] and 'Citações no' not in dados[i] and i not in linhas_tipos:\n",
    "                                    linhas_tipos.append(i)\n",
    "\n",
    "                            if i not in linhas_citacoes and i not in linhas_qtecitacoes and i not in linhas_retirar and i not in linhas_subsecoes and i not in linhas_tipos and \"Citações no\" not in dados[i] and dados[i] !='':\n",
    "                                linhas_dados.append(dados[i])\n",
    "\n",
    "                        ultima_linha=max(range(len(dados)))\n",
    "                        # print('Última linha:',ultima_linha)\n",
    "                        # print('DadoÚltLinha:',dados[ultima_linha])\n",
    "                        linhas_tipos.append(ultima_linha)\n",
    "\n",
    "                        # print(f'QteTotalLinhas:{len(dados)}, QteDados:{len(linhas_dados)}, QteRotulosCitações:{len(linhas_citacoes)}, QteCitações:{len(linhas_qtecitacoes)}, QteRotulosSubseção:{len(linhas_subsecoes)}, QteRotulosTipos:{len(linhas_tipos)}, QteRotulosRetirar:{len(linhas_retirar)}')\n",
    "\n",
    "                        ## Da lista de espacos monta os pares ordenados de rótulo/conteúdo\n",
    "                        limites_conteudos=[]\n",
    "                        lst_i=linhas_tipos[::1]\n",
    "                        lst_j=linhas_tipos[1::1]\n",
    "                        # print(len(lst_i),lst_i)\n",
    "                        # print(len(lst_j),lst_j)\n",
    "\n",
    "                        for i, j in zip(lst_i,lst_j):\n",
    "                            limites_conteudos.append((i,j))\n",
    "\n",
    "                        # print('LimitesConteúdo:',len(limites_conteudos),limites_conteudos)\n",
    "\n",
    "                        ## Para cada par ordenado separar em colunas de rótulos e coluna de conteúdos\n",
    "                        for i in range(len(limites_conteudos)):\n",
    "                            par=limites_conteudos[i]\n",
    "                            if i<(len(limites_conteudos)-1):\n",
    "                                try:\n",
    "                                    rotulos.append(dados[par[0]])\n",
    "                                    # print('Linhas:', par[0]+1,par[1])\n",
    "                                    conteudos.append(dados[par[0]+1:par[1]])\n",
    "                                    curriculos.append(NOME)\n",
    "                                except:\n",
    "                                    print('Erro ao montar pares ordenados de rótulo/conteúdo')\n",
    "                                    pass\n",
    "                            else:\n",
    "                                try:\n",
    "                                    rotulos.append(dados[par[0]])\n",
    "                                    # print('Linhas:', par[0]+1,par[1])\n",
    "                                    conteudos.append(dados[par[0]+1:par[1]+1])\n",
    "                                    curriculos.append(NOME)\n",
    "                                except:\n",
    "                                    print('Erro ao montar pares ordenados de rótulo/conteúdo')\n",
    "                                    pass        \n",
    "                        \n",
    "                    ## Seções de organização simples em rótulos e conteúdos\n",
    "                    elif sessao in sessoes:\n",
    "                        subtitulo = elementos[i].text.split('\\n')[0]\n",
    "                        dados = elementos[i].text.split('\\n')[1:]\n",
    "                        indices=[]\n",
    "                        for i in range(len(dados)):\n",
    "                            numbers = re.findall('[0-9]+', dados[i])\n",
    "                            hifen = re.findall('-', dados[i])\n",
    "                            if len(numbers) == 2 and len(hifen) == 1:\n",
    "                                indices.append(i)\n",
    "                                periodo = f' {numbers[0]}{hifen[0]}{numbers[1]}'\n",
    "                                rotulos.append(subtitulo+\" \"+periodo)\n",
    "                            else:\n",
    "                                pass                    \n",
    "                        \n",
    "                        finais=[]\n",
    "                        for i in indices[1::1]: # slice: a partir do elemento 1 até o final da lista, contanto de 1 em 1\n",
    "                            finais.append(i-1)\n",
    "                        finais.append(len(dados))\n",
    "\n",
    "                        for ini, fim in zip(indices, finais):\n",
    "                            conteudos.append(dados[ini+1:fim])\n",
    "                            curriculos.append(NOME)\n",
    "                \n",
    "                ## Fechar janela do currículo\n",
    "                browser.close()            \n",
    "                \n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                todas_janelas = browser.window_handles\n",
    "                browser.switch_to.window(todas_janelas[0])\n",
    "\n",
    "                ## Fechar a janela pop-up\n",
    "                close_popup = WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, \"//*[@id='idbtnfechar']\")))\n",
    "                close_popup.click()\n",
    "                \n",
    "                ## Nova Consulta\n",
    "                try:\n",
    "                    btn_novaconsulta = WebDriverWait(browser, delay).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, \"//*[@id='botaoBuscaFiltros']\")))\n",
    "                    btn_novaconsulta.click()\n",
    "                    time.sleep(1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print('Erro ao reiniciar consulta')\n",
    "                    traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "                    print(e,traceback_str) \n",
    "                \n",
    "                t5=time.time()\n",
    "                # print(f' {tempo(t0,t5)} | Tempo de Acesso |  Identificação |   Dados Brutos | Subtotal Tempo | Acumulado')\n",
    "                # print(f'  Decorrido  |   {tempo(t2,t3)}   |  {tempo(t3,t4)}   |  {tempo(t4,t5)}   |  {tempo(t2,t5)}   | {len(conteudos)} seções')\n",
    "                \n",
    "                sucesso.append(NOME)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print('Erro ao montar dataframe dados de artigos')\n",
    "                traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "                print(e,traceback_str)    \n",
    "                browser.quit()\n",
    "                \n",
    "                return np.NaN, np.NaN, np.NaN\n",
    "        except:\n",
    "            print(f'Currículo não encontrado: {NOME}')\n",
    "            browser.back()\n",
    "            pass\n",
    "\n",
    "    df_dados =pd.DataFrame({\n",
    "        'CURRICULO': pd.Series(curriculos),\n",
    "        'ROTULOS': pd.Series(rotulos),\n",
    "        'CONTEUDOS': pd.Series(conteudos),\n",
    "            })\n",
    "    \n",
    "    t6=time.time()\n",
    "    print('='*95)\n",
    "    # print(f' {len(sucesso)} currículos extraídos com sucesso')\n",
    "    print(f' Tempo total para extrair {len(conteudos)} seções dos currículos: {tempo(t0,t6)}')\n",
    "    # print('='*95)\n",
    "    browser.quit()\n",
    "    \n",
    "    return df_dados, sucesso, json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_teste = ['Carlos Jose Araujo Pinheiro', 'Claudia Stutz Zubieta','Antonio Marcos Aires Barbosa']\n",
    "# df_secoes_teste, sucesso_teste, json_data = extrair_dados(lista_teste, mestres=True, assunto=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_lista_discentes(lista_nomes, instituicao='Fiocruz', unidade='Ceará', termo='Fundação Oswaldo Cruz'):\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    from IPython.display import clear_output\n",
    "    import numpy as np\n",
    "    import time\n",
    "\n",
    "    delay=15\n",
    "    rotulos=[]\n",
    "    conteudos=[]\n",
    "    tipo_erro=[]\n",
    "    falhas=[]\n",
    "    duvidas=[]\n",
    "    sucessos=[]\n",
    "    \n",
    "    t0 = time.time()\n",
    "    browser = conectar_busca()\n",
    "    t1 = time.time()\n",
    "    \n",
    "    for k,NOME in enumerate(lista_nomes):\n",
    "        resta=len(lista_nomes)-(k+1)\n",
    "        tdec=time.time()-t1\n",
    "        tmed=tdec/(k+1)\n",
    "        trest=tmed*resta\n",
    "        print(f'{k+1}/{len(lista_nomes)}, restando {len(lista_nomes)-int(k+1)}. Decorrido: {horas(time.time()-t1)}. Previsão tempo para término: {horas(trest)}')\n",
    "        t2 = time.time()\n",
    "        limite=5\n",
    "\n",
    "        ## Definir filtros para busca de nomes\n",
    "        definir_filtros(browser, delay, assunto=False, todosniveis=True) # filtros customizados para todos níveis\n",
    "    \n",
    "        ## Colar a query desejada e guardar a janela corrente\n",
    "        preencher_busca(browser, delay, NOME)      \n",
    "        window_before  = browser.current_window_handle\n",
    "        \n",
    "        t2a = time.time()\n",
    "        elemento_achado, nome_falha, duvida, erro, browser = procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite)\n",
    "        # print('Elemento encontrado:',elemento_achado)\n",
    "        # print(nome_falha)\n",
    "        # print(tipo_erro)\n",
    "        \n",
    "        if str(elemento_achado) == 'nan':\n",
    "            print('Vínculo não encontrado passando ao próximo nome...')\n",
    "            falhas.append(nome_falha)\n",
    "            duvidas.append(duvida)\n",
    "            tipo_erro.append(erro)\n",
    "            clear_output(wait=True)\n",
    "            continue\n",
    "            \n",
    "        t2b = time.time()\n",
    "        print(f'{tempo(t2a,t2b)} para achar e escolher o currículo com vínculo: ')\n",
    "    \n",
    "        try:\n",
    "            print('Aguardando abertura da janela de currículo...')\n",
    "            ## Clicar no botão abrir currículo\n",
    "            cssselector_btn = \"#idbtnabrircurriculo\"\n",
    "            waitbtn = WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, cssselector_btn)))\n",
    "            time.sleep(0.5)\n",
    "            elemento_btn = browser.find_element(By.CSS_SELECTOR, cssselector_btn)             \n",
    "            texto_botao  = elemento_btn.text\n",
    "            # print('Clicando no botão:', texto_botao)\n",
    "            retry(elemento_btn.click(),\n",
    "                   wait_ms=20,\n",
    "                   limit=limite, \n",
    "                   on_exhaust=(f'Problema clicar em {texto_botao}, {limite} tentativas sem sucesso.'))\n",
    "            \n",
    "            ## Gerenciamento das janelas abertas no browser e mudar de aba\n",
    "            WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "            window_after = browser.window_handles\n",
    "            new_window   = [x for x in window_after if x != window_before][0]\n",
    "            browser.switch_to.window(new_window)\n",
    "            time.sleep(2) # tempo para garantir mudança de aba em conexão lenta\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('Erro na função extrair_lista_discentes(), ao clicar no botão para abrir o currículo')\n",
    "            print(e)\n",
    "            falhas.append(NOME)\n",
    "            duvidas.append(np.NaN)\n",
    "            tipo_erro.append(e)            \n",
    "            continue\n",
    "        \n",
    "        ## Extrair dados e salvar em dataframe\n",
    "        print('Iniciando a captura dos dados do currículo...')\n",
    "        time.sleep(0.5)\n",
    "        t3 = time.time()\n",
    "        css_dadoscentral = \".main-content > div:nth-child(1)\"\n",
    "        \n",
    "        ## O objeto elementos_id abaixo é uma lista de elementos onde as informações de identificação estão contidas\n",
    "        ## acessado através do marcador xpath='//div[@class=\"infpessoa\"]' no HTML para extrair de cada pesquisador\n",
    "        xpath='//div[@class=\"infpessoa\"]'\n",
    "        WebDriverWait(browser, delay).until(\n",
    "                EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "        elementos_id = browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "        ## primeira informação para cada pesquisador seja o caminho para sua foto e dados de identificação\n",
    "        try:\n",
    "            css_selector='.foto'\n",
    "            link_foto=WebDriverWait(browser, delay).until(\n",
    "                EC.visibility_of_element_located((By.CSS_SELECTOR, \".foto\"))).get_attribute(\"src\")\n",
    "            rotulos.append('Link Foto:')\n",
    "            conteudos.append(link_foto)            \n",
    "\n",
    "        except Exception as e:\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print('  !!Erro ao extrair imagem do currículo:',e,'\\n', traceback_str)\n",
    "            falhas.append(NOME)\n",
    "            duvidas.append(np.NaN)\n",
    "            tipo_erro.append(e)              \n",
    "            continue\n",
    "        \n",
    "        for i in range(len(elementos_id)):\n",
    "            dados = elementos_id[i].text.split('\\n')\n",
    "            for i in range(len(dados)):\n",
    "                if i==0:\n",
    "                    rotulos.append('Nome completo:')\n",
    "                    conteudos.append(dados[i])\n",
    "                elif 'Bolsista' in dados[i]:\n",
    "                    rotulos.append('Bolsista CNPq:')\n",
    "                    conteudos.append(dados[i])\n",
    "                elif 'Endereço para acessar este CV: ' in dados[i]:\n",
    "                    rotulos.append('Link Currículo:')\n",
    "                    conteudos.append(dados[i].strip('Endereço para acessar este CV: '))\n",
    "                elif 'ID Lattes: ' in dados[i]:\n",
    "                    rotulos.append('ID Lattes:')\n",
    "                    conteudos.append(dados[i].strip('ID Lattes: '))\n",
    "                    print('  ID_LATTES:',dados[i].strip('ID Lattes: '))\n",
    "                elif 'Última atualização do currículo em ' in dados[i]:\n",
    "                    rotulos.append('Data atualização:')\n",
    "                    conteudos.append(dados[i].strip('Última atualização do currículo em '))\n",
    "                    print('ATUALIZAÇÃO:',dados[i].strip('Última atualização do currículo em '))\n",
    "        try: \n",
    "            df_temp =pd.DataFrame({\n",
    "                'ROTULOS': pd.Series(rotulos),\n",
    "                'CONTEUDOS': pd.Series(conteudos),\n",
    "                    })\n",
    "            filtro    = 'Link Foto:'\n",
    "            fotos     = df_temp[(df_temp.ROTULOS == filtro)]['CONTEUDOS']\n",
    "            x         = fotos[-1:].index[0]\n",
    "            df_temp.drop(columns=['ROTULOS'], inplace=True)\n",
    "\n",
    "            try:\n",
    "                foto = HTML(df_temp[x:x+1].to_html(escape=False, formatters=dict(CONTEUDOS=path_to_image_html)))\n",
    "                display(foto)\n",
    "\n",
    "            except TimeoutException as t:\n",
    "                print('Demora na conexão com servidor, carregamento da foto cancelado')\n",
    "                print(t)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Erro ao extrair a foto do pesquisador')\n",
    "            print(e)\n",
    "            falhas.append(NOME)\n",
    "            duvidas.append(np.NaN)\n",
    "            tipo_erro.append(e)              \n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            css_resumo = \".resumo\"\n",
    "            resumo = browser.find_elements(By.CSS_SELECTOR, css_resumo)\n",
    "            for i in resumo:\n",
    "                print(i.text)\n",
    "                rotulos.append('Resumo:')\n",
    "                conteudos.append(i.text) \n",
    "        except Exception as e:\n",
    "            print('Erro ao extrair o resumo:', e)\n",
    "            falhas.append(NOME)\n",
    "            duvidas.append(np.NaN)\n",
    "            tipo_erro.append(e)              \n",
    "            continue\n",
    "        \n",
    "        t4=time.time() \n",
    "        print('Dados de cabeçalho carregados...', tempo(t3,t4))\n",
    "            \n",
    "        ## Extração de dados de produções e orientações\n",
    "        try:\n",
    "            ## O objeto ELEMENTOS abaixo é uma lista de elementos onde as informações estão contidas\n",
    "            time.sleep(2)\n",
    "            elementos = browser.find_elements(By.XPATH, '//div[@class=\"title-wrapper\"]')\n",
    "            # print('Len Cotainers 01:')\n",
    "            # pprint(elementos)\n",
    "            # for i in elementos:\n",
    "            #     print(i.text)\n",
    "        \n",
    "            ## Extração das demais sessões que se organizam em pares rótulo/conteúdo\n",
    "            for i in range(len(elementos)):\n",
    "                # print(len(elementos))\n",
    "                sessao=elementos[i].text.split('\\n')[0]\n",
    "                # print(sessao)\n",
    "                sessoes=['Formação acadêmica/titulação',\n",
    "                         'Pós-doutorado',\n",
    "                         'Formação Complementar',\n",
    "                         'Linhas de pesquisa',\n",
    "                         'Projetos de pesquisa',\n",
    "                         'Projetos de desenvolvimento',\n",
    "                         'Membro de corpo editorial',\n",
    "                         'Membro de comitê de assessoramento',\n",
    "                         'Revisor de periódico',\n",
    "                         'Áreas de atuação',\n",
    "                         'Prêmios e títulos',\n",
    "                         'Idiomas',\n",
    "                         'Educação e Popularização de C & T',\n",
    "                         'Outras informações relevantes',\n",
    "                        ]\n",
    "\n",
    "                ## Seção Orientações\n",
    "                if sessao == 'Orientações':                \n",
    "                    rotulos_subsecoes=[\n",
    "                        'Orientações e supervisões em andamento',\n",
    "                        'Orientações e supervisões concluídas',\n",
    "                    ]\n",
    "        \n",
    "                    rotulos_retirar=['Orientações e supervisões concluídas ', \n",
    "                                     'Orientações e supervisões concluídas']\n",
    "\n",
    "                    rotulos_tipo =[\n",
    "                        'Dissertação de mestrado',\n",
    "                        'Tese de doutorado',\n",
    "                        'Supervisão de pós-doutorado',\n",
    "                        'Dissertação de mestrado',\n",
    "                        'Trabalho de conclusão de curso de graduação',\n",
    "                        'Tese de doutorado',\n",
    "                        'Supervisão de pós-doutorado',\n",
    "                        # 'Iniciação científica',\n",
    "                        'Orientações de outra natureza',\n",
    "                    ]                 \n",
    "\n",
    "                    ## Montagem dos dados a partir da extração da seção específica dos containers:\n",
    "                    for i in range(len(elementos)):\n",
    "                        if i != 0:\n",
    "                            secao=elementos[i].text.split('\\n')[0]\n",
    "\n",
    "                            ## Filtra para receber dados somente da seção produção, monta cada linha de dados com a quebra de linha\n",
    "                            if secao == 'Orientações':\n",
    "                                dados=elementos[i].text.split('\\n')[1:]\n",
    "        \n",
    "                    ## Montar a lista de números de linha com os respectivos dados de interesse e dividir em rótulos e conteúdos\n",
    "                    linhas_subsecoes=[]\n",
    "                    linhas_tipos=[]\n",
    "                    linhas_dados=[]\n",
    "                    for i in range(len(dados)):\n",
    "                        for rotulo in rotulos_subsecoes:\n",
    "                            if rotulo in dados[i] and i not in linhas_subsecoes:\n",
    "                                linhas_subsecoes.append(i)\n",
    "\n",
    "                        for rotulo in rotulos_tipo:\n",
    "                            if rotulo in dados[i] and 'Citações:' not in dados[i] and 'Citações no' not in dados[i] and i not in linhas_tipos:\n",
    "                                linhas_tipos.append(i)\n",
    "\n",
    "                        if i not in linhas_subsecoes and i not in linhas_tipos and \"Citações no\" not in dados[i] and dados[i] !='':\n",
    "                            linhas_dados.append(dados[i])\n",
    "\n",
    "                    ultima_linha=max(range(len(dados)))\n",
    "                    # print('Última linha:',ultima_linha)\n",
    "                    # print('DadoÚltLinha:',dados[ultima_linha])\n",
    "                    linhas_tipos.append(ultima_linha)\n",
    "                    # print(f'QteTotalLinhas:{len(dados)}, QteDados:{len(linhas_dados)}, QteRotulosCitações:{len(linhas_citacoes)}, QteCitações:{len(linhas_qtecitacoes)}, QteRotulosSubseção:{len(linhas_subsecoes)}, QteRotulosTipos:{len(linhas_tipos)}, QteRotulosRetirar:{len(linhas_retirar)}')\n",
    "\n",
    "                    ## Da lista de espacos monta os pares ordenados de rótulo/conteúdo\n",
    "                    limites_conteudos=[]\n",
    "                    lst_i=linhas_tipos[::1]\n",
    "                    lst_j=linhas_tipos[1::1]\n",
    "                    # print(len(lst_i),lst_i)\n",
    "                    # print(len(lst_j),lst_j)\n",
    "\n",
    "                    for i, j in zip(lst_i,lst_j):\n",
    "                        limites_conteudos.append((i,j))\n",
    "                    # print('LimitesConteúdo:',len(limites_conteudos),limites_conteudos)\n",
    "\n",
    "                    ## Para cada par ordenado separar em colunas de rótulos e coluna de conteúdos\n",
    "                    for i in range(len(limites_conteudos)):\n",
    "                        par=limites_conteudos[i]\n",
    "                        if i<(len(limites_conteudos)-1):\n",
    "                            try:\n",
    "                                rotulos.append('Orientação '+dados[par[0]])\n",
    "                                # print('Linhas:', par[0]+1,par[1])\n",
    "                                conteudos.append(dados[par[0]+1:par[1]])\n",
    "                            except:\n",
    "                                print('Erro ao montar pares ordenados de rótulo/conteúdo')\n",
    "                                pass\n",
    "                        else:\n",
    "                            try:\n",
    "                                rotulos.append('Orientação '+dados[par[0]])\n",
    "                                # print('Linhas:', par[0]+1,par[1])\n",
    "                                conteudos.append(dados[par[0]+1:par[1]+1])\n",
    "                            except:\n",
    "                                print('Erro ao montar pares ordenados de rótulo/conteúdo')\n",
    "                                pass\n",
    "\n",
    "                    ## Montar dataframe com colunas de rótulos e conteúdos só sobre orientações\n",
    "                    ## df_orientacoes=pd.DataFrame(zip(rotulos,conteudos), columns=['rotulos','conteudos'])                    \n",
    "\n",
    "                if sessao == 'Identificação':\n",
    "                    dados = elementos[i].text.split('\\n')[1:]\n",
    "                    for i, j in zip(dados[::2], dados[1::2]):\n",
    "                        rotulos.append(i)\n",
    "                        if i != 'Nome em citações bibliográficas':\n",
    "                            conteudos.append(j)\n",
    "                        else:\n",
    "                            conteudos.append(j.split(';'))\n",
    "                        # print(f'{len(rotulos):>3} Linhas extraídas seção {sessao}...')\n",
    "\n",
    "                ## Seção Produções\n",
    "                if sessao == 'Produções':\n",
    "                    rotulos_retirar=[\n",
    "                        'Ordenar por', \n",
    "                        'Ordem Cronológica',\n",
    "                        'Número de citações Web of science',\n",
    "                        'Número de citações Scopus',\n",
    "                        'Numero de citações Scielo',\n",
    "                        'Primeiro autor',\n",
    "                        'Impacto JCR',\n",
    "                        'Ordem de Importância',\n",
    "                    ]\n",
    "                    rotulos_citacoes =  [\n",
    "                        'Web of Science',\n",
    "                        'Total de trabalhos',\n",
    "                        'Total de citações',\n",
    "                        'Fator H',\n",
    "                        'Pinheiro, Placido R  Data',\n",
    "                        'SciELO',\n",
    "                        'Total de trabalhos',\n",
    "                        'Total de citações',\n",
    "                        'Placido Rogerio Pinheiro  Data',\n",
    "                        'SCOPUS',\n",
    "                        'Total de trabalhos',\n",
    "                        'Total de citações',\n",
    "                        'Placido Rogerio Pinheiro (Fator h-index',\n",
    "                        'Outras',\n",
    "                        'Total de trabalhos',\n",
    "                    ]\n",
    "                    rotulos_subsecoes=[\n",
    "                        'Produção bibliográfica',\n",
    "                        'Produção técnica',\n",
    "                        'Demais tipos de produção técnica',\n",
    "                    ]\n",
    "                    rotulos_tipo =[\n",
    "                        'Citações',\n",
    "                        'Artigos completos publicados em periódicos',\n",
    "                        'Livros publicados/organizados ou edições',\n",
    "                        'Capítulos de livros publicados',\n",
    "                        'Textos em jornais de notícias/revistas',\n",
    "                        'Trabalhos completos publicados em anais de congressos',\n",
    "                        'Resumos expandidos publicados em anais de congressos',\n",
    "                        'Resumos publicados em anais de congressos',\n",
    "                        'Apresentações de Trabalho',\n",
    "                        'Outras produções bibliográficas',\n",
    "                        'Assessoria e consultoria',\n",
    "                        'Programas de computador sem registro',\n",
    "                        'Trabalhos técnicos',\n",
    "                        'Entrevistas, mesas redondas, programas e comentários na mídia',\n",
    "                        'Demais tipos de produção técnica',\n",
    "                    ]\n",
    "                    rotulos_qte_citacoes =[\n",
    "                        'Citações:',\n",
    "                    ]\n",
    "\n",
    "                    ## Montagem dos dados a partir da extração da seção específica dos containers:\n",
    "                    for i in range(len(elementos)):\n",
    "                        if i != 0:\n",
    "                            sessao=elementos[i].text.split('\\n')[0]\n",
    "\n",
    "                            ## Filtra para receber dados somente da seção produção, monta cada linha de dados com a quebra de linha\n",
    "                            if sessao == 'Produções':\n",
    "                                dados=elementos[i].text.split('\\n')[1:]\n",
    "\n",
    "                    ## Montar a lista de números de linha com os respectivos dados de interesse e dividir em rótulos e conteúdos\n",
    "                    linhas_citacoes=[]\n",
    "                    linhas_qtecitacoes=[]\n",
    "                    linhas_retirar=[]\n",
    "                    linhas_subsecoes=[]\n",
    "                    linhas_tipos=[]\n",
    "                    linhas_dados=[]\n",
    "                    for i in range(len(dados)):\n",
    "                        for rotulo in rotulos_citacoes:\n",
    "                            if rotulo in dados[i] and i not in linhas_citacoes:\n",
    "                                linhas_citacoes.append(i)\n",
    "                        for rotulo in rotulos_qte_citacoes:\n",
    "                            if rotulo in dados[i] and i not in linhas_qtecitacoes:\n",
    "                                linhas_qtecitacoes.append(i)\n",
    "                        for rotulo in rotulos_retirar:\n",
    "                            if rotulo in dados[i] and i not in linhas_retirar:\n",
    "                                linhas_retirar.append(i)\n",
    "                        for rotulo in rotulos_subsecoes:\n",
    "                            if rotulo in dados[i] and i not in linhas_subsecoes:\n",
    "                                linhas_subsecoes.append(i)\n",
    "                        for rotulo in rotulos_tipo:\n",
    "                            if rotulo in dados[i] and 'Citações:' not in dados[i] and 'Citações no' not in dados[i] and i not in linhas_tipos:\n",
    "                                linhas_tipos.append(i)\n",
    "                        if i not in linhas_citacoes and i not in linhas_qtecitacoes and i not in linhas_retirar and i not in linhas_subsecoes and i not in linhas_tipos and \"Citações no\" not in dados[i] and dados[i] !='':\n",
    "                            linhas_dados.append(dados[i])\n",
    "\n",
    "                    ultima_linha=max(range(len(dados)))\n",
    "                    linhas_tipos.append(ultima_linha)\n",
    "                    # print('Última linha:',ultima_linha)\n",
    "                    # print('DadoÚltLinha:',dados[ultima_linha])\n",
    "                    # print(f'QteTotalLinhas:{len(dados)}, QteDados:{len(linhas_dados)}, QteRotulosCitações:{len(linhas_citacoes)}, QteCitações:{len(linhas_qtecitacoes)}, QteRotulosSubseção:{len(linhas_subsecoes)}, QteRotulosTipos:{len(linhas_tipos)}, QteRotulosRetirar:{len(linhas_retirar)}')\n",
    "\n",
    "                    ## Da lista de espacos monta os pares ordenados de rótulo/conteúdo\n",
    "                    limites_conteudos=[]\n",
    "                    lst_i=linhas_tipos[::1]\n",
    "                    lst_j=linhas_tipos[1::1]\n",
    "                    # print(len(lst_i),lst_i)\n",
    "                    # print(len(lst_j),lst_j)\n",
    "\n",
    "                    for i, j in zip(lst_i,lst_j):\n",
    "                        limites_conteudos.append((i,j))\n",
    "                    # print('LimitesConteúdo:',len(limites_conteudos),limites_conteudos)\n",
    "\n",
    "                    ## Para cada par ordenado separar em colunas de rótulos e coluna de conteúdos\n",
    "                    for i in range(len(limites_conteudos)):\n",
    "                        par=limites_conteudos[i]\n",
    "                        if i<(len(limites_conteudos)-1):\n",
    "                            try:\n",
    "                                rotulos.append(dados[par[0]])\n",
    "                                # print('Linhas:', par[0]+1,par[1])\n",
    "                                conteudos.append(dados[par[0]+1:par[1]])\n",
    "                            except:\n",
    "                                print('Erro ao montar pares ordenados de rótulo/conteúdo')\n",
    "                                pass\n",
    "                        else:\n",
    "                            try:\n",
    "                                rotulos.append(dados[par[0]])\n",
    "                                # print('Linhas:', par[0]+1,par[1])\n",
    "                                conteudos.append(dados[par[0]+1:par[1]+1])\n",
    "                            except:\n",
    "                                print('Erro ao montar pares ordenados de rótulo/conteúdo')\n",
    "                                pass        \n",
    "\n",
    "                ## Seções de organização simples em rótulos e conteúdos\n",
    "                elif sessao in sessoes:\n",
    "                    subtitulo = elementos[i].text.split('\\n')[0]\n",
    "                    dados = elementos[i].text.split('\\n')[1:]\n",
    "                    indices=[]\n",
    "                    for i in range(len(dados)):\n",
    "                        numbers = re.findall('[0-9]+', dados[i])\n",
    "                        hifen = re.findall('-', dados[i])\n",
    "                        if len(numbers) == 2 and len(hifen) == 1:\n",
    "                            indices.append(i)\n",
    "                            periodo = f' {numbers[0]}{hifen[0]}{numbers[1]}'\n",
    "                            rotulos.append(subtitulo+\" \"+periodo)\n",
    "                        else:\n",
    "                            pass                    \n",
    "\n",
    "                    finais=[]\n",
    "                    for i in indices[1::1]: # slice: a partir do elemento 1 até o final da lista, contanto de 1 em 1\n",
    "                        finais.append(i-1)\n",
    "                    finais.append(len(dados))\n",
    "\n",
    "                    for ini, fim in zip(indices, finais):\n",
    "                        conteudos.append(dados[ini+1:fim])\n",
    "\n",
    "            t5=time.time()                       \n",
    "            print(\"=\"*95)\n",
    "            print(f' {tempo(t0,t5)} | Tempo de Acesso |  Identificação |   Dados Brutos | Subtotal Tempo | Acumulado')\n",
    "            print(f'  Decorrido  |   {tempo(t2,t3)}   |  {tempo(t3,t4)}   |  {tempo(t4,t5)}   |  {tempo(t2,t5)}   | {len(conteudos)} seções')\n",
    "            print(\"=\"*95)\n",
    "            sucessos.append(NOME)              \n",
    "        \n",
    "        except Exception as e:\n",
    "            print('Erro ao montar dataframe dados de artigos')\n",
    "            print(e)\n",
    "            falhas.append(NOME)\n",
    "            duvidas.append(np.NaN)\n",
    "            tipo_erro.append(e)\n",
    "            continue\n",
    "\n",
    "        print(f' Tempo para extrair currículo com {len(conteudos)} seções: {tempo(t2,t5)}')\n",
    "        print('='*95)\n",
    "        \n",
    "        ## Fechar janela do currículo\n",
    "        browser.close() \n",
    "\n",
    "        ## Gerenciamento das janelas abertas no browser\n",
    "        print('Retornando para aba de busca...')\n",
    "        browser.switch_to.window(window_before)\n",
    "        # todas_janelas = browser.window_handles \n",
    "        # browser.switch_to.window(todas_janelas[0])\n",
    "\n",
    "        ## Fechar a janela pop-up\n",
    "        # print('Fechando janela pop-up do currículo')\n",
    "        css_fecharpopup = \"#idbtnfechar\"\n",
    "        fecharpopup = WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, css_fecharpopup)))\n",
    "        fecharpopup = browser.find_element(By.CSS_SELECTOR, css_fecharpopup)\n",
    "        fecharpopup.click()\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        ## Nova Consulta\n",
    "        try:\n",
    "            # print('Retornando para página de busca...')\n",
    "            css_novaconsulta = \"#botaoBuscaFiltros\"\n",
    "            btn_novaconsulta = browser.find_element(By.CSS_SELECTOR, css_novaconsulta)             \n",
    "            retry(btn_novaconsulta.click(),\n",
    "                   wait_ms=50,\n",
    "                   limit=limite, \n",
    "                   on_exhaust=(f'Problema clicar em {css_novaconsulta}, {limite} tentativas sem sucesso.'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Erro ao reiniciar consulta')\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(e,traceback_str)\n",
    "            falhas.append(NOME)\n",
    "            duvidas.append(np.NaN)\n",
    "            tipo_erro.append(e)              \n",
    "            continue\n",
    "    \n",
    "    df_dados =pd.DataFrame({\n",
    "                            'ROTULOS': pd.Series(rotulos),\n",
    "                            'CONTEUDOS': pd.Series(conteudos),\n",
    "                            })\n",
    "    \n",
    "    t6=time.time()\n",
    "    print('='*95)\n",
    "    print(f' {len(sucessos)} currículos extraídos com sucesso')\n",
    "    print(f' Tempo total para extrair {len(conteudos)} seções dos currículos: {tempo(t0,t6)}')\n",
    "    print('='*95)\n",
    "    browser.quit()\n",
    "            \n",
    "    return df_dados, falhas, duvidas, tipo_erro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para dividir detalhes dos artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padrões de expressão regular\n",
    "pattern_m01a = r' \\. '\n",
    "pattern_m01b = r' et al. '\n",
    "pattern_m01c = r' \\; '\n",
    "pattern_m01d = r'\\. '\n",
    "pattern_vol  = r' v\\. (\\d+)'\n",
    "pattern_pag  = r' p\\. (.*?),'\n",
    "pattern_ano  = r'\\d{4}\\.$'\n",
    "\n",
    "def find_positions(pattern, string):\n",
    "    return [match.start() for match in re.finditer(pattern, string)]\n",
    "\n",
    "def find_vol(input_string):\n",
    "    pattern = r\" v\\. (\\d+)\"\n",
    "    match   = re.search(pattern, input_string)\n",
    "    try:\n",
    "        volume = match.group(1)\n",
    "        return volume, match.start()\n",
    "    except:\n",
    "        return ['','']\n",
    "\n",
    "def find_pag(input_string):\n",
    "    pattern = r\" p\\. (.*?),\"\n",
    "    match   = re.search(pattern, input_string)\n",
    "    try:\n",
    "        pages = match.group(1)\n",
    "        return pages, match.start()\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        return ['','']\n",
    "\n",
    "def find_year(input_string):\n",
    "    pattern = r'\\d{4}\\.$'\n",
    "    match = re.search(pattern, input_string)\n",
    "    year  = match.group().strip('.')\n",
    "    return year, match.start()\n",
    "\n",
    "def find_marker_positions(input_string, markers):\n",
    "    marker_positions = {}\n",
    "    for marker in markers:\n",
    "        position = input_string.find(marker)\n",
    "        if position != -1:\n",
    "            marker_positions[position] = marker\n",
    "    return marker_positions\n",
    "\n",
    "def find_odd(input_string, marker):\n",
    "    odds_positions = []\n",
    "    \n",
    "    for order, position in enumerate([pos for pos, char in enumerate(input_string) if input_string[pos:pos+len(marker)] == marker], start=1):\n",
    "        if order % 2 != 0:\n",
    "            odds_positions.append(position)\n",
    "    \n",
    "    return odds_positions\n",
    "\n",
    "def find_even(input_string, marker):\n",
    "    evens_positions = []\n",
    "    \n",
    "    for order, position in enumerate([pos for pos, char in enumerate(input_string) if input_string[pos:pos+len(marker)] == marker], start=0):\n",
    "        if order % 2 != 0:\n",
    "            evens_positions.append(position)\n",
    "    \n",
    "    return evens_positions\n",
    "\n",
    "\n",
    "def split_string_at_positions(input_string, positions):\n",
    "    substrings = []\n",
    "    start = 0\n",
    "\n",
    "    for position in positions:\n",
    "        substrings.append(input_string[start:position])\n",
    "        start = position\n",
    "\n",
    "    substrings.append(input_string[start:])\n",
    "\n",
    "    return substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_publication_data(input_string):\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # Extract Year\n",
    "    try:\n",
    "        year, year_position = find_year(input_string)\n",
    "        extracted_data['ANO'] = year\n",
    "        input_string = input_string[:year_position]\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred while extracting the year: {e}\")\n",
    "    \n",
    "    # List of patterns in order of priority\n",
    "    priority_patterns = [pattern_m01a, pattern_m01b, pattern_m01c, pattern_m01d]\n",
    "    m1_used = None\n",
    "    p1_end = None\n",
    "\n",
    "    # Identify m1 following the hierarchical preference\n",
    "    for pattern in priority_patterns:\n",
    "        positions = find_positions(pattern, input_string)\n",
    "        if positions:\n",
    "            m1_used = pattern\n",
    "            p1_end = positions[0]\n",
    "            break\n",
    "\n",
    "    # User interaction if no m1 marker is found\n",
    "    if m1_used is None:\n",
    "        user_marker = input(\"No marker found for splitting data for m1. Please specify a marker: \")\n",
    "        positions = find_positions(user_marker, input_string)\n",
    "        if positions:\n",
    "            m1_used = user_marker\n",
    "            p1_end = positions[0]\n",
    "\n",
    "    p1 = input_string[:p1_end].strip()\n",
    "    extracted_data['LISTA_AUTORES'] = p1\n",
    "    # extracted_data['M1_USED'] = m1_used\n",
    "    \n",
    "    # Identify m2 for p2 and p3\n",
    "    p2_end_candidates = [find_vol(input_string)[1], find_pag(input_string)[1]]\n",
    "    m2_labels = ['vol', 'pag']\n",
    "    m2_used = None\n",
    "    p2_end_candidates = [pos for pos in p2_end_candidates if pos is not None and pos != '']\n",
    "    \n",
    "    if len(p2_end_candidates) == 0:\n",
    "        last_dot_space = input_string.rfind('. ')\n",
    "        if last_dot_space != -1:\n",
    "            p2_end_candidates.append(last_dot_space)\n",
    "            m2_used = '. '\n",
    "        else:\n",
    "            user_input = input(\"No suitable marker found for p2 and p3. Specify either 'vol' or 'pag': \")\n",
    "            p2_end_candidates = find_positions(user_input, input_string)\n",
    "            m2_used = user_input\n",
    "    \n",
    "    else:\n",
    "        m2_used = m2_labels[p2_end_candidates.index(min(p2_end_candidates))]\n",
    "    \n",
    "    p2_end = min(p2_end_candidates)\n",
    "    p2 = input_string[p1_end:p2_end].lstrip(m1_used).strip()\n",
    "    extracted_data['ARTIGO_REVISTA'] = p2\n",
    "    # extracted_data['M2_USED'] = m2_used\n",
    "    \n",
    "    p3_start = max(p2_end_candidates)\n",
    "    p3 = input_string[p3_start:].strip()\n",
    "    \n",
    "    volume, _ = find_vol(p3)\n",
    "    pages, _ = find_pag(p3)\n",
    "    \n",
    "    if volume != '':\n",
    "        extracted_data['VOLUME'] = volume\n",
    "    if pages != '':\n",
    "        extracted_data['PAGES'] = pages\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para separar nomes de autores \n",
    "\n",
    "(Funcionando bem somente para separador ';' melhorar para ausência dele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_abreviation(substring):\n",
    "    pattern = r'(?: [a-zA-Z]\\.)|(?: [a-zA-Z] \\.)'\n",
    "    if re.search(pattern, substring):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_surname(substring):\n",
    "    return not is_abreviation(substring) and substring.endswith(', ')\n",
    "\n",
    "def compose_full_name(surname, parts, marker):\n",
    "    full_name = \"\"\n",
    "    for part in parts:\n",
    "        full_name += part + marker\n",
    "    full_name += ' '+surname\n",
    "    return full_name.strip()\n",
    "\n",
    "def split_authors(string, verbose=False):\n",
    "    authors_names = []\n",
    "    \n",
    "    m1a = \"; \"\n",
    "    m1b = \", \"\n",
    "    \n",
    "    if m1a in string and (string.count(m1a) <= string.count(m1b) or m1b not in string):\n",
    "        marker = m1a\n",
    "        authors_names = string.split(marker)\n",
    "        return [x.strip() for x in authors_names]\n",
    "    else:\n",
    "        marker = m1b\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Selected marker: \"{marker}\"')\n",
    "\n",
    "    parts_list = string.split(marker)\n",
    "    name    = \"\"\n",
    "    surname = \"\"\n",
    "    \n",
    "    for part in parts_list:\n",
    "        if is_abreviation(part):\n",
    "            classification = 'Abrev'\n",
    "            name += part + marker\n",
    "        else:\n",
    "            classification = 'Name'\n",
    "            if is_surname(part) or surname == \"\":\n",
    "                classification = 'SOBRENOME'\n",
    "                if surname:\n",
    "                    full_name = compose_full_name(surname.strip(), name.split(marker), ', ')\n",
    "                    if full_name not in authors_names:\n",
    "                        authors_names.append(full_name)\n",
    "                surname = part\n",
    "                name = part + marker\n",
    "            else:\n",
    "                name = part + marker\n",
    "        if verbose:\n",
    "            # print(f'Quantidade de m1a: {string.count(m1a)} \\nQuantidade de m1b: {string.count(m1b)}')\n",
    "            print(f'Autor: {part:40} | Forma: {classification}')\n",
    "\n",
    "        authors_names.append(part.strip(marker).strip())\n",
    "     \n",
    "    return [x.strip() for x in authors_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BARBOSA, ANTONIO MARCOS AIRES',\n",
      " 'ANTONIO MARCOS AIRES BARBOSA',\n",
      " 'BARBOSA FILHO, A. M. A.',\n",
      " 'SUZANA, B. B. A. B.']\n"
     ]
    }
   ],
   "source": [
    "# Testando a função\n",
    "input_string = 'BARBOSA, ANTONIO MARCOS AIRES ; ANTONIO MARCOS AIRES BARBOSA ; BARBOSA FILHO, A. M. A. ; SUZANA, B. B. A. B. '\n",
    "names = split_authors(input_string, 1)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected marker: \", \"\n",
      "Autor: BARBOSA                                  | Forma: SOBRENOME\n",
      "Autor: ANTONIO MARCOS AIRES                     | Forma: Name\n",
      "Autor: ANTONIO MARCOS AIRES BARBOSA             | Forma: Name\n",
      "Autor: BARBOSA FILHO                            | Forma: Name\n",
      "Autor: A. M. A.                                 | Forma: Abrev\n",
      "Autor: SUZANA                                   | Forma: Name\n",
      "Autor: B. B. A. B.                              | Forma: Abrev\n",
      "['BARBOSA',\n",
      " 'ANTONIO MARCOS AIRES',\n",
      " 'ANTONIO MARCOS AIRES BARBOSA',\n",
      " 'BARBOSA FILHO',\n",
      " 'A. M. A.',\n",
      " 'SUZANA',\n",
      " 'B. B. A. B.']\n"
     ]
    }
   ],
   "source": [
    "# Testando a função\n",
    "input_string = 'BARBOSA, ANTONIO MARCOS AIRES, ANTONIO MARCOS AIRES BARBOSA, BARBOSA FILHO, A. M. A., SUZANA, B. B. A. B.'\n",
    "names = split_authors(input_string, 1)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Antonio Marcos Aires BARBOSA'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compose_full_name('BARBOSA', 'Antonio Marcos Aires', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BARBOSA FILHO, A. M. A.',\n",
       " 'SUZANA, B. B. A. B.',\n",
       " 'ANTONIO MARCOS AIRES BARBOSA']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'BARBOSA FILHO, A. M. A. ; SUZANA, B. B. A. B. ; ANTONIO MARCOS AIRES BARBOSA '\n",
    "split_authors(string, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de montar dataframes a partir dos dados brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string(citation, verbose=False):\n",
    "    citation = citation.replace('CitaÃ§Ãµes:','Citações:')\n",
    "    # Define potential separators in order of preference\n",
    "    separators = [r' \\. ', r' ; ', r'(?<=[^A-Z\\s])\\. ']\n",
    "    \n",
    "    for sep in separators:\n",
    "            # Split the citation using regex\n",
    "            parts = re.split(sep, citation)\n",
    "            if len(parts) > 1:\n",
    "                authors = parts[0]\n",
    "                if verbose is True:\n",
    "                    print(type(authors))\n",
    "                    print(authors)\n",
    "                remaining_info = citation.split(authors)[1].lstrip(sep)\n",
    "\n",
    "                volume_match = re.search(r'v\\. (\\d+)', remaining_info)\n",
    "                volume = volume_match.group(1) if volume_match and volume_match is not None else ''\n",
    "                if volume_match:\n",
    "                    start_position = volume_match.start()\n",
    "                    title_journal = remaining_info[:start_position-2].strip()\n",
    "                    try:\n",
    "                        journal = title_journal.split('. ')[1]\n",
    "                    except:\n",
    "                        journal = ''\n",
    "                else:\n",
    "                    start_position = len(citation)\n",
    "                    title_journal = remaining_info[:start_position].strip()\n",
    "                    journal = ''\n",
    "                pages_match = re.search(r'p\\. ([\\d\\-]+)', remaining_info)\n",
    "                pages = pages_match.group(1) if pages_match and pages_match is not None else ''\n",
    "\n",
    "                year_match = re.search(r'(\\d{4}\\.)', remaining_info)\n",
    "                year = year_match.group(1).strip('.') if year_match else ''\n",
    "\n",
    "                title   = title_journal.split('. ')[0]\n",
    "                \n",
    "                if type(parts[0]) is str:\n",
    "                    authors = split_authors(parts[0], verbose=False)\n",
    "                else:\n",
    "                    authors = [authors]\n",
    "\n",
    "                return {\n",
    "                    'authors': authors,\n",
    "                    'title': title,\n",
    "                    'journal': journal,\n",
    "                    'volume': volume,\n",
    "                    'pages': pages,\n",
    "                    'year': year\n",
    "                }\n",
    "\n",
    "    return citation, ''\n",
    "    \n",
    "\n",
    "def parse_dataframe(df):\n",
    "    # Initialize an empty list to hold the parsed data\n",
    "    parsed_data = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Extract the 'ARTIGO' value\n",
    "        citation = row['ARTIGO']\n",
    "        \n",
    "        # Parse the citation using the previously defined 'parse_citation' function\n",
    "        parsed_citation = parse_string(citation)\n",
    "        \n",
    "        # Create a new dictionary that combines the parsed citation data with the remaining row data\n",
    "        new_row = parsed_citation.copy() if parsed_citation else {}\n",
    "        for col in ['ANO_PUB', 'STATUS', 'MATRÍCULA', 'NOME', 'ÁREA', 'CARGO', 'VÍNCULO', 'INGRESSO_FIOCE', 'NÍVEL', 'ANO_INGRESSO_FIOCE']:\n",
    "            new_row[col] = row[col]\n",
    "        \n",
    "        # Append this new row dictionary to the list of parsed data\n",
    "        parsed_data.append(new_row)\n",
    "        \n",
    "    # Convert the list of parsed data dictionaries into a new DataFrame\n",
    "    new_df = pd.DataFrame(parsed_data)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def find_authors_and_others(citation):\n",
    "    # Define the separator types and their priority\n",
    "    separators = [' \\. ', '; ', '. ']\n",
    "    \n",
    "    # Check for each separator in the priority order\n",
    "    for sep in separators:\n",
    "        parts = re.split(sep, citation)\n",
    "        \n",
    "        # Special case for '. ' where it might be part of an abbreviated author name\n",
    "        if sep == '. ':\n",
    "            author_parts = []\n",
    "            for i, part in enumerate(parts):\n",
    "                # If the previous part ends with an uppercase letter followed by a space, it is likely an author's abbreviated initial\n",
    "                if i > 0 and (parts[i-1][-1].isupper() and parts[i-1][-2] == ' '):\n",
    "                    author_parts.append(part)\n",
    "                else:\n",
    "                    # Once we encounter a part that doesn't meet the condition, we break and consider the authors' list complete\n",
    "                    break\n",
    "            \n",
    "            if author_parts:\n",
    "                authors = f\"{sep}\".join(parts[:len(author_parts) + 1])\n",
    "                remaining_info = f\"{sep}\".join(parts[len(author_parts) + 1:])\n",
    "                return authors, remaining_info\n",
    "        \n",
    "        else:\n",
    "            if len(parts) > 1:\n",
    "                authors = parts[0]\n",
    "                remaining_info = sep.join(parts[1:])\n",
    "                return authors, remaining_info\n",
    "                \n",
    "    return citation, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GERAR DATAFRAMES COM LISTA DE PUBLICAÇÕES DE CADA AUTOR\n",
    "def montar_publicacoes(df_secoes):\n",
    "    filtro  = 'Artigos completos publicados em periódicos'\n",
    "    artigos = df_secoes[(df_secoes.ROTULOS == filtro)]\n",
    "    print(f'Total de linhas de dados: {len(artigos)}')\n",
    "    \n",
    "    nomes   = df_secoes[df_secoes.ROTULOS=='Nome'].values\n",
    "    print(f'Total de nomes de servidores: {len(nomes)}')  \n",
    "\n",
    "    cont=[]\n",
    "    publicacoes=[]\n",
    "    l_curriculo=[]\n",
    "    l_autores=[]\n",
    "    l_titulo=[]\n",
    "    l_revista=[]\n",
    "    l_ano_pub=[]\n",
    "    l_volume=[]\n",
    "    l_paginas=[]\n",
    "    # l_primautor=[]\n",
    "    # l_ultimautor=[]\n",
    "    # l_coaut=[]\n",
    "    # l_numero=[]\n",
    "    # l_local=[]\n",
    "    # l_doi=[]\n",
    "    \n",
    "    remover =['Ordenar por','Ordem Cronológica','Número de citações Web of science','Número de citações Scopus','Numero de citações Scielo','Primeiro autor','Impacto JCR','Ordem de Importância','Livros publicados/organizados ou edições']\n",
    "    \n",
    "    for n,linha in enumerate(artigos['CONTEUDOS']):\n",
    "        linha = linha.replace('CitaÃ§Ãµes:','Citações:')\n",
    "        c=0\n",
    "        # print(type(linha))\n",
    "        for i in eval(linha):\n",
    "            if i not in remover and len(i)>15 and 'Citações:' not in i:\n",
    "                c+=1\n",
    "                cont.append(c)\n",
    "                publicacoes.append(i)\n",
    "                data = parse_string(i)\n",
    "                l_curriculo.append(nomes[n][0])\n",
    "                l_autores.append(data[\"authors\"])\n",
    "                l_titulo.append(data[\"title\"])\n",
    "                l_revista.append(data[\"journal\"])\n",
    "                l_volume.append(data[\"volume\"])\n",
    "                l_paginas.append(data[\"pages\"])\n",
    "                l_ano_pub.append(data[\"year\"])\n",
    "                #  prim_autor, ult_autor, coaut, titulo, revista, local, volume, numero, paginas, ano_publicacao, doi, problemas  = extrair_detalhes(i)\n",
    "                #  l_primautor.append(prim_autor)\n",
    "                #  l_ultimautor.append(ult_autor)\n",
    "                #  l_coaut.append(coaut)\n",
    "                #  l_numero.append(numero)\n",
    "                #  l_local.append(local)\n",
    "                #  l_doi.append(doi)\n",
    "            \n",
    "    ## Monta novo dataframe para ver primeiro e último autor separados dos demais colaboradores\n",
    "    df_artigos = pd.DataFrame({\n",
    "        'CURRICULO': pd.Series(l_curriculo),\n",
    "        'AUTORES': pd.Series(l_autores),\n",
    "        'TITULO': pd.Series(l_titulo),\n",
    "        'REVISTA': pd.Series(l_revista),\n",
    "        'ANO_PUB': pd.Series(l_ano_pub),\n",
    "        'VOLUME':pd.Series(l_volume),\n",
    "        'PAGINAS': pd.Series(l_paginas),\n",
    "        # 'PRIMEIRO_AUTOR': pd.Series(l_primautor),\n",
    "        # 'ULTIMO_AUTOR': pd.Series(l_ultimautor),\n",
    "        # 'COAUTORES': pd.Series(l_coaut),\n",
    "        # 'LOCAL': pd.Series(l_local),        \n",
    "        # 'DOI': pd.Series(l_doi),\n",
    "    })\n",
    "\n",
    "    return df_artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GERAR DATAFRAMES COM LISTA DE PUBLICAÇÕES DE CADA AUTOR E DATAFRAME COM LISTA DE COLABORAÇÕES UMA POR LINHA\n",
    "def dividir(linha_dados):\n",
    "    '''Identifica os separadores de nomes de autores em uma string\n",
    "     Recebe: a string e entrega uma lista dividida por possíveis separadores como o ;\n",
    "    Retorna: linha de dados dividida com o separador escolhido dentro da função\n",
    "      Autor: Marcos Aires Fev.2022\n",
    "    '''\n",
    "    \n",
    "    ## encontrar padrões com regular expressions \n",
    "    qte_divisor_autores  = re.compile(r';')            #Símbolo \";\" na string\n",
    "    nome_autor_inicio    = re.compile(r'^[A-ZÀ-ú]+,')  #Uma ou mais ocorrência de letras maiúsculas no início da string\n",
    "    # nome_abreviatura     = re.compile(r'^[A-Z].')      #Uma ou mais ocorrência de letras\n",
    "     \n",
    "    ## variáveis para encontrar padrões com expressões regulares\n",
    "    div_autores     = qte_divisor_autores.findall(linha_dados)\n",
    "    nomes_autores   = nome_autor_inicio.findall(linha_dados)\n",
    "    # div_abreviatura = nome_autor_inicio.findall(linha_dados)\n",
    "\n",
    "    ## parâmetros para classificar cada linha como número de ordem, dados de autor ou citações da publicação\n",
    "    cond_pntvrg = len(div_autores)>0\n",
    "    cond_pnt    = len(nomes_autores)>0\n",
    "    # cond_abrev  = len(div_abreviatura)>0\n",
    "\n",
    "    if cond_pntvrg is True:\n",
    "        div_inicial = div_pvirg=linha_dados.split(\";\") \n",
    "    elif cond_pnt is True:\n",
    "        div_inicial = div_pvirg=linha_dados.split(\". \")\n",
    "        \n",
    "    return div_inicial\n",
    "\n",
    "\n",
    "def montar_dfcolab_linhas(df_artigos):\n",
    "    '''Aplica filtro em df_dados para montar o dataframe de dados detalhados de artigos publicados\n",
    "        Recebe: Dataframe com os artigos gerado pela função montar_df_artigos(containers).\n",
    "       Utiliza: Função extrair_detalhes()\n",
    "       Retorna: Dataframe df_colabartigos com os dados de PRIMEIRO_AUTOR, ULTIMO_AUTOR, COAUTORES, TITULO, REVISTA, ANO_PUB.\n",
    "         Autor: Marcos Aires Fev.2022\n",
    "    '''\n",
    "    \n",
    "    dados_artigo=df_artigos['dados_artigo']\n",
    "    l_primautor, l_ultimautor, l_coaut, l_titulo, l_revista, l_local, l_volume, l_numero, l_paginas, l_ano_pub, l_doi = [],[],[],[],[],[],[],[],[],[],[]\n",
    "    \n",
    "    primeiro_autor=''\n",
    "    revista=''\n",
    "    doi=''\n",
    "    local=''\n",
    "    volume=''\n",
    "    numero=''\n",
    "    paginas=''\n",
    "    ano_publicacao=''\n",
    "    \n",
    "    ## Para cada artigo no dataframe extrai os dados detalhados com uso da função de quebra da string\n",
    "    for i in dados_artigo:\n",
    "        try:\n",
    "            primeiro_autor, ultimo_autor, coautores, titulo, revista, local, volume, numero, paginas, ano_publicacao, doi, problemas = extrair_detalhes(i)\n",
    "            l_primautor.append(primeiro_autor)\n",
    "            l_ultimautor.append(ultimo_autor)\n",
    "            l_coaut.append(coautores)\n",
    "            l_titulo.append(titulo)\n",
    "            l_revista.append(revista)\n",
    "            l_local.append(local)\n",
    "            l_volume.append(volume)\n",
    "            l_numero.append(numero)\n",
    "            l_paginas.append(paginas)\n",
    "            l_ano_pub.append(ano_publicacao)\n",
    "            clear_output(wait=True)\n",
    "        except Exception as e:\n",
    "            print('Erro ao montar colaborações na função montar_dfcolab_linhas()')\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(e,traceback_str)\n",
    "            \n",
    "    ## Monta novo dataframe de artigos com seus dados individualizados\n",
    "    df_colabartigos = pd.DataFrame({\n",
    "        'PRIMEIRO_AUTOR': pd.Series(l_primautor),\n",
    "        'ULTIMO_AUTOR': pd.Series(l_ultimautor),\n",
    "        'COAUTORES': pd.Series(l_coaut),\n",
    "        'TITULO': pd.Series(l_titulo),\n",
    "        'REVISTA': pd.Series(l_revista),\n",
    "        'ANO_PUB': pd.Series(l_ano_pub),\n",
    "        'local': pd.Series(l_local),\n",
    "        'rev_volume':pd.Series(l_volume),\n",
    "        'rev_numero':pd.Series(l_numero),\n",
    "        'paginas': pd.Series(l_paginas),\n",
    "    })\n",
    "    \n",
    "    return df_colabartigos, problemas\n",
    "\n",
    "\n",
    "def padronizar_titulo(titulo_bruto):\n",
    "    '''Retira acentos, expressão (Org.) e espaços vazios do título da publicação\n",
    "    Autor: Marcos Aires (Fev.2022)\n",
    "    '''\n",
    "    import unicodedata\n",
    "    import re\n",
    "    string = ''.join(ch for ch in unicodedata.normalize('NFKD', titulo_bruto) if not unicodedata.combining(ch))\n",
    "    string = string.replace('(Org)','').replace('(Org.)','').replace('(Org).','').replace('.','')\n",
    "    \n",
    "    titulo_padronizado = string.strip().strip('\"')\n",
    "    \n",
    "    return titulo_padronizado\n",
    "\n",
    "\n",
    "def montar_lista_autores(df_dadosgrupo, df_colunaautores):\n",
    "    ''' Cria lista com o nome padronizado para cada autor\n",
    "     Recebe: Dataframe com os dados brutos do grupo de pesquisa; Coluna de um Dataframe com nomes dos membros do grupo de pesquisa, com um nome de autor em cada linha; Lista de tuplas a trocar (origem, destino)\n",
    "    Utiliza: Funções padronizar_nome(nome), extrair_variantes(df_dadosgrupo)\n",
    "    Retorna: lista_padronizada com os nomes padronizados\n",
    "      Autor: Marco Aires (Fev.2022)\n",
    "    '''\n",
    "    \n",
    "#     lista_nomes=df_professores['PESQUISADORES']\n",
    "    lista_padronizada=[]\n",
    "    origem=[]\n",
    "    destino=[]\n",
    "    trocar=extrair_variantes(df_dadosgrupo)\n",
    "    for i,j in trocar:\n",
    "        origem.append(i)\n",
    "        destino.append(j)\n",
    "    \n",
    "    c=0\n",
    "    for autor in df_colunaautores:\n",
    "        # print(' ANTES:',autor)\n",
    "        c+=1\n",
    "        try:\n",
    "            nome_padronizado = padronizar_nome(autor.strip())\n",
    "        except:\n",
    "            nome_padronizado = autor.strip()\n",
    "            pass\n",
    "        if autor in origem:\n",
    "            for j,k in zip(origem,destino):\n",
    "                if autor.lower().strip()==j.lower().strip():\n",
    "                    l=k\n",
    "#             print(f'{c:3d}. {i:<35} ==> {j:<35} ==> {k}')\n",
    "            lista_padronizada.append(l)\n",
    "            # print('DEPOIS:',l)\n",
    "        else:\n",
    "            # print('DEPOIS:',nome_padronizado)\n",
    "            lista_padronizada.append(nome_padronizado)\n",
    "    \n",
    "    return lista_padronizada\n",
    "\n",
    "\n",
    "def montar_lista_coautores(df_dadosgrupo,df_colunacoautores):\n",
    "    '''\n",
    "    Cria lista com o nome padronizado de cada autor\n",
    "     Recebe: Coluna de um Dataframe com nomes dos membros do grupo de pesquisa, com vários nomes de autor em cada célula\n",
    "    Utiliza: Funções padronizar_nome(nome), extrair_variantes(df_dadosgrupo)\n",
    "    Retorna: Lista com os nomes padronizados\n",
    "      Autor: Marco Aires (Fev.2022)\n",
    "    '''\n",
    "    lista_coautores_padronizada=[]\n",
    "    origem=[]\n",
    "    destino=[]\n",
    "    \n",
    "    filtro1   = 'Nome'\n",
    "    lista_nomes = list(df_dadosgrupo[(df_dadosgrupo.ROTULOS == filtro1)]['CONTEUDOS'].values)\n",
    "    lista_nomes\n",
    "    \n",
    "    trocar=extrair_variantes(df_dadosgrupo)\n",
    "    for i,j in trocar:\n",
    "        origem.append(i)\n",
    "        destino.append(j)\n",
    "    \n",
    "    c=0\n",
    "    for coautores in df_colunacoautores.values:\n",
    "        c+=1\n",
    "        # print(' ANTES:',coautores)\n",
    "        lista_temp=[]\n",
    "        for coautor in coautores:\n",
    "            try:\n",
    "                nome_padronizado = padronizar_nome(coautor.strip())\n",
    "            except:\n",
    "                nome_padronizado = coautor.strip()\n",
    "            if coautor in origem or nome_padronizado in origem:             \n",
    "                for j,k in zip(origem,destino):\n",
    "                    if coautor.lower().strip()==j.lower().strip():\n",
    "                        l=k\n",
    "                lista_temp.append(l)\n",
    "            else:\n",
    "                lista_temp.append(nome_padronizado)      \n",
    "        # print('DEPOIS:',lista_temp)\n",
    "        lista_coautores_padronizada.append(lista_temp) \n",
    "    \n",
    "    return lista_coautores_padronizada\n",
    "\n",
    "\n",
    "\n",
    "## MONTAGEM E VISUALIZAÇÃO DOS DATAFRAMES QUE IRÃO GERAR OS GRAFOS\n",
    "def montar_bipartido(df):\n",
    "    '''Monta dataframes de colaboração a partir do dataframe de dados brutos df_dados extraídos de cada autor\n",
    "     Recebe: Dataframe df_dados com dados brutos de cada autor\n",
    "    Utiliza: Função parse_string(linha_conteudo)\n",
    "    Retorna: Dois dataframes um com lista de publicações separada por autoria e colaborações outro com uma linha por colaborador\n",
    "      Autor: Marcos Aires (Fev.2022)\n",
    "    '''\n",
    "    filtro='Artigos completos publicados em periódicos'\n",
    "    # df=df_dados_unico\n",
    "    dados = df[(df.ROTULOS == filtro)]\n",
    "    remover=['Ordenar por','Ordem Cronológica','Número de citações Web of science','Número de citações Scopus','Numero de citações Scielo','Primeiro autor','Impacto JCR','Ordem de Importância','Livros publicados/organizados ou edições']\n",
    "    publicacoes=[]\n",
    "    cont=[]\n",
    "    l_curriculo=[]\n",
    "    l_primautor=[]\n",
    "    l_ultimautor=[]\n",
    "    l_coaut=[]\n",
    "    l_titulo=[]\n",
    "    l_revista=[]\n",
    "    l_ano_pub=[]\n",
    "    l_local=[]\n",
    "    l_volume=[]\n",
    "    l_numero=[]\n",
    "    l_paginas=[]\n",
    "    l_doi=[]\n",
    "    l_problemas=[]\n",
    "\n",
    "    for n,j in enumerate(dados['CONTEUDOS']):\n",
    "        c=0\n",
    "        for i in j: \n",
    "            if i not in remover and len(i)>15 and 'Citações:' not in i:\n",
    "                c+=1\n",
    "                cont.append(c)\n",
    "                publicacoes.append(i)\n",
    "                data  = parse_string(i)\n",
    "                l_curriculo.append(dados['CURRICULO'].iloc[n])\n",
    "                l_autores.append(data[\"authors\"])\n",
    "                l_titulo.append(data[\"title\"])\n",
    "                l_revista.append(data[\"journal\"])\n",
    "                l_volume.append(data[\"volume\"])\n",
    "                l_paginas.append(data[\"pages\"])\n",
    "                l_ano_pub.append(data[\"year\"])\n",
    "                #  l_primautor.append(prim_autor)\n",
    "                #  l_ultimautor.append(ult_autor)\n",
    "                #  l_coaut.append(coaut)\n",
    "                #  l_numero.append(numero)\n",
    "                #  l_local.append(local)\n",
    "                #  l_doi.append(doi)\n",
    "\n",
    "    ## Monta novo dataframe para ver primeiro e último autor separados dos demais colaboradores\n",
    "    df_colabartigos = pd.DataFrame({\n",
    "        'CURRICULO': pd.Series(l_curriculo),\n",
    "        'AUTORES': pd.Series(l_autores),\n",
    "        'PRIMEIRO_AUTOR': pd.Series(l_primautor),\n",
    "        'ULTIMO_AUTOR': pd.Series(l_ultimautor),\n",
    "        'COAUTORES': pd.Series(l_coaut),\n",
    "        'TITULO': pd.Series(l_titulo),\n",
    "        'REVISTA': pd.Series(l_revista),\n",
    "        'ANO_PUB': pd.Series(l_ano_pub),\n",
    "    })\n",
    "\n",
    "\n",
    "    ## Monta novo dataframe para ver uma linha por cada colaborador na autoria \n",
    "    l_autores =[]\n",
    "    l_titulos =[]\n",
    "    l_anos    =[]\n",
    "    l_tipos   =[]\n",
    "\n",
    "    for i in range(len(df_colabartigos['TITULO'])):\n",
    "        l_anos.append(df_colabartigos['ANO_PUB'][i])\n",
    "        l_titulos.append(df_colabartigos['TITULO'][i])\n",
    "        l_autores.append(df_colabartigos['PRIMEIRO_AUTOR'][i])\n",
    "        l_tipos.append('primeiro_autor')\n",
    "\n",
    "        for j in df_colabartigos['COAUTORES'][i]:\n",
    "            l_anos.append(df_colabartigos['ANO_PUB'][i])\n",
    "            l_titulos.append(df_colabartigos['TITULO'][i])\n",
    "            try:\n",
    "                j=padronizar_nome(j.strip())\n",
    "            except:\n",
    "                pass\n",
    "            l_autores.append(j)\n",
    "            l_tipos.append('colaborador')\n",
    "\n",
    "    for k in range(len(df_colabartigos['TITULO'])):\n",
    "        if df_colabartigos['ULTIMO_AUTOR'][k] != '':\n",
    "            ultimo=df_colabartigos['ULTIMO_AUTOR'][k]\n",
    "            try:\n",
    "                ultimo=padronizar_nome(ultimo)\n",
    "            except:\n",
    "                pass\n",
    "            l_anos.append(df_colabartigos['ANO_PUB'][k])\n",
    "            l_titulos.append(df_colabartigos['TITULO'][k])\n",
    "            l_autores.append(ultimo)\n",
    "            l_tipos.append('ultimo_autor')\n",
    "\n",
    "    df_bipartido = pd.DataFrame({\n",
    "        'ANO_PUB': pd.Series(l_anos),\n",
    "        'TITULO': pd.Series(l_titulos),\n",
    "        'AUTORES': pd.Series(l_autores),\n",
    "        'TIPO': pd.Series(l_tipos)\n",
    "    })\n",
    "\n",
    "    df_bipartido.sort_values([\"ANO_PUB\", \"TITULO\"],\n",
    "                               axis = 0, \n",
    "                               ascending = True,\n",
    "                               inplace = True,\n",
    "                               na_position = \"first\")\n",
    "\n",
    "    df_bipartido.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # filtro='Supervisão de pós-doutorado'\n",
    "    # filtro='Monografia de conclusão de curso de aperfeiçoamento/especialização'\n",
    "    # filtro='Trabalho de conclusão de curso de graduação'\n",
    "    # filtro='Iniciação científica'\n",
    "    # filtro='Orientações de outra natureza'\n",
    "    \n",
    "    \n",
    "    filtro='Orientação Dissertação de mestrado'\n",
    "    dados = df[(df.ROTULOS == filtro)]\n",
    "    remover=['Ordenar por','Ordem Cronológica','Número de citações Web of science','Número de citações Scopus','Numero de citações Scielo','Primeiro autor','Impacto JCR','Ordem de Importância','Livros publicados/organizados ou edições']   \n",
    "    orientacoes_mestrado=[]\n",
    "    # remover=[]\n",
    "    cont=[]\n",
    "    for j in dados['CONTEUDOS']:\n",
    "        c=0\n",
    "        for i in j: \n",
    "            if i not in remover and len(i)>15 and 'Citações:' not in i:\n",
    "                c+=1\n",
    "                cont.append(c)\n",
    "                orientacoes_mestrado.append(i)   \n",
    "                \n",
    "    orientandos=[]\n",
    "    trabalhos_orientados=[]\n",
    "    ano_orientacao=[]\n",
    "    instituicoes_orientacao=[]\n",
    "    papeis_orientacao=[]\n",
    "\n",
    "    for j in orientacoes_mestrado:\n",
    "        i = j.split('. ')\n",
    "        orientando=i[0]\n",
    "        trabalho=i[1]\n",
    "        ano=i[2]\n",
    "        terceiro_dado=i[3]\n",
    "        # print(terceiro_dado)\n",
    "        if len(terceiro_dado) > 5:\n",
    "            instituicao=i[3]\n",
    "            papel=i[4].split(': ')[0]\n",
    "        else:\n",
    "            instituicao=i[4]\n",
    "            papel=i[5].split(': ')[0]\n",
    "\n",
    "        orientandos.append(orientando.title().strip())\n",
    "        trabalhos_orientados.append(trabalho)\n",
    "        ano_orientacao.append(ano)\n",
    "        instituicoes_orientacao.append(instituicao)\n",
    "        papeis_orientacao.append(papel)  \n",
    "    \n",
    "    df_orientacoes_mestrado = pd.DataFrame({\n",
    "        'ANO_CONCLUSÃO': pd.Series(ano_orientacao),\n",
    "        'TÍTULO': pd.Series(trabalhos_orientados),\n",
    "        'ORIENTADOS': pd.Series(orientandos),\n",
    "        'PAPEL_ORIENTAÇÃO': pd.Series(papeis_orientacao),\n",
    "        'INSTITUIÇÃO': pd.Series(instituicoes_orientacao),\n",
    "    })\n",
    "\n",
    "\n",
    "    filtro='Orientação Tese de doutorado'\n",
    "    dados = df[(df.ROTULOS == filtro)]\n",
    "    remover=['Ordenar por','Ordem Cronológica','Número de citações Web of science','Número de citações Scopus','Numero de citações Scielo','Primeiro autor','Impacto JCR','Ordem de Importância','Livros publicados/organizados ou edições']   \n",
    "    orientacoes_doutorado=[]\n",
    "    # remover=[]\n",
    "    cont=[]\n",
    "    for j in dados['CONTEUDOS']:\n",
    "        c=0\n",
    "        for i in j: \n",
    "            if i not in remover and len(i)>15:\n",
    "                c+=1\n",
    "                cont.append(c)\n",
    "                orientacoes_doutorado.append(i)   \n",
    "                \n",
    "    orientandos=[]\n",
    "    trabalhos_orientados=[]\n",
    "    ano_orientacao=[]\n",
    "    instituicoes_orientacao=[]\n",
    "    papeis_orientacao=[]\n",
    "\n",
    "    for j in orientacoes_doutorado:\n",
    "        i = j.split('. ')\n",
    "        orientando=i[0]\n",
    "        trabalho=i[1]\n",
    "        ano=i[2]\n",
    "        terceiro_dado=i[3]\n",
    "        # print(terceiro_dado)\n",
    "        if len(terceiro_dado) > 5:\n",
    "            instituicao=i[3]\n",
    "            papel=i[4].split(': ')[0]\n",
    "        else:\n",
    "            instituicao=i[4]\n",
    "            papel=i[5].split(': ')[0]\n",
    "\n",
    "        orientandos.append(orientando.title().strip())\n",
    "        trabalhos_orientados.append(trabalho)\n",
    "        ano_orientacao.append(ano)\n",
    "        instituicoes_orientacao.append(instituicao)\n",
    "        papeis_orientacao.append(papel)  \n",
    "    \n",
    "    df_orientacoes_doutorado = pd.DataFrame({\n",
    "        'ANO_CONCLUSÃO': pd.Series(ano_orientacao),\n",
    "        'TÍTULO': pd.Series(trabalhos_orientados),\n",
    "        'ORIENTADOS': pd.Series(orientandos),\n",
    "        'PAPEL_ORIENTAÇÃO': pd.Series(papeis_orientacao),\n",
    "        'INSTITUIÇÃO': pd.Series(instituicoes_orientacao),\n",
    "    })\n",
    "    \n",
    "    return df_colabartigos, df_bipartido, df_orientacoes_mestrado, df_orientacoes_doutorado\n",
    "\n",
    "\n",
    "def visualizar_bipartido(df_dados):\n",
    "    '''Renderiza, no notebook ou em uma página HTML, a visualização do grafo de todas as colaborações a partir do dataframe de dados bipartidos\n",
    "     Recebe: Dataframe com uma linha para relacionamento, ou seja, uma linha para cada autor do artigo repetindo o mesmo artigo para quantos coautores houver.\n",
    "    Utiliza: Módulo network da biblioteca pyvis\n",
    "    Retorna: Visualização em HTML do grafo\n",
    "    Autor: Marcos Aires (Mar.2022)\n",
    "    '''\n",
    "    from pyvis.network import Network\n",
    "\n",
    "    origem =[]\n",
    "    destino=[]\n",
    "    pesos=[]\n",
    "    titulo=[]\n",
    "    ano=[]\n",
    "    peso=[]\n",
    "\n",
    "    for i in range(len(df_dados['AUTORES_PADRONIZADOS'])):\n",
    "        origem.append(df_dados['AUTORES_PADRONIZADOS'][i].title())\n",
    "        destino.append(df_dados['TITULO'][i].title())\n",
    "        titulo.append(df_dados['TITULO'][i])\n",
    "        ano.append(df_dados['ANO_PUB'][i])\n",
    "        peso.append(1)\n",
    "\n",
    "    df_bipartido = pd.DataFrame({\n",
    "                            'AUTOR': pd.Series(origem),\n",
    "                            'TITULO': pd.Series(titulo),\n",
    "                            'ANO_PUB': pd.Series(ano),\n",
    "                            'PESO': pd.Series(peso),\n",
    "                           }) \n",
    "\n",
    "    colab_artigos = Network(height='750px', \n",
    "                    width='100%', \n",
    "                    bgcolor='#222222', \n",
    "                    font_color='white',\n",
    "                    # fontsize=24,\n",
    "                    notebook=True,\n",
    "                           )\n",
    "\n",
    "    # modelo de física para conformação da rede\n",
    "    colab_artigos.barnes_hut()\n",
    "\n",
    "    ## Adicionando subgrafo referente ao componente principal com as colaborações\n",
    "    sources = df_bipartido['AUTOR']\n",
    "    targets = df_bipartido['TITULO']\n",
    "    weights = df_bipartido['PESO']\n",
    "    ano_pub = df_bipartido['ANO_PUB']\n",
    "\n",
    "    edge_data = zip(sources, targets, weights)\n",
    "\n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        w   = e[2]\n",
    "\n",
    "        colab_artigos.add_node(src, src, title=src)\n",
    "        colab_artigos.add_node(dst, dst, title=dst)\n",
    "        colab_artigos.add_edge(src, dst, value=w)\n",
    "\n",
    "\n",
    "    neighbor_map = colab_artigos.get_adj_list()\n",
    "\n",
    "    # add neighbor data to node hover data\n",
    "    for node in colab_artigos.nodes:\n",
    "        node['title'] += ' COLABOROU_COM:<br>' + '<br>'.join(neighbor_map[node['id']])\n",
    "        node['value'] = len(neighbor_map[node['id']])\n",
    "\n",
    "    colab_artigos.show_buttons(filter_=['physics'])\n",
    "    # colab_artigos.show('coautorias.html')\n",
    "    \n",
    "    return colab_artigos.show('coautorias.html')\n",
    "\n",
    "\n",
    "def montar_adj(df_dadoscolab):\n",
    "    '''Monta uma lista de colaborações, tomando o primeiro autor como origem e colaboradores como destino\n",
    "    Recebe: DataFrame com dados detalhados dos artigos\n",
    "    Retorna: DataFrame com lista de aresta de colaborações\n",
    "    Autor: Marcos Aires Fev.2022\n",
    "    '''\n",
    "    origem=[]\n",
    "    destinos=[]\n",
    "    lista_destinos=[]\n",
    "    lista_titulos=[]\n",
    "    lista_anos=[]\n",
    "\n",
    "    for i in range(len(df_dadoscolab['PRIMEIRO_AUTOR'])):\n",
    "        lista_anos.append(df_dadoscolab['ANO_PUB'][i])\n",
    "        lista_titulos.append(df_dadoscolab['TITULO'][i])\n",
    "        origem.append(df_dadoscolab['PRIMEIRO_AUTOR'][i])\n",
    "\n",
    "        \n",
    "    for i in range(len(df_dadoscolab['COAUTORES'])):\n",
    "        l1=df_dadoscolab['COAUTORES'][i]\n",
    "        l2=[df_dadoscolab['ULTIMO_AUTOR'][i]]\n",
    "        destino=l1+l2\n",
    "        lista_destinos.append(destino)\n",
    "\n",
    "    df_adj = pd.DataFrame({\n",
    "                            'ANO_PUB': pd.Series(lista_anos),\n",
    "                            'TITULO': pd.Series(lista_titulos),\n",
    "                            'PRIMEIRO_AUTOR': pd.Series(origem),\n",
    "                            'COAUTORES': pd.Series(lista_destinos),\n",
    "                           }) \n",
    "\n",
    "    return df_adj\n",
    "\n",
    "\n",
    "def montar_adj_primult(df_dados):\n",
    "    '''Monta uma lista de colaborações, tomando o primeiro autor como origem e ultimo autor como destino\n",
    "    Recebe: DataFrame com dados detalhados dos artigos\n",
    "    Retorna: DataFrame com lista de aresta de colaborações\n",
    "    Autor: Marcos Aires Fev.2022\n",
    "    '''\n",
    "    origem=[]\n",
    "    destinos=[]\n",
    "    lista_destinos=[]\n",
    "    lista_titulos=[]\n",
    "    lista_anos=[]\n",
    "\n",
    "    for i in range(len(df_dados['PRIMEIRO_AUTOR'])):\n",
    "        origem.append(df_dados['PRIMEIRO_AUTOR'][i])\n",
    "        lista_anos.append(df_dados['ANO_PUB'][i])\n",
    "        lista_titulos.append(df_dados['TITULO'][i])\n",
    "\n",
    "        \n",
    "    for j in range(len(df_dados['ULTIMO_AUTOR'])):\n",
    "        destinos.append(df_dados['ULTIMO_AUTOR'][j])\n",
    "\n",
    "    df_adj_prim_ult = pd.DataFrame({\n",
    "                            'ANO_PUB': pd.Series(lista_anos),\n",
    "                            'TITULO': pd.Series(lista_titulos),\n",
    "                            'PRIMEIRO_AUTOR': pd.Series(origem),\n",
    "                            'ULTIMO_AUTOR': pd.Series(destinos),\n",
    "                           }) \n",
    "\n",
    "    return df_adj_prim_ult\n",
    "\n",
    "\n",
    "def visualizar_colaboracoes(df_dados):\n",
    "    '''Renderiza, no notebook ou em uma página HTML, a visualização do grafo de todas as colaborações\n",
    "    Como parâmetro, recebe um dataframe de dados, onde: df_dados=montar_adj(df_dados)\n",
    "    Autor: Marcos Aires (Mar.2022)\n",
    "    '''\n",
    "    from pyvis.network import Network\n",
    "\n",
    "    origem =[]\n",
    "    destino=[]\n",
    "    pesos=[]\n",
    "    titulo=[]\n",
    "    ano=[]\n",
    "    peso=[]\n",
    "    lista_autores=[]\n",
    "\n",
    "    for i in range(len(df_dados['PRIMEIRO_AUTOR'])):\n",
    "        if i not in lista_autores:\n",
    "            lista_autores.append(df_dados['PRIMEIRO_AUTOR'][i].title())\n",
    "        \n",
    "        for j in df_dados['COAUTORES'][i]:\n",
    "            origem.append(df_dados['PRIMEIRO_AUTOR'][i].title())\n",
    "            destino.append(j.title())\n",
    "            if j not in lista_autores:\n",
    "                lista_autores.append(j.title())\n",
    "            titulo.append(df_dados['TITULO'][i])\n",
    "            ano.append(df_dados['ANO_PUB'][i])\n",
    "            peso.append(1)\n",
    "\n",
    "    df_grafo = pd.DataFrame({\n",
    "                            'PRIMEIRO_AUTOR': pd.Series(origem),\n",
    "                            'COAUTORES': pd.Series(destino),\n",
    "                            'TITULO': pd.Series(titulo),\n",
    "                            'ANO_PUB': pd.Series(ano),\n",
    "                            'PESO': pd.Series(peso),\n",
    "                           }) \n",
    "\n",
    "    colab_artigos = Network(height='750px', \n",
    "                    width='100%', \n",
    "                    bgcolor='#222222', \n",
    "                    font_color='white',\n",
    "                    # fontsize=24,\n",
    "                    notebook=True,\n",
    "                           )\n",
    "\n",
    "    # modelo de física para conformação da rede\n",
    "    colab_artigos.barnes_hut()\n",
    "\n",
    "    ## Adicionando subgrafo referente ao componente principal com as colaborações\n",
    "    sources = df_grafo['PRIMEIRO_AUTOR']\n",
    "    targets = df_grafo['COAUTORES']\n",
    "    weights = df_grafo['PESO']\n",
    "\n",
    "    titulo  = df_grafo['TITULO']\n",
    "    ano_pub = df_grafo['ANO_PUB']\n",
    "\n",
    "    edge_data = zip(sources, targets, weights)\n",
    "\n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        w = e[2]\n",
    "\n",
    "        colab_artigos.add_node(src, src, title=src)\n",
    "        colab_artigos.add_node(dst, dst, title=dst)\n",
    "        colab_artigos.add_edge(src, dst, value=w)\n",
    "\n",
    "\n",
    "    # ## Adicionando subgrafo referente às variantes de nome em citações\n",
    "    # origem = NOME\n",
    "    # filtro = 'Nome em citações bibliográficas'\n",
    "    # variantes = df_dados[(df_dados.ROTULOS == filtro)]['CONTEUDOS'].values[0]\n",
    "\n",
    "    # origens = []\n",
    "    # pesos = []\n",
    "    # destinos = variantes\n",
    "\n",
    "    # # Atribui dados do dataset às variáveis que constroem o grafo\n",
    "    # for i in range(len(destinos)):\n",
    "    #     origens.append(origem)\n",
    "    #     pesos.append(100)\n",
    "\n",
    "    # arestas = zip(origens, destinos, pesos)\n",
    "    # for a in arestas:\n",
    "    #     src = a[0]\n",
    "    #     dst = a[1]\n",
    "    #     w = a[2]\n",
    "\n",
    "    #     colab_artigos.add_node(src, src, title=src, color = \"white\")\n",
    "    #     colab_artigos.add_node(dst, dst, title=dst)\n",
    "    #     colab_artigos.add_edge(src, dst, value=w)\n",
    "\n",
    "\n",
    "    neighbor_map = colab_artigos.get_adj_list()\n",
    "\n",
    "    # add neighbor data to node hover data\n",
    "    for node in colab_artigos.nodes:\n",
    "        node['title'] += ' COLABOROU_COM:<br>' + '<br>'.join(neighbor_map[node['id']])\n",
    "        node['value'] = len(neighbor_map[node['id']])\n",
    "\n",
    "    colab_artigos.show_buttons(filter_=['physics'])\n",
    "#     colab_artigos.show('coautorias.html')\n",
    "    \n",
    "    return colab_artigos.show('coautorias.html')\n",
    "\n",
    "\n",
    "def visualizar_colabprincipais(df_dados):\n",
    "    '''Renderiza, no notebook ou em uma página HTML, a visualização do grafo das principais colaborações.\n",
    "    Considera como principais as colaborações onde o autor é o primeiro ou o último autor.\n",
    "    Como parâmetro, recebe um dataframe de dados, onde: df_dados=montar_adj_primult(df_dados)\n",
    "    Autor: Marcos Aires (Mar.2022)\n",
    "    '''\n",
    "    from pyvis.network import Network\n",
    "\n",
    "    origem =[]\n",
    "    destino=[]\n",
    "    pesos=[]\n",
    "    titulo=[]\n",
    "    ano=[]\n",
    "    peso=[]\n",
    "    lista_autores=[]\n",
    "\n",
    "    for i in range(len(df_dados['PRIMEIRO_AUTOR'])):\n",
    "        origem.append(df_dados['PRIMEIRO_AUTOR'][i])\n",
    "        if i not in lista_autores:\n",
    "            lista_autores.append(df_dados['PRIMEIRO_AUTOR'][i])\n",
    "        \n",
    "    for j in range(len(df_dados['ULTIMO_AUTOR'])):\n",
    "        destino.append(df_dados['ULTIMO_AUTOR'][j])\n",
    "        if df_dados['ULTIMO_AUTOR'][j] not in lista_autores:\n",
    "            lista_autores.append(df_dados['ULTIMO_AUTOR'][j])\n",
    "    \n",
    "    titulo.append(df_dados['TITULO'][i])\n",
    "    ano.append(df_dados['ANO_PUB'][i])\n",
    "    peso.append(1)\n",
    "\n",
    "    df_grafo = pd.DataFrame({\n",
    "                            'PRIMEIRO_AUTOR': pd.Series(origem),\n",
    "                            'ULTIMO_AUTOR': pd.Series(destino),\n",
    "                            'TITULO': pd.Series(titulo),\n",
    "                            'ANO_PUB': pd.Series(ano),\n",
    "                            'PESO': pd.Series(peso),\n",
    "                           }) \n",
    "\n",
    "    colab_artigos = Network(height='750px', \n",
    "                    width='100%', \n",
    "                    bgcolor='#000000', \n",
    "                    font_color='white',\n",
    "                    # fontsize=24,\n",
    "                    notebook=True,\n",
    "                           )\n",
    "\n",
    "    # modelo de física para conformação da rede\n",
    "    colab_artigos.barnes_hut()\n",
    "\n",
    "    ## Adicionando subgrafo referente ao componente principal com as colaborações\n",
    "    sources = df_grafo['PRIMEIRO_AUTOR']\n",
    "    targets = df_grafo['ULTIMO_AUTOR']\n",
    "    weights = df_grafo['PESO']\n",
    "\n",
    "    titulo  = df_grafo['TITULO']\n",
    "    ano_pub = df_grafo['ANO_PUB']\n",
    "\n",
    "    edge_data = zip(sources, targets, weights)\n",
    "\n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        w   = e[2]\n",
    "\n",
    "        colab_artigos.add_node(src, src, title=src)\n",
    "        colab_artigos.add_node(dst, dst, title=dst)\n",
    "        colab_artigos.add_edge(src, dst, value=w)\n",
    "\n",
    "\n",
    "    # ## Adicionando subgrafo referente às variantes de nome em citações\n",
    "    # origem = NOME\n",
    "    # filtro = 'Nome em citações bibliográficas'\n",
    "    # variantes = df_dados[(df_dados.ROTULOS == filtro)]['CONTEUDOS'].values[0]\n",
    "\n",
    "    # origens = []\n",
    "    # pesos = []\n",
    "    # destinos = variantes\n",
    "\n",
    "    # # Atribui dados do dataset às variáveis que constroem o grafo\n",
    "    # for i in range(len(destinos)):\n",
    "    #     origens.append(origem)\n",
    "    #     pesos.append(100)\n",
    "\n",
    "    # arestas = zip(origens, destinos, pesos)\n",
    "    # for a in arestas:\n",
    "    #     src = a[0]\n",
    "    #     dst = a[1]\n",
    "    #     w = a[2]\n",
    "\n",
    "    #     colab_artigos.add_node(src, src, title=src, color = \"white\")\n",
    "    #     colab_artigos.add_node(dst, dst, title=dst)\n",
    "    #     colab_artigos.add_edge(src, dst, value=w)\n",
    "\n",
    "\n",
    "    neighbor_map = colab_artigos.get_adj_list()\n",
    "\n",
    "    # add neighbor data to node hover data\n",
    "    for node in colab_artigos.nodes:\n",
    "        node['title'] += ' COLABOROU_COM:<br>' + '<br>'.join(neighbor_map[node['id']])\n",
    "        node['value'] = len(neighbor_map[node['id']])\n",
    "\n",
    "    colab_artigos.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    return colab_artigos.show('coautorias_principais.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar funções de Plotagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def get_initials(name):\n",
    "    return ''.join([word[0] for word in name.split()])\n",
    "\n",
    "def plotar_artigos_ano(df_artigosperiodo):\n",
    "    # Contar os valores dos anos da coluna ANO_PUB\n",
    "    count_anos = df_artigosperiodo['ANO_PUB'].value_counts().sort_index()\n",
    "\n",
    "    # Criar o gráfico de barras com plotly express\n",
    "    fig = px.bar(x=range(len(count_anos.index)), \n",
    "                 y=count_anos.values,\n",
    "                 labels={'x':'Ano de publicação', 'y':'Quantidade de artigos'},\n",
    "                 title='Quantidade de Artigos por Ano')\n",
    "\n",
    "    # Ajustar os tick labels do eixo x para mostrar os anos\n",
    "    fig.update_xaxes(tickvals=list(range(len(count_anos.index))), ticktext=count_anos.index, showgrid=False)\n",
    "\n",
    "    # Remover linhas de grade horizontal (eixo y)\n",
    "    fig.update_yaxes(showgrid=False)\n",
    "\n",
    "    # Adicionar a anotação dos valores em cada barra\n",
    "    for index, value in enumerate(count_anos.values):\n",
    "        fig.add_annotation(\n",
    "            x=index,\n",
    "            y=value + (0.05 * max(count_anos.values)),  # Ajustar esta proporção conforme necessário\n",
    "            text=str(value),\n",
    "            showarrow=False,\n",
    "            font_size=20\n",
    "        )\n",
    "\n",
    "    # Adicionar a anotação com a quantidade total de artigos no período\n",
    "    total_artigos = sum(count_anos.values)\n",
    "    fig.add_annotation(\n",
    "        x=len(count_anos.index)/2,\n",
    "        y=max(count_anos.values) + (0.15 * max(count_anos.values)),  # Ajustar esta proporção conforme necessário\n",
    "        text=f\"Total de Participação em Artigos, após entrada na Fiocruz Ceará: {total_artigos}\",\n",
    "        showarrow=False,\n",
    "        font_size=18,\n",
    "        font_color=\"blue\"\n",
    "    )\n",
    "\n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1380,   # largura em pixels\n",
    "        height=600   # altura em pixels\n",
    "    )\n",
    "\n",
    "    # Mostrar o gráfico\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "def plotar_barras_agrupadas(df_artigosperiodo):\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    # Contar os artigos por ano e por CURRICULO\n",
    "    grouped_data = df_artigosperiodo.groupby(['ANO_PUB', 'NOME']).size().reset_index(name='count')\n",
    "\n",
    "    # Usar paleta de cores personalizada \"VIVID\"\n",
    "    colors_vivid = ['#FF595E', '#FFCA3A', '#8AC926', '#1982FC', '#6A0572'] # Adicione mais cores se necessário\n",
    "\n",
    "    # Criar o gráfico de barras com plotly express\n",
    "    fig = px.bar(grouped_data, \n",
    "                 x='ANO_PUB',\n",
    "                 y='count',\n",
    "                 color='CURRICULO',\n",
    "                 color_discrete_sequence=colors_vivid,\n",
    "                 labels={'ANO_PUB':'Ano', 'count':'Quantidade'},\n",
    "                 title='Quantidade de Participações em Artigos por Ano e por Currículo, após entrada na Fiocruz Ceará')\n",
    "\n",
    "    # Adicionar rótulos de dados\n",
    "    for trace, color in zip(fig.data, colors_vivid):\n",
    "        for x_val, y_val in zip(trace.x, trace.y):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[x_val],\n",
    "                    y=[y_val + (0.02 * max(grouped_data['count']))], \n",
    "                    text=[str(y_val)],\n",
    "                    mode=\"text\",\n",
    "                    showlegend=False,\n",
    "                    textfont=dict(color=color)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Remover linhas de grade\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    fig.update_yaxes(showgrid=False)\n",
    "\n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1380,   # largura em pixels\n",
    "        height=1200   # altura em pixels\n",
    "    )\n",
    "\n",
    "    # Mostrar o gráfico\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "def plotar_barras_estaqueadas(df_artigosperiodo):\n",
    "    \n",
    "    df_artigosperiodo['INICIAIS'] = df_artigosperiodo['NOME'].apply(get_initials)\n",
    "    grouped_data = df_artigosperiodo.groupby(['ANO_PUB', 'INICIAIS']).size().reset_index(name='count')\n",
    "\n",
    "    # Usar a paleta \"Plotly\"\n",
    "    colors = px.colors.qualitative.Vivid\n",
    "\n",
    "    fig = px.bar(grouped_data, \n",
    "                 x='ANO_PUB',\n",
    "                 y='count',\n",
    "                #  color='CURRICULO',\n",
    "                 color='INICIAIS',\n",
    "                 barmode='group',\n",
    "                 color_discrete_sequence=colors,\n",
    "                #  labels={'ANO_PUB':'Ano', 'count':'Quantidade'},\n",
    "                 labels={'ANO_PUB':'Ano de Publicação', 'count':'Quantidade de artigos', 'INICIAIS':'Currículo'},\n",
    "                 title='Quantidade de Participação em Artigos por Ano e por Currículo, após entrada na Fiocruz Ceará')\n",
    "\n",
    "    # Adicionar rótulos de dados\n",
    "#     for trace, color in zip(fig.data, colors):\n",
    "#         for x_val, y_val in zip(trace.x, trace.y):\n",
    "#             fig.add_trace(\n",
    "#                 go.Scatter(\n",
    "#                     x=[x_val + 0.01],\n",
    "#                     y=[y_val + (0.02 * max(grouped_data['count']))], \n",
    "#                     text=[str(y_val)],\n",
    "#                     mode=\"text\",\n",
    "#                     showlegend=False,\n",
    "#                     textfont=dict(color=color)\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "    # Posicionar a legenda abaixo do gráfico\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.3,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1600,   # largura em pixels\n",
    "        height=1200   # altura em pixels\n",
    "    )\n",
    "    \n",
    "    # Remover linhas de grade\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    fig.update_yaxes(showgrid=False)\n",
    "\n",
    "    # Mostrar o gráfico\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "def comparativo_curriculos(df_artigosperiodo):\n",
    "\n",
    "    df_artigosperiodo['CURRICULO_INITIALS'] = df_artigosperiodo['NOME'].apply(get_initials)\n",
    "    grouped_data = df_artigosperiodo.groupby(['ANO_PUB', 'CURRICULO_INITIALS']).size().reset_index(name='count')\n",
    "    years = sorted(grouped_data['ANO_PUB'].unique())\n",
    "\n",
    "    # Calcular a soma total para cada currículo\n",
    "    total_counts = grouped_data.groupby('CURRICULO_INITIALS')['count'].sum()\n",
    "\n",
    "    # Ordenar as iniciais dos currículos com base na soma total\n",
    "    sorted_initials = total_counts.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "    # Obter a paleta \"Greens\" e normalize-a para ter uma cor para cada ano\n",
    "    palette = px.colors.sequential.Greens\n",
    "    colors = reversed([palette[i * (len(palette) - 1) // (len(years) - 1)] for i in range(len(years))])\n",
    "    colors = list(colors)  # Converta o resultado para uma lista novamente\n",
    "\n",
    "    # Criar o gráfico com as iniciais ordenadas\n",
    "    traces = []\n",
    "    for idx, year in enumerate(reversed(years)):\n",
    "        year_data = grouped_data[grouped_data['ANO_PUB'] == year]\n",
    "\n",
    "        # Preencher valores ausentes com zero\n",
    "        year_data = year_data.set_index('CURRICULO_INITIALS').reindex(sorted_initials, fill_value=0).reset_index()\n",
    "\n",
    "        # Remover entradas com valores zerados\n",
    "        # year_data = year_data[year_data['count'] > 0]\n",
    "\n",
    "        traces.append(go.Bar(\n",
    "            y=year_data['CURRICULO_INITIALS'],\n",
    "            x=year_data['count'],\n",
    "            name=str(year),\n",
    "            orientation='h',\n",
    "            marker_color=colors[idx]  # Aplique a cor aqui\n",
    "        ))\n",
    "\n",
    "    # Adicionar as traces em ordem à figura\n",
    "    fig = go.Figure(data=traces)\n",
    "\n",
    "    # Configurar o layout para ser empilhado\n",
    "    fig.update_layout(\n",
    "        barmode='stack',\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        title='Quantidade de Participações em Artigos por Currículo e Ano, após entrada na Fiocruz Ceará'\n",
    "    )\n",
    "\n",
    "    # Criar um DataFrame para a soma total por currículo\n",
    "    total_per_curriculo = grouped_data.groupby('CURRICULO_INITIALS')['count'].sum().reset_index()\n",
    "    total_per_curriculo = total_per_curriculo.set_index('CURRICULO_INITIALS').reindex(sorted_initials, fill_value=0).reset_index()\n",
    "\n",
    "    # Calcular a soma total de todos os artigos\n",
    "    total_articles = grouped_data['count'].sum()\n",
    "\n",
    "    # Adicionar a anotação no centro superior da área do gráfico com a soma total de todos os artigos\n",
    "    fig.add_annotation(\n",
    "        x=0.5,  # posição horizontal centrada\n",
    "        y=0.99,  # posição vertical logo acima do topo do gráfico\n",
    "        xref=\"paper\",  # refere-se à posição proporcional do gráfico (0 à esquerda, 1 à direita)\n",
    "        yref=\"paper\",  # refere-se à posição proporcional do gráfico (0 na parte inferior, 1 na parte superior)\n",
    "        text=f\"Total de Participação em Artigos, após entrada na Fiocruz Ceará: {total_articles}\",\n",
    "        showarrow=False,\n",
    "        font=dict(color='blue', size=20),\n",
    "        align=\"center\",\n",
    "        valign=\"top\"\n",
    "    )\n",
    "\n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1380,   # largura em pixels\n",
    "        height=1200   # altura em pixels\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Levenshtein --trusted-host pypi.org --trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def evolucao_anual(df_artigosperiodo, df_pessoal):\n",
    "    # Removendo duplicidades\n",
    "    # df_artigosperiodo = df_artigosperiodo.drop_duplicates(subset='TITULO')\n",
    "\n",
    "    # Para todos os anos dos dados\n",
    "    min_year = df_artigosperiodo['ANO_PUB'].min()\n",
    "    max_year = df_artigosperiodo['ANO_PUB'].max()\n",
    "    all_years = list(range(min_year, max_year + 1))\n",
    "\n",
    "    # Quantidade total de artigos por ano\n",
    "    artigos_por_ano = df_artigosperiodo.groupby('ANO_PUB').size().reindex(all_years, fill_value=0).reset_index(name='count')\n",
    "\n",
    "    # Quantidade de pesquisadores únicos que entraram a cada ano\n",
    "    pesquisadores_por_ano = df_pessoal.groupby('ANO_INGRESSO_FIOCE')['NOME'].nunique().reindex(all_years, fill_value=0).reset_index(name='unique_researchers')\n",
    "\n",
    "    # Soma cumulativa de pesquisadores ao longo dos anos\n",
    "    pesquisadores_por_ano['cumulative_researchers'] = pesquisadores_por_ano['unique_researchers'].cumsum()\n",
    "\n",
    "    # Média de artigos por pesquisador\n",
    "    pesquisadores_por_ano['avg_articles_per_researcher'] = artigos_por_ano['count'] / pesquisadores_por_ano['cumulative_researchers']\n",
    "    pesquisadores_por_ano['avg_articles_per_researcher'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "    # Calcular a proporção entre os dois eixos y\n",
    "    max_y1 = artigos_por_ano['count'].max()+10\n",
    "    max_y2 = pesquisadores_por_ano['cumulative_researchers'].max()\n",
    "    ratio = max_y1 / max_y2\n",
    "\n",
    "    # Defina um buffer de 10%\n",
    "    buffer = 0.1\n",
    "    \n",
    "    # Criando a figura com as devidas traces\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Adicionando a trace de barras para a quantidade de artigos\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=artigos_por_ano['ANO_PUB'],\n",
    "        y=artigos_por_ano['count'],\n",
    "        name='Total de Artigos',\n",
    "        text=artigos_por_ano['count'],\n",
    "        textposition='outside'\n",
    "    ))\n",
    "\n",
    "    # Adicionando a trace de linha para a soma cumulativa de pesquisadores\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pesquisadores_por_ano['ANO_INGRESSO_FIOCE'],\n",
    "        y=pesquisadores_por_ano['cumulative_researchers'],\n",
    "        yaxis='y2',\n",
    "        name='Soma Cumulativa de Pesquisadores',\n",
    "        mode='lines+markers+text',\n",
    "        text=pesquisadores_por_ano['cumulative_researchers'],\n",
    "        textposition='top center'\n",
    "    ))\n",
    "\n",
    "    # Adicionando a trace de linha tracejada para a média de artigos por pesquisador\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pesquisadores_por_ano['ANO_INGRESSO_FIOCE'],\n",
    "        y=pesquisadores_por_ano['avg_articles_per_researcher'] * ratio,\n",
    "        yaxis='y2',\n",
    "        name='Média de Artigos por Pesquisador',\n",
    "        mode='lines+text',\n",
    "        line=dict(dash='dash'),\n",
    "        text=pesquisadores_por_ano['avg_articles_per_researcher'].round(2),\n",
    "        textposition='top center'\n",
    "    ))\n",
    "\n",
    "    # Atualizando o layout do gráfico\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(\n",
    "            title='Total de Artigos',\n",
    "            range=[0, max_y1 * (1 + buffer)]\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            title='Soma Cumulativa de Pesquisadores e Média de Artigos por Pesquisador',\n",
    "            overlaying='y',\n",
    "            side='right',\n",
    "            range=[0, max_y2 * (1 + buffer)],\n",
    "            tickvals=list(range(0, int(max_y2 * (1 + buffer)), int(max_y2/10))),\n",
    "            ticktext=list(range(0, int(max_y2 * (1 + buffer)), int(max_y2/10)))\n",
    "        ),\n",
    "        xaxis=dict(tickvals=all_years),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            x=0.2,\n",
    "            y=0.99\n",
    "        ),\n",
    "        title='Quantidade de Participações em Artigos, Soma Cumulativa de Pesquisadores e Média de Artigos por Pesquisador por Ano',\n",
    "    )\n",
    "\n",
    "    # Remover linhas de grade\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    fig.update_yaxes(showgrid=False)\n",
    "    \n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1380,   # largura em pixels\n",
    "        height=800   # altura em pixels\n",
    "    )\n",
    "        \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def evolucao_sem_duplicatas(df_artigosperiodo, df_pessoal, sim_limite=0.95):\n",
    "    import Levenshtein\n",
    "\n",
    "    # Função para verificar similaridade entre strings\n",
    "    def is_similar(str1, str2, threshold):\n",
    "        similarity = (len(str1) - Levenshtein.distance(str1, str2)) / float(len(str1))\n",
    "        return similarity >= threshold\n",
    "\n",
    "    # Removendo duplicidades\n",
    "    titles = df_artigosperiodo['ARTIGO'].tolist()\n",
    "    unique_titles = []\n",
    "    for title in titles:\n",
    "        if not any(is_similar(title, utitle, sim_limite) for utitle in unique_titles):\n",
    "            unique_titles.append(title)\n",
    "    df_artigosperiodo = df_artigosperiodo[df_artigosperiodo['ARTIGO'].isin(unique_titles)]\n",
    "\n",
    "    # Remover duplicatas apenas com base no título exato\n",
    "    df_artigosperiodo = df_artigosperiodo.drop_duplicates(subset='ARTIGO')\n",
    "\n",
    "    # Para todos os anos dos dados\n",
    "    min_year  = df_artigosperiodo['ANO_PUB'].min()\n",
    "    max_year  = df_artigosperiodo['ANO_PUB'].max()\n",
    "    all_years = list(range(min_year, max_year + 1))\n",
    "\n",
    "    # Quantidade total de artigos por ano\n",
    "    artigos_por_ano = df_artigosperiodo.groupby('ANO_PUB').size().reindex(all_years, fill_value=0).reset_index(name='count')\n",
    "\n",
    "    # Quantidade de pesquisadores únicos que entraram a cada ano\n",
    "    pesquisadores_por_ano = df_pessoal.groupby('ANO_INGRESSO_FIOCE')['NOME'].nunique().reindex(all_years, fill_value=0).reset_index(name='unique_researchers')\n",
    "\n",
    "    # Soma cumulativa de pesquisadores ao longo dos anos\n",
    "    pesquisadores_por_ano['cumulative_researchers'] = pesquisadores_por_ano['unique_researchers'].cumsum()\n",
    "\n",
    "    # Média de artigos por pesquisador\n",
    "    pesquisadores_por_ano['avg_articles_per_researcher'] = artigos_por_ano['count'] / pesquisadores_por_ano['cumulative_researchers']\n",
    "    pesquisadores_por_ano['avg_articles_per_researcher'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "    # Calcular a proporção entre os dois eixos y\n",
    "    max_y1 = artigos_por_ano['count'].max()+10\n",
    "    max_y2 = pesquisadores_por_ano['cumulative_researchers'].max()\n",
    "    ratio  = max_y1 / max_y2\n",
    "\n",
    "    # Defina um buffer de 10%\n",
    "    buffer = 0.1\n",
    "    \n",
    "    # Criando a figura com as devidas traces\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Adicionando a trace de barras para a quantidade de artigos\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=artigos_por_ano['ANO_PUB'],\n",
    "        y=artigos_por_ano['count'],\n",
    "        name='Total de Artigos',\n",
    "        text=artigos_por_ano['count'],\n",
    "        textposition='outside'\n",
    "    ))\n",
    "\n",
    "    # Adicionando a trace de linha para a soma cumulativa de pesquisadores\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pesquisadores_por_ano['ANO_INGRESSO_FIOCE'],\n",
    "        y=pesquisadores_por_ano['cumulative_researchers'],\n",
    "        yaxis='y2',\n",
    "        name='Soma Cumulativa de Pesquisadores',\n",
    "        mode='lines+markers+text',\n",
    "        text=pesquisadores_por_ano['cumulative_researchers'],\n",
    "        textposition='top center'\n",
    "    ))\n",
    "\n",
    "    # Adicionando a trace de linha tracejada para a média de artigos por pesquisador\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pesquisadores_por_ano['ANO_INGRESSO_FIOCE'],\n",
    "        y=pesquisadores_por_ano['avg_articles_per_researcher'] * ratio,\n",
    "        yaxis='y2',\n",
    "        name='Média de Artigos por Pesquisador',\n",
    "        mode='lines+text',\n",
    "        line=dict(dash='dash'),\n",
    "        text=pesquisadores_por_ano['avg_articles_per_researcher'].round(2),\n",
    "        textposition='top center'\n",
    "    ))\n",
    "\n",
    "    # Atualizando o layout do gráfico\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(\n",
    "            title='Total de Artigos',\n",
    "            range=[0, max_y1 * (1 + buffer)]\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            title='Soma Cumulativa de Pesquisadores e Média de Artigos por Pesquisador',\n",
    "            overlaying='y',\n",
    "            side='right',\n",
    "            range=[0, max_y2 * (1 + buffer)],\n",
    "            tickvals=list(range(0, int(max_y2 * (1 + buffer)), int(max_y2/10))),\n",
    "            ticktext=list(range(0, int(max_y2 * (1 + buffer)), int(max_y2/10)))\n",
    "        ),\n",
    "        xaxis=dict(tickvals=all_years),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            x=0.2,\n",
    "            y=0.99\n",
    "        ),\n",
    "        title='Quantidade de Artigos (sem duplicatas), Soma Cumulativa de Pesquisadores e Média de Artigos por Pesquisador por Ano',\n",
    "    )\n",
    "\n",
    "    # Identificar o último ano\n",
    "    last_year = max(all_years)\n",
    "    \n",
    "    # Adicionar a barra transparente para o último ano\n",
    "    mask_last_year = artigos_por_ano['ANO_PUB'] == last_year\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=[last_year],\n",
    "        y=[artigos_por_ano[mask_last_year]['count'].iloc[0]],\n",
    "        name='Total de Artigos no Ano Atual',\n",
    "        text=str(artigos_por_ano[mask_last_year]['count'].iloc[0]),  # Convertendo para string\n",
    "        textposition='outside',\n",
    "        marker=dict(color='rgba(0,0,0,0)',  # Transparente\n",
    "                    line=dict(color='#1f77b4', width=2))  # Borda com a cor padrão e espessura de 2\n",
    "    ))\n",
    "    \n",
    "    # Remover linhas de grade\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    fig.update_yaxes(showgrid=False)\n",
    "    \n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1380,   # largura em pixels\n",
    "        height=800   # altura em pixels\n",
    "    )\n",
    "        \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def boxplot_media_artigos(df_artigosperiodo, df_pessoal):\n",
    "    # Removendo duplicidades\n",
    "    df_artigosperiodo = df_artigosperiodo.drop_duplicates(subset='ARTIGO')\n",
    "    df_artigosperiodo['CURRICULO_INITIALS'] = df_artigosperiodo['NOME'].apply(get_initials)\n",
    "\n",
    "    # Calculando o total de artigos por pesquisador\n",
    "    artigos_por_pesquisador = df_artigosperiodo.groupby('CURRICULO_INITIALS').size()\n",
    "\n",
    "    # Calculando a média de artigos por pesquisador\n",
    "    anos_ativos = df_artigosperiodo.groupby('CURRICULO_INITIALS')['ANO_PUB'].nunique()\n",
    "    media_artigos_por_pesquisador = artigos_por_pesquisador / anos_ativos\n",
    "\n",
    "    # Retirar possíveis infinitos\n",
    "    media_artigos_por_pesquisador.replace(np.inf, 0, inplace=True)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Box plot\n",
    "    fig.add_trace(go.Box(\n",
    "        y=media_artigos_por_pesquisador,\n",
    "        boxpoints='all',\n",
    "        jitter=0.3,\n",
    "        pointpos=0,\n",
    "        name='Média de Artigos por Pesquisador'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Distribuição da Média de Artigos por Pesquisador',\n",
    "        yaxis_title='Média de Artigos por Pesquisador'\n",
    "    )\n",
    "\n",
    "    # Ajustar a altura e a largura\n",
    "    fig.update_layout(\n",
    "        width=1380,   # largura em pixels\n",
    "        height=800   # altura em pixels\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def evolucao_artigos(df_artigosperiodo, df_pessoal, threshold=0.8):\n",
    "    # Para todos os anos dos dados\n",
    "    min_year = df_artigosperiodo['ANO_PUB'].min()\n",
    "    max_year = df_artigosperiodo['ANO_PUB'].max()\n",
    "    all_years = list(range(min_year, max_year + 1))\n",
    "\n",
    "    # 1. Usando TF-IDF para calcular similaridade entre os títulos\n",
    "    vectorizer = TfidfVectorizer().fit_transform(df_artigosperiodo['ARTIGO'])\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_matrix = cosine_similarity(vectors)\n",
    "\n",
    "    # 2. Filtrando títulos com similaridade acima do threshold e removendo duplicatas\n",
    "    indices_to_drop = []\n",
    "    for i in range(cosine_matrix.shape[0]):\n",
    "        for j in range(i + 1, cosine_matrix.shape[1]):\n",
    "            if cosine_matrix[i][j] > threshold:\n",
    "                indices_to_drop.append(j)\n",
    "\n",
    "    indices_to_drop = list(set(indices_to_drop))\n",
    "    df_artigosperiodo = df_artigosperiodo.drop(df_artigosperiodo.index[indices_to_drop])\n",
    "\n",
    "    # 3. Plotar o gráfico de evolução\n",
    "\n",
    "    # Agrupando por ano\n",
    "    artigos_por_ano = df_artigosperiodo.groupby('ANO_PUB').size().reset_index(name='count')\n",
    "\n",
    "    # Preenchimento padrão para todos os anos\n",
    "    fill_colors = ['#1f77b4'] * artigos_por_ano.shape[0]\n",
    "\n",
    "    # Para o último ano, deixar sem preenchimento (somente borda)\n",
    "    last_year = artigos_por_ano['ANO_PUB'].iloc[-1]\n",
    "    fill_colors[-1] = 'rgba(0,0,0,0)'\n",
    "\n",
    "    # Plotagem\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Adicionar barras para cada ano\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=artigos_por_ano['ANO_PUB'],\n",
    "        y=artigos_por_ano['count'],\n",
    "        name='Total de Artigos por Ano',\n",
    "        marker=dict(color=fill_colors,\n",
    "                    line=dict(color='#1f77b4', width=2))\n",
    "    ))\n",
    "\n",
    "    # Adicionar linha de média\n",
    "    mean_articles = artigos_por_ano['count'].mean()\n",
    "    years = artigos_por_ano['ANO_PUB'].values\n",
    "\n",
    "    # Excluindo o último ano da linha de média\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=years[0],\n",
    "        y0=mean_articles,\n",
    "        x1=years[-2],\n",
    "        y1=mean_articles,\n",
    "        line=dict(color=\"Red\", width=2, dash=\"dashdot\")\n",
    "    )\n",
    "\n",
    "    # Configurações adicionais do gráfico\n",
    "    fig.update_layout(\n",
    "        title=\"Evolução da quantidade de participação em artigos por ano concluídos, média e artigos do ano em curso\",\n",
    "        xaxis_title=\"Ano\",\n",
    "        yaxis_title=\"Número de Artigos\",\n",
    "        legend_title=\"Legenda\",\n",
    "        xaxis=dict(tickvals=all_years),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            x=0.2,\n",
    "            y=0.99\n",
    "    ),\n",
    "\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Ingerir dados de entrada</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos arquivos de entrada de dados\n",
    "\n",
    "(listas de pessoal e listas da VPEIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(pathcsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(pathzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(pathout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extrair currículos da Plataforma Lattes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir grupo de indivíduos para análise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrair nomes de membros do programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Armenio Aguiar dos Santos': 'http://lattes.cnpq.br/6367176618425888',\n",
      " 'Eanes Delgado Barros Pereira': 'http://lattes.cnpq.br/1517783578435444',\n",
      " 'Elizabeth De Francesco Daher': 'http://lattes.cnpq.br/4855968398515646',\n",
      " 'Francisco Airton Castro da Rocha': 'http://lattes.cnpq.br/4916026652021507',\n",
      " 'Fábio Miyajima': 'http://lattes.cnpq.br/0998235420634887',\n",
      " 'Geanne Matos de Andrade': 'http://lattes.cnpq.br/9935129797137635',\n",
      " 'Gerly Anne de Castro Brito': 'http://lattes.cnpq.br/8991062042568398',\n",
      " 'Marcellus Henrique Loiola Ponte de Souza': 'http://lattes.cnpq.br/4001596522263940',\n",
      " 'Marcelo Alcantara Holanda': 'http://lattes.cnpq.br/4091972080928881',\n",
      " 'Miguel Ângelo Nobre e Souza': 'http://lattes.cnpq.br/2471578430392531',\n",
      " 'Pedro Braga Neto': 'http://lattes.cnpq.br/0524387231525638',\n",
      " 'Pedro Felipe Carvalhedo de Bruin': 'http://lattes.cnpq.br/9205614478199218',\n",
      " 'Raquel Carvalho Montenegro': 'http://lattes.cnpq.br/0043828437326839',\n",
      " 'Renan Magalhães Montenegro Junior': 'http://lattes.cnpq.br/7492450432942397',\n",
      " 'Ronald Feitosa Pinheiro': 'http://lattes.cnpq.br/4755251182720144',\n",
      " 'Rossana de Aguiar Cordeiro': 'http://lattes.cnpq.br/1934399087822977',\n",
      " 'Silvia Maria Meira Magalhães': 'http://lattes.cnpq.br/9118657720317683',\n",
      " 'Tainá Veras de Sandes Freitas': 'http://lattes.cnpq.br/8033606057154785',\n",
      " 'Veralice Meireles Sales de Bruin': 'http://lattes.cnpq.br/1875628960274922',\n",
      " 'Xinaida Taligare Vasconcelos Lima': 'http://lattes.cnpq.br/6289689171537125'}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "# Realizar Solicitação HTTP\n",
    "response = requests.get(\"https://ppgcm.ufc.br/pt/corpo-docente/\")\n",
    "\n",
    "# Parse do Conteúdo HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Limitar o Escopo da Busca\n",
    "main_content = soup.find('main')\n",
    "\n",
    "# Inicialização de Estruturas de Dados\n",
    "docente_info = {}\n",
    "\n",
    "# Iteração sobre os Elementos da Tabela\n",
    "for tr_tag in main_content.find_all('tr'):\n",
    "    \n",
    "    # Extração do Nome e do Link\n",
    "    p_tag = tr_tag.find('p')\n",
    "    a_tag = tr_tag.find('a', href=True)\n",
    "    \n",
    "    if p_tag and a_tag:\n",
    "        nome = p_tag.text.strip()\n",
    "        if '– ' in nome:\n",
    "            nome = nome.split('– ')[0].strip()\n",
    "        elif '- ' in nome:\n",
    "            nome = nome.split('- ')[0].strip()\n",
    "        link = a_tag['href']\n",
    "        \n",
    "        if \"http://lattes.cnpq.br/\" in link:\n",
    "            docente_info[nome] = link\n",
    "\n",
    "# Exibição do Dicionário com os Nomes e Links\n",
    "pprint(docente_info)\n",
    "nomes = docente_info.keys()\n",
    "lista_nomes = list(nomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Armenio Aguiar dos Santos',\n",
       " 'Eanes Delgado Barros Pereira',\n",
       " 'Elizabeth De Francesco Daher',\n",
       " 'Francisco Airton Castro da Rocha',\n",
       " 'Geanne Matos de Andrade',\n",
       " 'Gerly Anne de Castro Brito',\n",
       " 'Marcellus Henrique Loiola Ponte de Souza',\n",
       " 'Pedro Braga Neto',\n",
       " 'Pedro Felipe Carvalhedo de Bruin',\n",
       " 'Raquel Carvalho Montenegro',\n",
       " 'Renan Magalhães Montenegro Junior',\n",
       " 'Ronald Feitosa Pinheiro',\n",
       " 'Rossana de Aguiar Cordeiro',\n",
       " 'Silvia Maria Meira Magalhães',\n",
       " 'Tainá Veras de Sandes Freitas',\n",
       " 'Veralice Meireles Sales de Bruin',\n",
       " 'Fábio Miyajima',\n",
       " 'Marcelo Alcantara Holanda',\n",
       " 'Miguel Ângelo Nobre e Souza',\n",
       " 'Xinaida Taligare Vasconcelos Lima']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_nomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efetuar requisições à página do CNPq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando com o servidor do CNPq...\n",
      "00h 00m 07s Tempo de conexão ao servidor do CNPq\n",
      "Checkbox buscar todos níveis de formação marcado.\n",
      "------------------------------------------------------------------------------------------\n",
      "Extraindo currículo 1/2. Resta 1. Decorrido:00:00:07. Previsão de término em 00:00:16\n",
      "Vínculo encontrado no currículo de nome: Armenio Aguiar dos Santos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>CURRICULO</th>\n",
       "      <th>CONTEUDOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Armenio Aguiar dos Santos</td>\n",
       "      <td><img src=\"http://servicosweb.cnpq.br/wspessoa/servletrecuperafoto?tipo=1&id=K4787826P6\" style=max-height:124px;\"/></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizado em 28/03/2023 há 205 dias | Armenio Aguiar dos Santos\n",
      "Erro ao montar dataframe dados de artigos\n",
      "slice indices must be integers or None or have an __index__ method   File \"C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_45948\\628932237.py\", line 424, in extrair_dados\n",
      "    producao_bibliografica_div = sessao.find('div', {'id': 'artigos-completos'})\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "--------------------------------------------------\n",
      "00h 00m 45s Tempo extração dados do currículo\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marco\\fioce\\source\\adapters\\input\\jupyter_notebooks\\extrair_lattes_medicina_ufc.ipynb Célula 70\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marco/fioce/source/adapters/input/jupyter_notebooks/extrair_lattes_medicina_ufc.ipynb#Y160sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marco/fioce/source/adapters/input/jupyter_notebooks/extrair_lattes_medicina_ufc.ipynb#Y160sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(tempo(t_ini,time\u001b[39m.\u001b[39mtime()), \u001b[39m'\u001b[39m\u001b[39mTempo extração dados do currículo\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/marco/fioce/source/adapters/input/jupyter_notebooks/extrair_lattes_medicina_ufc.ipynb#Y160sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(sucesso_servidores)\u001b[39m}\u001b[39;00m\u001b[39m currículos encontrados e extraído\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marco/fioce/source/adapters/input/jupyter_notebooks/extrair_lattes_medicina_ufc.ipynb#Y160sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(sucesso_servidores)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(lista_nomes)\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m% do total de servidores com currículos na plataforma Lattes do CNPq\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t_ini=time.time()\n",
    "df_secoes_servidores, sucesso_servidores, json_data = extrair_dados(lista_nomes[:2], mestres=True, assunto=False)\n",
    "\n",
    "print('-'*50)\n",
    "print(tempo(t_ini,time.time()), 'Tempo extração dados do currículo')\n",
    "print(f'{len(sucesso_servidores)} currículos encontrados e extraído')\n",
    "print(f'{len(sucesso_servidores)/len(lista_nomes)*100:.2f}% do total de servidores com currículos na plataforma Lattes do CNPq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total de linhas de dados extraídas: {len(df_secoes_servidores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Servidores currículo Lattes não encontrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lista_servidores:\n",
    "    if i not in sucesso_servidores:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar Resultados em Arquivo de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_secoes_servidores.to_csv(pathout+'df_secoes_servidores.csv', sep=\";\", index=False)\n",
    "len(df_secoes_servidores[df_secoes_servidores['ROTULOS']=='Nome'].index)\n",
    "\n",
    "for n,i in enumerate(df_secoes_servidores['CURRICULO'].unique()):\n",
    "    print(f'{n+1:2} {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar funções para contar artigos e citações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    if 'SOMA_CITACOES' in df_idlattes.columns:\n",
    "        df_idlattes.drop('SOMA_CITACOES', axis=1, inplace=True)\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def sum_citations(text):\n",
    "    pattern = r\"Citações:((?:\\d+\\|)*\\d+)\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    total_sum = 0\n",
    "    \n",
    "    for match in matches:\n",
    "        numbers = map(int, match.split(\"|\"))\n",
    "        total_sum += sum(numbers)\n",
    "        \n",
    "    return total_sum\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        content_utf8 = [x.replace('CitaÃ§Ãµes:','Citações:') for x in content]\n",
    "        if not isinstance(content_utf8, (str, bytes)):\n",
    "            content = ' '.join(map(str, content_utf8))\n",
    "        citation_pattern = r\"Citações:((?:\\d+\\|)*\\d+)\"\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return total_citations\n",
    "\n",
    "    ds_linhas_conteudos = df_secoes[df_secoes['ROTULOS'] == 'Artigos completos publicados em periódicos'].CONTEUDOS\n",
    "    ds_linhas_conteudos = ds_linhas_conteudos.apply(lambda lista: [x.replace('CitaÃ§Ãµes:', 'Citações:') if isinstance(x, str) else x for x in lista] if isinstance(lista, list) else lista)\n",
    "    ds_qte_artigos      = ds_linhas_conteudos.apply(count_year_occurrences)\n",
    "    ds_qte_citacoes     = ds_linhas_conteudos.apply(extract_citations)\n",
    "    df_secoes_contadas  = df_secoes[df_secoes['ROTULOS'] == 'Artigos completos publicados em periódicos']\n",
    "    df_secoes_contadas['CONTEUDOS']     = ds_linhas_conteudos\n",
    "    df_secoes_contadas.drop('CONTEUDOS', axis=1, inplace=True)\n",
    "    df_secoes_contadas['QTE_ARTIGOS']   = ds_qte_artigos\n",
    "    df_secoes_contadas['SOMA_CITACOES'] = ds_qte_citacoes\n",
    "\n",
    "    return df_secoes_contadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_secoes = df_secoes_servidores\n",
    "df_idlattes = listar_idlattes(df_secoes)\n",
    "df_idlattes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte_artigos = contar_artigos(df_secoes)\n",
    "print(df_qte_artigos.QTE_ARTIGOS.sum())\n",
    "df_qte_artigos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar funções montar áreas de pesquisa CNPq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "pasta_arquivo = pathzip \n",
    "arquivo = 'cnpq_tabela-areas-conhecimento.pdf'\n",
    "caminho = pasta_arquivo+arquivo\n",
    "\n",
    "def verifica_ponto_virgula(df):\n",
    "    return df[df['Descricao'].str.contains(';', regex=False)]\n",
    "\n",
    "def verifica_virgula(df):\n",
    "    return df[df['Descricao'].str.contains(',', regex=False)]\n",
    "\n",
    "def verifica_formato_descricao(descricao):\n",
    "    excecoes = [\"de\", \"do\", \"da\", \"dos\", \"das\", \"a\", \"o\", \"e\", \"em\", \"com\", \"para\", \"por\", \"sem\"]\n",
    "    palavras = descricao.split()\n",
    "    \n",
    "    for i, palavra in enumerate(palavras):\n",
    "        if palavra.lower() in excecoes or palavra[0]==\"(\":\n",
    "            continue\n",
    "        if not palavra[0].isupper() or (palavra==palavras[-1] and palavra in excecoes):\n",
    "            return False, i  # Retornar False e o índice da palavra problemática\n",
    "    return True, None\n",
    "\n",
    "    # for idx, word in enumerate(palavras):\n",
    "    #     # Se a palavra inicia com letra minúscula e não é uma preposição ou artigo\n",
    "    #     if word[0].islower() and word not in excecoes:\n",
    "    #         # Aqui verificamos se a palavra anterior termina com uma letra e a palavra atual é uma preposição ou artigo\n",
    "    #         if idx > 0 and palavras[idx - 1][-1].isalpha() and word in excecoes:\n",
    "    #             return (False, idx)\n",
    "    #         # Ou apenas a condição de começar com minúscula e não ser preposição ou artigo\n",
    "    #         elif idx == 0 or (idx > 0 and not palavras[idx - 1][-1].isalpha()):\n",
    "    #             return (False, idx)    \n",
    "\n",
    "def corrigir_descricao(descricao, word_index):\n",
    "    excecoes = [\"de\", \"do\", \"da\", \"dos\", \"das\", \"a\", \"o\", \"e\", \"em\", \"com\", \"para\", \"por\", \"sem\"]\n",
    "    palavras = descricao.split()\n",
    "\n",
    "    # Se o índice anterior existir e a palavra atual começa com minúscula\n",
    "    if word_index > 0 and palavras[word_index][0].islower():\n",
    "        # Checar se a palavra é uma preposição ou artigo e se a anterior termina com uma letra\n",
    "        if palavras[word_index] in excecoes:\n",
    "            palavras[word_index - 1] += palavras[word_index]\n",
    "            del palavras[word_index]\n",
    "        else:\n",
    "            # Juntar palavra atual com a palavra anterior\n",
    "            palavras[word_index - 1] += palavras[word_index]\n",
    "            del palavras[word_index]\n",
    "\n",
    "    # Após as correções, juntamos as palavras de volta em uma única string\n",
    "    nova_descricao = ' '.join(palavras)\n",
    "\n",
    "    # Imprimindo para debug\n",
    "    # print(f\"Descrição ruim: {descricao}\")\n",
    "    # print(f\"Correção feita: {palavra_anterior} + {palavra_incorreta} = {correcao}\")\n",
    "    # print(f\"Nova descrição: {nova_descricao}\\n\")\n",
    "    \n",
    "    return nova_descricao\n",
    "\n",
    "\n",
    "def extrair_areas(caminho):\n",
    "    texto_completo = \"\"\n",
    "\n",
    "    reader = PdfReader(caminho)\n",
    "    \n",
    "    for npag, p in tqdm(enumerate(reader.pages), total=len(reader.pages), desc=\"Processando páginas do PDF das Áreas de pesquisa do CNPq..\"):\n",
    "        texto_completo += p.extract_text()\n",
    "\n",
    "    texto_completo = texto_completo.replace('\\n', ' ').replace(\" -\",\"-\").replace(\" ,\",\",\").strip().replace(\"ã o\",\"ão\")\n",
    "    texto_completo = re.sub(r'\\s?(\\d)\\s?(\\.)\\s?(\\d{2})\\s?(\\.)\\s?(\\d{2})\\s?(\\.)\\s?(\\d{2})\\s?(-)\\s?(\\d)\\s?', r'\\1\\2\\3\\4\\5\\6\\7\\8\\9', texto_completo)\n",
    "\n",
    "    pattern = r'(\\d\\.\\d{2}\\.\\d{2}\\.\\d{2}-\\d)([^0-9]+)'\n",
    "    matches = re.findall(pattern, texto_completo)\n",
    "\n",
    "    codigos = [match[0] for match in matches]\n",
    "    descricoes = [match[1].strip() for match in matches]\n",
    "\n",
    "    print(f'Total dos códigos   identificados: {len(codigos)}')\n",
    "    print(f'Total de descrições identificadas: {len(descricoes)}')\n",
    "\n",
    "    df_linhas = pd.DataFrame({'Codigo': codigos, 'Descricao': descricoes})\n",
    "\n",
    "    # Verificação da divisão correta dos códigos/descrições\n",
    "    descricoes_com_numeros = df_linhas[df_linhas['Descricao'].str.contains(r'\\d')]\n",
    "    if not descricoes_com_numeros.empty:\n",
    "        print(f\"Conferência: {len(descricoes_com_numeros)} descrições contêm números!\")\n",
    "    else:\n",
    "        print(f\"Nenhum erro de códigos em descrições!\")\n",
    "\n",
    "    # Identificando e printando a quantidade de possíveis erros\n",
    "    erros = sum(1 for descricao in descricoes if not verifica_formato_descricao(descricao)[0])\n",
    "    print(f\"{erros} possíveis erros de descrição detectados.\")\n",
    "\n",
    "    # Barra de progresso para correção das descrições\n",
    "    with tqdm(total=df_linhas.shape[0], desc=\"Corrigindo descrições...\") as pbar:\n",
    "        for index, row in df_linhas.iterrows():\n",
    "            is_valid, word_index = verifica_formato_descricao(row['Descricao'])\n",
    "            loop_count = 0\n",
    "            while not is_valid and loop_count < 10:\n",
    "                row['Descricao'] = corrigir_descricao(row['Descricao'], word_index)\n",
    "                is_valid, word_index = verifica_formato_descricao(row['Descricao'])\n",
    "                loop_count += 1\n",
    "            if loop_count == 10:\n",
    "                print(f\"Problema corrigindo descrição: {row['Descricao']}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    return df_linhas\n",
    "\n",
    "df_areas = extrair_areas(caminho)\n",
    "\n",
    "def count_unique_for_level(level: int):\n",
    "    return df_areas['Codigo'].str.split('.', expand=True)[level].nunique()\n",
    "\n",
    "# Remover o sufixo após o hífen\n",
    "def count_unique_for_last_level():\n",
    "    return df_areas['Codigo'].str.split('.', expand=True).iloc[:, -1].str.split('-').str[0].nunique()\n",
    "\n",
    "levels = df_areas['Codigo'].str.count(\"\\.\").iloc[0]  # conta a quantidade de pontos, para determinar o número de níveis\n",
    "\n",
    "unique_counts = [count_unique_for_level(i) for i in range(levels)]\n",
    "unique_counts.append(count_unique_for_last_level())\n",
    "\n",
    "# qte_grandeareas, qte_areas, qte_subareas, qte_especialidades = unique_counts\n",
    "# print(f'Quantidades de codigos:')\n",
    "# print(f'  Grande_Área: {qte_grandeareas:2}')\n",
    "# print(f'         Área: {qte_areas:2}')\n",
    "# print(f'      Subárea: {qte_subareas:2}')\n",
    "# print(f'Especialidade: {qte_especialidades:2}')\n",
    "\n",
    "# Dividir a coluna 'Codigo' em várias colunas\n",
    "df_split = df_areas['Codigo'].str.split('.', expand=True)\n",
    "\n",
    "# Remover o último hífen e dígito das colunas \n",
    "df_split.iloc[:, -1] = df_split.iloc[:, -1].str.split('-').str[0]\n",
    "\n",
    "def count_sublevels(df, level):\n",
    "    if level == 0:\n",
    "        return df[0].nunique()\n",
    "    else:\n",
    "        return df.groupby(list(range(level)))[level].nunique().reset_index(name=\"count\")[\"count\"].to_list()\n",
    "\n",
    "sublevels_counts = [count_sublevels(df_split, i) for i in range(df_split.shape[1])]\n",
    "\n",
    "# Criar uma coluna para armazenar a contagem de subníveis\n",
    "# df_areas['SublevelCount'] = df_split.apply(lambda row: [sublevels_counts[col][row[:col].astype(str).tolist().index(row[col-1]) if row[col-1] in row[:col].astype(str).tolist() else -1] if col > 0 else sublevels_counts[col] for col in df_split.columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_areas[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir por nível de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def contar_marcadores(texto):\n",
    "    padrao = r'\\.00'\n",
    "    ocorrencias = re.findall(padrao, texto)\n",
    "    return len(ocorrencias)\n",
    "\n",
    "cat_grandeareas=[]\n",
    "cat_subareas=[]\n",
    "cat_areas=[]\n",
    "cat_especialidades=[]\n",
    "\n",
    "for cod,des in zip(df_areas['Codigo'],df_areas['Descricao']):\n",
    "    k = contar_marcadores(cod)\n",
    "    if k==3:\n",
    "        cat_grandeareas.append((cod,des))\n",
    "    elif k==2:\n",
    "        cat_subareas.append((cod,des))\n",
    "    elif k==1:\n",
    "        cat_areas.append((cod,des))\n",
    "    elif k==0:\n",
    "        cat_especialidades.append((cod,des))\n",
    "    else:\n",
    "        print('Erro na separação')\n",
    "        print(f'{k} {cod}{des}')\n",
    "\n",
    "print(f'{len(cat_grandeareas):4} Grandes Áreas')\n",
    "print(f'{len(cat_subareas):4} Subáreas')\n",
    "print(f'{len(cat_areas):4} Áreas')\n",
    "print(f'{len(cat_especialidades):4} Especialidades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_grandeareas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_subareas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_areas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_especialidades[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_areas.to_csv(pathout+'cnpq_areas-pesquisa.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela de Áreas de Conhecimento CNPQ\n",
    "\n",
    "http://lattes.cnpq.br/documents/11871/24930/TabeladeAreasdoConhecimento.pdf/d192ff6b-3e0a-4074-a74d-c280521bd5f7\n",
    "\n",
    "http://lattes.cnpq.br/web/dgp/arvore-do-conhecimento\n",
    "\n",
    "<b> Árvore do conhecimento </b>\n",
    "\n",
    "    Ciências Agrárias.\n",
    "    Ciências Biológicas.\n",
    "    Ciências da Saúde.\n",
    "    Ciências Exatas e da Terra.\n",
    "    Engenharias.\n",
    "    Ciências Humanas.\n",
    "    Ciências Sociais Aplicadas.\n",
    "    Lingüística, Letras e Artes.\n",
    "\n",
    "<b> Setores de Aplicação </b>\n",
    "\n",
    "https://lattes.cnpq.br/web/dgp/setores-de-aplicacao-2002-2010 (Link Quebrado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>ESTUDAR COMO CLASSIFICAR ATIVIDADES DE PESQUISA</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atividades econômicas na CNAE 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação nacional de atividades econômicas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fonte:\n",
    "https://concla.ibge.gov.br/classificacoes/por-tema/atividades-economicas/classificacao-nacional-de-atividades-economicas.html\n",
    "\n",
    "Estrutura: \n",
    "\n",
    "    1º nível: 21 Seções\n",
    "    2º nível: 87 Divisões\n",
    "    3° nível: 285 Grupos\n",
    "    4º nível: 673 Classes\n",
    "    5º nível: 1301 Subclasses\n",
    "\n",
    "Descrição: A Classificação Nacional de Atividades Econômicas-CNAE é a classificação oficialmente adotada pelo Sistema Estatístico Nacional e pelos órgãos federais gestores de registros administrativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fonte:\n",
    "https://concla.ibge.gov.br/busca-online-cnae.html?view=estrutura\n",
    "\n",
    "Seções:\n",
    "\n",
    "    A\t01 .. 03\tAGRICULTURA, PECUÁRIA, PRODUÇÃO FLORESTAL, PESCA E AQÜICULTURA\n",
    "    B\t05 .. 09\tINDÚSTRIAS EXTRATIVAS\n",
    "    C\t10 .. 33\tINDÚSTRIAS DE TRANSFORMAÇÃO\n",
    "    D\t35 .. 35\tELETRICIDADE E GÁS\n",
    "    E\t36 .. 39\tÁGUA, ESGOTO, ATIVIDADES DE GESTÃO DE RESÍDUOS E DESCONTAMINAÇÃO\n",
    "    F\t41 .. 43\tCONSTRUÇÃO\n",
    "    G\t45 .. 47\tCOMÉRCIO; REPARAÇÃO DE VEÍCULOS AUTOMOTORES E MOTOCICLETAS\n",
    "    H\t49 .. 53\tTRANSPORTE, ARMAZENAGEM E CORREIO\n",
    "    I\t55 .. 56\tALOJAMENTO E ALIMENTAÇÃO\n",
    "    J\t58 .. 63\tINFORMAÇÃO E COMUNICAÇÃO\n",
    "    K\t64 .. 66\tATIVIDADES FINANCEIRAS, DE SEGUROS E SERVIÇOS RELACIONADOS\n",
    "    L\t68 .. 68\tATIVIDADES IMOBILIÁRIAS\n",
    "    M\t69 .. 75\tATIVIDADES PROFISSIONAIS, CIENTÍFICAS E TÉCNICAS\n",
    "    N\t77 .. 82\tATIVIDADES ADMINISTRATIVAS E SERVIÇOS COMPLEMENTARES\n",
    "    O\t84 .. 84\tADMINISTRAÇÃO PÚBLICA, DEFESA E SEGURIDADE SOCIAL\n",
    "    P\t85 .. 85\tEDUCAÇÃO\n",
    "    Q\t86 .. 88\tSAÚDE HUMANA E SERVIÇOS SOCIAIS\n",
    "    R\t90 .. 93\tARTES, CULTURA, ESPORTE E RECREAÇÃO\n",
    "    S\t94 .. 96\tOUTRAS ATIVIDADES DE SERVIÇOS\n",
    "    T\t97 .. 97\tSERVIÇOS DOMÉSTICOS\n",
    "    U\t99 .. 99\tORGANISMOS INTERNACIONAIS E OUTRAS INSTITUIÇÕES EXTRATERRITORIAIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busca em atividades econômicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sintaxe para busca\n",
    "\n",
    "O mecanismo de busca da CNAE aceita consultas a partir de palavra, palavras completas e incompletas e texto. Os resultados serão exibidos ordenados inicialmente pelo código podendo o usuário optar pelo tipo de ordenação (código ou descrição).\n",
    "\n",
    "Uma dica importante é usar apenas palavras chaves na sua busca. Ao invés de buscar, por exemplo, Gado de Corte, busque por Gado Corte. O uso de preposições pode levar a resultados indesejados ou de pouca relevância para sua busca. O mecanismo de busca não faz distinção entre maiúscula e minúscula e acentuação.\n",
    "\n",
    "<b> Hierarquia </b>\n",
    "\n",
    "    Seção:\t    M\tATIVIDADES PROFISSIONAIS, CIENTÍFICAS E TÉCNICAS\n",
    "    Divisão:\t72 PESQUISA E DESENVOLVIMENTO CIENTÍFICO\n",
    "    Grupo:              72.1 Pesquisa e desenvolvimento experimental em ciências físicas e naturais\n",
    "                        72.2 Pesquisa e desenvolvimento experimental em ciências sociais e humanas\n",
    "\n",
    "\n",
    "    Seção:\t    M\tATIVIDADES PROFISSIONAIS, CIENTÍFICAS E TÉCNICAS\n",
    "    Divisão: \t72 PESQUISA E DESENVOLVIMENTO CIENTÍFICO\n",
    "    Grupo:\t            72.1 Pesquisa e desenvolvimento experimental em ciências físicas e naturais\n",
    "    Classe:\t                72.10-0 Pesquisa e desenvolvimento experimental em ciências físicas e naturais\n",
    "    Subclasse:                  7210-0/00 Pesquisa e desenvolvimento experimental em ciências físicas e naturais\n",
    "\n",
    "Notas Explicativas:\n",
    "Esta classe compreende:\n",
    "\n",
    "    - as atividades de pesquisa e desenvolvimento realizadas no âmbito das ciências da vida, tais como: medicina, biologia, bioquímica, farmácia, agronomia e conexas\n",
    "\n",
    "    - as atividades de pesquisa e desenvolvimento realizadas no âmbito das ciências físicas e de engenharia, tais como: matemática, física, astronomia, química, geociências e conexas\n",
    "\n",
    "Lista de Descritores\n",
    "Registros encontrados: 16\n",
    "\n",
    "    Código\tDescrição\n",
    "    7210-0\tAGRONOMIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tBIOQUÍMICA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tFARMÁCIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tFAZENDA EXPERIMENTAL; PESQUISA\n",
    "    7210-0\tLABORATÓRIO DE PESQUISA FÍSICA, COMERCIAL E NÃO COMERCIAL\n",
    "    7210-0\tLABORATÓRIO DE PESQUISA QUÍMICA, COMERCIAL E NÃO COMERCIAL\n",
    "    7210-0\tLABORATÓRIO INDUSTRIAL; PESQUISA\n",
    "    7210-0\tMEDICINA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tPESQUISA BIOGENÉTICA\n",
    "    7210-0\tPESQUISA BIOLÓGICA\n",
    "    7210-0\tPESQUISA E DESENVOLVIMENTO COM ACESSO A PATRIMÔNIO GENÉTICO EXISTENTE NO TERRITÓRIO NACIONAL; ATIVIDADES DE\n",
    "    7210-0\tPESQUISA E DESENVOLVIMENTO DAS CIÊNCIAS FÍSICAS E NATURAIS\n",
    "    7210-0\tPESQUISA E DESENVOLVIMENTO EXPERIMENTAL EM CIÊNCIAS FÍSICAS E NATURAIS; ATIVIDADES DE\n",
    "    7210-0\tPESQUISA MATEMÁTICA, FÍSICA, ASTRONOMIA; DESENVOLVIMENTO DE\n",
    "    7210-0\tPESQUISA MÉDICA NÃO COMERCIAL\n",
    "    7210-0\tQUÍMICA; PESQUISA E DESENVOLVIMENTO EM\n",
    "\n",
    "<b> Exemplo de atividades relacionadas ao termo \"Pesquisa\" no CNAE 2.0 </b>\n",
    "\n",
    "Classes encontradas: 45\n",
    "\n",
    "    Código\tDescrição\n",
    "    0159-8\tANIMAIS PARA PESQUISA; CRIAÇÃO DE\n",
    "    0159-8\tBIOTÉRIO; CRIAÇÃO DE ANIMAIS PARA PESQUISA\n",
    "    0170-9\tCAPTURA DE ANIMAIS, MORTOS OU VIVOS, PARA PESQUISA, UTILIZAÇÃO EM ZOOLÓGICOS; SERVIÇOS DE\n",
    "    2651-5\tAPARELHOS E EQUIPAMENTOS PARA LABORATÓRIOS DE PESQUISA CIENTÍFICA; FABRICAÇÃO DE\n",
    "    2651-5\tAPARELHOS E EQUIPAMENTOS PARA LABORATÓRIOS DE PESQUISA E DESENVOLVIMENTO; FABRICAÇÃO DE\n",
    "    2829-1\tCENTRIFUGADOR PARA LABORATÓRIO DE ANÁLISE. ENSAIO OU PESQUISA CIENTÍFICA; FABRICAÇÃO DE\n",
    "    3011-3\tNAVIOS-HOSPITAIS, NAVIOS DE GUERRA, EMBARCAÇÕES PARA PESQUISA CIENTÍFICA E OUTRAS EMBARCAÇÕES SEMELHANTES; FABRICAÇÃO DE\n",
    "    3312-1\tAPARELHOS E EQUIPAMENTOS PARA LABORATÓRIOS DE PESQUISA CIENTÍFICA, MANUTENÇAO E REPARACAO DE\n",
    "    3312-1\tAPARELHOS E EQUIPAMENTOS PARA LABORATÓRIOS DE PESQUISA E DESENVOLVIMENTO, MANUTENÇAO E REPARACAO DE\n",
    "    5030-1\tTRANSPORTE DE MERCADORIAS E PESSOAS PARA SUPRIMENTO E APOIO A PLATAFORMAS DE PESQUISA\n",
    "    6319-4\tBANCO DE INFORMAÇÃO PARA PESQUISA E ANÁLISE; SERVIÇOS DE\n",
    "    7119-7\tPROSPECÇÃO, PESQUISA MINERAL; SERVIÇOS DE\n",
    "    7210-0\tAGRONOMIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tBIOQUÍMICA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tFARMÁCIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tFAZENDA EXPERIMENTAL; PESQUISA\n",
    "    7210-0\tLABORATÓRIO DE PESQUISA FÍSICA, COMERCIAL E NÃO COMERCIAL\n",
    "    7210-0\tLABORATÓRIO DE PESQUISA QUÍMICA, COMERCIAL E NÃO COMERCIAL\n",
    "    7210-0\tLABORATÓRIO INDUSTRIAL; PESQUISA\n",
    "    7210-0\tMEDICINA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7210-0\tPESQUISA BIOGENÉTICA\n",
    "    7210-0\tPESQUISA BIOLÓGICA\n",
    "    7210-0\tPESQUISA E DESENVOLVIMENTO COM ACESSO A PATRIMÔNIO GENÉTICO EXISTENTE NO TERRITÓRIO NACIONAL; ATIVIDADES DE\n",
    "    7210-0\tPESQUISA E DESENVOLVIMENTO DAS CIÊNCIAS FÍSICAS E NATURAIS\n",
    "    7210-0\tPESQUISA E DESENVOLVIMENTO EXPERIMENTAL EM CIÊNCIAS FÍSICAS E NATURAIS; ATIVIDADES DE\n",
    "    7210-0\tPESQUISA MATEMÁTICA, FÍSICA, ASTRONOMIA; DESENVOLVIMENTO DE\n",
    "    7210-0\tPESQUISA MÉDICA NÃO COMERCIAL\n",
    "    7210-0\tQUÍMICA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7220-7\tARQUEOLOGIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7220-7\tARTES; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7220-7\tDIREITO; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7220-7\tECONOMIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7220-7\tLINGÜÍSTICA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7220-7\tPESQUISA E DESENVOLVIMENTO EM CIÊNCIAS SOCIAIS E HUMANAS\n",
    "    7220-7\tPESQUISA ECONÔMICA, COMERCIAL E NÃO COMERCIAL\n",
    "    7220-7\tPESQUISA EDUCACIONAL\n",
    "    7220-7\tSOCIOLOGIA; PESQUISA E DESENVOLVIMENTO EM\n",
    "    7320-3\tPESQUISA DE MERCADO E DE OPINIÃO PÚBLICA\n",
    "    7320-3\tPESQUISA DE OPINIÃO PÚBLICA\n",
    "    7320-3\tPESQUISA E COLETA DE DADOS PARA PESQUISAS DE MERCADO E OPINIÃO\n",
    "    7320-3\tPESQUISA MERCADOLÓGICA\n",
    "    7320-3\tPESQUISA POLÍTICA; SERVIÇOS DE\n",
    "    8411-6\tFUNDAÇÃO DE APOIO À PESQUISA E EXTENSÃO\n",
    "    8650-0\tCONSULTORIA EM BIOMEDICINA, EXCETO PARA PESQUISA E DESENVOLVIMENTO; ATIVIDADES DE\n",
    "    9101-5\tDOCUMENTAÇÃO E PESQUISA BIBLIOGRÁFICA; ATIVIDADE DE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Áreas de avaliação da CAPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Áreas do Conhecimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gov.br/capes/pt-br/acesso-a-informacao/acoes-e-programas/avaliacao/instrumentos/documentos-de-apoio-1/tabela-de-areas-de-conhecimento-avaliacao\n",
    "\n",
    "Fonte: www.gov.br, publicado em 19/09/2020 19h38 Atualizado em 24/10/2022 18h03\n",
    "\n",
    "A classificação das Áreas do Conhecimento tem finalidade eminentemente prática, objetivando proporcionar às Instituições de ensino, pesquisa e inovação uma maneira ágil e funcional de sistematizar e prestar informações concernentes a projetos de pesquisa e recursos humanos aos órgãos gestores da área de ciência e tecnologia.\n",
    "\n",
    "A organização das Áreas do Conhecimento na tabela apresenta uma hierarquização em quatro níveis, do mais geral ao mais específico, abrangendo nove grandes áreas nas quais se distribuem as 49 áreas de avaliação da CAPES. Estas áreas de avaliação, por sua vez, agrupam áreas básicas (ou áreas do conhecimento), subdivididas em subáreas e especialidades:\n",
    "\n",
    "1º nível - Grande Área: aglomeração de diversas áreas do conhecimento, em virtude da afinidade de seus objetos, métodos cognitivos e recursos instrumentais refletindo contextos sociopolíticos específicos;\n",
    "\n",
    "2º nível – Área do Conhecimento (Área Básica): conjunto de conhecimentos inter-relacionados, coletivamente construído, reunido segundo a natureza do objeto de investigação com finalidades de ensino, pesquisa e aplicações práticas;\n",
    "\n",
    "3º nível - Subárea: segmentação da área do conhecimento (ou área básica) estabelecida em função do objeto de estudo e de procedimentos metodológicos reconhecidos e amplamente utilizados;\n",
    "\n",
    "4º nível - Especialidade: caracterização temática da atividade de pesquisa e ensino. Uma mesma especialidade pode ser enquadrada em diferentes grandes áreas, áreas básicas e subáreas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorias do Periodicos Capes\n",
    "\n",
    "https://buscador-periodicos-capes-gov-br.ez68.periodicos.capes.gov.br/V/QB984FN2Y5SQTN9JQMSD3BEF1LYT9I6UP1Q51GN5L4YC6JV1EX-14773?func=find-db-info&doc_num=000002739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_capes = pd.read_csv(pathcsv+'categorias_capes.txt', header=None, sep=';')\n",
    "class_capes.columns = ['CATEGORIAS_SUBCATEGORIAS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_capes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoria=[]\n",
    "subcategoria=[]\n",
    "\n",
    "for i in class_capes['CATEGORIAS_SUBCATEGORIAS'].values:\n",
    "    try:\n",
    "        cat, subcat = i.split('/ ')\n",
    "    except:\n",
    "        print(f'Erro ao dividir: {i}')\n",
    "    categoria.append(cat.strip())\n",
    "    subcategoria.append(subcat.strip())\n",
    "\n",
    "qte_cat = len(pd.Series(categoria).unique())\n",
    "qte_subcat = len(pd.Series(subcategoria).unique())\n",
    "\n",
    "desc_grandeareas = [tupla[-1] for tupla in cat_grandeareas]\n",
    "desc_subareas = [tupla[-1] for tupla in cat_subareas]\n",
    "desc_areas = [tupla[-1] for tupla in cat_areas]\n",
    "desc_especialidades = [tupla[-1] for tupla in cat_especialidades]\n",
    "\n",
    "print('ANÁLISE DA CLASSIFICAÇÃO EXTRAÍDA DO PORTAL DE PERIÓDICOS DA CAPES')\n",
    "\n",
    "print(f'\\n=>CATEGORIAS CAPES: Contém {qte_cat} Categorias no total, podendo ser:')\n",
    "for i in pd.Series(categoria).unique():\n",
    "    if i in desc_grandeareas:\n",
    "        print(f'  É uma das     Áreas do CNPq: \"{i}\"')\n",
    "    elif i in desc_subareas:\n",
    "        print(f'  É uma das  Subáreas do CNPq: \"{i}\"')\n",
    "    elif i in desc_areas:\n",
    "        print(f'  É uma Especialidade do CNPq: \"{i}\"')\n",
    "    elif i in desc_especialidades:\n",
    "        print(f'  É uma Especialidade do CNPq: \"{i}\"')\n",
    "    else:\n",
    "        print(f'Classificação ausente no CNPq: \"{i}\"')\n",
    "\n",
    "print(f'\\n=>SUBCATEGORIAS CAPES: Contém {qte_subcat} Subcategorias no total, podendo ser:')\n",
    "for i in pd.Series(subcategoria).unique():\n",
    "    if i in desc_grandeareas:\n",
    "        print(f'  É uma das     Áreas do CNPq: \"{i}\"')\n",
    "    elif i in desc_subareas:\n",
    "        print(f'  É uma das  Subáreas do CNPq: \"{i}\"')\n",
    "    elif i in desc_areas:\n",
    "        print(f'  É uma Especialidade do CNPq: \"{i}\"')\n",
    "    elif i in desc_especialidades:\n",
    "        print(f'  É uma Especialidade do CNPq: \"{i}\"')\n",
    "    else:\n",
    "        print(f'Classificação ausente no CNPq: \"{i}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Valores possíveis de k: {list(range(len(df_areas.index)//600+1))}')\n",
    "k=0\n",
    "n=600\n",
    "df_areas[n*k:(k+1)*n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_areas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibilizar CNPq com Áreas de Avaliação CAPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gov.br/capes/pt-br/acesso-a-informacao/acoes-e-programas/avaliacao/sobre-a-avaliacao/areas-avaliacao/sobre-as-areas-de-avaliacao/sobre-as-areas-de-avaliacao\n",
    "\n",
    "Atualizado em 04/08/2023 12h20 (Acesso em Agosto de 2023)\n",
    "\n",
    "Áreas da Avaliação\n",
    "Com o intuito de facilitar o desenvolvimento das atividades de avaliação, as 49 áreas de avaliação são agregadas, por critério de afinidade, em dois níveis:\n",
    "\n",
    "    • Primeiro nível: Colégios (COLÉGIO DE CIÊNCIAS DA VIDA | COLÉGIO DE HUMANIDADES | COLÉGIO DE CIÊNCIAS EXATAS, TECNOLÓGICAS E MULTIDISCIPLINAR)\n",
    "    • Segundo nível: Grandes Áreas."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.gov.br/capes/pt-br/acesso-a-informacao/acoes-e-programas/avaliacao/sobre-a-avaliacao/areas-avaliacao/sobre-as-areas-de-avaliacao/sobre-as-areas-de-avaliacao#areas\n",
    "\n",
    "ÁREAS DE AVALIAÇÃO\n",
    "\n",
    "COLÉGIO DE CIÊNCIAS DA VIDA\n",
    "    Ciências Agrárias\n",
    "        Ciências Agrárias I\n",
    "        Ciência de Alimentos\n",
    "        Medicina Veterinária\n",
    "        Zootecnia/Recursos Pesqueiros\n",
    "\n",
    "    Ciências Biológicas\n",
    "        Biodiversidade\n",
    "        Ciências Biológicas I\n",
    "        Ciências Biológicas II\n",
    "        Ciências Biológicas III\n",
    "\n",
    "    Ciências da Saúde\n",
    "        Educação Física\n",
    "        Enfermagem\n",
    "        Farmácia\n",
    "        Medicina I\n",
    "        Medicina II\n",
    "        Medicina III\n",
    "        Nutrição\n",
    "        Odontologia\n",
    "        Saúde Coletiva\n",
    "\n",
    "COLÉGIO DE HUMANIDADES\n",
    "    CIÊNCIAS HUMANAS\t \t\n",
    "        Antropologia / Arqueologia\n",
    "        Ciência Política e Relações Internacionais\n",
    "        Ciências da Religião e Teologia\n",
    "        Educação\n",
    "        Filosofia\n",
    "        Geografia\n",
    "        História\n",
    "        Psicologia\n",
    "        Sociologia\n",
    "\n",
    "    CIÊNCIAS SOCIAIS APLICADAS\t \t\n",
    "        Administração Pública e de Empresas, Ciências Contábeis e Turismo\n",
    "        Arquitetura, Urbanismo e Design\n",
    "        Comunicação e Informação\n",
    "        Direito\n",
    "        Economia\n",
    "        Planejamento Urbano e Regional / Demografia\n",
    "        Serviço Social\n",
    "\n",
    "    LINGUÍSTICA, LETRAS E ARTES\n",
    "\t \t Artes\n",
    "\t \t Linguística e Literatura\n",
    "\n",
    "\n",
    "COLÉGIO DE CIÊNCIAS EXATAS, TECNOLÓGICAS E MULTIDISCIPLINAR\n",
    "    CIÊNCIAS EXATAS E DA TERRA\n",
    "        Astronomia / Física\n",
    "        Ciência da Computação\n",
    "        Geociências\n",
    "        Matemática / Probabilidade e Estatística\n",
    "        Química\n",
    "\n",
    "    ENGENHARIAS\n",
    "        Engenharias I\n",
    "        Engenharias II\n",
    "        Engenharias III\n",
    "        Engenharias IV\n",
    "\n",
    "    MULTIDISCIPLINAR\n",
    "        Biotecnologia\n",
    "        Ciências Ambientais\n",
    "        Ensino\n",
    "        Interdisciplinar\n",
    "        Materiais"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Montar a interação com as áras de avaliação da CAPES\n",
    "\n",
    "    1.00.00.00-3        Ciências exatas e da Terra\n",
    "        ÁREA DE AVALIAÇÃO: MATEMÁTICA / PROBABILIDADE E ESTATÍSTICA\n",
    "            1.01.00.00-8    Matemática\n",
    "            1.02.00.00-2    Probabilidade e Estatística\n",
    "\n",
    "        ÁREA DE AVALIAÇÃO: CIÊNCIA DA COMPUTAÇÃO\n",
    "            1.03.00.00-7    Ciência da Computação\n",
    "        \n",
    "        ÁREA DE AVALIAÇÃO: ASTRONOMIA / FÍSICA \n",
    "            1.04.00.00-1    Astronomia\n",
    "            1.05.00.00-6\tFísica\n",
    "\n",
    "        ÁREA DE AVALIAÇÃO: QUÍMICA\n",
    "            1.06.00.00-0\tQuímica\n",
    "\n",
    "        ÁREA DE AVALIAÇÃO: GEOCIÊNCIAS\n",
    "            1.07.00.00-5\tGeoCiências\n",
    "\n",
    "    2.00.00.00-6\tCiências Biológicas\n",
    "        ÁREA DE AVALIAÇÃO: CIÊNCIAS BIOLÓGICAS I\n",
    "            1.08.00.00-0\tOceanografia\n",
    "            \n",
    "            2.01.00.00-0\tBiologia Geral\n",
    "        \n",
    "            2.02.00.00-5\tGenética\n",
    "\n",
    "            2.03.00.00-0\tBotânica    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Estudar recortes de grupos de pessoas para análise</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar/extrair dados servidores doutores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler o arquivo Excel usando apenas as colunas selecionadas\n",
    "fioce_pessoal = pd.read_excel(pathzip+'fioce_colaboradores-2023.xls', skiprows=3, header=0, usecols=selected_columns)\n",
    "fioce_pessoal['NOME'] = fioce_pessoal['NOME'].str.strip()\n",
    "\n",
    "filtro1=fioce_pessoal.VÍNCULO=='SERVIDOR'\n",
    "fioce_pessoal = fioce_pessoal[filtro1]\n",
    "\n",
    "filtro2=fioce_pessoal.STATUS=='ATIVO'\n",
    "fioce_pessoal = fioce_pessoal[filtro2]\n",
    "\n",
    "filtro_combinado = (fioce_pessoal['NÍVEL'] == 'DOUTORADO') | (fioce_pessoal['NÍVEL'] == 'PHD ')\n",
    "fioce_pessoal_doutores = fioce_pessoal[filtro_combinado]\n",
    "\n",
    "lista_servidores_doutores = fioce_pessoal_doutores['NOME']\n",
    "lista_servidores_doutores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Retirar os currículos que não tem artigos publicados\n",
    "# retirar_doutores  = ['Dayane Alves Costa']\n",
    "\n",
    "# ## Montar lista que será buscada no Lattes\n",
    "# lista_servidores_doutores = [item for item in lista_servidores_doutores if item not in retirar_doutores]\n",
    "# print(f'{len(lista_servidores_doutores)} servidores com doutorado a extrair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# t_ini=time.time()\n",
    "# df_secoes_doutores, sucesso_doutores, json_data = extrair_dados(lista_servidores_doutores, mestres=False, assunto=False)\n",
    "\n",
    "# print('-'*50)\n",
    "# print(tempo(t_ini,time.time()), 'Tempo extração dados do currículo')\n",
    "# print(f'{len(sucesso_doutores)/len(lista_servidores_doutores)*100:.2f}% de sucesso na extração dos servidores com doutorado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_secoes_doutores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_doutores = listar_artigos(df_secoes_doutores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_doutores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_contagem_artigos = contar_artigos(df_artigos_doutores)\n",
    "# df_contagem_artigos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar/extrair servidores ativos, nível mestrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ler o arquivo Excel usando apenas as colunas selecionadas\n",
    "# fioce_pessoal = pd.read_excel(pathzip+'fioce_colaboradores-2023.xls', skiprows=3, header=0, usecols=selected_columns)\n",
    "# fioce_pessoal['NOME'] = fioce_pessoal['NOME'].str.strip()\n",
    "\n",
    "# filtro1=fioce_pessoal.VÍNCULO=='SERVIDOR'\n",
    "# fioce_pessoal = fioce_pessoal[filtro1]\n",
    "\n",
    "# filtro2=fioce_pessoal.STATUS=='ATIVO'\n",
    "# fioce_pessoal = fioce_pessoal[filtro2]\n",
    "\n",
    "# filtro3=fioce_pessoal.NÍVEL=='MESTRADO'\n",
    "# fioce_pessoal = fioce_pessoal[filtro3]\n",
    "\n",
    "# lista_servidores_mestres = fioce_pessoal['NOME']\n",
    "# lista_servidores_mestres.sort_values()\n",
    "\n",
    "# print(f'{len(lista_servidores_mestres)} servidores com, no máximo, mestrado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Retirar os currículos que não tem artigos publicados\n",
    "# retirar_mestres = ['Bruno Bezerra Carvalho',\n",
    "#                    'Luis Fernando Pessoa De Andrade']\n",
    "\n",
    "# lista_servidores_mestres = [item for item in lista_servidores_mestres if item not in retirar_mestres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'{len(lista_servidores_mestres)} currículos de mestres associados à Fiocruz Ceará em 2023')\n",
    "# for n,i in enumerate(lista_servidores_mestres):\n",
    "#     print(f'{n:2}: {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# t_ini=time.time()\n",
    "# df_secoes_mestres, sucesso_mestres, json_data = extrair_dados(lista_servidores_mestres, mestres=True, assunto=False)\n",
    "\n",
    "# print('-'*50)\n",
    "# print(tempo(t_ini,time.time()), 'Tempo extração dados do currículo')\n",
    "# print(f'{len(sucesso_mestres)/len(lista_servidores_mestres)*100:.2f}% de sucesso na extração dos servidores com mestrado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_mestres = montar_publicacoes(df_secoes_mestres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_mestres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_artigos_mestres.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_mestres['CURRICULO'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unir os dois grupos de formação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'{len(df_secoes_mestres.index):4} linhas de dados de {len(lista_servidores_mestres)} mestres')\n",
    "# print(f'{len(df_secoes_doutores.index)} linhas de dados de {len(lista_servidores_doutores)} doutores')\n",
    "# df_secoes = pd.concat([df_secoes_mestres, df_secoes_doutores], ignore_index=True)\n",
    "# print(f'{len(df_secoes.index)} linhas de dados de {len(lista_servidores_mestres)+len(lista_servidores_doutores)} servidores no total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_secoes[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_secoes.to_csv(pathout+'df_secoes_doutores_mestres.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_secoes[df_secoes['ROTULOS']=='Nome'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # pathout='C:/kgfioce/output/'\n",
    "# df_secoes_doutores_mestres = pd.read_csv(pathout+'df_secoes_doutores_mestres.csv', sep=\";\")\n",
    "# print(len(df_secoes_doutores_mestres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_servidores_doutorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_servidores_mestrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar lista de Publicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retirar os currículos que não tem artigos publicados, adicionar mestres com mais de 02 participações em publicações\n",
    "# lista_publicadores = []\n",
    "# retirar_doutores   = ['Dayane Alves Costa','Luciana Coelho Serafim']\n",
    "# adicionar_mestres  = ['Anna Carolina Machado Marinho', 'Claudia Stutz Zubieta','Luciana Silvério Alleluia Higino Da Silva','Marlos De Medeiros Chaves']\n",
    "\n",
    "# # Montar lista que será buscada no Lattes\n",
    "# for i in lista_servidores_doutorado:\n",
    "#     if i not in retirar_doutores:\n",
    "#         lista_publicadores.append(i)\n",
    "\n",
    "# for i in adicionar_mestres:\n",
    "#     lista_publicadores.append(i)\n",
    "\n",
    "# print(f'{len(lista_publicadores)} currículos de mestres/doutores associados à Fiocruz Ceará em 2023, com mais de 02 participações em artigos')\n",
    "# for n,i in enumerate(lista_publicadores):\n",
    "#     print(f'{n:2}: {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# t_ini=time.time()\n",
    "# df_secoes_publicadores, sucesso_publicadores, json_data = extrair_dados(lista_publicadores, mestres=True, assunto=False)\n",
    "\n",
    "# print('-'*50)\n",
    "# print(tempo(t_ini,time.time()), 'Tempo extração dados do currículo')\n",
    "# print(f'Extraídos dados de {len(sucesso_publicadores)}, {len(sucesso_publicadores)/len(lista_publicadores)*100:.2f}% de sucesso na extração dos servidores, com mestrado/doutorado e 02 ou mais participações em artigos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_secoes_publicadores.to_csv(pathout+'df_secoes_publicadores.csv', sep=\";\", index=False)\n",
    "# len(df_secoes_publicadores[df_secoes_publicadores['ROTULOS']=='Nome'].index)\n",
    "\n",
    "# for n,i in enumerate(df_secoes_publicadores['CURRICULO'].unique()):\n",
    "#     print(f'{n+1:2} {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testar extração e montagem da lista de artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teste = ['Fabio Miyajima']\n",
    "# teste = ['Jaime Ribeiro Filho']\n",
    "# teste = ['']\n",
    "# df_secoes_teste, sucesso_teste, json_data = extrair_dados(teste, mestres=True, assunto=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_teste.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_teste = montar_publicacoes(df_secoes_teste)\n",
    "# df_artigos_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair dados de Servidores pesquisadores, ou com artigos no Lattes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # pathout='C:/kgfioce/output/'\n",
    "# df_secoes_publicadores = pd.read_csv(pathout+'df_secoes_publicadores.csv', sep=\";\")\n",
    "# print(len(df_secoes_publicadores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_secoes_publicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos, curriculo_to_articles = montar_lista_pub(df_secoes_publicadores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar participações artigos publicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def montar_dfpub(df):\n",
    "    filtro  = 'Artigos completos publicados em periódicos'\n",
    "    artigos = df[(df.ROTULOS == filtro)]\n",
    "    print(f'Total de linhas de dados: {len(artigos)}')\n",
    "\n",
    "    nomes   = df[df.ROTULOS=='Nome'].values\n",
    "    print(f'Total de servidores: {len(nomes)}')  \n",
    "\n",
    "    # Initialize lists to populate DataFrame\n",
    "    curriculos = []\n",
    "    dados_publ = []\n",
    "    artigos_list = []\n",
    "    autores_list = []\n",
    "\n",
    "    remover = ['Ordenar por','Ordem Cronológica','Número de citações Web of science',\n",
    "               'Número de citações Scopus','Numero de citações Scielo','Primeiro autor',\n",
    "               'Impacto JCR','Ordem de Importância','Livros publicados/organizados ou edições']\n",
    "\n",
    "    for n, row in artigos.iterrows():\n",
    "        linha = row['CONTEUDOS']\n",
    "        curriculo = row['CURRICULO']\n",
    "\n",
    "        for i in eval(linha):\n",
    "            if i not in remover and len(i) > 15 and 'Citações:' not in i:\n",
    "                curriculos.append(curriculo)\n",
    "                dados_publ.append(i)\n",
    "                try:\n",
    "                    parts = i.split(' . ')\n",
    "                    if len(parts) == 2:\n",
    "                        autores_list = parts[0]\n",
    "                        autores_list = parts[1]\n",
    "                    else:\n",
    "                        autores_list = ''\n",
    "                        autores_list = ''\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    # Create DataFrame with 'CURRICULO' as the index\n",
    "    df_publicacoes = pd.DataFrame({\n",
    "        'CURRICULO': curriculos,\n",
    "        'ARTIGOS': dados_publ,\n",
    "        'AUTORES': autores_list,\n",
    "        'ARTIGO': artigos_list,\n",
    "    }).set_index('CURRICULO')\n",
    "\n",
    "    return df_publicacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def montar_lista_pub(df):\n",
    "    filtro = 'Artigos completos publicados em periódicos'\n",
    "    artigos = df[(df.ROTULOS == filtro)]\n",
    "\n",
    "    # Initialize lists to hold DataFrame data\n",
    "    curriculos = []\n",
    "    artigos_list = []\n",
    "    anos_pub = []\n",
    "\n",
    "    # Initialize dictionary to hold curriculo to row mapping\n",
    "    curriculo_to_articles = {}\n",
    "\n",
    "    remover = ['Ordenar por', 'Ordem Cronológica', 'Número de citações Web of science',\n",
    "               'Número de citações Scopus', 'Numero de citações Scielo', 'Primeiro autor',\n",
    "               'Impacto JCR', 'Ordem de Importância', 'Livros publicados/organizados ou edições']\n",
    "\n",
    "    for n, row in artigos.iterrows():\n",
    "        linha = row['CONTEUDOS']\n",
    "        curriculo = row['CURRICULO']\n",
    "        artigos_temp = []  # Temporary list to hold articles for the current curriculum\n",
    "\n",
    "        for i in eval(linha):\n",
    "            if i not in remover and len(i) > 15 and 'Citações:' not in i:\n",
    "                # Extract year of publication using regex\n",
    "                match = re.search(r'\\d{4}\\.$', i)\n",
    "                if match:\n",
    "                    ano_pub = match.group().strip('.')\n",
    "                    anos_pub.append(int(ano_pub))\n",
    "                else:\n",
    "                    anos_pub.append(None)\n",
    "\n",
    "                curriculos.append(curriculo)\n",
    "                artigos_list.append(i)\n",
    "                artigos_temp.append(i)\n",
    "        \n",
    "        # Update the dictionary with the current row of articles\n",
    "        curriculo_to_articles[curriculo] = artigos_temp\n",
    "\n",
    "    # Create DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        'CURRICULO': curriculos,\n",
    "        'ARTIGO': artigos_list,\n",
    "        'ANO_PUB': anos_pub\n",
    "    })\n",
    "    \n",
    "    # Set CURRICULO as the index\n",
    "    output_df.set_index('CURRICULO', inplace=True)\n",
    "\n",
    "    return output_df, curriculo_to_articles\n",
    "\n",
    "# Example usage\n",
    "# df = pd.read_csv(\"your_dataframe.csv\")\n",
    "# output_df, curriculo_to_articles = montar_lista_pub(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_publicacoes = montar_dfpub(df_secoes_publicadores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos, curriculo_to_articles = montar_lista_pub(df_secoes_publicadores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_doutores_mestres, curriculo_to_articles_doutores_mestres = montar_lista_pub(df_secoes_doutores_mestres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curriculo_to_articles_doutores_mestres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos_doutores_mestres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pathout=\"C:/kgfioce/output/\"\n",
    "df_secoes_servidores = pd.read_csv(pathout+'df_secoes_servidores.csv', sep=\";\")\n",
    "df_artigos_servidores, curriculo_to_articles_servidores = montar_lista_pub(df_secoes_servidores)\n",
    "print(len(df_artigos_servidores.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotbar_tudo(df_artigos):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Contagem do número de artigos para cada 'CURRICULO'\n",
    "    article_counts = df_artigos.index.value_counts()\n",
    "    \n",
    "    # Cálculo da soma total de artigos\n",
    "    total_articles = article_counts.sum()\n",
    "\n",
    "    # Criação do gráfico de barras\n",
    "    plt.figure(figsize=(19, 12))\n",
    "    ax = article_counts.plot(kind='bar', color='skyblue')\n",
    "\n",
    "    # Adição dos rótulos de colunas\n",
    "    for i, value in enumerate(article_counts):\n",
    "        plt.text(i, value + 0.5, str(value), ha='center', va='bottom')\n",
    "\n",
    "    # Adição do rótulo centralizado com a soma total de artigos\n",
    "    plt.annotate(f'Total de Artigos: {total_articles}', xy=(0.5, 0.95), xycoords='axes fraction',\n",
    "                 fontsize=24, ha='center', va='center')\n",
    "    \n",
    "    # Configurações adicionais\n",
    "    plt.xlabel('CURRICULO')\n",
    "    plt.ylabel('Quantidade de Artigos')\n",
    "    plt.title('Quantidade de participações em artigos por currículo de servidores, em qualquer tempo', fontsize=24)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotbar_tudo(df_artigos_servidores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrar por datas de ingresso na Fiocruz Ceará"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fioce_pessoal['INGRESSO_FIOCE'] = pd.to_datetime(fioce_pessoal['INGRESSO_FIOCE'])\n",
    "\n",
    "# Certificando-se de que 'INGRESSO_FIOCE' é do tipo int (se for data no formato yyyy, por exemplo)\n",
    "fioce_pessoal['ANO_INGRESSO_FIOCE'] = fioce_pessoal['INGRESSO_FIOCE'].dt.year\n",
    "\n",
    "# Merge entre df_artigos e fioce_pessoal usando 'AUTORES' e 'NOME' como chaves\n",
    "merged_df = df_artigos_servidores.merge(fioce_pessoal, left_on='CURRICULO', right_on='NOME', how='inner')\n",
    "\n",
    "# Filtrar as linhas de acordo com a condição do ano de publicação e da data de ingresso\n",
    "df_artigos_servidores_ingresso_fioce = merged_df[merged_df['ANO_PUB'] >= merged_df['ANO_INGRESSO_FIOCE']]\n",
    "\n",
    "# Opcional: Dropar colunas redundantes ou não necessárias, por exemplo, 'NOME' que é igual a 'AUTORES'\n",
    "# result_df = result_df.drop(columns=['NOME'])\n",
    "\n",
    "# Agora, result_df é o dataframe final desejado\n",
    "df_artigos_servidores_ingresso_fioce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos_servidores_ingresso_fioce.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_df = parse_dataframe(df_artigos_servidores_ingresso_fioce)\n",
    "teste_df.iloc[:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of columns you are interested in\n",
    "selected_columns = ['NOME','MATRÍCULA','ARTIGO', 'ÁREA', 'ANO_PUB', 'ANO_INGRESSO_FIOCE']\n",
    "\n",
    "# Create a new DataFrame containing only the selected columns\n",
    "df_servidores_ingresso_fioce = df_artigos_servidores_ingresso_fioce[selected_columns]\n",
    "df_servidores_ingresso_fioce.rename(columns={'ÁREA': 'SETOR_FIOCE'}, inplace=True)\n",
    "df_servidores_ingresso_fioce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the DataFrame to Excel\n",
    "df_artigos_servidores_ingresso_fioce.to_excel(pathout+\"df_artigos_servidores_ingresso_fioce.xlsx\", sheet_name='FiocruzCeara', index=False)\n",
    "df_artigos_servidores_ingresso_fioce.to_csv(pathout+\"df_artigos_servidores_ingresso_fioce.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos = montar_publicacoes(df_secoes_servidores)\n",
    "print(len(df_artigos.index))\n",
    "df_artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos = montar_publicacoes(df_secoes_servidores)\n",
    "\n",
    "# Converta strings vazias para NaN\n",
    "df_artigos['ANO_PUB'].replace('', pd.NA, inplace=True)\n",
    "\n",
    "# Preencha NaN com 0\n",
    "df_artigos['ANO_PUB'].fillna(0, inplace=True)\n",
    "\n",
    "# Agora converta a coluna para int\n",
    "df_artigos['ANO_PUB'] = df_artigos['ANO_PUB'].astype(int)\n",
    "\n",
    "# Salve o DataFrame\n",
    "df_artigos.to_csv(pathout+'df_artigos.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes das funções de separação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dicts(teste_dict, verbose=False):\n",
    "    \n",
    "    try:\n",
    "        dict2 = parse_string(teste_dict[\"input\"], verbose)\n",
    "    except Exception as e:\n",
    "        print('Nenhum separador funcionou bem para o caso:')\n",
    "        print(e)\n",
    "        print(teste_dict)\n",
    "        return -1.0  # Retorna -1.0 em caso de erro\n",
    "\n",
    "    dict1 = teste_dict[\"expected_output\"]\n",
    "    indices = len(dict1.keys())\n",
    "    sucesso = 0\n",
    "    desvios = 0\n",
    "    lenght = 75\n",
    "    for key in dict1.keys() & dict2.keys():  # Intersecção de chaves dos dois dicionários\n",
    "        if dict1[key] != dict2[key]:\n",
    "            desvios += 1\n",
    "            if verbose:\n",
    "                print('-' * lenght)\n",
    "                print('FALHOU na divisão de:')\n",
    "                print(f'Campo \"{key}\":')\n",
    "                print(f'   Esperado: {dict1[key]}')\n",
    "                print(f'     Obtido: {dict2[key]}')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('-' * lenght)\n",
    "                print('SUCESSO na divisão de:')\n",
    "                print(f'Campo \"{key}\":')\n",
    "                print(f'   Esperado: {dict1[key]}')\n",
    "                print(f'     Obtido: {dict2[key]}')\n",
    "            sucesso += 1\n",
    "    \n",
    "    percentual_sucesso = f'{sucesso / indices * 100:.2f}'\n",
    "    \n",
    "    return percentual_sucesso\n",
    "\n",
    "\n",
    "def run_testes(test_dict, verbose=False):\n",
    "    resultados = {}\n",
    "    if not isinstance(test_dict, dict):\n",
    "        print(\"O parâmetro deve ser um dicionário\")\n",
    "        return resultados\n",
    "    \n",
    "    for name, value in test_dict.items():\n",
    "        result = compare_dicts(value, verbose)\n",
    "        if result is not None:  # Verifica se o resultado é None\n",
    "            print(f'Resultado do teste {name:3}: {result}% de conformidade entre obtido e esperado')\n",
    "            resultados[name] = float(result)\n",
    "        else:\n",
    "            print(f'Falha ao executar o teste {name}')\n",
    "            \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dictionary = {\n",
    "    \"t1\": {\n",
    "        'input': \"PEREIRA, F. O. ; ARRUA, J. M. M. ; RIBEIRO-FILHO, J. . In vitro and ex vivo antibiofilm activity of riparin 1, and its nor and dinor homologs, against dermatophytes. MYCOLOGIA, p. 1-10, 2023.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['PEREIRA, F. O.', 'ARRUA, J. M. M.', 'RIBEIRO-FILHO, J.'],\n",
    "            'title': 'In vitro and ex vivo antibiofilm activity of riparin 1, and its nor and dinor homologs, against dermatophytes',\n",
    "            'journal': 'MYCOLOGIA',\n",
    "            'local': '',\n",
    "            'volume': '',\n",
    "            'page': '1-10',\n",
    "            'year': '2023'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t2\": {\n",
    "        'input':\"VIEIRA-MEYER, Anya Pimentel Gomes Fernandes. RAIZES E PONTES NO FORTALECIMENTO DO SUS. Revista da ESP, v. 17, p. e1712, 2023.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['VIEIRA-MEYER, Anya Pimentel Gomes Fernandes'],\n",
    "            'title': 'RAIZES E PONTES NO FORTALECIMENTO DO SUS',\n",
    "            'journal': 'Revista da ESP',\n",
    "            'local': '',\n",
    "            'volume': '17',\n",
    "            'page': 'e1712',\n",
    "            'year': '2023'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t3\": {\n",
    "        'input':\"Nicolete, Roberto; Rius, Cristina ; Piqueras, Laura ; Jose, Peter J ; Sorgi, Carlos A ; Soares, Edson G ; Sanz, Maria J ; Faccioli, Lúcia H . Leukotriene B4-loaded microspheres: a new therapeutic strategy to modulate cell activation, v. 9, p. 36, 2008.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['Nicolete, Roberto', 'Rius, Cristina', 'Piqueras, Laura','Jose, Peter J', 'Sorgi, Carlos A', 'Soares, Edson G', 'Sanz, Maria J', 'Faccioli, Lúcia H'],\n",
    "            'title': 'Leukotriene B4-loaded microspheres: a new therapeutic strategy to modulate cell activation',\n",
    "            'journal': '',\n",
    "            'local': '',\n",
    "            'volume': '9',\n",
    "            'page': '36',\n",
    "            'year': '2008'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t4\": {\n",
    "        'input':\"TELES, Y. C. F. ; RIBEIRO-FILHO, J. ; BOZZA, Patrícia T. ; AGRA, M. F. ; SIHERI, W. ; IGOLI, J. O. ; GRAY, A. I. ; SOUZA, M. F. V. . Phenolic constituents from (L.) C. Presl. and anti-inflammatory activity of 7,4--di- -methylisoscutellarein. Natural Product Research (Print), p. 1-5, 2015.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['TELES, Y. C. F.', 'RIBEIRO-FILHO, J.', 'BOZZA, Patrícia T.', 'AGRA, M. F.', 'SIHERI, W.', 'IGOLI, J. O.', 'GRAY, A. I.', 'SOUZA, M. F. V.'],\n",
    "            'title': 'Phenolic constituents from (L.) C Presl and anti-inflammatory activity of 7,4--di- -methylisoscutellarein',\n",
    "            'journal': 'Natural Product Research (Print)',\n",
    "            'local': '',\n",
    "            'volume': '',\n",
    "            'page': '1-5',\n",
    "            'year': '2015'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t5\": {\n",
    "        'input':\"BILD, N. ; CHAPEAUROUGE, D. A. ; GFELLER, S. ; BIENZ, S. . The [M-1]+ quasi-molecular Ion in Chemical Ionization Mass Spectrometry, Fragmentation of Bis (benzyloxy) silanes by Intramolecular Reactions. Org. Mass Spectrom., v. 27, p. 896-900, 1992.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['BILD, N.', 'CHAPEAUROUGE, D. A.', 'GFELLER, S.','BIENZ, S.'],\n",
    "            'title': 'The [M-1]+ quasi-molecular Ion in Chemical Ionization Mass Spectrometry, Fragmentation of Bis (benzyloxy) silanes by Intramolecular Reactions',\n",
    "            'journal': 'Mass Spectrom',\n",
    "            'local': '',\n",
    "            'volume': '27',\n",
    "            'page': '896-900',\n",
    "            'year': '1992'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t6\": {\n",
    "        'input':\"DARUGE, E. ; MIYAJIMA, F. ; PARANHOS, L. R. ; DUZ, S. . Identificação Humana por meio de Superposição de Imagens: Caso Clínico. JBC. Jornal Brasileiro de Clínica & Estética em Odontologia, Curitiba - PR, v. 3, n.Mar/Abr, p. 90-96, 1999.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['DARUGE, E.', 'MIYAJIMA, F.', 'PARANHOS, L. R.','DUZ, S.'],\n",
    "            'title': 'Identificação Humana por meio de Superposição de Imagens: Caso Clínico',\n",
    "            'journal': 'Jornal Brasileiro de Clínica & Estética em Odontologia',\n",
    "            'local': 'Curitiba - PR',\n",
    "            'volume': '3',\n",
    "            'page': '90-96',\n",
    "            'year': '1999'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t7\": {\n",
    "        'input':\"SALOMON, TASSILA BUSS, LEWIS F WHITTAKER, CHARLES PRETE, CARLOS A OIKAWA, MARCIO K PEREIRA, RAFAEL HM MOURA, ISABEL CG DELERINO, LUCAS BARRAL-NETTO, MANOEL TAVARES, NATALIA M FRANCA, RAFAEL FO BOAVENTURA, VIVIANE S MIYAJIMA, FABIO MENDRONE-JUNIOR, ALFREDO DE ALMEIDA-NETO, CESAR SALLES, NANCI A FERREIRA, SUZETE C FLADZINSKI, KARINE A DE SOUZA, LUANA M SCHIER, LUCIANE K INOUE, PATRICIA M XABREGAS, LILYANE A CRISPIM, MYUKI AE FRAIJI, NELSON ARAUJO, FERNANDO LV , et al. ; SARS-CoV-2 antibody dynamics in blood donors and COVID-19 epidemiology in eight Brazilian state capitals: A serial cross-sectional study. eLife, v. 11, p. e78233, 2022.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['SALOMON, TASSILA BUSS, LEWIS F WHITTAKER, CHARLES PRETE, CARLOS A OIKAWA, MARCIO K PEREIRA, RAFAEL HM MOURA, ISABEL CG DELERINO, LUCAS BARRAL-NETTO, MANOEL TAVARES, NATALIA M FRANCA, RAFAEL FO BOAVENTURA, VIVIANE S MIYAJIMA, FABIO MENDRONE-JUNIOR, ALFREDO DE ALMEIDA-NETO, CESAR SALLES, NANCI A FERREIRA, SUZETE C FLADZINSKI, KARINE A DE SOUZA, LUANA M SCHIER, LUCIANE K INOUE, PATRICIA M XABREGAS, LILYANE A CRISPIM, MYUKI AE FRAIJI, NELSON ARAUJO, FERNANDO LV , et al'],\n",
    "            'title': 'SARS-CoV-2 antibody dynamics in blood donors and COVID-19 epidemiology in eight Brazilian state capitals: A serial cross-sectional study',\n",
    "            'journal': 'eLife',\n",
    "            'local': '',\n",
    "            'volume': '11',\n",
    "            'page': 'e78233',\n",
    "            'year': '2022'\n",
    "        }\n",
    "    }, \n",
    "\n",
    "    \"t8\": {\n",
    "        'input':\"PEREIRA, F. O. ; ARRUA, J. M. M. ; RIBEIRO-FILHO, J. . In vitro and ex vivo antibiofilm activity of riparin 1, and its nor and dinor homologs, against dermatophytes. MYCOLOGIA, p. 1-10, 2023.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['PEREIRA, F. O.', 'ARRUA, J. M. M.', 'RIBEIRO-FILHO, J.'],\n",
    "            'title': 'In vitro and ex vivo antibiofilm activity of riparin 1, and its nor and dinor homologs, against dermatophytes',\n",
    "            'journal': 'MYCOLOGIA',\n",
    "            'local': '',\n",
    "            'volume': '',\n",
    "            'page': '1-10',\n",
    "            'year': '2023'\n",
    "        }\n",
    "    },       \n",
    "\n",
    "    \"t9\": {\n",
    "        'input':\"MARTIN, A. L. A. R. ; PEREIRA, R. L. S. ; RIBEIRO-FILHO, J. ; MENEZES, I. R. A. ; COUTINHO, H. D. M. ; FONTELES, M. M. F. . In vitro and in silico evidences about the inhibition of MepA efflux pump by coumarin derivatives. MICROBIAL PATHOGENESIS, p. 106246, 2023.\",\n",
    "        'expected_output': {\n",
    "            'authors': ['MARTIN, A. L. A. R.', 'PEREIRA, R. L. S.', 'RIBEIRO-FILHO, J.', 'MENEZES, I. R. A.', 'COUTINHO, H. D. M.', 'FONTELES, M. M. F.'],\n",
    "            'title': 'In vitro and in silico evidences about the inhibition of MepA efflux pump by coumarin derivatives',\n",
    "            'journal': 'MICROBIAL PATHOGENESIS',\n",
    "            'local': '',\n",
    "            'volume': '',\n",
    "            'page': '106246',\n",
    "            'year': '2023'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t10\": {\n",
    "        'input':\"TALKOWSKI, MICHAEL E. MCCANN, KATHLEEN L. CHEN, MICHAEL MCCLAIN, LORA BAMNE, MIKHIL WOOD, JOEL CHOWDARI, KODAVALI V. WATSON, ANNIE PRASAD, KONASALE M. KIROV, GEORGE GEORGIEVA, LYUDMILLA TONCHEVA, DRAGA MANSOUR, HADER LEWIS, DAVID A. OWEN, MICHAEL O'DONOVAN, MICHAEL PAPASAIKAS, PANAGIOTIS SULLIVAN, PATRICK RUDERFER, DOUGLAS YAO, JEFFREY K LEONARD, SHERRY THOMAS, PRAMOD MIYAJIMA, FABIO QUINN, JOHN LOPEZ, A. JAVIER , et al. ; Fine-mapping reveals novel alternative splicing of the dopamine transporter. American Journal of Medical Genetics. Part B, Neuropsychiatric Genetics, v. 153B, p. 1434-1447, 2010.\",\n",
    "        'expected_output': {\n",
    "            'authors': [\"TALKOWSKI, MICHAEL E. MCCANN, KATHLEEN L. CHEN, MICHAEL MCCLAIN, LORA BAMNE, MIKHIL WOOD, JOEL CHOWDARI, KODAVALI V. WATSON, ANNIE PRASAD, KONASALE M. KIROV, GEORGE GEORGIEVA, LYUDMILLA TONCHEVA, DRAGA MANSOUR, HADER LEWIS, DAVID A. OWEN, MICHAEL O'DONOVAN, MICHAEL PAPASAIKAS, PANAGIOTIS SULLIVAN, PATRICK RUDERFER, DOUGLAS YAO, JEFFREY K LEONARD, SHERRY THOMAS, PRAMOD MIYAJIMA, FABIO QUINN, JOHN LOPEZ, A. JAVIER , et al\"],\n",
    "            'title': 'Fine-mapping reveals novel alternative splicing of the dopamine transporter',\n",
    "            'journal': 'American Journal of Medical Genetics. Part B, Neuropsychiatric Genetics',\n",
    "            'local': '',\n",
    "            'volume': '153B',\n",
    "            'page': '1434-1447',\n",
    "            'year': '2010'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t11\": {\n",
    "        'input':\"DA SILVA, LUCAS YURE SANTOS ; PAULO, CICERA LAURA ROQUE ; MOURA, TALYSSON FELISMINO ; ALVES, DANIEL SAMPAIO ; PESSOA, RENATA TORRES ; ARAÚJO, ISAAC MOURA ; DE MORAIS OLIVEIRA-TINTINO, CÍCERA DATIANE ; TINTINO, SAULO RELISON ; NONATO, CARLA DE FATIMA ALVES ; DA COSTA, JOSÉ GALBERTO MARTINS ; RIBEIRO-FILHO, JAIME ; COUTINHO, HENRIQUE DOUGLAS MELO ; KOWALSKA, GRA'YNA ; MITURA, PRZEMYS'AW ; BAR, MAREK ; KOWALSKI, RADOS'AW ; MENEZES, IRWIN ROSE ALENCAR DE . Antibacterial Activity of the Essential Oil of Piper tuberculatum Jacq. Fruits against Multidrug-Resistant Strains: Inhibition of Efflux Pumps and β-Lactamase. PLANTS, v. 12, p. 2377, 2023.\",\n",
    "        'expected_output': {\n",
    "            'authors': [\"DA SILVA, LUCAS YURE SANTOS ; PAULO, CICERA LAURA ROQUE ; MOURA, TALYSSON FELISMINO ; ALVES, DANIEL SAMPAIO ; PESSOA, RENATA TORRES ; ARAÚJO, ISAAC MOURA ; DE MORAIS OLIVEIRA-TINTINO, CÍCERA DATIANE ; TINTINO, SAULO RELISON ; NONATO, CARLA DE FATIMA ALVES ; DA COSTA, JOSÉ GALBERTO MARTINS ; RIBEIRO-FILHO, JAIME ; COUTINHO, HENRIQUE DOUGLAS MELO ; KOWALSKA, GRA'YNA ; MITURA, PRZEMYS'AW ; BAR, MAREK ; KOWALSKI, RADOS'AW ; MENEZES, IRWIN ROSE ALENCAR DE\"],\n",
    "            'title': 'Antibacterial Activity of the Essential Oil of Piper tuberculatum Jacq. Fruits against Multidrug-Resistant Strains: Inhibition of Efflux Pumps and β-Lactamase',\n",
    "            'journal': 'PLANTS',\n",
    "            'local': '',\n",
    "            'volume': '12',\n",
    "            'page': '2377',\n",
    "            'year': '2023'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t12\": {\n",
    "        'input':\"MIYAJIMA, F.; OLLIER, W. ; MAYES, A. ; JACKSON, A. ; THACKER, N. ; RABBITT, P. ; Pendleton, N. ; HORAN, M. ; PAYTON, A. . Brain-derived neurotrophic factor polymorphism Val66Met influences cognitive abilities in the elderly. GENES, BRAIN AND BEHAVIOR (ONLINE), v. ON, p. 31/10/2007, 2007.\",\n",
    "        'expected_output': {\n",
    "            'authors': [\"MIYAJIMA, F.; OLLIER, W. ; MAYES, A. ; JACKSON, A. ; THACKER, N. ; RABBITT, P. ; Pendleton, N. ; HORAN, M. ; PAYTON, A\"],\n",
    "            'title': 'Brain-derived neurotrophic factor polymorphism Val66Met influences cognitive abilities in the elderly',\n",
    "            'journal': 'GENES, BRAIN AND BEHAVIOR (ONLINE)',\n",
    "            'local': '',\n",
    "            'volume': 'ON',\n",
    "            'page': '31/10/2007',\n",
    "            'year': '2007'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"t13\": {\n",
    "        'input':\"MIYAJIMA, F.; LIMA, V. P. . Exames em DNA: a superestimação da inovação. Revista de Direito (Itatiba), Leme - SP, v. 1, n.2002, p. 63-65, 2002.\",\n",
    "        'expected_output': {\n",
    "            'authors': [\"MIYAJIMA, F.; LIMA, V. P.\"],\n",
    "            'title': 'Exames em DNA: a superestimação da inovação',\n",
    "            'journal': 'Revista de Direito (Itatiba)',\n",
    "            'local': 'Leme - SP',\n",
    "            'volume': '1',\n",
    "            'page': '63-65',\n",
    "            'year': '2002'\n",
    "        }\n",
    "    },                  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_testes(test_dictionary, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa todos os testes e armazena os resultados em 'resultados_todos_testes'\n",
    "resultados_todos_testes = run_testes(test_dictionary)\n",
    "\n",
    "# Filtra os testes que não atingiram 100% de conformidade\n",
    "testes_filtrados = {k: v for k, v in resultados_todos_testes.items() if v < 100.0}\n",
    "\n",
    "# Exibe os testes filtrados\n",
    "print(\"Testes com menos de 100% de conformidade:\", testes_filtrados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_testes(testes_filtrados, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_excecao = {\"t7\": \"SALOMON, TASSILA BUSS, LEWIS F WHITTAKER, CHARLES PRETE, CARLOS A OIKAWA, MARCIO K PEREIRA, RAFAEL HM MOURA, ISABEL CG DELERINO, LUCAS BARRAL-NETTO, MANOEL TAVARES, NATALIA M FRANCA, RAFAEL FO BOAVENTURA, VIVIANE S MIYAJIMA, FABIO MENDRONE-JUNIOR, ALFREDO DE ALMEIDA-NETO, CESAR SALLES, NANCI A FERREIRA, SUZETE C FLADZINSKI, KARINE A DE SOUZA, LUANA M SCHIER, LUCIANE K INOUE, PATRICIA M XABREGAS, LILYANE A CRISPIM, MYUKI AE FRAIJI, NELSON ARAUJO, FERNANDO LV , et al. ; SARS-CoV-2 antibody dynamics in blood donors and COVID-19 epidemiology in eight Brazilian state capitals: A serial cross-sectional study. eLife, v. 11, p. e78233, 2022.\"}\n",
    "\n",
    "run_testes(input_excecao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converta strings vazias para NaN\n",
    "df_artigos['ANO_PUB'].replace('', pd.NA, inplace=True)\n",
    "\n",
    "# Preencha NaN com 0\n",
    "df_artigos['ANO_PUB'].fillna(0, inplace=True)\n",
    "\n",
    "# Agora converta a coluna para int\n",
    "df_artigos['ANO_PUB'] = df_artigos['ANO_PUB'].astype(int)\n",
    "\n",
    "# Salve o DataFrame\n",
    "df_artigos.to_csv(pathout+'df_artigos.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_servidores_ingresso_fioce.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m,i in enumerate(df_secoes_publicadores['CURRICULO'].unique()):\n",
    "#     if i not in df_artigos['CURRICULO'].unique():\n",
    "#         print(f'Não encontrado nome de: \"{i}\" na lista e artigos montados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos[df_artigos['TITULO'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos[600:1200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrar por datas para recorte mestres/doutores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fioce_pessoal['INGRESSO_FIOCE'] = pd.to_datetime(fioce_pessoal['INGRESSO_FIOCE'])\n",
    "\n",
    "# # Certificando-se de que 'INGRESSO_FIOCE' é do tipo int (se for data no formato yyyy, por exemplo)\n",
    "# fioce_pessoal['ANO_INGRESSO_FIOCE'] = fioce_pessoal['INGRESSO_FIOCE'].dt.year\n",
    "\n",
    "# # Merge entre df_artigos e fioce_pessoal usando 'AUTORES' e 'NOME' como chaves\n",
    "# merged_df = df_artigos_doutores_mestres.merge(fioce_pessoal, left_on='CURRICULO', right_on='NOME', how='inner')\n",
    "\n",
    "# # Filtrar as linhas de acordo com a condição do ano de publicação e da data de ingresso\n",
    "# df_doutores_mestres_ingresso_fioce = merged_df[merged_df['ANO_PUB'] >= merged_df['ANO_INGRESSO_FIOCE']]\n",
    "\n",
    "# # Opcional: Dropar colunas redundantes ou não necessárias, por exemplo, 'NOME' que é igual a 'AUTORES'\n",
    "# # result_df = result_df.drop(columns=['NOME'])\n",
    "\n",
    "# # Agora, result_df é o dataframe final desejado\n",
    "# df_doutores_mestres_ingresso_fioce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_doutores_mestres_ingresso_fioce.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the list of columns you are interested in\n",
    "# selected_columns = ['NOME','MATRÍCULA','ARTIGO', 'ÁREA', 'ANO_PUB', 'ANO_INGRESSO_FIOCE']\n",
    "\n",
    "# # Create a new DataFrame containing only the selected columns\n",
    "# df_artigos_doutores_mestres = df_artigos_doutores_mestres[selected_columns]\n",
    "# df_artigos_doutores_mestres.rename(columns={'ÁREA': 'SETOR_FIOCE'}, inplace=True)\n",
    "# df_artigos_doutores_mestres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exporting the DataFrame to Excel\n",
    "# df_artigos_doutores_mestres.to_excel(pathout+\"df_artigos_doutores_mestres.xlsx\", sheet_name='FiocruzCeara', index=False)\n",
    "# df_artigos_doutores_mestres.to_csv(pathout+\"df_artigos_doutores_mestres.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar recorte de Profissionais Publicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the list of columns you are interested in\n",
    "# selected_columns = ['NOME','MATRÍCULA','ARTIGO', 'ÁREA', 'ANO_PUB', 'ANO_INGRESSO_FIOCE']\n",
    "\n",
    "# # Create a new DataFrame containing only the selected columns\n",
    "# filtered_df = result_df[selected_columns]\n",
    "# filtered_df.rename(columns={'ÁREA': 'SETOR_FIOCE'}, inplace=True)\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exporting the DataFrame to Excel\n",
    "# filtered_df.to_excel(pathout+\"artigos_desde_ano_ingresso_fiocruz_ceara.xlsx\", sheet_name='FiocruzCeara', index=False)\n",
    "# filtered_df.to_csv(pathout+\"artigos_desde_ano_ingresso_fiocruz_ceara.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(result_df['ARTIGO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publicacoes = result_df['ARTIGO'].to_list()\n",
    "# len(publicacoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar lista específica de nomes para extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_busca=[]\n",
    "# for i in lista_servidores_doutores:\n",
    "#     if i != 'Raphael Trevizani Roque De Oliveira':\n",
    "#         lista_busca.append(i)\n",
    "#     else:\n",
    "#         lista_busca.append('Raphael Trevizani')\n",
    "\n",
    "# lista_busca.sort()\n",
    "# # Salvar lista de servidores em arquivo CSV\n",
    "# with open(pathcsv+'lista_servidores-fioce.csv', 'w', newline='') as arquivo:\n",
    "#     escritor = csv.writer(arquivo)\n",
    "#     for item in lista_busca:\n",
    "#         escritor.writerow([item])\n",
    "\n",
    "# # Criar lista de busca com interesse de pesquisa\n",
    "# retirar = ['Carlos Jose Araujo Pinheiro', 'Charles Cerqueira De Abreu', 'Dayane Alves Costa',\n",
    "#            'Ezequiel Valentim De Melo','João Baptista Estabile Neto','Luciano Pinto Zorzanelli',\n",
    "#            'Luciana Coelho Serafim', 'Nilton Luiz Costa Machado','Renato Caldeira De Souza',\n",
    "#            'Sergio Dos Santos Reis']\n",
    "\n",
    "# lista_busca = [item for item in lista_busca if item not in retirar]\n",
    "# with open(pathcsv+'lista_lattes-fioce.csv', 'w', newline='') as arquivo:\n",
    "#     escritor = csv.writer(arquivo)\n",
    "#     for item in lista_busca:\n",
    "#         escritor.writerow([item])\n",
    "\n",
    "# # Ler do arquivo CSV salvo para dataframe\n",
    "# df_busca = pd.read_csv(pathcsv+'lista_lattes-fioce.csv', header=None)\n",
    "# df_busca.columns = ['SERVIDORES_FIOCE']\n",
    "# print(f'{len(df_busca.index)} currículos a extrair')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair dados visando avaliar edital FUNCAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# ## Arquivos fontes para orientadores FUNCAP\n",
    "# arquivo_qualis     = 'classificações_publicadas_todas_as_areas_avaliacao1672761192111.csv'\n",
    "# lista_orientadores = pd.read_csv(pathcsv+'lista_orientadores.csv')\n",
    "\n",
    "# lista_busca = lista_orientadores['ORIENTADOR'].unique()\n",
    "# lista_busca.sort()\n",
    "# print(f'Total de pesquisadores a extrair: {len(lista_orientadores[1:])}')\n",
    "# for i in lista_busca:\n",
    "#     print('    ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_artigos(lista_nomes, mestres=True, assunto=False):\n",
    "    '''Extrai todas as informações brutas de publicações (artigos e livros) de cada currículo da Plataforma Lattes do CNPQ\n",
    "     Recebe: Um nome a ser buscado na base do currículo Lattes\n",
    "    Utiliza: Funções: definir_filtros(), montar_dfcolab_linhas()\n",
    "    Retorna: Três dataframes: df_identificacao com dados da identificação; df_dados com dados de todas produções; e df_colabartigos com dados das colaborações em artigos\n",
    "    Autor: Marcos Aires (Jan 2022)\n",
    "    '''\n",
    "    import time\n",
    "    from datetime import date\n",
    "    \n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    t0=time.time()\n",
    "    \n",
    "    ## INÍCIO DO SCRIPT DE RASPAGEM DA PÁGINA HTML DO CURRÍCULO LATTES\n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    browser   = webdriver.Chrome(options=options)\n",
    "    url_buscaespecialista = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=false&textoBusca='\n",
    "    browser.get(url_buscaespecialista) # acessa a url de busca do CNPQ   \n",
    "    # browser.set_window_position(2100, 0)\n",
    "    # browser.set_window_size(1096, 1896)\n",
    "    # browser.maximize_window()\n",
    "    \n",
    "    browser.set_window_position(-20, -10)\n",
    "    size          = browser.get_window_size()\n",
    "    width1        = size.get(\"width\")\n",
    "    height1       = size.get(\"height\")\n",
    "    browser.set_window_size(170, 1896)\n",
    "    browser.mouse = webdriver.ActionChains(browser)\n",
    "    \n",
    "    delay   = 10  # seconds \n",
    "    buscas        = []\n",
    "    resultados    = []\n",
    "    \n",
    "    df_dados          = pd.DataFrame()   \n",
    "    rotulos           = []\n",
    "    conteudos         = []\n",
    "    parcial_rotulos   = []\n",
    "    parcial_conteudos = []\n",
    "    sucesso           = []\n",
    "    falhas            = []\n",
    "    impactos = []\n",
    "    linhas_dados = []\n",
    "    artigos = []\n",
    "\n",
    "    df_parcial = pd.DataFrame({     \n",
    "            'NOMES': pd.Series(sucesso),\n",
    "            'ROTULOS': pd.Series(rotulos),\n",
    "            'CONTEUDOS': pd.Series(conteudos),                    \n",
    "        })\n",
    "\n",
    "    t1=time.time()\n",
    "    print(tempo(t0,t1), 'Tempo de conexão ao servidor do CNPq')\n",
    "    time.sleep(0.00001)\n",
    "\n",
    "    count=0\n",
    "    for NOME in lista_nomes:\n",
    "        print('-'*100)\n",
    "        count+=1\n",
    "        t2       = time.time()\n",
    "        tdec     = np.round(t2-t0,2)\n",
    "        restante = len(lista_nomes)-count\n",
    "        print(f'Extraindo currículo {count}/{len(lista_nomes)}. Resta {restante}. Decorrido:{horas(tdec)}. Previsão de término em {horas(np.round(tdec/count,0)*(restante+1))}')\n",
    "        \n",
    "        # Definir filtros para busca de nomes\n",
    "        definir_filtros(browser, mestres, assunto)\n",
    "        preencher_busca(browser, delay, NOME)      \n",
    "        window_before  = browser.current_window_handle\n",
    "        limite=5\n",
    "        ## Clicar no botão abrir currículo e mudar de aba\n",
    "        try:\n",
    "            ## Aguarda, encontra, clica em buscar nome\n",
    "            link_nome    = achar_busca(browser, delay)\n",
    "            nome_buscado = []\n",
    "            nome_achado  = []\n",
    "            nome_buscado.append(NOME)\n",
    "            \n",
    "            if link_nome.text == None:\n",
    "                xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "                # 'Stale file handle'\n",
    "                print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "                retry(WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "                #    expected_ex_type=ZeroDivisionError, \n",
    "                   wait_ms=200,\n",
    "                   limit=limite, \n",
    "                #    logger=logger, \n",
    "                   on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "            try:\n",
    "                ActionChains(browser).click(link_nome).perform()\n",
    "                nome_achado.append(link_nome.text)\n",
    "            except:\n",
    "                print(f'Currículo não encontrado para: {NOME}.')\n",
    "                return\n",
    "            \n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "                #    expected_ex_type=ZeroDivisionError, \n",
    "                   wait_ms=200,\n",
    "                   limit=limite, \n",
    "                #    logger=logger, \n",
    "                   on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "            \n",
    "            btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "            time.sleep(0.2)\n",
    "            ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "            ## Gerenciamento das janelas abertas no browser\n",
    "            WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "            window_after = browser.window_handles\n",
    "            new_window   = [x for x in window_after if x != window_before][0]\n",
    "            browser.switch_to.window(new_window)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('Erro',e)\n",
    "            print('Tentando nova requisição ao servidor')\n",
    "            time.sleep(1)\n",
    "            btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "            ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "            WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "            ## Gerenciamento das janelas abertas no browser\n",
    "            window_after = browser.window_handles\n",
    "            new_window   = [x for x in window_after if x != window_before][0]\n",
    "            browser.switch_to.window(new_window)\n",
    "            time.sleep(1)\n",
    "\n",
    "        t3=time.time()\n",
    "\n",
    "        ## O objeto elementos_id abaixo é uma lista de elementos onde as informações de identificação estão contidas\n",
    "        # acessado através do marcador xpath='//div[@class=\"infpessoa\"]' no HTML para extrair de cada pesquisador\n",
    "        time.sleep(1)\n",
    "        xpath='//div[@class=\"infpessoa\"]'\n",
    "        WebDriverWait(browser, delay).until(\n",
    "                EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "        elementos_id = browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "        # Fazer com que a primeira informação para cada pesquisador seja o caminho para sua foto e dados de identificação\n",
    "        try:\n",
    "            css_selector='.foto'\n",
    "            link_foto=WebDriverWait(browser, delay).until(\n",
    "                EC.visibility_of_element_located((By.CSS_SELECTOR, \".foto\"))).get_attribute(\"src\")\n",
    "            rotulos.append('Link Foto:')\n",
    "            conteudos.append(link_foto)            \n",
    "\n",
    "        except Exception as e:\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print('  !!Erro ao extrair imagem do currículo:',e,'\\n', traceback_str)\n",
    "\n",
    "        for i in range(len(elementos_id)):\n",
    "            dados = elementos_id[i].text.split('\\n')\n",
    "            for i in range(len(dados)):\n",
    "                if i==0:\n",
    "                    rotulos.append('Nome completo:')\n",
    "                    conteudos.append(dados[i])\n",
    "                elif 'Bolsista' in dados[i]:\n",
    "                    rotulos.append('Bolsista CNPq:')\n",
    "                    conteudos.append(dados[i])\n",
    "                elif 'Endereço para acessar este CV: ' in dados[i]:\n",
    "                    rotulos.append('Link Currículo:')\n",
    "                    conteudos.append(dados[i].strip('Endereço para acessar este CV: '))\n",
    "                elif 'ID Lattes: ' in dados[i]:\n",
    "                    rotulos.append('ID Lattes:')\n",
    "                    conteudos.append(dados[i].strip('ID Lattes: '))\n",
    "                elif 'Última atualização do currículo em ' in dados[i]:\n",
    "                    rotulos.append('Data atualização:')\n",
    "                    conteudos.append(dados[i].strip('Última atualização do currículo em '))\n",
    "                    dt_atualizacao = dados[i].strip('Última atualização do currículo em ')\n",
    "                    dtt = datetime.strptime(dt_atualizacao, '%d/%m/%Y').date()\n",
    "                    defasagem = (date.today()-dtt).days        \n",
    "\n",
    "        try: \n",
    "            df_temp =pd.DataFrame({\n",
    "                'ROTULOS': pd.Series(rotulos),\n",
    "                'CONTEUDOS': pd.Series(conteudos),\n",
    "                    })\n",
    "            filtro    = 'Link Foto:'\n",
    "            fotos     = df_temp[(df_temp.ROTULOS == filtro)]['CONTEUDOS']\n",
    "            x         = fotos[-1:].index[0]\n",
    "            df_temp.drop(columns=['ROTULOS'], inplace=True)\n",
    "\n",
    "            try:\n",
    "                foto = HTML(df_temp[x:x+1].to_html(escape=False, formatters=dict(CONTEUDOS=path_to_image_html)))\n",
    "                display(foto)\n",
    "                print(f'Atualizado em {dt_atualizacao} há {defasagem:>2} dias | {NOME}')    \n",
    "                \n",
    "\n",
    "            except TimeoutException as t:\n",
    "                print('Demora na conexão com servidor, carregamento da foto cancelado')\n",
    "                traceback_str = ''.join(traceback.format_tb(t.__traceback__))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('Erro ao extrair a foto do pesquisador')\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(e,traceback_str)\n",
    "        \n",
    "        t4=time.time() \n",
    "\n",
    "        ## TRECHO PARA EXTRAIR DADOS DOS ARTIGOS\n",
    "        try:    \n",
    "            page_source = browser.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            print(len(soup.text))\n",
    "            info_spans = soup.find_all('span', class_='informacao-artigo')\n",
    "            for span in info_spans:\n",
    "                linhas_dados.append(span.text)\n",
    "            artigos.append(linhas_dados)\n",
    "\n",
    "            ## Fechar janela do currículo\n",
    "            browser.close()            \n",
    "            \n",
    "            ## Gerenciamento das janelas abertas no browser\n",
    "            todas_janelas = browser.window_handles\n",
    "            browser.switch_to.window(todas_janelas[0])\n",
    "\n",
    "            ## Fechar a janela pop-up\n",
    "            close_popup = WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//*[@id='idbtnfechar']\")))\n",
    "            close_popup.click()\n",
    "            \n",
    "            # ## Nova Consulta\n",
    "            # try:\n",
    "            #     nova_consulta = WebDriverWait(browser, delay).until(\n",
    "            #         EC.element_to_be_clickable((By.XPATH, \"//*[@id='botaoBuscaFiltros']\")))\n",
    "            #     nova_consulta.click()\n",
    "            #     time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Erro ao reiniciar consulta')\n",
    "                traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "                print(e,traceback_str) \n",
    "            \n",
    "            t5=time.time()                       \n",
    "\n",
    "            # print(f' {tempo(t0,t5)} | Tempo de Acesso |  Identificação |   Dados Brutos | Subtotal Tempo | Acumulado')\n",
    "            # print(f'  Decorrido  |   {tempo(t2,t3)}   |  {tempo(t3,t4)}   |  {tempo(t4,t5)}   |  {tempo(t2,t5)}   | {len(conteudos)} seções')\n",
    "            \n",
    "            sucesso.append(NOME)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('Erro ao montar dataframe dados de artigos')\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(e,traceback_str)    \n",
    "            browser.quit()\n",
    "            \n",
    "            return df_dados\n",
    "    \n",
    "    # df_dados =pd.DataFrame({\n",
    "    #     'ROTULOS': pd.Series(rotulos),\n",
    "    #     'CONTEUDOS': pd.Series(conteudos),\n",
    "    #         })\n",
    "    \n",
    "    t6=time.time()\n",
    "    print('='*95)\n",
    "    # print(f' {len(sucesso)} currículos extraídos com sucesso')\n",
    "    print(f' Tempo total para extrair {len(artigos)} artigos dos currículos: {tempo(t0,t6)}')\n",
    "    # print('='*95)\n",
    "    browser.quit()\n",
    "    \n",
    "    # return df_dados, sucesso, parcial_rotulos, parcial_conteudos\n",
    "    return pd.DataFrame(artigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arquivos fontes para orientadores FUNCAP:\n",
    "# arquivo_qualis     = 'classificações_publicadas_todas_as_areas_avaliacao1672761192111.csv'\n",
    "# lista_orientadores = pd.read_csv(pathcsv+'lista_orientadores.csv')\n",
    "\n",
    "# lista_busca = lista_orientadores['ORIENTADOR'].unique()\n",
    "# lista_busca.sort()\n",
    "# print(f'Total de pesquisadores a extrair: {len(lista_orientadores[1:])}')\n",
    "# for i in lista_busca:\n",
    "#     print('    ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dados_artigos = extrair_artigos(lista_busca[0:1])\n",
    "# for i in dados_artigos.values:\n",
    "#     print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in dados_artigos.values:\n",
    "#     c=0\n",
    "#     for n,j in enumerate(i):\n",
    "#         c+=1\n",
    "#         if c==1:\n",
    "#             print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dados_artigos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Funções padronização de strings e remoção de variantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PADRONIZAÇÃO DE NOMES DE AUTOR E ANÁLISE DE SIMILARIDADES\n",
    "def padronizar_nome(linha_texto):\n",
    "    '''Procura sobrenomes e abreviaturas e monta nome completo\n",
    "     Recebe: String com todos os sobrenomes e nomes, abreviados ou não\n",
    "    Retorna: Nome completo no formato padronizado em SOBRENOME AGNOME, Prenomes\n",
    "      Autor: Marcos Aires (Mar.2022)\n",
    "    '''\n",
    "    import unicodedata\n",
    "    import re\n",
    "    # print('               Analisando:',linha_texto)\n",
    "    string = ''.join(ch for ch in unicodedata.normalize('NFKD', linha_texto) if not unicodedata.combining(ch))\n",
    "    string = string.replace('(Org)','').replace('(Org.)','').replace('(Org).','').replace('.','').replace('\\'','')\n",
    "    string = string.replace(',,,',',').replace(',,',',')\n",
    "    string = re.sub(r'[0-9]+', '', string)\n",
    "        \n",
    "    # Expressões regulares para encontrar padrões de divisão de nomes de autores\n",
    "    sobrenome_inicio   = re.compile(r'^[A-ZÀ-ú-a-z]+,')                  # Sequência de letras maiúsculas no início da string\n",
    "    sobrenome_composto = re.compile(r'^[A-ZÀ-ú-a-z]+[ ][A-ZÀ-ú-a-z]+,')  # Duas sequências de letras no início da string, separadas por espaço, seguidas por vírgula\n",
    "    letra_abrevponto   = re.compile(r'^[A-Z][.]')                        # Uma letra maiúscula no início da string, seguida por ponto\n",
    "    letra_abrevespaco  = re.compile(r'^[A-Z][ ]')                        # Uma letra maiúscula no início da string, seguida por espaço\n",
    "    letras_dobradas    = re.compile(r'[A-Z]{2}')                         # Duas letras maiúsculas juntas no início da string, seguida por espaço\n",
    "    letras_dobradasini = re.compile(r'[A-Z]{2}[ ]')                      # Duas letras maiúsculas juntas no início da string, seguida por espaço\n",
    "    letras_dobradasfim = re.compile(r'[ ][A-Z]{2}')                      # Duas letras maiúsculas juntas no final da string, precedida por espaço\n",
    "    letras_duasconsnts = re.compile(r'[B-DF-HJ-NP-TV-XZ]{2}')            # Duas Letras maiúsculas e consoantes juntas\n",
    "    letras_tresconsnts = re.compile(r'[B-DF-HJ-NP-TV-XZ]{3}')            # Três Letras maiúsculas e consoantes juntas\n",
    "    \n",
    "    # Agnomes e preprosições a tratar, agnomes vão maiúsculas para sobrenome e preposições vão para minúsculas nos nomes\n",
    "    nomes=[]\n",
    "    agnomes       = ['NETO','JUNIOR','FILHO','SEGUNDO','TERCEIRO']\n",
    "    preposicoes   = ['da','de','do','das','dos']\n",
    "    nome_completo = ''\n",
    "    \n",
    "    # Ajustar lista de termos, identificar sobrenomes compostos e ajustar sobrenome com ou sem presença de vírgula\n",
    "    div_sobrenome      = sobrenome_inicio.findall(string)\n",
    "    div_sbrcomposto    = sobrenome_composto.findall(string)\n",
    "    \n",
    "    # print('-'*100)\n",
    "    # print('                 Recebido:',string)\n",
    "    \n",
    "    # Caso haja vírgulas na string, tratar sobrenomes e sobrenomes compostos\n",
    "    if div_sobrenome != [] or div_sbrcomposto != []:\n",
    "        # print('CASO_01: Há víruglas na string')\n",
    "        div = string.split(', ')\n",
    "        sobrenome     = div[0].strip().upper()\n",
    "        try:\n",
    "            div_espaco    = div[1].split(' ')\n",
    "        except:\n",
    "            div_espaco    = ['']\n",
    "        primeiro      = div_espaco[0].strip('.')\n",
    "        \n",
    "        # print('     Dividir por vírgulas:',div)\n",
    "        # print('      Primeira DivVirgula:',sobrenome)\n",
    "        # print('Segunda DivVrg/DivEspaços:',div_espaco)\n",
    "        # print('      Primeira DivEspaços:',primeiro)\n",
    "               \n",
    "        # Caso primeiro nome sejam somente duas letras maiúsculas juntas, trata-se de duas iniciais\n",
    "        if len(primeiro)==2 or letras_tresconsnts.findall(primeiro):\n",
    "            # print('CASO_01.a: Há duas letras ou três letras consoantes juntas, são iniciais')\n",
    "            primeiro_nome=primeiro[0].strip()\n",
    "            # print('          C01.a1_PrimNome:',primeiro_nome)\n",
    "            nomes.append(primeiro[1].strip().upper())\n",
    "            try:\n",
    "                nomes.append(primeiro[2].strip().upper())\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            # print('CASO_01.b: Primeiro nome maior que 2 caracteres')\n",
    "            primeiro_nome = div_espaco[0].strip().title()\n",
    "            # print('          C01.a2_PrimNome:',primeiro_nome)\n",
    "        \n",
    "        # Montagem da lista de nomes do meio\n",
    "        for nome in div_espaco:\n",
    "            # print('CASO_01.c: Para cada nome da divisão por espaços após divisão por vírgula')\n",
    "            if nome not in nomes and nome.lower()!=primeiro_nome.lower() and nome.lower() not in primeiro_nome.lower() and nome!=sobrenome:   \n",
    "                # print('CASO_01.c1: Se o nome não está nem como primeiro nome, nem sobrenomes')\n",
    "                # print(nome, len(nome))\n",
    "                \n",
    "                # Avaliar se é abreviatura seguida de ponto e remover o ponto\n",
    "                if len(nome)<=2 and nome.lower() not in preposicoes:\n",
    "                    # print('    C01.c1.1_Nome<=02:',nome)\n",
    "                    for inicial in nome:\n",
    "                        # print(inicial)\n",
    "                        if inicial not in nomes and inicial not in primeiro_nome:\n",
    "                            nomes.append(inicial.replace('.','').strip().title())\n",
    "                elif len(nome)==3 and nome.lower() not in preposicoes:\n",
    "                        # print('    C01.c1.2_Nome==03:',nome)\n",
    "                        for inicial in nome:\n",
    "                            if inicial not in nomes and inicial not in primeiro_nome:\n",
    "                                nomes.append(inicial.replace('.','').strip().title())\n",
    "                else:\n",
    "                    if nome not in nomes and nome!=primeiro_nome and nome!=sobrenome and nome!='':\n",
    "                        if nome.lower() in preposicoes:\n",
    "                            nomes.append(nome.replace('.','').strip().lower())\n",
    "                        else:\n",
    "                            nomes.append(nome.replace('.','').strip().title())\n",
    "                        # print(nome,'|',primeiro_nome)\n",
    "                        \n",
    "        #caso haja sobrenome composto que não esteja nos agnomes considerar somente primeiro como sobrenome\n",
    "        if div_sbrcomposto !=[] and sobrenome.split(' ')[1] not in agnomes and sobrenome.split(' ')[0].lower() not in preposicoes:\n",
    "            # print('CASO_01.d: Sobrenome composto sem agnomes')\n",
    "            # print(div_sbrcomposto)\n",
    "            # print('Sobrenome composto:',sobrenome)\n",
    "            \n",
    "            nomes.append(sobrenome.split(' ')[1].title())\n",
    "            sobrenome = sobrenome.split(' ')[0].upper()\n",
    "            # print('Sobrenome:',sobrenome)\n",
    "            \n",
    "            for i in nomes:\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('    Nomes:',nomes)\n",
    "        \n",
    "        #caso haja preposição como agnome desconsiderar e passar para final dos nomes\n",
    "        if div_sbrcomposto !=[] and sobrenome.split(' ')[0].lower() in preposicoes:\n",
    "            # print('CASO_01.e: Preposição no Sobrenome passar para o final dos nomes')\n",
    "            # print('   div_sbrcomposto:', div_sbrcomposto)\n",
    "            # print('Sobrenome composto:',div_sbrcomposto)\n",
    "            \n",
    "            nomes.append(div_sbrcomposto[0].split(' ')[0].lower())\n",
    "            # print('    Nomes:',nomes)\n",
    "            sobrenome = div_sbrcomposto[0].split(' ')[1].upper().strip(',')\n",
    "            # print('Sobrenome:',sobrenome)\n",
    "            \n",
    "            for i in nomes:\n",
    "                # print('CASO_01.e1: Para cada nome avaliar se o sobrenome está na lista')\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('  Nomes:',nomes)\n",
    "        \n",
    "        # print('Ao final do Caso 01')\n",
    "        # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('           Lista de nomes:',nomes, len(nomes),'nomes')\n",
    "        \n",
    "    # Caso não haja vírgulas na string considera sobrenome o último nome da string dividida com espaço vazio\n",
    "    else:\n",
    "        # print('CASO_02: Não há víruglas na string')\n",
    "        try:\n",
    "            div = string.split(' ')\n",
    "            # print('      Divisões por espaço:',div)\n",
    "            \n",
    "            if div[-1] in agnomes: # nome final é um agnome\n",
    "                sobrenome     = div[-2].upper().strip()+' '+div[-1].upper().strip()\n",
    "                for i in div[1:-2]:\n",
    "                    if i not in sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.title().strip())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.lower().strip())\n",
    "            else:\n",
    "                if len(div[-1]) > 2:\n",
    "                    sobrenome     = div[-1].upper().strip()\n",
    "                    primeiro_nome = div[1].title().strip()\n",
    "                    for i in div[1:-1]:\n",
    "                        if i != sobrenome and i not in preposicoes:\n",
    "                            nomes.append(i.title().strip())\n",
    "                        if i in preposicoes:\n",
    "                            nomes.append(i.lower().strip())\n",
    "                else:\n",
    "                    sobrenome     = div[-2].upper().strip()\n",
    "                    for i in div[-1]:\n",
    "                        nomes.append(i.title())\n",
    "                    primeiro_nome = nomes[0].title().strip()\n",
    "                    for i in div[1:-1]:\n",
    "                        if i != sobrenome and i not in preposicoes:\n",
    "                            nomes.append(i.title().strip())\n",
    "                        if i in preposicoes:\n",
    "                            nomes.append(i.lower().strip())\n",
    "        except:\n",
    "            sobrenome = div[-1].upper().strip()\n",
    "            for i in div[1:-1]:\n",
    "                    if i != sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.title().strip())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.lower().strip())\n",
    "            \n",
    "        if sobrenome.lower() != div[0].lower().strip():\n",
    "            primeiro_nome=div[0].title().strip()\n",
    "        else:\n",
    "            primeiro_nome=''\n",
    "        \n",
    "        # print('Ao final do Caso 02')\n",
    "        # print('    Sobrenome sem vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome sem vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('Nomes do meio sem vírgula:',nomes, len(nomes),'nomes')\n",
    "    \n",
    "    # Encontrar e tratar como abreviaturas termos com apenas uma ou duas letras iniciais juntas, com ou sem ponto\n",
    "    for j in nomes:\n",
    "        # print('CASO_03: Avaliar cada nome armazenado na variável nomes')\n",
    "        # Procura padrões com expressões regulares na string\n",
    "        div_sobrenome      = sobrenome_inicio.findall(j)\n",
    "        div_sbrcomposto    = sobrenome_composto.findall(j)\n",
    "        div_abrevponto     = letra_abrevponto.findall(j)\n",
    "        div_abrevespaco    = letra_abrevespaco.findall(j)\n",
    "        div_ltrdobradasini = letras_dobradasini.findall(j)\n",
    "        div_ltrdobradasfim = letras_dobradasfim.findall(j)\n",
    "        div_ltrdobradas    = letras_dobradas.findall(j)\n",
    "        tamanho=len(j)\n",
    "        # print('\\n', div_ltrdobradasini, div_ltrdobradasfim, tamanho, 'em:',j,len(j))\n",
    "        \n",
    "        #caso houver abreviatura com uma letra em maiúscula nos nomes\n",
    "        if div_abrevponto !=[] or tamanho==1:\n",
    "            # print('CASO_03.1: Há abreviaturas uma letra maiúscula nos nomes')\n",
    "            nome = j.replace('.','').strip()\n",
    "            if nome not in nomes and nome != sobrenome and nome != primeiro_nome:\n",
    "                # print('CASO_03.1a: Há abreviaturas uma letra maiúscula nos nomes')\n",
    "                nomes.append(nome.upper())\n",
    "        \n",
    "        #caso houver duas inicias juntas em maiúsculas\n",
    "        elif div_ltrdobradasini !=[] or div_ltrdobradasfim !=[] or div_ltrdobradas !=[] :\n",
    "            # print('CASO_03.2: Há abreviaturas uma letra maiúscula nos nomes')\n",
    "            for letra in j:\n",
    "                # print('CASO_03.2a: Avaliar cada inicial do nome')\n",
    "                if letra not in nomes and letra != sobrenome and letra != primeiro_nome:\n",
    "                    # print('CASO_03.2a.1: Se não estiver adicionar inicial aos nomes')\n",
    "                    nomes.append(letra.upper())\n",
    "        \n",
    "        #caso haja agnomes ao sobrenome\n",
    "        elif sobrenome in agnomes:\n",
    "            # print('CASO_03.3: Há agnomes nos sobrenomes')\n",
    "            sobrenome = nomes[-1].upper()+' '+sobrenome\n",
    "            # print(sobrenome.split(' '))\n",
    "            # print('Sobrenome composto:',sobrenome)\n",
    "            for i in nomes:\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('Nomes do meio:',nomes)\n",
    "            \n",
    "        else:\n",
    "            # print('CASO_03.4: Não há agnomes nos sobrenomes')\n",
    "            if j not in nomes and j not in sobrenome and j != primeiro_nome:\n",
    "                if len(nomes) == 1:\n",
    "                    nomes.append(j.upper())\n",
    "                elif 1 < len(nomes) <= 3:\n",
    "                    nomes.append(j.lower())\n",
    "                else:\n",
    "                    nomes.append(j.title())\n",
    "         \n",
    "        # print('Ao final do Caso 03')\n",
    "        # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('Nomes do meio com vírgula:',nomes, len(nomes),'nomes')\n",
    "        \n",
    "    nomes_meio=' '.join([str for str in nomes]).strip()\n",
    "    # print('        Qte nomes do meio:',nomes,len(nomes))\n",
    "    \n",
    "    if primeiro_nome.lower() == sobrenome.lower():\n",
    "        # print('CASO_04: Primeiro nome é igual ao sobrenome')\n",
    "        try:\n",
    "            primeiro_nome=nomes_meio.split(' ')[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            nomes_meio.remove(sobrenome)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        # print('Ao final do caso 04')\n",
    "        # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('Nomes do meio com vírgula:',nomes, len(nomes),'nomes')\n",
    "    \n",
    "    # Caso sobrenome seja só de 1 letra passá-lo para nomes e considerar o próximo nome como sobrenome\n",
    "    for i in range(len(div)):\n",
    "        if len(sobrenome)==1 or sobrenome.lower() in preposicoes:\n",
    "            # print('CASO_05: Mudar sobrenomes até o adequado')\n",
    "            div    = string.split(', ')\n",
    "            # print('Divisão por vírgulas:',div)\n",
    "            avaliar0       = div[0].split(' ')[0].strip()\n",
    "            if 1< len(avaliar0) < 3:\n",
    "                # print('CASO_05.1: 1 < Sobrenome < 3 fica em minúsculas')\n",
    "                sbrn0          = avaliar0.lower()\n",
    "            else:\n",
    "                # print('CASO_05.2: Sobrenome de tamanho 1 ou maior que 3 fica em maiúsculas')\n",
    "                sbrn0          = avaliar0.title()\n",
    "            # print('sbrn0:',sbrn0, len(sbrn0))\n",
    "            \n",
    "            try:\n",
    "                avaliar1=div[0].split(' ')[1].strip()\n",
    "                # print('avaliar0',avaliar0)\n",
    "                # print('avaliar1',avaliar1)\n",
    "                if 1 < len(avaliar1) <=3:\n",
    "                    sbrn1     = avaliar1.lower()\n",
    "                else:\n",
    "                    sbrn1     = avaliar1.title()\n",
    "                # print('sbrn1:',sbrn1, len(sbrn1))\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if div != []:\n",
    "                # print('CASO_05.3: Caso haja divisão por vírgulas na string')\n",
    "                try:\n",
    "                    div_espaco     = div[1].split(' ')\n",
    "                except:\n",
    "                    div_espaco     = div[0].split(' ')\n",
    "                sobrenome      = div_espaco[0].strip().upper()\n",
    "                try:\n",
    "                    primeiro_nome  = div_espaco[1].title().strip()\n",
    "                except:\n",
    "                    primeiro_nome  = div_espaco[0].title().strip()\n",
    "                if len(sbrn0) == 1:\n",
    "                    # print('CASO_05.3a: Avalia primeiro sobrenome de tamanho 1')\n",
    "                    # print('Vai pros nomes:',str(sbrn0).title())\n",
    "                    nomes_meio = nomes_meio+str(' '+sbrn0.title())\n",
    "                    # print('   NomesMeio:',nomes_meio)\n",
    "\n",
    "                elif 1 < len(sbrn0) <= 3:\n",
    "                    # print('CASO_05.3b: Avalia primeiro sobrenome 1< tamanho <=3')\n",
    "                    # print('Vão pros nomes sbrn0:',sbrn0, 'e sbrn1:',sbrn1)\n",
    "\n",
    "                    div_tresconsoantes = letras_tresconsnts.findall(sobrenome)\n",
    "                    if div_tresconsoantes != []:\n",
    "                        # print('CASO_05.4: Três consoantes como sobrenome')\n",
    "                        for letra in sobrenome:\n",
    "                            nomes.append(letra)\n",
    "\n",
    "                        if len(sobrenome) >2:\n",
    "                            sobrenome=nomes[0]\n",
    "                        else:\n",
    "                            sobrenome=nomes[1]\n",
    "                        nomes.remove(sobrenome)\n",
    "                        primeiro_nome=nomes[0]\n",
    "                        nomes_meio=' '.join([str for str in nomes[1:]]).strip()\n",
    "                        nome_completo=sobrenome.upper()+', '+nomes_meio                \n",
    "                    \n",
    "                    try:                       \n",
    "                        # print(' 05.3b    Lista de Nomes:',nomes_meio)\n",
    "                        nomes_meio=nomes_meio.replace(sbrn0,'')\n",
    "                        # print(' 05.3b ReplaceSobrenome0:',nomes_meio)\n",
    "                        nomes_meio=nomes_meio.replace(sbrn1,'')\n",
    "                        # print(' 05.3b ReplaceSobrenome1:',nomes_meio)\n",
    "                    except Exception as e:\n",
    "                        # print('   Erro ReplaceSobrenome:',e)\n",
    "                        pass\n",
    "                    try:\n",
    "                        nomes_meio.replace(primeiro_nome.title(),'')\n",
    "                        nomes_meio.replace(primeiro_nome.lower(),'')\n",
    "                        nomes_meio.replace(primeiro_nome,'')\n",
    "                        # print(' 05.3b Replace PrimNome:',nomes_meio)\n",
    "                    except Exception as e:\n",
    "                        print('Erro no try PrimeiroNome:',e)\n",
    "                        pass\n",
    "                    nomes_meio = nomes_meio.replace(sobrenome,'')\n",
    "                    try:\n",
    "                        for n,i in enumerate(avaliar1):\n",
    "                            nomes.append(i.upper())\n",
    "                            sbrn1     = avaliar1[0]\n",
    "                        else:\n",
    "                            sbrn1     = avaliar1.title()\n",
    "                        # print('sbrn1:',sbrn1, len(sbrn1))\n",
    "                        nomes_meio = nomes_meio+str(' '+sbrn0)+str(' '+sbrn1)\n",
    "                    except:\n",
    "                        nomes_meio = nomes_meio+str(' '+sbrn0)\n",
    "                    nomes      = nomes_meio.strip().strip(',').split(' ')\n",
    "                    # print(' 05.3b NomesMeio:',nomes_meio)\n",
    "                    # print(' 05.3b     Nomes:',nome)\n",
    "\n",
    "                else:\n",
    "                    # print('CASO_05.3c: Avalia primeiro sobrenome >3')\n",
    "                    nomes_meio = nomes_meio+str(' '+div[0].strip().title())\n",
    "                    nomes      = nomes_meio.strip().split(' ')\n",
    "                    # print(' 05.3c NomesMeio:',nomes_meio)\n",
    "                    # print(' 05.3c     Nomes:',nomes)\n",
    "\n",
    "                nomes_meio=nomes_meio.replace(sobrenome,'').replace(',','').strip()\n",
    "                nomes_meio=nomes_meio.replace(primeiro_nome,'').strip()\n",
    "\n",
    "            # print('Ao final do caso 05')\n",
    "            # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "            # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "            # print('Nomes do meio com vírgula:',nomes, len(nomes),'nomes')\n",
    "    \n",
    "    if sobrenome != '' and primeiro_nome !='':\n",
    "        nome_completo=sobrenome.upper().replace(',','')+', '+primeiro_nome.replace(',','')+' '+nomes_meio.replace(sobrenome,'').replace(',','')\n",
    "    elif sobrenome != '':\n",
    "        nome_completo=sobrenome.upper().replace(',','')+', '+nomes_meio.replace(sobrenome,'').replace(',','')\n",
    "    else:\n",
    "        nome_completo=sobrenome.upper()\n",
    "    \n",
    "#     print('Após ajustes finais')\n",
    "#     print('     Sobrenome:',sobrenome)\n",
    "#     print(' Primeiro Nome:',primeiro_nome)\n",
    "#     print('         Nomes:',nomes)\n",
    "#     print('     NomesMeio:',nomes_meio)        \n",
    "        \n",
    "#     print('                Resultado:',nome_completo)\n",
    "    \n",
    "    return nome_completo.strip()\n",
    "\n",
    "\n",
    "def iniciais_nome(linha_texto):\n",
    "    '''Função para retornar sobrenome+iniciais dos nomes, na forma: SOBRENOME, X Y Z\n",
    "     Recebe: String com nome\n",
    "    Retorna: Tupla com nome e sua versão padronizada em sobrenome+agnomes em maiúsculas, seguida de vírgula e iniciais dos nomes \n",
    "      Autor: Marcos Aires (Mar.2022)\n",
    "    '''\n",
    "    import unicodedata\n",
    "    import re\n",
    "    # print('               Analisando:',linha_texto)\n",
    "    string = ''.join(ch for ch in unicodedata.normalize('NFKD', linha_texto) if not unicodedata.combining(ch))\n",
    "    string = string.replace('(Org)','').replace('(Org.)','').replace('(Org).','').replace('.','')\n",
    "        \n",
    "    # Expressões regulares para encontrar padrões de divisão de nomes de autores\n",
    "    sobrenome_inicio   = re.compile(r'^[A-ZÀ-ú-a-z]+,')                 # Sequência de letras maiúsculas no início da string\n",
    "    sobrenome_composto = re.compile(r'^[A-ZÀ-ú-a-z]+[ ][A-ZÀ-ú-a-z]+,') # Duas sequências de letras no início da string, separadas por espaço, seguidas por vírgula\n",
    "    letra_abrevponto   = re.compile(r'^[A-Z][.]')                       # Uma letra maiúscula no início da string, seguida por ponto\n",
    "    letra_abrevespaco  = re.compile(r'^[A-Z][ ]')                       # Uma letra maiúscula no início da string, seguida por espaço\n",
    "    letras_dobradas    = re.compile(r'[A-Z]{2}')                        # Duas letras maiúsculas juntas no início da string, seguida por espaço\n",
    "    letras_dobradasini = re.compile(r'[A-Z]{2}[ ]')                     # Duas letras maiúsculas juntas no início da string, seguida por espaço\n",
    "    letras_dobradasfim = re.compile(r'[ ][A-Z]{2}')                     # Duas letras maiúsculas juntas no final da string, precedida por espaço\n",
    "        \n",
    "    nomes=[]\n",
    "    agnomes       = ['NETO','JUNIOR','FILHO','SEGUNDO','TERCEIRO']\n",
    "    preposicoes   = ['da','de','do','das','dos','DA','DE','DOS','DAS','DOS','De']\n",
    "    nome_completo = ''\n",
    "    \n",
    "    # Ajustar lista de termos, identificar sobrenomes compostos e ajustar sobrenome com ou sem presença de vírgula\n",
    "    div_sobrenome      = sobrenome_inicio.findall(string)\n",
    "    div_sbrcomposto    = sobrenome_composto.findall(string)\n",
    "    \n",
    "    # Caso haja vírgulas na string, tratar sobrenomes e sobrenomes compostos\n",
    "    if div_sobrenome != [] or div_sbrcomposto != []:\n",
    "        div   = string.split(', ')\n",
    "        sobrenome     = div[0].strip().upper()\n",
    "        try:\n",
    "            div_espaco    = div[1].split(' ')\n",
    "        except:\n",
    "            div_espaco  = ['']\n",
    "        primeiro      = div_espaco[0].strip('.')\n",
    "        \n",
    "        # Caso primeiro nome sejam somente duas letras maiúsculas juntas, trata-se de duas iniciais\n",
    "        if len(primeiro)==2:\n",
    "            primeiro_nome=primeiro[0].strip()\n",
    "            nomes.append(primeiro[1].strip())\n",
    "        else:\n",
    "            primeiro_nome = div_espaco[0].strip().title()\n",
    "        \n",
    "        # Montagem da lista de nomes do meio\n",
    "        for nome in div_espaco:\n",
    "            if nome not in nomes and nome.lower()!=primeiro_nome.lower() and nome.lower() not in primeiro_nome.lower() and nome!=sobrenome:   \n",
    "                # print(nome, len(nome))\n",
    "                \n",
    "                # Avaliar se é abreviatura seguida de ponto e remover o ponto\n",
    "                if len(nome)<=2 and nome.lower() not in preposicoes:\n",
    "                    for inicial in nome:\n",
    "                        # print(inicial)\n",
    "                        if inicial not in nomes and inicial not in primeiro_nome:\n",
    "                            nomes.append(inicial.replace('.','').strip().title())\n",
    "                else:\n",
    "                    if nome not in nomes and nome!=primeiro_nome and nome!=sobrenome and nome!='':\n",
    "                        if nome.lower() in preposicoes:\n",
    "                            nomes.append(nome.replace('.','').strip().lower())\n",
    "                        else:\n",
    "                            nomes.append(nome.replace('.','').strip().title())\n",
    "                        # print(nome,'|',primeiro_nome)\n",
    "                        \n",
    "        #caso haja sobrenome composto que não esteja nos agnomes considerar somente primeiro como sobrenome\n",
    "        if div_sbrcomposto !=[] and sobrenome.split(' ')[1] not in agnomes:\n",
    "            # print(div_sbrcomposto)\n",
    "            # print('Sobrenome composto:',sobrenome)\n",
    "            nomes.append(sobrenome.split(' ')[1].title())\n",
    "            sobrenome = sobrenome.split(' ')[0].upper()\n",
    "            # print('Sobrenome:',sobrenome.split(' '))\n",
    "            for i in nomes:\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('Nomes do meio:',nomes)\n",
    "        \n",
    "        # print('    Sobrenome com vírgula:',sobrenome, len(sobrenome),'letras')\n",
    "        # print('Primeiro nome com vírgula:',primeiro_nome, len(primeiro_nome),'letras')\n",
    "        # print('Nomes do meio com vírgula:',nomes, len(nomes),'nomes')\n",
    "        \n",
    "    # Caso não haja vírgulas na string considera sobrenome o último nome da string dividida com espaço vazio\n",
    "    else:\n",
    "        try:\n",
    "            div       = string.split(' ')\n",
    "            if div[-2] in agnomes:\n",
    "                sobrenome = div[-2].upper()+' '+div[-1].strip().upper()\n",
    "                for i in nomes[1:-2]:\n",
    "                    if i not in sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.strip().title())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.strip().lower())\n",
    "            else:\n",
    "                sobrenome = div[-1].strip().upper()\n",
    "                for i in div[1:-1]:\n",
    "                    if i not in sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.strip().title())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.strip().lower())\n",
    "        except:\n",
    "            sobrenome = div[-1].strip().upper()\n",
    "            for i in div[1:-1]:\n",
    "                    if i not in sobrenome and i not in preposicoes:\n",
    "                        nomes.append(i.strip().title())\n",
    "                    if i in preposicoes:\n",
    "                        nomes.append(i.strip().lower())\n",
    "            \n",
    "        if sobrenome.lower() != div[0].strip().lower():\n",
    "            primeiro_nome=div[0].strip().title()\n",
    "        else:\n",
    "            primeiro_nome=''\n",
    "        \n",
    "        # print('    Sobrenome sem vírgula:',sobrenome)\n",
    "        # print('Primeiro nome sem vírgula:',primeiro_nome)\n",
    "        # print('Nomes do meio sem vírgula:',nomes)\n",
    "    \n",
    "    # Encontrar e tratar como abreviaturas termos com apenas uma ou duas letras iniciais juntas, com ou sem ponto\n",
    "    for j in nomes:\n",
    "        # Procura padrões com expressões regulares na string\n",
    "        div_sobrenome      = sobrenome_inicio.findall(j)\n",
    "        div_sbrcomposto    = sobrenome_composto.findall(j)\n",
    "        div_abrevponto     = letra_abrevponto.findall(j)\n",
    "        div_abrevespaco    = letra_abrevespaco.findall(j)\n",
    "        div_ltrdobradasini = letras_dobradasini.findall(j)\n",
    "        div_ltrdobradasfim = letras_dobradasfim.findall(j)\n",
    "        div_ltrdobradas    = letras_dobradas.findall(j)\n",
    "        tamanho=len(j)\n",
    "        # print('\\n', div_ltrdobradasini, div_ltrdobradasfim, tamanho, 'em:',j,len(j))\n",
    "        \n",
    "        #caso houver abreviatura com uma letra em maiúscula nos nomes\n",
    "        if div_abrevponto !=[] or tamanho==1:\n",
    "            cada_nome = j.replace('.','').strip()\n",
    "            if cada_nome not in nomes and cada_nome != sobrenome and nome != primeiro_nome:\n",
    "                nomes.append(cada_nome)\n",
    "        \n",
    "        #caso houver duas inicias juntas em maiúsculas\n",
    "        elif div_ltrdobradasini !=[] or div_ltrdobradasfim !=[] or div_ltrdobradas !=[] :\n",
    "            for letra in j:\n",
    "                if letra not in nomes and letra != sobrenome and letra != primeiro_nome:\n",
    "                    nomes.append(letra)\n",
    "        \n",
    "        #caso haja agnomes ao sobrenome\n",
    "        elif sobrenome in agnomes:\n",
    "            sobrenome = nomes[-1].upper()+' '+sobrenome\n",
    "            # print(sobrenome.split(' '))\n",
    "            # print('Sobrenome composto:',sobrenome)\n",
    "            for i in nomes:\n",
    "                if i.lower() in sobrenome.lower():\n",
    "                    nomes.remove(i)\n",
    "            # print('Nomes do meio:',nomes)\n",
    "            \n",
    "        else:\n",
    "            if j not in nomes and j not in sobrenome and j != primeiro_nome:\n",
    "                nomes.append(j)\n",
    "    \n",
    "    nomes_meio=' '.join([str[0] for str in nomes]).strip()\n",
    "    # print('Qte nomes do meio',len(nomes),nomes)\n",
    "    if sobrenome != '' and primeiro_nome !='':\n",
    "        sobrenome_iniciais = sobrenome+', '+primeiro_nome[0]+' '+nomes_meio\n",
    "    elif sobrenome != '':\n",
    "        sobrenome_iniciais = sobrenome\n",
    "    \n",
    "    return sobrenome_iniciais.strip()\n",
    "\n",
    "\n",
    "def similares(lista_autores, lista_grupo, limite_jarowinkler, distancia_levenshtein):\n",
    "    \"\"\"Função para aplicar padronização no nome de autor da lista de pesquisadores e buscar similaridade na lista de coautores\n",
    "     Recebe: Lista de pesquisadores do grupo em análise gerada pela lista de nomes dos coautores das publicações em análise\n",
    "    Utiliza: get_jaro_distance(), editdistance()\n",
    "    Retorna: Lista de autores com fusão de nomes cuja similaridade esteja dentro dos limites definidos nesta função\n",
    "      Autor: Marcos Aires (Fev.2022)\n",
    "      \n",
    "    Refazer: Inserir crítica de, mantendo sequência ordem alfabética, retornar no final nome mais extenso em caso de similaridade;\n",
    "    \"\"\"\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    from IPython.display import clear_output\n",
    "    import editdistance\n",
    "    import numpy as np\n",
    "    import time\n",
    "    \n",
    "    t0=time.time()\n",
    "    \n",
    "    # limite_jarowinkler=0.85\n",
    "    # distancia_levenshtein=6\n",
    "    similares_jwl=[]\n",
    "    similares_regras=[]\n",
    "    similares=[]\n",
    "    tempos=[]\n",
    "    \n",
    "    count=0\n",
    "    t1=time.time()\n",
    "    for i in lista_autores:\n",
    "        count+=1\n",
    "        if count > 0:\n",
    "            tp=time.time()-t1\n",
    "            tmed=tp/count*2\n",
    "            tempos.append(tp)\n",
    "    #     print(\"Analisar similaridades com: \", nome_padronizado)\n",
    "        \n",
    "        count1=0\n",
    "        for nome in lista_autores:\n",
    "            if count1 > 0:\n",
    "                resta=len(lista_autores)-count\n",
    "                print(f'Analisando {count1:3}/{len(lista_autores)} resta analisar {resta:3} nomes. Previsão de término em {np.round(tmed*resta/60,1)} minutos')\n",
    "            else:\n",
    "                print(f'Analisando {count1:3}/{len(lista_autores)} resta analisar {len(lista_autores)-count1} nomes.')\n",
    "            \n",
    "            t2=time.time()\n",
    "            count1+=1            \n",
    "\n",
    "            try:\n",
    "                similaridade_jarowinkler = get_jaro_distance(i, nome)\n",
    "                print(f'{i:40} | {nome:40} | Jaro-Winkler: {np.round(similaridade_jarowinkler,2):4} Levenshtein: {editdistance.eval(i, nome)}')\n",
    "                similaridade_levenshtein = editdistance.eval(i, nome)\n",
    "\n",
    "                # inferir similaridade para nomes que estejam acima do limite ponderado definido, mas não idênticos e não muito distantes em edição\n",
    "                if  similaridade_jarowinkler > limite_jarowinkler and similaridade_jarowinkler!=1 and similaridade_levenshtein < distancia_levenshtein:\n",
    "                    # Crítica no nome mais extenso como destino no par (origem, destino)\n",
    "                    \n",
    "                    similares_jwl.append((i,nome))\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            clear_output(wait=True)\n",
    "    \n",
    "    # Conjunto de regras de validação de similaridade\n",
    "    # Monta uma lista de nomes a serem retirados antes de montar a lista de troca\n",
    "    trocar=[]\n",
    "    retirar=[]\n",
    "    for i in similares_jwl:\n",
    "        sobrenome_i = i[0].split(',')[0]\n",
    "        sobrenome_j = i[1].split(',')[0]\n",
    "\n",
    "        try:\n",
    "            iniciais_i  = iniciais_nome(i[0]).split(',')[1].strip()\n",
    "        except:\n",
    "            iniciais_i  = ''\n",
    "\n",
    "        try:\n",
    "            iniciais_j  = iniciais_nome(i[1]).split(',')[1].strip()\n",
    "        except:\n",
    "            iniciais_j  = ''\n",
    "\n",
    "        try:\n",
    "            primnome_i = i[0].split(',')[1].strip().split(' ')[0].strip()\n",
    "        except:\n",
    "            primnome_i = ''\n",
    "\n",
    "        try:\n",
    "            primnome_j = i[1].split(',')[1].strip().split(' ')[0].strip()\n",
    "        except:\n",
    "            primnome_j = ''    \n",
    "\n",
    "        try:\n",
    "            inicial_i = i[0].split(',')[1].strip()[0]\n",
    "        except:\n",
    "            inicial_i = ''\n",
    "\n",
    "        try:\n",
    "            resto_i   = i[0].split(',')[1].strip().split(' ')[0][1:]\n",
    "        except:\n",
    "            resto_i   = ''\n",
    "\n",
    "        try:\n",
    "            inicial_j = i[1].split(',')[1].strip()[0]\n",
    "        except:\n",
    "            inicial_j = ''\n",
    "\n",
    "        try:\n",
    "            resto_j   = i[1].split(',')[1].strip().split(' ')[0][1:]\n",
    "        except:\n",
    "            resto_j = ''\n",
    "\n",
    "        # Se a distância de edição entre os sobrenomes\n",
    "        if editdistance.eval(sobrenome_i, sobrenome_j) > 2 or inicial_i!=inicial_j:\n",
    "            retirar.append(i)\n",
    "        else:\n",
    "            if primnome_i!=primnome_j and len(primnome_i)>1:\n",
    "                retirar.append(i)\n",
    "            if primnome_i!=primnome_j and len(primnome_i)>1 and len(primnome_j)>1:\n",
    "                retirar.append(i)\n",
    "            if resto_i!=resto_j and resto_i!='':\n",
    "                retirar.append(i)\n",
    "            if len(i[1]) < len(i[0]):\n",
    "                retirar.append(i)\n",
    "            if len(iniciais_i) != len(iniciais_j):\n",
    "                retirar.append(i)\n",
    "\n",
    "    for i in similares_jwl:\n",
    "        if i not in retirar:\n",
    "            trocar.append(i)\n",
    "\n",
    "        if iniciais_nome(i[0]) in iniciais_nome(i[1]) and len(i[0]) < len(i[1]):\n",
    "            trocar.append(i)\n",
    "\n",
    "        if iniciais_nome(i[0]) == iniciais_nome(i[1]) and len(i[0]) < len(i[1]):\n",
    "             trocar.append(i)\n",
    "\n",
    "    # Exemplo de inserção de conhecimentos extra Lattes para melhor resolução de entidades\n",
    "    lista_extra = [\n",
    "                    # ('ALBUQUERQUE, Adriano B', 'ALBUQUERQUE, Adriano Bessa'),\n",
    "                    # ('ALBUQUERQUE, Adriano', 'ALBUQUERQUE, Adriano Bessa'),\n",
    "                    # ('COELHO, Andre L V', 'COELHO, Andre Luis Vasconcelos'),\n",
    "                    # ('DUARTE, Joao B F', 'DUARTE, Joao Batista Furlan'),\n",
    "                    # ('FILHO, Raimir H','HOLANDA FILHO, Raimir'),\n",
    "                    # ('FILHO, Raimir','HOLANDA FILHO, Raimir'),\n",
    "                    # ('FORMIGO, A','FORMICO, Maria Andreia Rodrigues'),\n",
    "                    # ('FORMICO, A','FORMICO, Maria Andreia Rodrigues'),\n",
    "                    # ('FURLAN, J B D', 'FURLAN, Joao Batista Duarte'),\n",
    "                    # ('FURTADO, Elizabeth', 'FURTADO, Maria Elizabeth Sucupira'),\n",
    "                    # ('FURTADO, Elizabeth S', 'FURTADO, Maria Elizabeth Sucupira'),\n",
    "                    # ('FURTADO, Elizabeth Sucupira','FURTADO, Maria Elizabeth Sucupira'),\n",
    "                    # ('FURTADO, M E S', 'FURTADO, Maria Elizabeth Sucupira'),\n",
    "                    # ('FURTADO, Vasco', 'FURTADO, Joao Jose Vasco Peixoto'),\n",
    "                    # ('FURTADO, J P', 'FURTADO, Joao Jose Vasco Peixoto'),\n",
    "                    # ('FURTADO, J V P', 'FURTADO, Joao Jose Vasco Peixoto'),\n",
    "                    # ('FURTADO, Vasco', 'FURTADO, Joao Jose Vasco Peixoto'),\n",
    "                    # ('FURTADO, Elizabeth','FURTADO, Maria Elizabeth Sucupira'),\n",
    "                    # ('HOLANDA, Raimir', 'HOLANDA FILHO, Raimir'),\n",
    "                    # ('LEITE, G S', 'LEITE, Gleidson Sobreira'),\n",
    "                    # ('PEQUENO, T H C', 'PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                    # ('PEQUENO, Tarcisio','PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                    # ('PEQUENO, Tarcisio Cavalcante', 'PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                    # ('PINHEIRO, Placido R', 'PINHEIRO, Placido Rogerio'),\n",
    "                    # ('PINHEIRO, Vladia', 'PINHEIRO, Vladia Celia Monteiro'),\n",
    "                    # ('RODRIGUES, M A F', 'RODRIGUES, Maria Andreia Formico'),\n",
    "                    # ('RODRIGUES, Andreia', 'RODRIGUES, Maria Andreia Formico'),\n",
    "                    # ('JOAO, Batista F Duarte,', 'FURLAN, Joao Batista Duarte'),\n",
    "                    # ('MACEDO, Antonio Roberto M de', 'MACEDO, Antonio Roberto Menescal de'),\n",
    "                    # ('MACEDO, D V', 'MACEDO, Daniel Valente'),\n",
    "                    # ('MENDONCA, Nabor C', 'MENDONCA, Nabor das Chagas'),\n",
    "                    # ('PEQUENO, Tarcisio', 'PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                    # ('PEQUENO, Tarcisio H', 'PEQUENO, Tarcisio Haroldo Cavalcante'),\n",
    "                    # ('PINHEIRO, Mirian C D', 'PINHEIRO, Miriam Caliope Dantas'),\n",
    "                    # ('PINHEIRO, Mirian Caliope Dantas', 'PINHEIRO, Miriam Caliope Dantas'),\n",
    "                    # ('PINHEIRO, P G C D', 'PINHEIRO, Pedro Gabriel Caliope Dantas'),\n",
    "                    # ('PINHEIRO, Pedro G C', 'PINHEIRO, Pedro Gabriel Caliope Dantas'),\n",
    "                    # ('PINHEIRO, Placido R', 'PINHEIRO, Placido Rogerio'),\n",
    "                    # ('PINHEIRO, Vladia', 'PINHEIRO, Vladia Celia Monteiro'),\n",
    "                    # ('ROGERIO, Placido Pinheiro', 'PINHEIRO, Placido Rogerio'),\n",
    "                    # ('REBOUCRAS FILHO, Pedro', 'REBOUCAS FILHO, Pedro Pedrosa'),\n",
    "                    # ('SAMPAIO, A', 'SAMPAIO, Americo Tadeu Falcone'),\n",
    "                    # ('SAMPAIO, Americo', 'SAMPAIO, Americo Tadeu Falcone'),\n",
    "                    # ('SAMPAIO, Americo Falcone', 'SAMPAIO, Americo Tadeu Falcone'),\n",
    "                    # ('SUCUPIRA, Elizabeth Furtado','FURTADO, Maria Elizabeth Sucupira'),\n",
    "                  ]\n",
    "    \n",
    "    trocar=trocar+lista_extra\n",
    "    trocar.sort()\n",
    "    \n",
    "    return trocar\n",
    "\n",
    "\n",
    "def extrair_variantes(df_dadosgrupo):\n",
    "    ''' Utiliza campo de Nome em Citações do currículo como filtro para obter variantes do nome de cada membro\n",
    "     Recebe: Dataframe com os dados brutos do grupo de pesquisa agrupados; lista de nomes de pesquisadores de interesse\n",
    "    Retorna: Lista de tuplas com pares a serem trocados da variante pelo nome padronizado na forma (origem, destino)\n",
    "    '''\n",
    "    filtro1   = 'Nome'\n",
    "    lista_nomes = df_dadosgrupo[(df_dadosgrupo.ROTULOS == filtro1)]['CONTEUDOS'].values\n",
    "\n",
    "    variantes=[]\n",
    "    filtro='Nome em citações bibliográficas'\n",
    "    variantes=df_dadosgrupo[(df_dadosgrupo.ROTULOS == filtro)]['CONTEUDOS'].to_list()\n",
    "\n",
    "    trocar=[]\n",
    "    for j in range(len(variantes)):\n",
    "        padrao_destino = padronizar_nome(lista_nomes[j])\n",
    "        trocar.append((lista_nomes[j], padrao_destino))\n",
    "        for k in variantes[j]:\n",
    "            padrao_origem = padronizar_nome(k)\n",
    "            trocar.append((k, padrao_destino))\n",
    "            trocar.append((padrao_origem, padrao_destino))\n",
    "    \n",
    "    return trocar\n",
    "\n",
    "\n",
    "def inferir_variantes(nome):\n",
    "    ''' Quebra um nome inicialmente por vírgula para achar sobrenomes, e depois por ' ' para achar nomes\n",
    "     Recebe: Par de nomes a comparar, nome1 é nome padronizado na função padronizar_nome(), nome2 é o que será analisado\n",
    "    Utiliza: Função padronizar_nome(nome)\n",
    "    Retorna: Lista de tuplas, no formato (origem, destino), com variantes de nome a serem trocadas pela forma padronizada\n",
    "      Autor: Marco Aires (Fev.2022)\n",
    "    '''\n",
    "    trocar = []\n",
    "    nomes  = []\n",
    "    try:\n",
    "        div0  = nome.split(',').strip()\n",
    "        sobrenome=div0[0]\n",
    "        try:\n",
    "            div1 = div0[1].split(' ').strip()\n",
    "            for i in div1:\n",
    "                nomes.append(i)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    trocar.append(nome, iniciais_nome(nome))\n",
    "    \n",
    "    return trocar\n",
    "\n",
    "\n",
    "def comparar_nomes(nome1,nome2):\n",
    "    ''' Compara dois nomes por seus sobrenomes e iniciais do primeiro nome\n",
    "     Recebe: Par de nomes a comparar, nome1 é nome padronizado na função padronizar_nome(), nome2 é o que será analisado\n",
    "    Utiliza: Função padronizar_nome(nome)\n",
    "    Retorna: Lista de tuplas, no formato (origem, destino), com variantes de nome a serem trocadas pela forma padronizada\n",
    "      Autor: Marco Aires (Fev.2022)\n",
    "    '''\n",
    "    trocar=[]\n",
    "    qte_nomes1=0\n",
    "    nome_padronizado1 = padronizar_nome(nome1)\n",
    "    sobrenome1        = nome_padronizado1.split(',')[0]\n",
    "    if sobrenome1!='':\n",
    "        qte_nomes1+=1\n",
    "    primeiro_nome1    = nome_padronizado1.split(',')[1].split(' ')[0]\n",
    "    if primeiro_nome1!='':\n",
    "        qte_nomes1+=1\n",
    "    inicial_primnome1 = primeiro_nome1[0]\n",
    "    demais_nomes1     = nome_padronizado1.split(',')[1].split(' ')[1:]\n",
    "    qte_nomes1=qte_nomes1+len(demais_nomes1)\n",
    "    \n",
    "    qte_nomes2=0\n",
    "    nome_padronizado2 = padronizar_nome(nome2)\n",
    "    sobrenome2        = nome_padronizado2.split(',')[0]\n",
    "    if sobrenome2!='':\n",
    "        qte_nomes2+=1    \n",
    "    primeiro_nome2    = nome_padronizado2.split(',')[1].split(' ')[0]\n",
    "    if primeiro_nome2!='':\n",
    "        qte_nomes2+=1\n",
    "    inicial_primnome2 = primeiro_nome2[0]\n",
    "    demais_nomes2     = nome_padronizado2.split(',')[1].split(' ')[1:]\n",
    "    qte_nomes2=qte_nomes2+len(demais_nomes2)\n",
    "    \n",
    "    if sobrenome1==sobrenome2 and primeiro_nome1==primeiro_nome2:\n",
    "        trocar.append((nome1,nome_padronizado2))\n",
    "\n",
    "    if sobrenome1==sobrenome2 and primeiro_nome1==primeiro_nome2:\n",
    "        trocar.append((nome1,nome_padronizado2))\n",
    "        \n",
    "    return trocar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"Vasconcelos, GS ; Fernandes, MCR ; Matsui, TC ; Luciano, MCS ; Costa, CL ; Ferraz, CPM ; Dias, FBS ; MIYAJIMA, FABIO ; Araújo, FMC ; Fonseca, MHG . Persistent SARS-COV-2 infection in vaccinated individual with three doses of COVID-19 vaccine. VACCINE, v. 41, p. 1778-1782, 2023.\"\n",
    "# extrair_detalhes(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtro='Artigos completos publicados em periódicos'\n",
    "# artigos = df_secoes[df_secoes.ROTULOS == filtro]\n",
    "\n",
    "# nome = lista_busca[1]\n",
    "# artigos_individual = artigos[artigos.CURRICULO==nome]\n",
    "# lista_individual = artigos_individual['CONTEUDOS']\n",
    "# lista_individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>FILTRAR PERÍODO DE TEMPO UNIFICADO</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos = pd.read_csv(pathout+'df_artigos_servidores_ingresso_fioce.csv', sep='\\t')\n",
    "print(len(df_artigos.index))\n",
    "# df_artigos[601:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_inico = 2008\n",
    "ano_final = 2023\n",
    "df_artigosperiodo = df_artigos[(df_artigos.ANO_PUB >=ano_inico)&(df_artigos.ANO_PUB <=ano_final)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigosperiodo[df_artigosperiodo.ANO_PUB==np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_artigosperiodo[:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>FILTRAR PERÍODO DE TEMPO INDIVIDUALIZADO</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_artigos = df_artigos_servidores\n",
    "\n",
    "# fioce_pessoal['INGRESSO_FIOCE'] = pd.to_datetime(fioce_pessoal['INGRESSO_FIOCE'])\n",
    "\n",
    "# # Certificando-se de que 'INGRESSO_FIOCE' é do tipo int (se for data no formato yyyy, por exemplo)\n",
    "# fioce_pessoal['ANO_INGRESSO_FIOCE'] = fioce_pessoal['INGRESSO_FIOCE'].dt.year\n",
    "\n",
    "# # Merge entre df_artigos e fioce_pessoal usando 'AUTORES' e 'NOME' como chaves\n",
    "# merged_df = df_artigos.merge(fioce_pessoal, left_on='CURRICULO', right_on='NOME', how='inner')\n",
    "\n",
    "# # Filtrar as linhas de acordo com a condição do ano de publicação e da data de ingresso\n",
    "# result_df = merged_df[merged_df['ANO_PUB'] >= merged_df['ANO_INGRESSO_FIOCE']]\n",
    "\n",
    "# # Opcional: Dropar colunas redundantes ou não necessárias, por exemplo, 'NOME' que é igual a 'AUTORES'\n",
    "# result_df = result_df.drop(columns=['NOME'])\n",
    "\n",
    "# # Agora, result_df é o dataframe final desejado\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df[result_df.ANO_PUB==2030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_result_df = merged_df[merged_df['ANO_PUB'] < merged_df['ANO_INGRESSO_FIOCE']]\n",
    "# print(len(result_df))\n",
    "# print(len(out_result_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exibir informações extratídas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigosperiodo.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolucao_artigos(df_artigosperiodo, fioce_pessoal, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigosperiodo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_artigosperiodo.NOME.value_counts()))\n",
    "df_artigosperiodo.NOME.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fioce_pessoal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_media_artigos(df_artigosperiodo, fioce_pessoal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolucao_anual(df_artigosperiodo, fioce_pessoal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolucao_sem_duplicatas(df_artigosperiodo, fioce_pessoal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotar_artigos_ano(df_artigosperiodo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparativo_curriculos(df_artigosperiodo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotar_barras_estaqueadas(df_artigosperiodo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artigosperiodo[df_artigosperiodo.NOME=='Anya Pimentel Gomes Fernandes Vieira Meyer'].sort_values(by='ANO_PUB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df[result_df.NOME=='Raphael Trevizani']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qualitatives = px.colors.qualitative.swatches()\n",
    "# sequentials = px.colors.sequential.swatches()\n",
    "\n",
    "# qualitatives.show()\n",
    "# sequentials.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def try_folders(drives_win, drives_lin,pastas,pastasraiz):\n",
    "#     sistema_operacional =sys.platform\n",
    "#     for r in pastasraiz:\n",
    "#         for p in pastas:\n",
    "#             if 'linux' in sistema_operacional:\n",
    "#                 so = 'Linux'\n",
    "#                 for d in drives_lin:\n",
    "#                     caminho = d+p+r\n",
    "#             elif 'win32' in sistema_operacional:\n",
    "#                 so = 'Windows'\n",
    "#                 for d in drives_win:\n",
    "#                     pastaraiz = d+p+r\n",
    "#             else: \n",
    "#                 print('MacOS não contemplado ainda')\n",
    "#             try:\n",
    "#                 print(f'Procurar em: {pastaraiz}')\n",
    "#                 caminho = pastaraiz+'/chromedriver'\n",
    "#                 if 'chromedriver' in os.listdir(caminho):\n",
    "#                     print(f'Folder Chromedriver encontrado em: {pastaraiz}')\n",
    "#                     print(f'Sistema Operacional: {so}')\n",
    "#                     print(f'  Pasta de trabalho: {caminho}')\n",
    "#                     return pastaraiz\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#                 caminho = None\n",
    "#                 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_chromedriver(drives_win, drives_lin, drives_mac, users, root_folders):\n",
    "#     \"\"\"\n",
    "#     This function aims to locate the directory containing Chromedriver.\n",
    "#     \"\"\"\n",
    "#     os_type = sys.platform\n",
    "\n",
    "#     # Determine the OS and relevant drives\n",
    "#     if 'linux' in os_type:\n",
    "#         os_name = 'Linux'\n",
    "#         drives = drives_lin\n",
    "#     elif 'win32' in os_type:\n",
    "#         os_name = 'Windows'\n",
    "#         drives = drives_win\n",
    "#     elif 'darwin' in os_type:\n",
    "#         os_name = 'macOS'\n",
    "#         drives = drives_mac\n",
    "#     else:\n",
    "#         print(\"Unsupported operating system.\")\n",
    "#         return None\n",
    "\n",
    "#     # Iterate over potential paths to find Chromedriver\n",
    "#     for root in root_folders:\n",
    "#         for user in users:\n",
    "#             for drive_path in drives:\n",
    "#                 search_path = os.path.join(drive_path, user, root)\n",
    "#                 print(f'Buscando em {search_path}')\n",
    "#                 try:\n",
    "#                     if 'chromedriver' in os.listdir(search_path):\n",
    "#                         chromedriver_path = os.path.join(search_path, 'chromedriver')\n",
    "#                         print(f'Chromedriver found at: {chromedriver_path}')\n",
    "#                         print(f'Operating System: {os_name}')\n",
    "#                         return search_path\n",
    "#                 except FileNotFoundError as e:\n",
    "#                     continue\n",
    "\n",
    "#     print(\"Chromedriver not found.\")\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drives_win   = ['C:/Users']\n",
    "# drives_lin   = ['/home']\n",
    "# drives_mac   = []\n",
    "# users        = ['/marcos.aires', '/marcos', '/marco']\n",
    "# root_folders = ['/kgfioce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.join(drives_win[0], users[0], root_folders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_chromedriver(drives_win, drives_lin, drives_mac, users, root_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# def find_chromedriver(drives_win, drives_lin, drives_mac, users, root_folders):\n",
    "#     \"\"\"\n",
    "#     This function aims to locate the directory containing Chromedriver.\n",
    "#     \"\"\"\n",
    "#     os_type = sys.platform\n",
    "\n",
    "#     # Determine the OS and relevant drives\n",
    "#     if 'linux' in os_type:\n",
    "#         os_name = 'Linux'\n",
    "#         drives = drives_lin\n",
    "#     elif 'win32' in os_type:\n",
    "#         os_name = 'Windows'\n",
    "#         drives = drives_win\n",
    "#     elif 'darwin' in os_type:\n",
    "#         os_name = 'macOS'\n",
    "#         drives = drives_mac\n",
    "#     else:\n",
    "#         print(\"Unsupported operating system.\")\n",
    "#         return None\n",
    "\n",
    "#     # Iterate over potential paths to find Chromedriver\n",
    "#     for root in root_folders:\n",
    "#         for user in users:\n",
    "#             for drive in drives:\n",
    "#                 search_path = f\"{drive}{os.sep}{user}{os.sep}{root}\"\n",
    "#                 print(f'Searching in: {search_path}')\n",
    "                \n",
    "#                 try:\n",
    "#                     if 'chromedriver' in os.listdir(search_path):\n",
    "#                         chromedriver_path = f\"{search_path}{os.sep}chromedriver\"\n",
    "#                         print(f'Chromedriver found at: {chromedriver_path}')\n",
    "#                         print(f'Operating System: {os_name}')\n",
    "#                         return search_path\n",
    "#                 except FileNotFoundError as e:\n",
    "#                     continue\n",
    "\n",
    "#     print(\"Chromedriver not found.\")\n",
    "#     return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_chromedriver(drives_win, drives_lin, drives_mac, users, root_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade python-docx\n",
    "\n",
    "# import pandas as pd\n",
    "# from docx import Document\n",
    "\n",
    "# def read_table_from_docx(docx_path, table_index=0):\n",
    "#     # Abrir o arquivo Word\n",
    "#     doc = Document(docx_path)\n",
    "    \n",
    "#     # Verificar se o documento contém tabelas\n",
    "#     if len(doc.tables) == 0:\n",
    "#         return \"O documento não contém tabelas.\"\n",
    "    \n",
    "#     # Escolher a tabela pelo índice (começando por 0)\n",
    "#     table = doc.tables[table_index]\n",
    "    \n",
    "#     # Ler as linhas da tabela e armazená-las em uma lista de listas\n",
    "#     data = []\n",
    "#     for row in table.rows:\n",
    "#         row_data = []\n",
    "#         for cell in row.cells:\n",
    "#             row_data.append(cell.text)\n",
    "#         data.append(row_data)\n",
    "    \n",
    "#     # Converter a lista de listas em um dataframe do pandas\n",
    "#     df = pd.DataFrame(data)\n",
    "    \n",
    "#     # Utilizar a primeira linha como cabeçalho\n",
    "#     df.columns = df.iloc[0]\n",
    "#     df = df[1:]\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Caminho para o arquivo Word\n",
    "# docx_path = \"24102022_Tabela_1844948_TabelaAreasConhecimento_atualizada_2022.docx\"\n",
    "\n",
    "# # Ler a primeira tabela no arquivo para um dataframe\n",
    "# df = read_table_from_docx(pathcsv+docx_path)\n",
    "\n",
    "# # Exibir o dataframe\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
